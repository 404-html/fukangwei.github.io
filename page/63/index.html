<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="泥腿子出身">
<meta property="og:url" content="http://fukangwei.gitee.io/page/63/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="泥腿子出身">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/63/">





  <title>泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/深度学习实现XOR/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/深度学习实现XOR/" itemprop="url">深度学习实现XOR</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T21:17:59+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;单层感知器不能解决异或问题是神经网络的一个常识，而简单的两层神经网络却能将其轻易解决。<br>&emsp;&emsp;<code>import</code>需要的模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;输入数据的<code>placeholder</code>：包含一个<code>data</code>(训练数据)和一个<code>label</code>(训练标签)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = tf.placeholder(tf.float32, shape=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line">label = tf.placeholder(tf.float32, shape=(<span class="number">4</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>由于异或只有<code>4</code>种输入和对应的<code>4</code>个输出，所以定义固定的<code>shape</code>。<br>&emsp;&emsp;基于输入数据的<code>placeholder</code>构建<code>model</code>：异或需要两层神经网络，每层分别需要一个<code>weight</code>和一个<code>bias</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weight = tf.get_variable(name=<span class="string">'weight'</span>, shape=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    bias = tf.get_variable(name=<span class="string">'bias'</span>, shape=(<span class="number">2</span>,))</span><br><span class="line">    x = tf.nn.sigmoid(tf.matmul(data, weight) + bias)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weight = tf.get_variable(name=<span class="string">'weight'</span>, shape=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    bias = tf.get_variable(name=<span class="string">'bias'</span>, shape=(<span class="number">1</span>,))</span><br><span class="line">    x = tf.matmul(x, weight) + bias</span><br></pre></td></tr></table></figure>
<p>因为后面的<code>loss</code>要使用<code>sigmoid_cross_entropy_with_logits</code>函数，所以第二层网络的输出没有使用<code>sigmoid</code>函数。<br>&emsp;&emsp;定义<code>loss</code>：其实这里可以灵活地选用各种<code>loss</code>函数，例如<code>MSE</code>。这里还是选用了在<code>CNN</code>中广泛使用的<code>cross entropy</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">preds = tf.nn.sigmoid(x)</span><br><span class="line">loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.sigmoid_cross_entropy_with_logits(labels=label, logits=x))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;定义<code>Optimizer</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.placeholder(tf.float32)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure>
<p>这里为<code>learning rate</code>定义了一个<code>placeholder</code>，当然也可以直接使用常量定义<code>learning rate</code>。<br>&emsp;&emsp;输入数据并开始训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">train_data = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">train_label = np.array([[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">0</span>]])</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">        <span class="keyword">if</span> step &lt; <span class="number">3000</span>:</span><br><span class="line">            lr = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> step &lt; <span class="number">6000</span>:</span><br><span class="line">            lr = <span class="number">0.1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lr = <span class="number">0.01</span></span><br><span class="line">        _, l, pred = sess.run([optimizer, loss, preds],</span><br><span class="line">                                feed_dict=&#123;</span><br><span class="line">                                    data: train_data,</span><br><span class="line">                                    label: train_label,</span><br><span class="line">                                    learning_rate: lr&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span>:</span><br><span class="line">            print(<span class="string">'Step: &#123;&#125; -&gt; Loss: &#123;&#125; -&gt; Predictions: &#123;&#125;'</span>.format(step, l, pred))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<strong>补充说明</strong>：对于<code>sess.run</code>函数，其第一个参数是你期望看到的输出信息，函数执行完以后，将这些信息对应地赋值给等号右边的变量；对于<code>feed_dict</code>，就是向自定义的神经网络中的变量赋值，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_, l, pred = sess.run([optimizer, loss, preds],</span><br><span class="line">                        feed_dict=&#123;</span><br><span class="line">                            data: train_data,</span><br><span class="line">                            label: train_label,</span><br><span class="line">                            learning_rate: lr&#125;)</span><br></pre></td></tr></table></figure>
<hr>
<p>&emsp;&emsp;<code>Keras</code>实现<code>XOR</code>的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">​</span><br><span class="line">X = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">0</span>]])</span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">8</span>, input_dim=<span class="number">2</span>))</span><br><span class="line">model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line">​</span><br><span class="line">sgd = SGD(lr=<span class="number">0.1</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=sgd)</span><br><span class="line">​</span><br><span class="line">model.fit(X, y, batch_size=<span class="number">1</span>, nb_epoch=<span class="number">1000</span>)</span><br><span class="line">print(model.predict_proba(X))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.00254393</span>]</span><br><span class="line"> [<span class="number">0.9953328</span> ]</span><br><span class="line"> [<span class="number">0.9948784</span> ]</span><br><span class="line"> [<span class="number">0.00588351</span>]]</span><br></pre></td></tr></table></figure>
<hr>
<p>&emsp;&emsp;<code>Pytorch</code>实现<code>XOR</code>的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">X = torch.Tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">Y = torch.Tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">​</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XOR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">2</span>, output_dim=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(XOR, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(input_dim, <span class="number">2</span>)</span><br><span class="line">        self.lin2 = nn.Linear(<span class="number">2</span>, output_dim)</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.lin1(x)</span><br><span class="line">        x = torch.sigmoid(x)</span><br><span class="line">        x = self.lin2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">​</span><br><span class="line">model = XOR()</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">​</span><br><span class="line">epochs = <span class="number">2001</span></span><br><span class="line">steps = X.size(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(steps):</span><br><span class="line">        data_point = np.random.randint(X.size(<span class="number">0</span>))</span><br><span class="line">        x_var = Variable(X[data_point], requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        y_var = Variable(Y[data_point], requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        y_hat = model(x_var)</span><br><span class="line">        loss = loss_func.forward(y_hat, y_var)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch: &#123;0&#125;, Loss: &#123;1&#125;, "</span>.format(i, loss.item()))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras基础知识/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras基础知识/" itemprop="url">Keras基础知识</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T20:30:49+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>Keras</code>是一个用<code>Python</code>编写的高级神经网络<code>API</code>，它能够以<code>TensorFlow</code>、<code>CNTK</code>或者<code>Theano</code>作为后端运行。<code>Keras</code>的开发重点是支持快速的实验，能够以最小的时间把你的想法转换为实验结果，是做好研究的关键。<br>&emsp;&emsp;无论是<code>Theano</code>还是<code>TensorFlow</code>，都是一个符号式的库，这也使得<code>Keras</code>的编程与传统的<code>Python</code>代码有所差别。笼统的说，符号主义的计算首先定义各种变量，然后建立一个计算图，该计算图规定了各个变量之间的计算关系。建立好的计算图需要编译以确定其内部细节，然而此时的计算图还是一个空壳子，里面没有任何实际的数据，只有当你把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。就像用管道搭建供水系统，当你在拼水管的时候，里面是没有水的。只有所有的管子都接完了，才能送水。<br>&emsp;&emsp;<code>Keras</code>的模型搭建形式就是这种方法，在你搭建<code>Keras</code>模型完毕后，你的模型就是一个空壳子，只有实际生成可调用的函数(<code>K.function</code>)，并输入数据，才会形成真正的数据流。<br>&emsp;&emsp;如果你在以下情况下需要深度学习库，请使用<code>Keras</code>：</p>
<ul>
<li>允许简单而快速的原型设计(用户友好、高度模块化、可扩展性强)。</li>
<li>同时支持卷积神经网络和循环神经网络，以及两者的组合。</li>
<li>在<code>CPU</code>和<code>GPU</code>上无缝运行。</li>
</ul>
<h3 id="指导原则"><a href="#指导原则" class="headerlink" title="指导原则"></a>指导原则</h3><p>&emsp;&emsp;<strong>用户友好</strong>。<code>Keras</code>是为人类而不是为机器设计的<code>API</code>，它把用户体验放在首要和中心位置。<code>Keras</code>提供一致且简单的<code>API</code>，将常见用例所需的用户操作数量降至最低，并且在用户错误时提供清晰和可操作的反馈。<br>&emsp;&emsp;<strong>模块化</strong>。模型被理解为由独立的、完全可配置的模块构成的序列或图，这些模块可以以尽可能少的限制组装在一起。特别是神经网络层、损失函数、优化器、初始化方法、激活函数、正则化方法，它们都是可以结合起来构建新模型的模块。<br>&emsp;&emsp;<strong>易扩展性</strong>。新模块是很容易添加的(作为新的类和函数)，现有的模块已经提供了充足的示例。由于能够轻松地创建可以提高表现力的新模块，<code>Keras</code>更加适合高级研究。<br>&emsp;&emsp;<strong>基于<code>Python</code>实现</strong>。<code>Keras</code>没有特定格式的单独配置文件。模型定义在<code>Python</code>代码中，这些代码紧凑，易于调试，并且易于扩展。<br>&emsp;&emsp;<strong>无缝集成</strong>。因为<code>Keras</code>与底层深度学习语言(特别是<code>TensorFlow</code>)集成在一起，所以它可以让你实现任何你可以用基础语言编写的东西。特别的，<code>tf.keras</code>作为<code>Keras API</code>可以与<code>TensorFlow</code>工作流无缝集成。</p>
<h3 id="快速开始：30秒上手Keras"><a href="#快速开始：30秒上手Keras" class="headerlink" title="快速开始：30秒上手Keras"></a>快速开始：30秒上手Keras</h3><p>&emsp;&emsp;<code>Keras</code>的核心数据结构是<code>model</code>，一种组织网络层的方式。最简单的模型是<code>Sequential</code>顺序模型，它是由多个网络层线性堆叠的栈。对于更复杂的结构，你应该使用<code>Keras</code>函数式<code>API</code>，它允许构建任意的神经网络图。<br>&emsp;&emsp;<code>Sequential</code>顺序模型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line">model = Sequential()</span><br></pre></td></tr></table></figure>
<p>可以简单地使用<code>add</code>函数来堆叠模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line">model.add(Dense(units=<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_dim=<span class="number">100</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<p>在完成了模型的构建后，可以使用<code>compile</code>函数来配置学习过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'sgd'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>如果需要的话，你还可以进一步地配置你的优化器。<code>Keras</code>的核心原则是使事情变得相当简单，同时又允许用户在需要的时候能够进行完全的控制(终极的控制是源代码的易扩展性)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    loss=keras.losses.categorical_crossentropy,</span><br><span class="line">    optimizer=keras.optimizers.SGD(lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>))</span><br></pre></td></tr></table></figure>
<p>现在你可以批量地在训练数据上进行迭代了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_train, y_train, epochs=<span class="number">5</span>, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p>或者你可以手动地将批次的数据提供给模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.train_on_batch(x_batch, y_batch)</span><br></pre></td></tr></table></figure>
<p>只需一行代码就能评估模型性能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_and_metrics = model.evaluate(x_test, y_test, batch_size=<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<p>或者对新的数据生成预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classes = model.predict(x_test, batch_size=<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<p>构建一个问答系统，一个图像分类模型，一个神经图灵机，或者其他的任何模型，就是这么的快。</p>
<h3 id="data-format"><a href="#data-format" class="headerlink" title="data_format"></a>data_format</h3><p>&emsp;&emsp;这是一个无可奈何的问题，在如何表示一组彩色图片的问题上，<code>Theano</code>和<code>TensorFlow</code>发生了分歧。<code>th</code>模式(即<code>Theano</code>模式)会把<code>100</code>张<code>RGB</code>三通道的<code>16*32</code>(高为<code>16</code>宽为<code>32</code>)彩色图表示为<code>(100, 3, 16, 32)</code>，<code>Caffe</code>采取的也是这种方式。第<code>0</code>个维度是样本维，代表样本的数目；第<code>1</code>个维度是通道维，代表颜色通道数；后面两个就是高和宽了。这种<code>theano</code>风格的数据组织方法，称为<code>channels_first</code>，即通道维靠前。<br>&emsp;&emsp;<code>TensorFlow</code>的表达形式是<code>(100, 16, 32, 3)</code>，也就是把通道维放在了最后，这种数据组织方式称为<code>channels_last</code>。<br>&emsp;&emsp;<code>Keras</code>默认的数据组织形式在<code>~/.keras/keras.json</code>中规定，可查看该文件的<code>image_data_format</code>，也可在代码中通过<code>K.image_data_format</code>函数返回，请在网络的训练和测试中保持维度顺序一致。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>&emsp;&emsp;深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式：</p>
<ul>
<li>遍历全部数据集计算一次损失函数，然后计算函数对各个参数的梯度，最后更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为<code>Batch gradient descent</code>，即<code>批梯度下降</code>。</li>
<li>每看一个数据就算一下损失函数，然后求梯度更新参数，称为<code>随机梯度下降</code>(<code>stochastic gradient descent</code>)。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，<code>hit</code>不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</li>
</ul>
<p>&emsp;&emsp;为了克服两种方法的缺点，现在一般采用的是一种折中手段<code>mini-batch gradient decent</code>。这种方法把数据分为若干个批，按批来更新参数，这样一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面，因为批的样本数与整个数据集相比小了很多，计算量也不是很大。<br>&emsp;&emsp;基本上现在的梯度下降都是基于<code>mini-batch</code>的，所以<code>Keras</code>的模块中经常会出现<code>batch_size</code>，就是指批的大小。顺便说一句，<code>Keras</code>中用的优化器<code>SGD</code>是<code>stochastic gradient descent</code>的缩写，但不代表是一个样本就更新一回，它还是基于<code>mini-batch</code>的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras之损失和评价函数/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras之损失和评价函数/" itemprop="url">Keras之损失和评价函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T19:48:08+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>&emsp;&emsp;损失函数(或称<code>目标函数</code>、<code>优化评分函数</code>)是编译模型时所需的两个参数之一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=<span class="string">'sgd'</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> losses</span><br><span class="line">model.compile(loss=losses.mean_squared_error, optimizer=<span class="string">'sgd'</span>)</span><br></pre></td></tr></table></figure>
<p>你可以传递一个现有的损失函数名，或者一个<code>TensorFlow/Theano</code>符号函数。该符号函数为每个数据点返回一个标量，有以下两个参数：</p>
<ul>
<li><code>y_true</code>：真实的数据标签，<code>TensorFlow/Theano</code>张量。</li>
<li><code>y_pred</code>：预测值，<code>TensorFlow/Theano</code>张量，其<code>shape</code>与<code>y_true</code>相同。</li>
</ul>
<p>实际的优化目标是所有数据点的输出数组的平均值。</p>
<h3 id="可用损失函数"><a href="#可用损失函数" class="headerlink" title="可用损失函数"></a>可用损失函数</h3><h4 id="mean-squared-error"><a href="#mean-squared-error" class="headerlink" title="mean_squared_error"></a>mean_squared_error</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_squared_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="mean-absolute-error"><a href="#mean-absolute-error" class="headerlink" title="mean_absolute_error"></a>mean_absolute_error</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_absolute_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="mean-absolute-percentage-error"><a href="#mean-absolute-percentage-error" class="headerlink" title="mean_absolute_percentage_error"></a>mean_absolute_percentage_error</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_absolute_percentage_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="mean-squared-logarithmic-error"><a href="#mean-squared-logarithmic-error" class="headerlink" title="mean_squared_logarithmic_error"></a>mean_squared_logarithmic_error</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_squared_logarithmic_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="squared-hinge"><a href="#squared-hinge" class="headerlink" title="squared_hinge"></a>squared_hinge</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">squared_hinge(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="hinge"><a href="#hinge" class="headerlink" title="hinge"></a>hinge</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hinge(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="categorical-hinge"><a href="#categorical-hinge" class="headerlink" title="categorical_hinge"></a>categorical_hinge</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">categorical_hinge(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="logcosh"><a href="#logcosh" class="headerlink" title="logcosh"></a>logcosh</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logcosh(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>该函数用于预测误差的双曲余弦的对数。参数<code>y_true</code>是目标真实值的张量，<code>y_pred</code>是目标预测值的张量。<br>&emsp;&emsp;对于小的<code>x</code>，<code>log(cosh(x))</code>近似等于<code>(x ** 2) / 2</code>；对于大的<code>x</code>，近似等于<code>abs(x) - log(2)</code>。这表示<code>logcosh</code>与均方误差大致相同，但是不会受到偶尔疯狂的错误预测的强烈影响。</p>
<h4 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>也称作<code>多类的对数损失</code>，注意使用该目标函数时，需要将标签转化为形如<code>(nb_samples, nb_classes)</code>的二值序列。<br>&emsp;&emsp;当使用<code>categorical_crossentropy</code>计算损失时，你的目标值应该是分类格式(即如果你有<code>10</code>个类，每个样本的目标值应该是一个<code>10</code>维的向量，这个向量除了表示类别的那个索引为<code>1</code>，其他均为<code>0</code>)。为了将整数目标值转换为分类目标值，你可以使用<code>Keras</code>实用函数<code>to_categorical</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line">categorical_labels = to_categorical(int_labels, num_classes=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a>sparse_categorical_crossentropy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparse_categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">binary_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="kullback-leibler-divergence"><a href="#kullback-leibler-divergence" class="headerlink" title="kullback_leibler_divergence"></a>kullback_leibler_divergence</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kullback_leibler_divergence(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>从预测值概率分布<code>Q</code>到真值概率分布<code>P</code>的信息增益，用以度量两个分布的差异。</p>
<h4 id="poisson"><a href="#poisson" class="headerlink" title="poisson"></a>poisson</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poisson(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h4 id="cosine-proximity"><a href="#cosine-proximity" class="headerlink" title="cosine_proximity"></a>cosine_proximity</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosine_proximity(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>预测值与真实标签的余弦距离平均值的相反数。</p>
<h3 id="Keras自定义loss函数"><a href="#Keras自定义loss函数" class="headerlink" title="Keras自定义loss函数"></a>Keras自定义loss函数</h3><p>&emsp;&emsp;在自定义<code>loss</code>函数之前，可以看看<code>Keras</code>官方是如何定义<code>loss</code>函数的。进入文件<code>keras/keras/losses.py</code>，可以看到很多<code>Keras</code>自带<code>loss</code>的实现代码，比如最简单的均方误差损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> K.mean(K.square(y_pred - y_true), axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>其中<code>y_true</code>为网络给出的预测值，<code>y_true</code>即是标签，两者均为<code>tensor</code>。在<code>loss</code>中直接操作这两个变量即可实现自己想要的<code>loss</code>函数，例如将其改为四次方的平均值来作为新的<code>loss</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error2</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> K.mean(K.square(K.square(y_pred - y_true)), axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>在<code>model</code>编译阶段将<code>loss</code>指定为自定义的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=mean_squared_error2)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="评价函数"><a href="#评价函数" class="headerlink" title="评价函数"></a>评价函数</h3><p>&emsp;&emsp;评价函数用于评估当前训练模型的性能。当模型编译后，评价函数应该作为<code>metrics</code>的参数来输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=<span class="string">'sgd'</span>, metrics=[<span class="string">'mae'</span>, <span class="string">'acc'</span>])​</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> metrics</span><br><span class="line">model.compile(</span><br><span class="line">    loss=<span class="string">'mean_squared_error'</span>, optimizer=<span class="string">'sgd'</span>,</span><br><span class="line">    metrics=[metrics.mae, metrics.categorical_accuracy])</span><br></pre></td></tr></table></figure>
<p>评价函数和损失函数相似，只不过评价函数的结果不会用于训练过程中。我们可以传递已有的评价函数名称，或者传递一个自定义的<code>Theano/TensorFlow</code>函数来使用：</p>
<ul>
<li><code>y_true</code>：真实标签，<code>Theano/Tensorflow</code>张量。</li>
<li><code>y_pred</code>：预测值，和<code>y_true</code>相同尺寸的<code>Theano/TensorFlow</code>张量。</li>
</ul>
<p>返回一个表示全部数据点平均值的张量。</p>
<h3 id="可使用的评价函数"><a href="#可使用的评价函数" class="headerlink" title="可使用的评价函数"></a>可使用的评价函数</h3><h4 id="binary-accuracy"><a href="#binary-accuracy" class="headerlink" title="binary_accuracy"></a>binary_accuracy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">binary_accuracy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>对二分类问题，计算在所有预测值上的平均正确率。</p>
<h4 id="categorical-accuracy"><a href="#categorical-accuracy" class="headerlink" title="categorical_accuracy"></a>categorical_accuracy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">categorical_accuracy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>对多分类问题，计算在所有预测值上的平均正确率。</p>
<h4 id="sparse-categorical-accuracy"><a href="#sparse-categorical-accuracy" class="headerlink" title="sparse_categorical_accuracy"></a>sparse_categorical_accuracy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparse_categorical_accuracy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<p>与<code>categorical_accuracy</code>相同，在对稀疏的目标值预测时有用。</p>
<h4 id="top-k-categorical-accuracy"><a href="#top-k-categorical-accuracy" class="headerlink" title="top_k_categorical_accuracy"></a>top_k_categorical_accuracy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top_k_categorical_accuracy(y_true, y_pred, k=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>计算<code>top-k</code>正确率，当预测值的前<code>k</code>个值中存在目标类别即认为预测正确。</p>
<h4 id="sparse-top-k-categorical-accuracy"><a href="#sparse-top-k-categorical-accuracy" class="headerlink" title="sparse_top_k_categorical_accuracy"></a>sparse_top_k_categorical_accuracy</h4><p>&emsp;&emsp;函数原型如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparse_top_k_categorical_accuracy(y_true, y_pred, k=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>与<code>top_k_categorical_accracy</code>作用相同，但适用于稀疏情况。</p>
<h3 id="自定义评价函数"><a href="#自定义评价函数" class="headerlink" title="自定义评价函数"></a>自定义评价函数</h3><p>&emsp;&emsp;自定义评价函数应该在编译时传递进去，该函数需要以<code>(y_true, y_pred)</code>作为输入参数，并返回一个张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_pred</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> K.mean(y_pred)</span><br><span class="line">​</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>, mean_pred])</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras之常用数据集/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras之常用数据集/" itemprop="url">Keras之常用数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T15:49:45+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="CIFAR10小图像分类数据集"><a href="#CIFAR10小图像分类数据集" class="headerlink" title="CIFAR10小图像分类数据集"></a>CIFAR10小图像分类数据集</h3><p>&emsp;&emsp;<code>50000</code>张<code>32*32</code>彩色训练图像数据，以及<code>10000</code>张测试图像数据，总共分为<code>10</code>个类别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> cifar10</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br></pre></td></tr></table></figure>
<p>该函数返回<code>2</code>个元组：</p>
<ul>
<li><code>(x_train, x_test)</code>：<code>uint8</code>数组表示的<code>RGB</code>图像数据，尺寸为<code>(num_samples, 3, 32, 32)</code>。</li>
<li><code>(y_train, y_test)</code>：<code>uint8</code>数组表示的类别标签(范围在<code>0</code>至<code>9</code>之间的整数)，尺寸为<code>(num_samples,)</code>。</li>
</ul>
<h3 id="CIFAR100小图像分类数据集"><a href="#CIFAR100小图像分类数据集" class="headerlink" title="CIFAR100小图像分类数据集"></a>CIFAR100小图像分类数据集</h3><p>&emsp;&emsp;该数据库具有<code>50000</code>个<code>32*32</code>的彩色图片作为训练集，<code>10000</code>个图片作为测试集。图片一共有<code>100</code>个类别，每个类别有<code>600</code>张图片。这<code>100</code>个类别又分为<code>20</code>个大类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> cifar100</span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=<span class="string">'fine'</span>)</span><br></pre></td></tr></table></figure>
<p>该函数返回<code>2</code>个元组：</p>
<ul>
<li><code>(x_train, x_test)</code>：<code>uint8</code>数组表示的<code>RGB</code>图像数据，尺寸为<code>(num_samples, 3, 32, 32)</code>。</li>
<li><code>(y_train, y_test)</code>：<code>uint8</code>数组表示的类别标签(范围在<code>0</code>至<code>99</code>之间的整数)，尺寸为<code>(num_samples,)</code>。</li>
</ul>
<p>参数<code>label_mode</code>为<code>fine</code>或<code>coarse</code>之一，用于控制标签的精细度。<code>fine</code>获得的标签是<code>100</code>个小类的标签，<code>coarse</code>获得的标签是<code>20</code>个大类的标签。</p>
<h3 id="IMDB电影评论情感分类数据集"><a href="#IMDB电影评论情感分类数据集" class="headerlink" title="IMDB电影评论情感分类数据集"></a>IMDB电影评论情感分类数据集</h3><p>&emsp;&emsp;数据集来自<code>IMDB</code>的<code>25000</code>条电影评论，以情绪(正面/负面)标记。每一条评论已经过预处理，并编码为词索引(整数)的序列表示。为了方便起见，将词按数据集中出现的频率进行索引，例如整数<code>3</code>编码数据中第三个最频繁的词。这允许快速筛选操作，例如只考虑前<code>10000</code>个最常用的词，但排除前<code>20</code>个最常见的词。<br>&emsp;&emsp;作为惯例，<code>0</code>不代表特定的单词，而是被用于编码任何未知单词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(</span><br><span class="line">    path=<span class="string">"imdb.npz"</span>, num_words=<span class="keyword">None</span>, skip_top=<span class="number">0</span>, maxlen=<span class="keyword">None</span>,</span><br><span class="line">    seed=<span class="number">113</span>, start_char=<span class="number">1</span>, oov_char=<span class="number">2</span>, index_from=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>该函数返回<code>2</code>个元组：</p>
<ul>
<li><code>(x_train, x_test)</code>：序列的列表，即词索引的列表。如果指定了<code>num_words</code>参数，则可能的最大索引值是<code>(num_words - 1)</code>；如果指定了<code>maxlen</code>参数，则可能的最大序列长度为<code>maxlen</code>。</li>
<li><code>(y_train, y_test)</code>：序列的标签，是一个二值<code>list</code>。</li>
</ul>
<p>参数如下：</p>
<ul>
<li><code>path</code>：如果你在本机上已有此数据集(位于<code>~/.keras/datasets/ + path</code>)，则载入；否则数据将下载到该目录下。</li>
<li><code>num_words</code>：整数或<code>None</code>，要考虑的最常用的词语。任何不太频繁的词将在序列数据中显示为<code>oov_char</code>值。</li>
<li><code>skip_top</code>：整数，忽略最常出现的若干单词，这些单词将会被编码为<code>oov_char</code>的值。</li>
<li><code>maxlen</code>：整数，最大序列长度，任何更长的序列都将被截断。</li>
<li><code>seed</code>：整数，用于数据重排的随机数种子。</li>
<li><code>start_char</code>：整数，序列的开始将用这个字符标记，默认设置为<code>1</code>，因为<code>0</code>通常作为填充字符。</li>
<li><code>oov_char</code>：整数，由于<code>num_words</code>或<code>skip_top</code>限制而被删除的单词将被替换为此字符。</li>
<li><code>index_from</code>：整数，真实的单词(而不是类似于<code>start_char</code>的特殊占位符)将从这个下标开始。</li>
</ul>
<h3 id="路透社新闻主题分类"><a href="#路透社新闻主题分类" class="headerlink" title="路透社新闻主题分类"></a>路透社新闻主题分类</h3><p>&emsp;&emsp;数据集来源于路透社的<code>11228</code>条新闻文本，总共分为<code>46</code>个主题。与<code>IMDB</code>数据集一样，每条新闻被编码为一个词下标的序列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line">(x_train, y_train), (x_test, y_test) = reuters.load_data(</span><br><span class="line">    path=<span class="string">"reuters.npz"</span>, num_words=<span class="keyword">None</span>, skip_top=<span class="number">0</span>, maxlen=<span class="keyword">None</span>,</span><br><span class="line">    test_split=<span class="number">0.2</span>, seed=<span class="number">113</span>, start_char=<span class="number">1</span>, oov_char=<span class="number">2</span>, index_from=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>参数的含义与<code>IMDB</code>同名参数相同，唯一多的参数是<code>test_split</code>，用于指定从原数据中分割出作为测试集的比例。该数据库支持获取用于编码序列的词下标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_index = reuters.get_word_index(path=<span class="string">"reuters_word_index.json"</span>)</span><br></pre></td></tr></table></figure>
<p>对于参数<code>path</code>，如果你在本机上已有此数据集(位于<code>~/.keras/datasets/ + path</code>)，则载入；否则数据将下载到该目录下。上面代码的返回值是一个以单词为关键字，以其下标为值的字典。例如，<code>word_index[&#39;giraffe&#39;]</code>的值可能为<code>1234</code>。</p>
<h3 id="MNIST手写字符数据集"><a href="#MNIST手写字符数据集" class="headerlink" title="MNIST手写字符数据集"></a>MNIST手写字符数据集</h3><p>&emsp;&emsp;训练集为<code>60000</code>张<code>28*28</code>像素灰度图像，测试集为<code>10000</code>张同规格图像，总共<code>10</code>类数字标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br></pre></td></tr></table></figure>
<p>该函数返回两个<code>Tuple</code>：</p>
<ul>
<li><code>(x_train, x_test)</code>：形状是<code>(nb_samples, 28, 28)</code>的灰度图片数据，数据类型是无符号8位整形(<code>uint8</code>)。</li>
<li><code>(y_train, y_test)</code>：形状是<code>(nb_samples,)</code>的标签数据，标签的范围是<code>0</code>至<code>9</code>。</li>
</ul>
<h3 id="Fashion-MNIST时尚物品数据集"><a href="#Fashion-MNIST时尚物品数据集" class="headerlink" title="Fashion-MNIST时尚物品数据集"></a>Fashion-MNIST时尚物品数据集</h3><p>&emsp;&emsp;训练集为<code>60000</code>张<code>28*28</code>像素灰度图像，测试集为<code>10000</code>同规格图像，总共<code>10</code>类时尚物品标签。该数据集可以用作<code>MNIST</code>的直接替代品。类别标签如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>描述</th>
<th>中文</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td><code>T-shirt/top</code></td>
<td><code>T恤/上衣</code></td>
</tr>
<tr>
<td><code>1</code></td>
<td><code>Trouser</code></td>
<td><code>裤子</code></td>
</tr>
<tr>
<td><code>2</code></td>
<td><code>Pullover</code></td>
<td><code>套头衫</code></td>
</tr>
<tr>
<td><code>3</code></td>
<td><code>Dress</code></td>
<td><code>连衣裙</code></td>
</tr>
<tr>
<td><code>4</code></td>
<td><code>Coat</code></td>
<td><code>外套</code></td>
</tr>
<tr>
<td><code>5</code></td>
<td><code>Sandal</code></td>
<td><code>凉鞋</code></td>
</tr>
<tr>
<td><code>6</code></td>
<td><code>Shirt</code></td>
<td><code>衬衫</code></td>
</tr>
<tr>
<td><code>7</code></td>
<td><code>Sneaker</code></td>
<td><code>运动鞋</code></td>
</tr>
<tr>
<td><code>8</code></td>
<td><code>Bag</code></td>
<td><code>背包</code></td>
</tr>
<tr>
<td><code>9</code></td>
<td><code>Ankle boot</code></td>
<td><code>短靴</code></td>
</tr>
</tbody>
</table>
</div>
<p>用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> fashion_mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</span><br></pre></td></tr></table></figure>
<p>该函数返回2个元组：</p>
<ul>
<li><code>(x_train, x_test)</code>：<code>uint8</code>数组表示的灰度图像，尺寸为<code>(num_samples, 28, 28)</code>。</li>
<li><code>(y_train, y_test)</code>：<code>uint8</code>数组表示的数字标签(范围在<code>0</code>至<code>9</code>之间的整数)，尺寸为<code>(num_samples,)</code>。</li>
</ul>
<h3 id="Boston房价回归数据集"><a href="#Boston房价回归数据集" class="headerlink" title="Boston房价回归数据集"></a>Boston房价回归数据集</h3><p>&emsp;&emsp;数据集来自卡内基梅隆大学维护的<code>StatLib</code>库。样本包含<code>1970</code>年代的在波士顿郊区不同位置的房屋信息，总共有<code>13</code>种房屋属性。目标值是一个位置的房屋的中值(单位是<code>k$</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(x_train, y_train), (x_test, y_test) = boston_housing.load_data()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras之融合层/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras之融合层/" itemprop="url">Keras之融合层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T15:23:32+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>Merge</code>层提供了一系列用于融合两个层或两个张量的层对象和方法。以大写首字母开头的是<code>Layer</code>类，以小写字母开头的是张量的函数。小写字母开头的张量函数在内部实际上是调用了大写字母开头的层。</p>
<h3 id="Add"><a href="#Add" class="headerlink" title="Add"></a>Add</h3><p>&emsp;&emsp;该函数计算一个列表的输入张量的和：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>相加层接受一个列表的张量，所有的张量必须有相同的输入尺寸，然后返回一个张量(和输入张量尺寸相同)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">​</span><br><span class="line">input1 = keras.layers.Input(shape=(<span class="number">16</span>,))</span><br><span class="line">x1 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input1)</span><br><span class="line">input2 = keras.layers.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">x2 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input2)</span><br><span class="line"><span class="comment"># 相当于“added = keras.layers.add([x1, x2])”</span></span><br><span class="line">added = keras.layers.Add()([x1, x2])</span><br><span class="line">out = keras.layers.Dense(<span class="number">4</span>)(added)</span><br><span class="line">model = keras.models.Model(inputs=[input1, input2], outputs=out)</span><br></pre></td></tr></table></figure>
<h3 id="Subtract"><a href="#Subtract" class="headerlink" title="Subtract"></a>Subtract</h3><p>&emsp;&emsp;该函数计算两个输入张量的差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Subtract()</span><br></pre></td></tr></table></figure>
<p>相减层接受一个长度为<code>2</code>的张量列表，两个张量必须有相同的尺寸，然后返回一个值为(<code>inputs[0] - inputs[1]</code>)的张量，输出张量和输入张量尺寸相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">​</span><br><span class="line">input1 = keras.layers.Input(shape=(<span class="number">16</span>,))</span><br><span class="line">x1 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input1)</span><br><span class="line">input2 = keras.layers.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">x2 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input2)</span><br><span class="line"><span class="comment"># 相当于“subtracted = keras.layers.subtract([x1, x2])”</span></span><br><span class="line">subtracted = keras.layers.Subtract()([x1, x2])</span><br><span class="line">out = keras.layers.Dense(<span class="number">4</span>)(subtracted)</span><br><span class="line">model = keras.models.Model(inputs=[input1, input2], outputs=out)</span><br></pre></td></tr></table></figure>
<h3 id="Multiply"><a href="#Multiply" class="headerlink" title="Multiply"></a>Multiply</h3><p>&emsp;&emsp;该函数计算一个列表的输入张量的(逐元素间的)乘积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Multiply()</span><br></pre></td></tr></table></figure>
<p>相乘层接受一个列表的张量，所有的张量必须有相同的输入尺寸，然后返回一个张量(和输入张量尺寸相同)。</p>
<h3 id="Average"><a href="#Average" class="headerlink" title="Average"></a>Average</h3><p>&emsp;&emsp;该函数计算一个列表的输入张量的平均值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Average()</span><br></pre></td></tr></table></figure>
<p>平均层接受一个列表的张量，所有的张量必须有相同的输入尺寸，然后返回一个张量(和输入张量尺寸相同)。</p>
<h3 id="Maximum"><a href="#Maximum" class="headerlink" title="Maximum"></a>Maximum</h3><p>&emsp;&emsp;该函数计算一个列表的输入张量的(逐元素间的)最大值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Maximum()</span><br></pre></td></tr></table></figure>
<p>最大层接受一个列表的张量，所有的张量必须有相同的输入尺寸，然后返回一个张量(和输入张量尺寸相同)。</p>
<h3 id="Concatenate"><a href="#Concatenate" class="headerlink" title="Concatenate"></a>Concatenate</h3><p>&emsp;&emsp;该函数串联一个列表的输入张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Concatenate(axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>串联层接受一个列表的张量(除了串联轴之外，其他的尺寸都必须相同)，然后返回一个由所有输入张量串联起来的输出张量。参数<code>axis</code>是串联的轴。</p>
<h3 id="Dot"><a href="#Dot" class="headerlink" title="Dot"></a>Dot</h3><p>&emsp;&emsp;该函数计算两个张量之间样本的点积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dot(axes, normalize=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>例如，如果作用于输入尺寸为<code>(batch_size, n)</code>的两个张量<code>a</code>和<code>b</code>，那么输出结果就会是尺寸为<code>(batch_size, 1)</code>的一个张量。结果张量每个<code>batch</code>的数据都是<code>a[i,:]</code>和<code>b[i,:]</code>的矩阵(向量)点积。</p>
<ul>
<li><code>axes</code>：整数或者整数元组，一个或者几个进行点积的轴。</li>
<li><code>normalize</code>：是否在点积之前对即将进行点积的轴进行<code>L2</code>标准化。如果设置成<code>True</code>，那么输出两个样本之间的余弦相似值。</li>
</ul>
<h3 id="add"><a href="#add" class="headerlink" title="add"></a>add</h3><p>&emsp;&emsp;该函数<code>Add</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.add(inputs)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是一个列表的输入张量(列表大小至少为<code>2</code>)。该函数返回一个张量，即所有输入张量的和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">​</span><br><span class="line">input1 = keras.layers.Input(shape=(<span class="number">16</span>,))</span><br><span class="line">x1 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input1)</span><br><span class="line">input2 = keras.layers.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">x2 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input2)</span><br><span class="line">added = keras.layers.add([x1, x2])</span><br><span class="line">out = keras.layers.Dense(<span class="number">4</span>)(added)</span><br><span class="line">model = keras.models.Model(inputs=[input1, input2], outputs=out)</span><br></pre></td></tr></table></figure>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><p>&emsp;&emsp;该函数是<code>Subtract</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.subtract(inputs)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是一个列表的输入张量(列表大小准确为<code>2</code>)。该函数返回一个张量，即两个输入张量的差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">​</span><br><span class="line">input1 = keras.layers.Input(shape=(<span class="number">16</span>,))</span><br><span class="line">x1 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input1)</span><br><span class="line">input2 = keras.layers.Input(shape=(<span class="number">32</span>,))</span><br><span class="line">x2 = keras.layers.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(input2)</span><br><span class="line">subtracted = keras.layers.subtract([x1, x2])</span><br><span class="line">out = keras.layers.Dense(<span class="number">4</span>)(subtracted)</span><br><span class="line">model = keras.models.Model(inputs=[input1, input2], outputs=out)</span><br></pre></td></tr></table></figure>
<h3 id="multiply"><a href="#multiply" class="headerlink" title="multiply"></a>multiply</h3><p>&emsp;&emsp;该函数是<code>Multiply</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.multiply(inputs)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是一个列表的输入张量(列表大小至少为<code>2</code>)。该函数返回一个张量，即所有输入张量的逐元素乘积。</p>
<h3 id="average"><a href="#average" class="headerlink" title="average"></a>average</h3><p>&emsp;&emsp;该函数是<code>Average</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.average(inputs)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是一个列表的输入张量(列表大小至少为<code>2</code>)。该函数返回一个张量，即所有输入张量的平均值。</p>
<h3 id="maximum"><a href="#maximum" class="headerlink" title="maximum"></a>maximum</h3><p>&emsp;&emsp;该函数是<code>Maximum</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.maximum(inputs)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是一个列表的输入张量(列表大小至少为<code>2</code>)。该函数返回一个张量，所有张量的逐元素的最大值。</p>
<h3 id="concatenate"><a href="#concatenate" class="headerlink" title="concatenate"></a>concatenate</h3><p>&emsp;&emsp;该函数是<code>Concatenate</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.concatenate(inputs, axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是一个列表的输入张量(列表大小至少为<code>2</code>)，<code>axis</code>是串联的轴。该函数返回一个张量，即所有输入张量通过<code>axis</code>轴串联起来的输出张量。</p>
<h3 id="dot"><a href="#dot" class="headerlink" title="dot"></a>dot</h3><p>&emsp;&emsp;该函数是<code>Dot</code>层的函数式接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.dot(inputs, axes, normalize=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：一个列表的输入张量(列表大小至少为<code>2</code>)。</li>
<li><code>axes</code>：整数或者整数元组，一个或者几个进行点积的轴。</li>
<li><code>normalize</code>：是否在点积之前对即将进行点积的轴进行<code>L2</code>标准化。如果设置成<code>True</code>，那么输出两个样本之间的余弦相似值。</li>
</ul>
<p>该函数返回一个张量，即所有输入张量样本之间的点积。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/TensorFlow之参数初始化/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/TensorFlow之参数初始化/" itemprop="url">TensorFlow之参数初始化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T14:23:16+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;CNN中最重要的就是参数了，包括<code>W</code>和<code>b</code>。训练<code>CNN</code>的最终目的就是得到最好的参数，使得目标函数取得最小值。参数的初始化也同样重要，因此微调受到很多人的重视。<code>tf</code>提供的所有初始化方法都定义在<code>tensorflow/python/ops/init_ops.py</code>。</p>
<h3 id="tf-constant-initializer"><a href="#tf-constant-initializer" class="headerlink" title="tf.constant_initializer"></a>tf.constant_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.Constant</code>，初始化为常数，通常偏置项就是用它初始化的。由它衍生出两个初始化方法：</p>
<ul>
<li><code>tf.zeros_initializer</code>：可以简写为<code>tf.Zeros</code>。</li>
<li><code>tf.ones_initializer</code>：可以简写为<code>tf.Ones</code>。</li>
</ul>
<p>在卷积层中，将偏置项<code>b</code>初始化为<code>0</code>，有多种写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conv1 = tf.layers.conv2d(  <span class="comment"># 方法1</span></span><br><span class="line">    batch_images, filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, activation=tf.nn.relu,</span><br><span class="line">    kernel_initializer=tf.TruncatedNormal(stddev=<span class="number">0.01</span>), bias_initializer=tf.Constant(<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bias_initializer = tf.constant_initializer(<span class="number">0</span>)  <span class="comment"># 方法2</span></span><br><span class="line">bias_initializer = tf.zeros_initializer()  <span class="comment"># 方法3</span></span><br><span class="line">bias_initializer = tf.Zeros()  <span class="comment"># 方法4</span></span><br></pre></td></tr></table></figure>
<p>将<code>W</code>初始化成拉普拉斯算子的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">value = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-8</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">init = tf.constant_initializer(value)</span><br><span class="line">W = tf.get_variable(<span class="string">'W'</span>, shape=[<span class="number">3</span>, <span class="number">3</span>], initializer=init)</span><br></pre></td></tr></table></figure>
<h3 id="tf-truncated-normal-initializer"><a href="#tf-truncated-normal-initializer" class="headerlink" title="tf.truncated_normal_initializer"></a>tf.truncated_normal_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.TruncatedNormal</code>，生成截断正态分布的随机数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.TruncatedNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=<span class="keyword">None</span>, dtype=dtypes.float32)</span><br></pre></td></tr></table></figure>
<p>这四个参数分别用于指定均值、标准差、随机数种子和随机数的数据类型，一般只需要设置<code>stddev</code>这一个参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conv1 = tf.layers.conv2d(  <span class="comment"># 代码示例1</span></span><br><span class="line">    batch_images, filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, activation=tf.nn.relu,</span><br><span class="line">    kernel_initializer=tf.TruncatedNormal(stddev=<span class="number">0.01</span>), bias_initializer=tf.Constant(<span class="number">0</span>))</span><br><span class="line">​</span><br><span class="line">conv1 = tf.layers.conv2d(  <span class="comment"># 代码示例2</span></span><br><span class="line">    batch_images, filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, activation=tf.nn.relu,</span><br><span class="line">    kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    bias_initializer=tf.zero_initializer())</span><br></pre></td></tr></table></figure>
<h3 id="tf-random-normal-initializer"><a href="#tf-random-normal-initializer" class="headerlink" title="tf.random_normal_initializer"></a>tf.random_normal_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.RandomNormal</code>，生成标准正态分布的随机数，参数和<code>truncated_normal_initializer</code>一样。</p>
<h3 id="tf-random-uniform-initializer"><a href="#tf-random-uniform-initializer" class="headerlink" title="tf.random_uniform_initializer"></a>tf.random_uniform_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.RandomUniform</code>，生成均匀分布的随机数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.RandomUniform(minval=<span class="number">0</span>, maxval=<span class="keyword">None</span>, seed=<span class="keyword">None</span>, dtype=dtypes.float32)</span><br></pre></td></tr></table></figure>
<p>这四个参数分别用于指定最小值、最大值、随机数种子和类型。</p>
<h3 id="tf-uniform-unit-scaling-initializer"><a href="#tf-uniform-unit-scaling-initializer" class="headerlink" title="tf.uniform_unit_scaling_initializer"></a>tf.uniform_unit_scaling_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.UniformUnitScaling</code>，和均匀分布差不多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.UniformUnitScaling(factor=<span class="number">1.0</span>, seed=<span class="keyword">None</span>, dtype=dtypes.float32)</span><br></pre></td></tr></table></figure>
<p>只是这个初始化方法不需要指定最小最大值，它们是通过计算得到的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max_val = math.sqrt(<span class="number">3</span> / input_size) * factor</span><br><span class="line">min_val = - max_val</span><br></pre></td></tr></table></figure>
<p>这里的<code>input_size</code>是指输入数据的维数，假设输入为<code>x</code>，运算为<code>x * W</code>，则<code>input_size= W.shape[0]</code>，它的分布区间为<code>[-max_val, max_val]</code>。</p>
<h3 id="tf-variance-scaling-initializer"><a href="#tf-variance-scaling-initializer" class="headerlink" title="tf.variance_scaling_initializer"></a>tf.variance_scaling_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.VarianceScaling</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.VarianceScaling(scale=<span class="number">1.0</span>, mode=<span class="string">"fan_in"</span>, distribution=<span class="string">"normal"</span>, seed=<span class="keyword">None</span>, dtype=dtypes.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>scale</code>：缩放尺度(正浮点数)。</li>
<li><code>mode</code>：<code>fan_in</code>、<code>fan_out</code>和<code>fan_avg</code>中的一个，用于计算标准差<code>stddev</code>的值。</li>
<li><code>distribution</code>：分布类型，<code>normal</code>或<code>uniform</code>中的一个。</li>
</ul>
<ol>
<li>当<code>distribution = &quot;normal&quot;</code>时，生成<code>truncated normal distribution</code>(截断正态分布)的随机数，其中<code>stddev = sqrt(scale / n)</code>，<code>n</code>的计算与<code>mode</code>参数有关。如果<code>mode = &quot;fan_in&quot;</code>，<code>n</code>为输入单元的结点数；如果<code>mode = &quot;fan_out&quot;</code>，<code>n</code>为输出单元的结点数；如果<code>mode = &quot;fan_avg&quot;</code>，<code>n</code>为输入和输出单元结点数的平均值</li>
<li>当<code>distribution = &quot;uniform&quot;</code>时，生成均匀分布的随机数，假设分布区间为<code>[-limit, limit]</code>，则：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">limit = sqrt(<span class="number">3</span> * scale / n)</span><br></pre></td></tr></table></figure>
<h3 id="tf-orthogonal-initializer"><a href="#tf-orthogonal-initializer" class="headerlink" title="tf.orthogonal_initializer"></a>tf.orthogonal_initializer</h3><p>&emsp;&emsp;可以简写为<code>tf.Orthogonal</code>，生成正交矩阵的随机数。当需要生成的参数是<code>2</code>维时，这个正交矩阵是由均匀分布的随机数矩阵经过<code>SVD</code>分解而来。</p>
<h3 id="tf-glorot-uniform-initializer"><a href="#tf-glorot-uniform-initializer" class="headerlink" title="tf.glorot_uniform_initializer"></a>tf.glorot_uniform_initializer</h3><p>&emsp;&emsp;也称为<code>Xavier uniform initializer</code>，由一个均匀分布(<code>uniform distribution</code>)来初始化数据。假设均匀分布的区间是<code>[-limit, limit]</code>，则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">limit = sqrt(<span class="number">6</span> / (fan_in + fan_out))</span><br></pre></td></tr></table></figure>
<p>其中的<code>fan_in</code>和<code>fan_out</code>分别表示输入单元的结点数和输出单元的结点数。</p>
<h3 id="glorot-normal-initializer"><a href="#glorot-normal-initializer" class="headerlink" title="glorot_normal_initializer"></a>glorot_normal_initializer</h3><p>&emsp;&emsp;也称之为<code>Xavier normal initializer</code>，由一个<code>truncated normal distribution</code>来初始化数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stddev = sqrt(<span class="number">2</span> / (fan_in + fan_out))</span><br></pre></td></tr></table></figure>
<p>其中的<code>fan_in</code>和<code>fan_out</code>分别表示输入单元的结点数和输出单元的结点数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras之模型保存和加载/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras之模型保存和加载/" itemprop="url">Keras之模型保存和加载</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T14:04:48+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;不建议使用<code>pickle</code>或<code>cPickle</code>来保存<code>Keras</code>模型。你可以使用<code>model.save(filepath)</code>将<code>Keras</code>模型保存到单个<code>HDF5</code>文件中，该文件将包含：</p>
<ul>
<li>模型的结构，允许重新创建模型。</li>
<li>模型的权重。</li>
<li>训练配置项(损失函数和优化器)。</li>
<li>优化器状态，允许准确地从你上次结束的地方继续训练。</li>
</ul>
<p>你可以使用<code>keras.models.load_model(filepath)</code>重新实例化模型。<code>load_model</code>还将负责使用保存的训练配置项来编译模型(除非模型从未编译过)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">​</span><br><span class="line">model.save(<span class="string">'my_model.h5'</span>)  <span class="comment"># 创建HDF5文件“my_model.h5”</span></span><br><span class="line"><span class="keyword">del</span> model  <span class="comment"># 删除现有模型</span></span><br><span class="line">model = load_model(<span class="string">'my_model.h5'</span>)  <span class="comment"># 返回一个编译好的模型，与之前那个相同</span></span><br></pre></td></tr></table></figure>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line">X = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>)  <span class="comment"># create some data</span></span><br><span class="line">np.random.shuffle(X)  <span class="comment"># randomize the data</span></span><br><span class="line">Y = <span class="number">0.5</span> * X + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, (<span class="number">200</span>,))</span><br><span class="line">X_train, Y_train = X[:<span class="number">160</span>], Y[:<span class="number">160</span>]  <span class="comment"># first 160 data points</span></span><br><span class="line">X_test, Y_test = X[<span class="number">160</span>:], Y[<span class="number">160</span>:]  <span class="comment"># last 40 data points</span></span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(output_dim=<span class="number">1</span>, input_dim=<span class="number">1</span>))</span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'sgd'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">301</span>):</span><br><span class="line">    cost = model.train_on_batch(X_train, Y_train)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># save</span></span><br><span class="line">print(<span class="string">'test before save: '</span>, model.predict(X_test[<span class="number">0</span>:<span class="number">2</span>]))</span><br><span class="line">model.save(<span class="string">'my_model.h5'</span>)  <span class="comment"># HDF5 file, you have to pip3 install h5py if don't have it</span></span><br><span class="line"><span class="keyword">del</span> model  <span class="comment"># deletes the existing model</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = load_model(<span class="string">'my_model.h5'</span>)</span><br><span class="line">print(<span class="string">'test after load: '</span>, model.predict(X_test[<span class="number">0</span>:<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="只保存-加载模型的结构"><a href="#只保存-加载模型的结构" class="headerlink" title="只保存/加载模型的结构"></a>只保存/加载模型的结构</h3><p>&emsp;&emsp;如果你只需要保存模型的结构，而非其权重或训练配置项，则可以执行以下操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">json_string = model.to_json() <span class="comment"># 保存为JSON</span></span><br><span class="line">yaml_string = model.to_yaml() <span class="comment"># 保存为YAML</span></span><br></pre></td></tr></table></figure>
<p>生成的<code>JSON</code>或<code>YAML</code>文件是人类可读的，如果需要的话还可以手动编辑。<br>&emsp;&emsp;你可以从这些数据建立一个新的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从JSON重建模型：</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> model_from_json</span><br><span class="line">model = model_from_json(json_string)</span><br><span class="line"><span class="comment"># 从YAML重建模型：</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> model_from_yaml</span><br><span class="line">model = model_from_yaml(yaml_string)</span><br></pre></td></tr></table></figure>
<h3 id="只保存-加载模型的权重"><a href="#只保存-加载模型的权重" class="headerlink" title="只保存/加载模型的权重"></a>只保存/加载模型的权重</h3><p>&emsp;&emsp;如果您只需要模型的权重，可以使用下面的代码以<code>HDF5</code>格式进行保存。请注意，首先需要安装<code>HDF5</code>的<code>Python</code>库<code>h5py</code>，它不包含在<code>Keras</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_weights(<span class="string">'my_model_weights.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>假设你有用于实例化模型的代码，则可以将保存的权重加载到具有相同结构的模型中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(<span class="string">'my_model_weights.h5'</span>)</span><br></pre></td></tr></table></figure>
<p>如果你需要将权重加载到不同的结构(有一些共同层)的模型中，例如<code>fine-tune</code>或<code>transfer-learning</code>，则可以按层的名字来加载权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(<span class="string">'my_model_weights.h5'</span>, by_name=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">假设原始模型如下所示：</span></span><br><span class="line"><span class="string">model = Sequential()</span></span><br><span class="line"><span class="string">model.add(Dense(2, input_dim=3, name='dense_1'))</span></span><br><span class="line"><span class="string">model.add(Dense(3, name='dense_2'))</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">model.save_weights(fname)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model = Sequential()  <span class="comment"># 新模型</span></span><br><span class="line">model.add(Dense(<span class="number">2</span>, input_dim=<span class="number">3</span>, name=<span class="string">'dense_1'</span>))  <span class="comment"># 将被加载</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>, name=<span class="string">'new_dense'</span>))  <span class="comment"># 将不被加载</span></span><br><span class="line">model.load_weights(fname, by_name=<span class="keyword">True</span>)  <span class="comment"># 从第一个模型加载权重，只会影响第一层(dense_1)</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras之初始化方法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras之初始化方法/" itemprop="url">Keras之初始化方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T13:03:30+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;初始化方法定义了对<code>Keras</code>层设置初始化权重的方法。不同的层可能使用不同的关键字来传递初始化方法，一般来说，指定初始化方法的关键字是<code>kernel_initializer</code>和<code>bias_initializer</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(Dense(<span class="number">64</span>, kernel_initializer=<span class="string">'random_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>))</span><br></pre></td></tr></table></figure>
<p>一个初始化器可以由字符串指定(必须是下面的预定义初始化器之一)，或一个<code>callable</code>的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> initializers</span><br><span class="line">​</span><br><span class="line">model.add(Dense(<span class="number">64</span>, kernel_initializer=initializers.random_normal(stddev=<span class="number">0.01</span>)))</span><br><span class="line"><span class="comment"># also works, it will use the default parameters.</span></span><br><span class="line">model.add(Dense(<span class="number">64</span>, kernel_initializer=<span class="string">'random_normal'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="Initializer"><a href="#Initializer" class="headerlink" title="Initializer"></a>Initializer</h3><p>&emsp;&emsp;<code>Initializer</code>是所有初始化方法的父类，不能直接使用，如果想要定义自己的初始化方法，请继承此类。</p>
<h3 id="预定义初始化方法"><a href="#预定义初始化方法" class="headerlink" title="预定义初始化方法"></a>预定义初始化方法</h3><h4 id="Zeros"><a href="#Zeros" class="headerlink" title="Zeros"></a>Zeros</h4><p>&emsp;&emsp;全<code>0</code>初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.Zeros()</span><br></pre></td></tr></table></figure>
<h4 id="Ones"><a href="#Ones" class="headerlink" title="Ones"></a>Ones</h4><p>&emsp;&emsp;全<code>1</code>初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.Ones()</span><br></pre></td></tr></table></figure>
<h4 id="Constant"><a href="#Constant" class="headerlink" title="Constant"></a>Constant</h4><p>&emsp;&emsp;初始化为固定值<code>value</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.Constant(value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="RandomNormal"><a href="#RandomNormal" class="headerlink" title="RandomNormal"></a>RandomNormal</h4><p>&emsp;&emsp;正态分布初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.RandomNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.05</span>, seed=<span class="keyword">None</span>))</span><br></pre></td></tr></table></figure>
<p>参数<code>mean</code>是均值，<code>stddev</code>是标准差，<code>seed</code>是随机数种子。</p>
<h4 id="RandomUniform"><a href="#RandomUniform" class="headerlink" title="RandomUniform"></a>RandomUniform</h4><p>&emsp;&emsp;均匀分布初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.RandomUniform(minval=<span class="number">-0.05</span>, maxval=<span class="number">0.05</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>minval</code>是均匀分布下边界，<code>maxval</code>是均匀分布上边界，<code>seed</code>是随机数种子。</p>
<h4 id="TruncatedNormal"><a href="#TruncatedNormal" class="headerlink" title="TruncatedNormal"></a>TruncatedNormal</h4><p>&emsp;&emsp;截尾高斯分布初始化，该初始化方法与<code>RandomNormal</code>类似，但位于均值两个标准差以外的数据将会被丢弃并重新生成，形成截尾分布。该分布是神经网络权重和滤波器的推荐初始化方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.TruncatedNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.05</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>mean</code>是均值，<code>stddev</code>是标准差，<code>seed</code>是随机数种子。</p>
<h4 id="VarianceScaling"><a href="#VarianceScaling" class="headerlink" title="VarianceScaling"></a>VarianceScaling</h4><p>&emsp;&emsp;该初始化方法能够自适应目标张量的<code>shape</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.VarianceScaling(scale=<span class="number">1.0</span>, mode=<span class="string">'fan_in'</span>, distribution=<span class="string">'normal'</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>scale</code>：放缩因子，正浮点数。</li>
<li><code>mode</code>：字符串，<code>fan_in</code>、<code>fan_out</code>或<code>fan_avg</code>。</li>
<li><code>distribution</code>：字符串，<code>normal</code>或<code>uniform</code>。</li>
<li><code>seed</code>：随机数种子。</li>
</ul>
<p>当<code>distribution = &quot;normal&quot;</code>时，样本从<code>0</code>均值，标准差为<code>sqrt(scale/n)</code>的截尾正态分布中产生。其中：</p>
<ul>
<li><code>mode = fan_in</code>：权重张量的输入单元数。</li>
<li><code>mode = fan_out</code>：权重张量的输出单元数。</li>
<li><code>mode = fan_avg</code>：权重张量的输入输出单元数的均值。</li>
</ul>
<p>当<code>distribution = &quot;uniform&quot;</code>时，权重从<code>[-limit, limit]</code>范围内均匀采样，其中<code>limit = limit = sqrt(3 * scale / n)</code>。</p>
<h4 id="Orthogonal"><a href="#Orthogonal" class="headerlink" title="Orthogonal"></a>Orthogonal</h4><p>&emsp;&emsp;用随机正交矩阵初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.Orthogonal(gain=<span class="number">1.0</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>gain</code>是正交矩阵的乘性系数，<code>seed</code>是随机数种子。</p>
<h4 id="Identiy"><a href="#Identiy" class="headerlink" title="Identiy"></a>Identiy</h4><p>&emsp;&emsp;使用单位矩阵初始化，仅适用于<code>2D</code>方阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.initializers.Identity(gain=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>gain</code>是单位矩阵的乘性系数。</p>
<h4 id="lecun-uniform"><a href="#lecun-uniform" class="headerlink" title="lecun_uniform"></a>lecun_uniform</h4><p>&emsp;&emsp;<code>LeCun</code>均匀分布初始化方法，参数由<code>[-limit, limit]</code>的区间中均匀采样获得，其中<code>limit = sqrt(3 / fan_in)</code>，<code>fin_in</code>是权重向量的输入单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lecun_uniform(seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机数种子。</p>
<h4 id="lecun-normal"><a href="#lecun-normal" class="headerlink" title="lecun_normal"></a>lecun_normal</h4><p>&emsp;&emsp;<code>LeCun</code>正态分布初始化方法，参数由<code>0</code>均值，标准差为<code>stddev = sqrt(1 / fan_in)</code>的正态分布产生，<code>fin_in</code>是权重向量的输入单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lecun_normal(seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机数种子。</p>
<h4 id="glorot-normal"><a href="#glorot-normal" class="headerlink" title="glorot_normal"></a>glorot_normal</h4><p>&emsp;&emsp;<code>Glorot</code>正态分布初始化方法，也称作<code>Xavier</code>正态分布初始化，参数由<code>0</code>均值，标准差为<code>sqrt(2 / (fan_in + fan_out))</code>的正态分布产生，其中<code>fan_in</code>和<code>fan_out</code>是权重张量的输入和输出单元数目：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glorot_normal(seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机数种子。</p>
<h4 id="glorot-uniform"><a href="#glorot-uniform" class="headerlink" title="glorot_uniform"></a>glorot_uniform</h4><p>&emsp;&emsp;<code>Glorot</code>均匀分布初始化方法，又称为<code>Xavier</code>均匀初始化，参数从<code>[-limit, limit]</code>的均匀分布产生，其中<code>limit = sqrt(6 / (fan_in + fan_out))</code>，<code>fan_in</code>为权值张量的输入单元数，<code>fan_out</code>是权重张量的输出单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glorot_uniform(seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机数种子。</p>
<h4 id="he-normal"><a href="#he-normal" class="headerlink" title="he_normal"></a>he_normal</h4><p>&emsp;&emsp;<code>He</code>正态分布初始化方法，参数由<code>0</code>均值，标准差为<code>sqrt(2 / fan_in)</code>的正态分布产生，其中<code>fan_in</code>是权重张量的输入单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">he_normal(seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机数种子。</p>
<h4 id="he-uniform"><a href="#he-uniform" class="headerlink" title="he_uniform"></a>he_uniform</h4><p>&emsp;&emsp;<code>He</code>均匀分布初始化方法，参数由<code>[-limit, limit]</code>的区间中均匀采样获得，其中<code>limit = sqrt(6 / fan_in)</code>，<code>fin_in</code>是权重向量的输入单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">he_normal(seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机数种子。</p>
<h4 id="自定义初始化器"><a href="#自定义初始化器" class="headerlink" title="自定义初始化器"></a>自定义初始化器</h4><p>&emsp;&emsp;如果需要传递自定义的初始化器，则该初始化器必须是<code>callable</code>的，并且接收<code>shape</code>(将被初始化的张量<code>shape</code>)和<code>dtype</code>(数据类型)两个参数，并返回符合<code>shape</code>和<code>dtype</code>的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_init</span><span class="params">(shape, dtype=None)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> K.random_normal(shape, dtype=dtype)</span><br><span class="line">​</span><br><span class="line">model.add(Dense(<span class="number">64</span>, init=my_init))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/可视化CNN中间层/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/可视化CNN中间层/" itemprop="url">可视化CNN中间层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T11:23:20+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;主要的实现思路如下：</p>
<p>&emsp;&emsp;1. 处理单张图片作为网络输入。<br>&emsp;&emsp;2. 根据给定的<code>layer</code>层，获取该层的输出结果<code>features</code>。<br>&emsp;&emsp;3. 考虑到<code>features</code>的形状为<code>[batch_size, filter_nums, H, W]</code>，提取其中的第一个过滤器得到的结果<code>feature</code>。<br>&emsp;&emsp;4. 以一张图片作为输入的情况下，我们得到的<code>feature</code>即为<code>[H, W]</code>大小的<code>tensor</code>。<br>&emsp;&emsp;5. 将<code>tensor</code>转为<code>numpy</code>，然后归一化到<code>[0, 1]</code>，最后乘以<code>255</code>，使得范围为<code>[0, 255]</code>。<br>&emsp;&emsp;6. 得到灰度图像并保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_image</span><span class="params">(cv2im, resize_im=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    function: Processes image for CNNs.</span></span><br><span class="line"><span class="string">    Args: PIL_img (PIL_img): Image to process; resize_im (bool): Resize to 224 or not.</span></span><br><span class="line"><span class="string">    returns: im_as_var (Pytorch variable): Variable that contains processed float tensor</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># mean and std list for channels (Imagenet)</span></span><br><span class="line">    mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">    std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> resize_im:  <span class="comment"># Resize image</span></span><br><span class="line">        cv2im = cv2.resize(cv2im, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"></span><br><span class="line">    im_as_arr = np.float32(cv2im)</span><br><span class="line">    im_as_arr = np.ascontiguousarray(im_as_arr[..., ::<span class="number">-1</span>])</span><br><span class="line">    im_as_arr = im_as_arr.transpose(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># Convert array to D,W,H</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> channel, _ <span class="keyword">in</span> enumerate(im_as_arr):  <span class="comment"># Normalize the channels</span></span><br><span class="line">        im_as_arr[channel] /= <span class="number">255</span></span><br><span class="line">        im_as_arr[channel] -= mean[channel]</span><br><span class="line">        im_as_arr[channel] /= std[channel]</span><br><span class="line">​</span><br><span class="line">    im_as_ten = torch.from_numpy(im_as_arr).float()  <span class="comment"># Convert to float tensor</span></span><br><span class="line">    im_as_ten.unsqueeze_(<span class="number">0</span>)  <span class="comment"># Add one more channel to the beginning. Tensor shape = 1,3,224,224</span></span><br><span class="line">    im_as_var = Variable(im_as_ten, requires_grad=<span class="keyword">True</span>)  <span class="comment"># Convert to Pytorch variable</span></span><br><span class="line">    <span class="keyword">return</span> im_as_var</span><br><span class="line">​</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureVisualization</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_path, selected_layer)</span>:</span></span><br><span class="line">        self.img_path = img_path</span><br><span class="line">        self.selected_layer = selected_layer</span><br><span class="line">        self.pretrained_model = models.vgg16(pretrained=<span class="keyword">True</span>).features</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(self)</span>:</span></span><br><span class="line">        img = cv2.imread(self.img_path)</span><br><span class="line">        img = preprocess_image(img)</span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature</span><span class="params">(self)</span>:</span></span><br><span class="line">        input = self.process_image()</span><br><span class="line">        print(<span class="string">"get_feature:"</span>, input.shape)</span><br><span class="line">        x = input</span><br><span class="line">        <span class="keyword">for</span> index, layer <span class="keyword">in</span> enumerate(self.pretrained_model):</span><br><span class="line">            x = layer(x)</span><br><span class="line">            <span class="keyword">if</span> (index == self.selected_layer):</span><br><span class="line">                <span class="keyword">return</span> x</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_single_feature</span><span class="params">(self)</span>:</span></span><br><span class="line">        features = self.get_feature()</span><br><span class="line">        print(<span class="string">"get_single_feature_1:"</span>, features.shape)</span><br><span class="line">        feature = features[:, <span class="number">0</span>, :, :]</span><br><span class="line">        print(<span class="string">"get_single_feature_2:"</span>, feature.shape)</span><br><span class="line">        feature = feature.view(feature.shape[<span class="number">1</span>], feature.shape[<span class="number">2</span>])</span><br><span class="line">        print(<span class="string">"get_single_feature_3:"</span>, feature.shape)</span><br><span class="line">        <span class="keyword">return</span> feature</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_feature_to_img</span><span class="params">(self)</span>:</span></span><br><span class="line">        feature = self.get_single_feature()</span><br><span class="line">        feature = feature.data.numpy()</span><br><span class="line">        feature = <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(<span class="number">-1</span> * feature))  <span class="comment"># use sigmod to [0, 1]</span></span><br><span class="line">        feature = np.round(feature * <span class="number">255</span>)  <span class="comment"># to [0, 255]</span></span><br><span class="line">        cv2.imwrite(<span class="string">'./img.jpg'</span>, feature)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    myClass = FeatureVisualization(<span class="string">'./tu.jpg'</span>, <span class="number">5</span>)</span><br><span class="line">    print(myClass.pretrained_model)</span><br><span class="line">    myClass.save_feature_to_img()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/01/深度学习/Keras之优化器/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/深度学习/Keras之优化器/" itemprop="url">Keras之优化器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T10:04:12+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;优化器(<code>optimizer</code>)是编译<code>Keras</code>模型的所需的两个参数之一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">64</span>, kernel_initializer=<span class="string">'uniform'</span>, input_shape=(<span class="number">10</span>,)))</span><br><span class="line">model.add(Activation(<span class="string">'tanh'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">​</span><br><span class="line">sgd = optimizers.SGD(lr=<span class="number">0.01</span>, decay=<span class="number">1e-6</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>)</span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=sgd)</span><br></pre></td></tr></table></figure>
<p>你可以先实例化一个优化器对象，然后将它传入<code>model.compile</code>，像上述示例中一样；或者你可以通过名称来调用优化器。在后一种情况下，将使用优化器的默认参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=<span class="string">'sgd'</span>)  <span class="comment"># 传入优化器名称，默认参数将被采用</span></span><br></pre></td></tr></table></figure>
<h3 id="Keras优化器的公共参数"><a href="#Keras优化器的公共参数" class="headerlink" title="Keras优化器的公共参数"></a>Keras优化器的公共参数</h3><p>&emsp;&emsp;参数<code>clipnorm</code>和<code>clipvalue</code>能在所有的优化器中使用，用于控制梯度裁剪(<code>Gradient Clipping</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="comment"># 所有参数梯度将被裁剪，让其l2范数最大为1：“g * 1 / max(1, l2_norm)”</span></span><br><span class="line">sgd = optimizers.SGD(lr=<span class="number">0.01</span>, clipnorm=<span class="number">1.</span>)</span><br><span class="line"><span class="comment"># --------------------------</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="comment"># 所有参数d梯度将被裁剪到数值范围内：最大值0.5，最小值“-0.5”</span></span><br><span class="line">sgd = optimizers.SGD(lr=<span class="number">0.01</span>, clipvalue=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>&emsp;&emsp;该函数是随机梯度下降优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.SGD(lr=<span class="number">0.01</span>, momentum=<span class="number">0.0</span>, decay=<span class="number">0.0</span>, nesterov=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>包含扩展功能的支持：动量(<code>momentum</code>)优化、学习率衰减(每次参数更新后)和<code>Nestrov</code>动量(<code>NAG</code>)优化。</p>
<ul>
<li><code>lr</code>：<code>float &gt;= 0</code>，学习率。</li>
<li><code>momentum</code>：<code>float &gt;= 0</code>，用于加速<code>SGD</code>在相关方向上前进，并抑制震荡。</li>
<li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值。</li>
<li><code>nesterov</code>：<code>boolean</code>型，是否使用<code>Nesterov</code>动量。</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>&emsp;&emsp;该函数是<code>RMSProp</code>优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.RMSprop(lr=<span class="number">0.001</span>, rho=<span class="number">0.9</span>, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>建议使用优化器的默认参数(除了学习率<code>lr</code>，它可以被自由调节)，这个优化器通常是训练循环神经网络<code>RNN</code>的不错选择。</p>
<ul>
<li><code>lr</code>：<code>float &gt;= 0</code>，学习率。</li>
<li><code>rho</code>：<code>float &gt;= 0</code>，<code>RMSProp</code>梯度平方的移动均值的衰减率。</li>
<li><code>epsilon</code>：<code>float &gt;= 0</code>，模糊因子。若为<code>None</code>，默认为<code>K.epsilon</code>。</li>
<li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值。</li>
</ul>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>&emsp;&emsp;该函数是<code>Adagrad</code>优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adagrad(lr=<span class="number">0.01</span>, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>建议使用优化器的默认参数。</p>
<ul>
<li><code>lr</code>：<code>float &gt;= 0</code>，学习率。</li>
<li><code>epsilon</code>：<code>float &gt;= 0</code>，若为<code>None</code>，默认为<code>K.epsilon</code>。</li>
<li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值。</li>
</ul>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>&emsp;&emsp;该函数是<code>Adagrad</code>优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adadelta(lr=<span class="number">1.0</span>, rho=<span class="number">0.95</span>, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>建议使用优化器的默认参数。</p>
<p><code>lr</code>：<code>float &gt;= 0</code>，学习率，建议保留默认值。<br><code>rho</code>：<code>float &gt;= 0</code>，<code>Adadelta</code>梯度平方移动均值的衰减率。<br><code>epsilon</code>：<code>float &gt;= 0</code>，模糊因子。若为<code>None</code>，默认为<code>K.epsilon</code>。<br><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>&emsp;&emsp;该函数是<code>Adam</code>优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adam(lr=<span class="number">0.001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>, amsgrad=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>建议使用优化器的默认参数。</p>
<ul>
<li><code>lr</code>：<code>float &gt;= 0</code>，学习率。</li>
<li><code>beta_1</code>：<code>float</code>型，<code>0 &lt; beta &lt; 1</code>，通常接近于<code>1</code>。</li>
<li><code>beta_2</code>：<code>float</code>型，<code>0 &lt; beta &lt; 1</code>，通常接近于<code>1</code>。</li>
<li><code>epsilon</code>：<code>float &gt;= 0</code>，模糊因子。若为<code>None</code>，默认为<code>K.epsilon</code>。</li>
<li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值。</li>
<li><code>amsgrad</code>：<code>boolean</code>型，是否应用此算法的<code>AMSGrad</code>变种，来自论文<code>On the Convergence of Adam and Beyond</code>。</li>
</ul>
<h3 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h3><p>&emsp;&emsp;该函数是<code>Adamax</code>优化器，来自<code>Adam</code>论文(<code>Adam - A Method for Stochastic Optimization</code>)的第七小节：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adamax(lr=<span class="number">0.002</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>它是<code>Adam</code>算法基于无穷范数(<code>infinity norm</code>)的变种。</p>
<ul>
<li><code>lr</code>：<code>float &gt;= 0</code>，学习率。</li>
<li><code>beta_1/beta_2</code>：<code>float</code>型，<code>0 &lt; beta &lt; 1</code>，通常接近于1。</li>
<li><code>epsilon</code>：<code>float &gt;= 0</code>，模糊因子。若为<code>None</code>，默认为<code>K.epsilon</code>。</li>
<li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值。</li>
</ul>
<h3 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h3><p>&emsp;&emsp;该函数是<code>Nesterov</code>版本<code>Adam</code>优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Nadam(lr=<span class="number">0.002</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, epsilon=<span class="keyword">None</span>, schedule_decay=<span class="number">0.004</span>)</span><br></pre></td></tr></table></figure>
<p>正像<code>Adam</code>本质上是<code>RMSProp</code>与动量<code>momentum</code>的结合，<code>Nadam</code>是采用<code>Nesterov momentum</code>版本的<code>Adam</code>优化器。建议使用优化器的默认参数。</p>
<ul>
<li><code>lr</code>：<code>float &gt;= 0</code>，学习率。</li>
<li><code>beta_1/beta_2</code>：<code>float</code>型，<code>0 &lt; beta &lt; 1</code>，通常接近于<code>1</code>。</li>
<li><code>epsilon</code>：<code>float &gt;= 0</code>，模糊因子。若为<code>None</code>，默认为<code>K.epsilon</code>。</li>
</ul>
<h3 id="TFOptimizer"><a href="#TFOptimizer" class="headerlink" title="TFOptimizer"></a>TFOptimizer</h3><p>&emsp;&emsp;该函数是原生<code>TensorFlow</code>优化器的包装类(<code>wrapper class</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.TFOptimizer(optimizer)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/62/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/62/">62</a><span class="page-number current">63</span><a class="page-number" href="/page/64/">64</a><span class="space">&hellip;</span><a class="page-number" href="/page/94/">94</a><a class="extend next" rel="next" href="/page/64/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">939</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
