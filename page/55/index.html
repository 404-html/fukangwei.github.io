<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="暴徒">
<meta property="og:url" content="http://fukangwei.gitee.io/page/55/index.html">
<meta property="og:site_name" content="暴徒">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="暴徒">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/55/">





  <title>暴徒</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">暴徒</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/TensorFlow之layers模块/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/TensorFlow之layers模块/" itemprop="url">TensorFlow之layers模块</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T14:56:39+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>TensorFlow</code>的<code>layers</code>模块提供用于深度学习的更高层次封装的<code>API</code>，利用它可以轻松地构建模型。<code>tf.layers</code>模块提供的方法有：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Input</code></td>
<td>用于实例化一个输入<code>Tensor</code>，作为神经网络的输入</td>
</tr>
<tr>
<td><code>average_pooling1d</code></td>
<td>一维平均池化层</td>
</tr>
<tr>
<td><code>average_pooling2d</code></td>
<td>二维平均池化层</td>
</tr>
<tr>
<td><code>average_pooling3d</code></td>
<td>三维平均池化层</td>
</tr>
<tr>
<td><code>batch_normalization</code></td>
<td>批量标准化层</td>
</tr>
<tr>
<td><code>conv1d</code></td>
<td>一维卷积层</td>
</tr>
<tr>
<td><code>conv2d</code></td>
<td>二维卷积层</td>
</tr>
<tr>
<td><code>conv2d_transpose</code></td>
<td>二维反卷积层</td>
</tr>
<tr>
<td><code>conv3d</code></td>
<td>三维卷积层</td>
</tr>
<tr>
<td><code>conv3d_transpose</code></td>
<td>三维反卷积层</td>
</tr>
<tr>
<td><code>dense</code></td>
<td>全连接层</td>
</tr>
<tr>
<td><code>dropout</code></td>
<td><code>Dropout</code>层</td>
</tr>
<tr>
<td><code>flatten</code></td>
<td><code>Flatten</code>层，把一个<code>Tensor</code>展平</td>
</tr>
<tr>
<td><code>max_pooling1d</code></td>
<td>一维最大池化层</td>
</tr>
<tr>
<td><code>max_pooling2d</code></td>
<td>二维最大池化层</td>
</tr>
<tr>
<td><code>max_pooling3d</code></td>
<td>三维最大池化层</td>
</tr>
<tr>
<td><code>separable_conv2d</code></td>
<td>二维深度可分离卷积层</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>&emsp;&emsp;<code>tf.layers.Input</code>(目前已更名为<code>tf.keras.Input</code>)这个方法用于输入数据，类似于<code>tf.placeholder</code>，相当于一个占位符，可以通过传入<code>tensor</code>参数来进行赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input(</span><br><span class="line">    shape=<span class="keyword">None</span>, batch_size=<span class="keyword">None</span>, name=<span class="keyword">None</span>,</span><br><span class="line">    dtype=tf.float32, sparse=<span class="keyword">False</span>, tensor=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>shape</code>：可选参数，是一个由数字组成的元组或列表。这个<code>shape</code>比较特殊，它不包含<code>batch_size</code>，比如传入的<code>shape</code>为<code>[32]</code>，那么它会将<code>shape</code>转化为<code>[?, 32]</code>。</li>
<li><code>batch_size</code>：可选参数，代表输入数据的<code>batch_size</code>，可以是数字或者<code>None</code>。</li>
<li><code>name</code>：可选参数，输入层的名称。</li>
<li><code>dtype</code>：可选参数，元素的类型。</li>
<li><code>sparse</code>：可选参数，指定是否以稀疏矩阵的形式来创建<code>placeholder</code>。</li>
<li><code>tensor</code>：可选参数，如果指定的话，那么创建的内容便不再是一个<code>placeholder</code>，会用此<code>Tensor</code>初始化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.dense(x, <span class="number">16</span>, activation=tf.nn.softmax)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>首先用<code>Input</code>方法初始化了一个<code>placeholder</code>，注意这时我们没有传入<code>tensor</code>参数。然后调用了<code>dense</code>方法构建了一个全连接网络，激活函数使用<code>softmax</code>。执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 32), dtype=float32)</span><br><span class="line">Tensor("dense/Softmax:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>注意此时<code>shape</code>给我们做了转化，本来是<code>[32]</code>，结果转化成了<code>[?, 32]</code>，第一维代表<code>batch_size</code>。所以我们需要注意，在调用此方法时不需要去关心<code>batch_size</code>这一维。<br>&emsp;&emsp;如果我们在初始化时传入一个<code>Tensor</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">data = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = tf.keras.Input(tensor=data)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<p>执行结果如下，可以看到它可以自动计算出其<code>shape</code>和<code>dtype</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"Const:0"</span>, shape=(<span class="number">3</span>,), dtype=int32)</span><br></pre></td></tr></table></figure>
<h3 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch_normalization"></a>batch_normalization</h3><p>&emsp;&emsp;此方法是批量标准化的方法，对数据经过处理之后可以加快训练速度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_normalization(</span><br><span class="line">    inputs, axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="keyword">True</span>, scale=<span class="keyword">True</span>,</span><br><span class="line">    beta_initializer=tf.zeros_initializer(), gamma_initializer=tf.ones_initializer(),</span><br><span class="line">    moving_mean_initializer=tf.zeros_initializer(), moving_variance_initializer=tf.ones_initializer(),</span><br><span class="line">    beta_regularizer=<span class="keyword">None</span>, gamma_regularizer=<span class="keyword">None</span>, beta_constraint=<span class="keyword">None</span>, gamma_constraint=<span class="keyword">None</span>,</span><br><span class="line">    training=<span class="keyword">False</span>, trainable=<span class="keyword">True</span>, name=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>, renorm=<span class="keyword">False</span>, renorm_clipping=<span class="keyword">None</span>,</span><br><span class="line">    renorm_momentum=<span class="number">0.99</span>, fused=<span class="keyword">None</span>, virtual_batch_size=<span class="keyword">None</span>, adjustment=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：输入数据。</li>
<li><code>axis</code>：可选参数，进行标准化操作时操作数据的维度。</li>
<li><code>momentum</code>：可选参数，动态均值的动量。</li>
<li><code>epsilon</code>：可选参数，大于<code>0</code>的小浮点数，用于防止除<code>0</code>错误。</li>
<li><code>center</code>：可选参数，若设为<code>True</code>，将会把<code>beta</code>作为偏置加上去，否则忽略参数<code>beta</code>。</li>
<li><code>scale</code>：可选参数，若设为<code>True</code>，则会乘以<code>gamma</code>，否则不使用<code>gamma</code>。</li>
<li><code>beta_initializer</code>：可选参数，<code>beta</code>权重的初始方法。</li>
<li><code>gamma_initializer</code>：可选参数，<code>gamma</code>的初始化方法。</li>
<li><code>moving_mean_initializer</code>：可选参数，动态均值的初始化方法。</li>
<li><code>moving_variance_initializer</code>：可选参数，动态方差的初始化方法。</li>
<li><code>beta_regularizer</code>: 可选参数，<code>beta</code>的正则化方法。</li>
<li><code>gamma_regularizer</code>: 可选参数，<code>gamma</code>的正则化方法。</li>
<li><code>beta_constraint</code>: 可选参数，加在<code>beta</code>上的约束项。</li>
<li><code>gamma_constraint</code>: 可选参数，加在<code>gamma</code>上的约束项。</li>
<li><code>training</code>：可选参数，返回结果是<code>training</code>模式。</li>
<li><code>trainable</code>：可选参数，布尔类型。如果为<code>True</code>，则将变量添加到<code>GraphKeys.TRAINABLE_VARIABLES</code>中。</li>
<li><code>name</code>：可选参数，层名称。</li>
<li><code>reuse</code>：可选参数，根据层名判断是否重复利用。</li>
<li><code>renorm</code>：可选参数，是否要用<code>Batch Renormalization</code>。</li>
<li><code>renorm_clipping</code>：可选参数，是否要用<code>rmax</code>、<code>rmin</code>、<code>dmax</code>来<code>scalar Tensor</code>。</li>
<li><code>renorm_momentum</code>：可选参数，用来更新动态均值和标准差的<code>Momentum</code>值。</li>
<li><code>fused</code>：可选参数，是否使用一个更快的、融合的实现方法。</li>
<li><code>virtual_batch_size</code>：可选参数，<code>int</code>类型数字，指定一个虚拟<code>batch size</code>。</li>
<li><code>adjustment</code>：可选参数，对标准化后的结果进行适当调整。详细用法参考<code>https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization</code>。</li>
</ul>
<p>该函数的用法是在输入数据后面加一层<code>batch_normalization</code>即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">x = tf.layers.batch_normalization(x)</span><br><span class="line">y = tf.layers.dense(x, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h3 id="dense"><a href="#dense" class="headerlink" title="dense"></a>dense</h3><p>&emsp;&emsp;<code>dense</code>是全连接网络，<code>layers</code>模块提供了一个<code>dense</code>方法来实现此操作，定义在<code>tensorflow/python/layers/core.py</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dense(</span><br><span class="line">    inputs, units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(), kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">    bias_constraint=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, name=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：输入数据。</li>
<li><code>units</code>：神经元的数量。</li>
<li><code>activation</code>：可选参数，如果为<code>None</code>，则是线性激活。</li>
<li><code>use_bias</code>：可选参数，是否使用偏置。</li>
<li><code>kernel_initializer</code>：可选参数，权重的初始化方法。如果为<code>None</code>，则使用默认的<code>Xavier</code>初始化方法。</li>
<li><code>bias_initializer</code>：可选参数，偏置的初始化方法。</li>
<li><code>kernel_regularizer</code>：可选参数，施加在权重上的正则项。</li>
<li><code>bias_regularizer</code>：可选参数，施加在偏置上的正则项。</li>
<li><code>activity_regularizer</code>：可选参数，施加在输出上的正则项。</li>
<li><code>kernel_constraint</code>：可选参数，施加在权重上的约束项。</li>
<li><code>bias_constraint</code>：可选参数，施加在偏置上的约束项。</li>
<li><code>trainable</code>：可选参数，布尔类型。如果为<code>True</code>，则将变量添加到<code>GraphKeys.TRAINABLE_VARIABLES</code>中。</li>
<li><code>name</code>：可选参数，卷积层的名称。</li>
<li><code>reuse</code>：可选参数，布尔类型。</li>
</ul>
<p>该函数返回全连接网络处理后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">print(x)</span><br><span class="line">y1 = tf.layers.dense(x, <span class="number">16</span>, activation=tf.nn.relu)</span><br><span class="line">print(y1)</span><br><span class="line">y2 = tf.layers.dense(y1, <span class="number">5</span>, activation=tf.nn.sigmoid)</span><br><span class="line">print(y2)</span><br></pre></td></tr></table></figure>
<p>首先我们用<code>Input</code>定义了<code>[?, 32]</code>的输入数据，然后经过第一层全连接网络，此时指定了神经元个数为<code>16</code>，激活函数为<code>relu</code>。接着输出结果经过第二层全连接网络，此时指定了神经元个数为<code>5</code>，激活函数为<code>sigmoid</code>。执行结果如下，可以看到输出结果的最后一维度就等于神经元的个数：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 32), dtype=float32)</span><br><span class="line">Tensor("dense/Relu:0", shape=(?, 16), dtype=float32)</span><br><span class="line">Tensor("dense_1/Sigmoid:0", shape=(?, 5), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="convolution"><a href="#convolution" class="headerlink" title="convolution"></a>convolution</h3><p>&emsp;&emsp;<code>convolution</code>就是卷积，<code>layers</code>层提供了多个卷积方法，例如<code>conv1d</code>、<code>conv2d</code>和<code>conv3d</code>分别代表一维、二维、三维卷积。另外还有<code>conv2d_transpose</code>、<code>conv3d_transpose</code>，分别代表二维和三维反卷积，还有<code>separable_conv2d</code>方法代表二维深度可分离卷积。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    inputs, filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(), kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">    trainable=<span class="keyword">True</span>, name=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：需要进行操作的输入数据。</li>
<li><code>filters</code>：输出通道的个数，即<code>output_channels</code>。</li>
<li><code>kernel_size</code>：卷积核大小，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>strides</code>：可选参数，卷积步长，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>padding</code>：可选参数，<code>padding</code>的模式，有<code>valid</code>和<code>same</code>两种，大小写不区分。</li>
<li><code>data_format</code>：可选参数，分为<code>channels_last</code>和<code>channels_first</code>两种模式，代表了输入数据的维度类型。如果是<code>channels_last</code>，那么输入数据的<code>shape</code>为(<code>batch, height, width, channels</code>)；如果是<code>channels_first</code>，那么输入数据的<code>shape</code>为(<code>batch, channels, height, width</code>)。</li>
<li><code>dilation_rate</code>：可选参数，卷积的扩张率。例如当扩张率为<code>2</code>时，卷积核内部就会有边距，<code>3 * 3</code>的卷积核就会变成<code>5 * 5</code>。</li>
<li><code>activation</code>：可选参数。如果为<code>None</code>，则是线性激活。</li>
<li><code>use_bias</code>：可选参数，是否使用偏置。</li>
<li><code>kernel_initializer</code>：可选参数，权重的初始化方法。如果为<code>None</code>，则使用默认的<code>Xavier</code>初始化方法。</li>
<li><code>bias_initializer</code>：可选参数，偏置的初始化方法。</li>
<li><code>kernel_regularizer</code>：可选参数，施加在权重上的正则项。</li>
<li><code>bias_regularizer</code>：可选参数，施加在偏置上的正则项。</li>
<li><code>activity_regularizer</code>：可选参数，施加在输出上的正则项。</li>
<li><code>kernel_constraint</code>：可选参数，施加在权重上的约束项。</li>
<li><code>bias_constraint</code>：可选参数，施加在偏置上的约束项。</li>
<li><code>trainable</code>：可选参数，布尔类型。如果为<code>True</code>，则将变量添加到<code>GraphKeys.TRAINABLE_VARIABLES</code>中。</li>
<li><code>name</code>：可选参数，卷积层的名称。</li>
<li><code>reuse</code>：可选参数，布尔类型。</li>
</ul>
<p>该函数返回卷积后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>首先声明了一个<code>[?, 20, 20, 3]</code>的输入<code>x</code>，然后将其传给<code>conv2d</code>方法。<code>filters</code>设定为<code>6</code>，即输出通道为<code>6</code>；<code>kernel_size</code>为<code>2</code>，即卷积核大小为<code>2 * 2</code>；<code>padding</code>方式设置为<code>same</code>，那么输出结果的宽高和原来一定是相同的，结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 20, 20, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果我们让<code>padding</code>使用默认的<code>valid</code>模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果如下所示，这是因为步长默认为<code>1</code>，卷积核大小为<code>2 * 2</code>，所以得到的结果的高宽即为<code>(20 - (2 - 1)) * (20 - (2 - 1)) = 19 * 19</code>：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 19, 19, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>对于卷积核的大小，我们可以传入一个列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 19, 18, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>这时卷积核的大小变成了<code>2 * 3</code>，即高为<code>2</code>，宽为<code>3</code>，结果就变成了<code>[?, 19, 18, 6]</code>。这是因为步长默认为<code>1</code>，卷积核大小为<code>2 * 2</code>，所以结果的高宽即为<code>(20 - (2 - 1)) * (20 - (3 - 1)) = 19 * 18</code>。<br>&emsp;&emsp;对于步长，我们也可以传入一个列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=[<span class="number">2</span>, <span class="number">3</span>], strides=[<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>这时卷积核大小变成了<code>2 * 3</code>，步长变成了<code>2 * 2</code>，所以结果的高宽为<code>ceil(20 - (2 - 1)) / 2 * ceil(20 - (3 - 1)) / 2 = 10 * 9</code>，得到的结果即为<code>[?, 10, 9, 6]</code>：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 10, 9, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>我们还可以传入激活函数，或者禁用<code>bias</code>等操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, activation=tf.nn.relu, use_bias=<span class="keyword">False</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/Relu:0", shape=(?, 19, 19, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;另外还有反卷积操作，反卷积顾名思义即卷积的反向操作，即输入卷积的结果，输出卷积前的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d_transpose(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>例如此处输入的图像高宽为<code>20 * 20</code>，经过卷积核为<code>2</code>，步长为<code>2</code>的反卷积处理，得到的结果高宽就变为了<code>40 * 40</code>，执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d_transpose/BiasAdd:0", shape=(?, 40, 40, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><p>&emsp;&emsp;<code>pooling</code>即池化层，<code>layers</code>模块提供了多个池化方法，这些池化方法都是类似的，包括<code>max_pooling1d</code>、<code>max_pooling2d</code>、<code>max_pooling3d</code>、<code>average_pooling1d</code>、<code>average_pooling2d</code>和<code>average_pooling3d</code>，分别代表一维、二维、三维、最大和平均池化方法，它们都定义在<code>tensorflow/python/layers/pooling.py</code>中。这里以<code>max_pooling2d</code>方法为例进行介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">max_pooling2d(</span><br><span class="line">    inputs, pool_size, strides, padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>: 需要池化的输入对象，必须是4维的。</li>
<li><code>pool_size</code>：池化窗口大小，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>strides</code>：池化步长，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>padding</code>：可选参数，<code>padding</code>的方法，可选<code>valid</code>或者<code>same</code>，大小写不区分。</li>
<li><code>data_format</code>：可选参数，分为<code>channels_last</code>和<code>channels_first</code>两种模式，代表了输入数据的维度类型。如果是<code>channels_last</code>，那么输入数据的<code>shape</code>为(<code>batch, height, width, channels</code>)；如果是<code>channels_first</code>，那么输入数据的<code>shape</code>为(<code>batch, channels, height, width</code>)。</li>
<li><code>name</code>：可选参数，池化层的名称。</li>
</ul>
<p>该函数返回经过池化处理后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">print(y)</span><br><span class="line">p = tf.layers.max_pooling2d(y, pool_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br><span class="line">print(p)</span><br></pre></td></tr></table></figure>
<p>首先指定了输入<code>x</code>的<code>shape</code>为<code>[20, 20, 3]</code>，然后对其进行了卷积以及池化操作，最后得到池化后的结果。执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 20, 20, 3), dtype=float32)</span><br><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 20, 20, 6), dtype=float32)</span><br><span class="line">Tensor("max_pooling2d/MaxPool:0", shape=(?, 10, 10, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>这里池化窗口的大小是<code>2 * 2</code>，步长也是<code>2</code>，所以原本卷积后的<code>shape</code>为<code>[?, 20, 20, 6]</code>，结果就变成了<code>[?, 10, 10, 6]</code>。</p>
<h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>&emsp;&emsp;<code>dropout</code>是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，可以用来防止过拟合。<code>layers</code>模块提供了<code>dropout</code>方法来实现这一操作，定义在<code>tensorflow/python/layers/core.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dropout(inputs, rate=<span class="number">0.5</span>, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>, training=<span class="keyword">False</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：输入数据。</li>
<li><code>rate</code>：可选参数，即<code>dropout rate</code>。如果设置为<code>0.1</code>，则会丢弃<code>10%</code>的神经元。</li>
<li><code>noise_shape</code>：可选参数，<code>int32</code>类型的一维<code>Tensor</code>，它代表了<code>dropout mask</code>的<code>shape</code>。</li>
<li><code>seed</code>：可选参数，产生随机数的种子值。</li>
<li><code>training</code>：可选参数，布尔类型，代表是否标志为<code>training</code>模式。</li>
<li><code>name</code>：可选参数，<code>dropout</code>层的名称。</li>
</ul>
<p>该函数返回经过<code>dropout</code>层之后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.dense(x, <span class="number">16</span>, activation=tf.nn.softmax)</span><br><span class="line">print(y)</span><br><span class="line">d = tf.layers.dropout(y, rate=<span class="number">0.2</span>)</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure>
<p>这里我们使用<code>dropout</code>方法实现了<code>droput</code>操作，并制定<code>dropout rate</code>为<code>0.2</code>，最后输出结果的<code>shape</code>和原来是一致的：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 32), dtype=float32)</span><br><span class="line">Tensor("dense/Softmax:0", shape=(?, 16), dtype=float32)</span><br><span class="line">Tensor("dropout/Identity:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h3><p>&emsp;&emsp;<code>flatten</code>方法可以对<code>Tensor</code>进行展平操作，定义在<code>tensorflow/python/layers/core.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatten(inputs, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是输入数据，<code>name</code>是该层的名称，该函数返回展平后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.flatten(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>这里输入数据的<code>shape</code>为<code>[?, 5, 6]</code>，经过<code>flatten</code>层之后，就会变成<code>[?, 30]</code>，也就是将除了第一维的数据维度相乘，对原<code>Tensor</code>进行展平：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 5, 6), dtype=float32)</span><br><span class="line">Tensor("flatten/Reshape:0", shape=(?, 30), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果第一维是一个已知数据的话，它依然进行同样的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(shape=[<span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.flatten(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"Placeholder:0"</span>, shape=(<span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">"flatten/Reshape:0"</span>, shape=(<span class="number">5</span>, <span class="number">12</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/TensorFlow指定设备/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/TensorFlow指定设备/" itemprop="url">TensorFlow指定设备</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T14:08:23+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="支持的设备"><a href="#支持的设备" class="headerlink" title="支持的设备"></a>支持的设备</h3><p>&emsp;&emsp;在一套标准的系统上通常有多个计算设备，<code>TensorFlow</code>支持<code>CPU</code>和GPU这两种设备。我们用指定字符串<code>strings</code>来标识这些设备：</p>
<ul>
<li><code>/cpu:0</code>：机器中的<code>CPU</code>。</li>
<li><code>/gpu:0</code>：机器中的<code>GPU</code>，如果你有一个的话。</li>
<li><code>/gpu:1</code>：机器中的第二个<code>GPU</code>，以此类推。</li>
</ul>
<p>如果一个<code>TensorFlow</code>的<code>operation</code>中兼有<code>CPU</code>和<code>GPU</code>的实现，当这个算子被指派设备时，<code>GPU</code>有优先权。</p>
<h3 id="记录设备指派情况"><a href="#记录设备指派情况" class="headerlink" title="记录设备指派情况"></a>记录设备指派情况</h3><p>&emsp;&emsp;为了获取你的<code>operations</code>和<code>Tensor</code>被指派到哪个设备上运行，用<code>log_device_placement</code>新建一个<code>session</code>，并设置为<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 新建一个graph</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># 新建“session with log_device_placement”，并设置为True</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(c))  <span class="comment"># 运行这个op</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, \</span><br><span class="line">    name: GeForce MX150,</span><br><span class="line">pci bus id: <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span>, compute capability: <span class="number">6.1</span></span><br><span class="line">MatMul: (MatMul): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">b: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">a: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">[[<span class="number">22.</span> <span class="number">28.</span>]</span><br><span class="line"> [<span class="number">49.</span> <span class="number">64.</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="手工指派设备"><a href="#手工指派设备" class="headerlink" title="手工指派设备"></a>手工指派设备</h3><p>&emsp;&emsp;如果你不想使用系统来为<code>operation</code>指派设备，而是手工指派设备，可以用<code>with tf.device</code>创建一个设备环境，这个环境下的<code>operation</code>都统一运行在环境指定的设备上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">    b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">​</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, \</span><br><span class="line">    name: GeForce MX150,</span><br><span class="line">pci bus id: <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span>, compute capability: <span class="number">6.1</span></span><br><span class="line">MatMul: (MatMul): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">b: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span></span><br><span class="line">a: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span></span><br><span class="line">[[<span class="number">22.</span> <span class="number">28.</span>]</span><br><span class="line"> [<span class="number">49.</span> <span class="number">64.</span>]]</span><br></pre></td></tr></table></figure>
<p>你会发现<code>a</code>和<code>b</code>操作都被指派给了<code>cpu:0</code>。<br>&emsp;&emsp;为了避免出现指定设备不存在的情况，可以在创建的<code>session</code>里把参数<code>allow_soft_placement</code>设置为<code>True</code>，这样<code>tensorFlow</code>会自动选择一个存在并且支持的设备来运行<code>operation</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:2'</span>):</span><br><span class="line">    a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">    b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>,</span><br><span class="line">                                        log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure>
<h3 id="使用多个GPU"><a href="#使用多个GPU" class="headerlink" title="使用多个GPU"></a>使用多个GPU</h3><p>&emsp;&emsp;如果你想让<code>TensorFlow</code>在多个<code>GPU</code>上运行，你可以建立<code>multi-tower</code>结构，在这个结构里，每个<code>tower</code>分别被指配给不同的<code>GPU</code>运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">c = []</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> [<span class="string">'/gpu:2'</span>, <span class="string">'/gpu:3'</span>]:</span><br><span class="line">    <span class="keyword">with</span> tf.device(d):</span><br><span class="line">        a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">        b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">        c.append(tf.matmul(a, b))</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    sum = tf.add_n(c)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 新建“session with log_device_placement”，并设置为True</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(sum))  <span class="comment"># 运行这个op</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">02</span>:<span class="number">00.0</span></span><br><span class="line">/job:localhost/replica:0/task:0/gpu:1 -&gt; device: 1, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">03</span>:<span class="number">00.0</span></span><br><span class="line">/job:localhost/replica:0/task:0/gpu:2 -&gt; device: 2, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">83</span>:<span class="number">00.0</span></span><br><span class="line">/job:localhost/replica:0/task:0/gpu:3 -&gt; device: 3, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">84</span>:<span class="number">00.0</span></span><br><span class="line">Const_3: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">3</span></span><br><span class="line">Const_2: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">3</span></span><br><span class="line">MatMul_1: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">3</span></span><br><span class="line">Const_1: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">2</span></span><br><span class="line">Const: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">2</span></span><br><span class="line">MatMul: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">2</span></span><br><span class="line">AddN: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/cpu:<span class="number">0</span></span><br><span class="line">[[<span class="number">44.</span> <span class="number">56.</span>]</span><br><span class="line"> [<span class="number">98.</span> <span class="number">128.</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="限制GPU资源使用"><a href="#限制GPU资源使用" class="headerlink" title="限制GPU资源使用"></a>限制GPU资源使用</h3><p>&emsp;&emsp;为了加快运行效率，<code>TensorFlow</code>在初始化时会尝试分配所有可用的<code>GPU</code>显存资源给自己，这在多人使用的服务器上工作时就会导致<code>GPU</code>占用，别人无法使用<code>GPU</code>工作的情况。<br>&emsp;&emsp;<code>tf</code>提供了两种控制<code>GPU</code>资源使用的方法，一是让<code>TensorFlow</code>在运行过程中动态申请显存，需要多少就申请多少，第二种方式就是限制<code>GPU</code>的使用率。<br>&emsp;&emsp;动态申请显存如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;限制<code>GPU</code>使用率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line"><span class="comment"># 占用40%显存</span></span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.4</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.4</span>)</span><br><span class="line">config=tf.ConfigProto(gpu_options=gpu_options)</span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;设置使用哪块<code>GPU</code>，可以是在<code>python</code>程序中设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0'</span>  <span class="comment"># 使用“GPU 0”</span></span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0,1'</span>  <span class="comment"># 使用“GPU 0”和“GPU 1”</span></span><br></pre></td></tr></table></figure>
<hr>
<p>&emsp;&emsp;<code>tf.device</code>是<code>tf.Graph.device</code>的一个包装，是一个用于指定新创建的操作(<code>operation</code>)的默认设备的环境管理器。参数为<code>device_name_or_function</code>，可以传入一个设备字符串或者环境操作函数(如<code>tf.DeviceSpec</code>)。</p>
<ul>
<li>如果传入的是一个设备名称字符串，那么在此环境中构造的所有操作都将被分配给带有该名称的设备，除非被其他嵌套的设备环境(其他的<code>tf.device</code>)所覆盖。</li>
<li>如果传入的是一个函数，它将被当作一个从操作对象到设备名称字符串的函数，并在每次创建新操作时调用它。操作将被分配给带有返回名称的设备。</li>
<li>如果是<code>None</code>，所有的来自代码段上下文的设备调用将被忽略。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.device(<span class="string">'/device:GPU:0'</span>):</span><br><span class="line">    <span class="comment"># All operations constructed in this context will be placed on GPU 0</span></span><br><span class="line"><span class="keyword">with</span> g.device(<span class="keyword">None</span>):</span><br><span class="line">    <span class="comment"># All operations constructed in this context will have no assigned device</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Defines a function from "Operation" to device string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matmul_on_gpu</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n.type == <span class="string">"MatMul"</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"/device:GPU:0"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"/cpu:0"</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> g.device(matmul_on_gpu):</span><br><span class="line">    <span class="comment"># All operations of type "MatMul" constructed in this context will be</span></span><br><span class="line">    <span class="comment"># placed on GPU 0; all other operations will be placed on CPU 0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>tf.DeviceSpec</code>返回的是部分或者全部的设备指定，在整个<code>graph</code>中来描述状态存储和计算发生的位置，并且允许解析设备规范的字符串，以验证它们的有效性，然后合并它们或以编码方式组合它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Place the operations on device "GPU:0" in the "ps" job</span></span><br><span class="line">device_spec = DeviceSpec(job=<span class="string">"ps"</span>, device_type=<span class="string">"GPU"</span>, device_index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(device_spec):</span><br><span class="line">    <span class="comment"># Both my_var and squared_var will be placed on /job:ps/device:GPU:0</span></span><br><span class="line">    my_var = tf.Variable(..., name=<span class="string">"my_variable"</span>)</span><br><span class="line">    squared_var = tf.square(my_var)</span><br></pre></td></tr></table></figure>
<p>如果一个<code>DeviceSpec</code>被部分指定，将根据定义的范围与其他<code>DeviceSpecs</code>合并，在内部内定义的<code>DeviceSpec</code>组件优先于在外层内定义的组件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(DeviceSpec(job=<span class="string">"train"</span>,)):</span><br><span class="line">    <span class="keyword">with</span> tf.device(DeviceSpec(job=<span class="string">"ps"</span>, device_type=<span class="string">"GPU"</span>, device_index=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># Nodes created here will be assigned to /job:ps/device:GPU:0</span></span><br><span class="line">    <span class="keyword">with</span> tf.device(DeviceSpec(device_type=<span class="string">"GPU"</span>, device_index=<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Nodes created here will be assigned to /job:train/device:GPU:1</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>DeviceSpec</code>的参数如下：</p>
<ul>
<li><code>job</code>：作业名称。</li>
<li><code>task</code>：任务索引。</li>
<li><code>device_type</code>：设备类型(<code>CPU</code>或<code>GPU</code>)。</li>
<li><code>device_index</code>：设备索引，如果未指定，则可以使用任意的设备。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/CNN与SVM联用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/CNN与SVM联用/" itemprop="url">CNN与SVM联用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T13:31:51+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;本文演示如何使用卷积神经网络来提取手写数字图片特征，并使用<code>SVM</code>进行分类。该实验使用了<code>UCI</code>手写数字数据集，其中前<code>256</code>维是<code>16 * 16</code>的图片，后<code>10</code>维是<code>one hot</code>编码的标签，即<code>1000000000</code>代表<code>0</code>，<code>0010000000</code>代表<code>2</code>。<br>&emsp;&emsp;使用<code>CNN</code>提取特征的原因如下：</p>
<ul>
<li>由于卷积和池化计算的性质，使得图像中的平移部分对于最后的特征向量是没有影响的。从这一角度说，提取到的特征更不容易过拟合。</li>
<li>可以利用不同的卷积、池化和最后输出的特征向量的大小控制整体模型的拟合能力。在过拟合时可以降低特征向量的维数，在欠拟合时可以提高卷积层的输出维数。</li>
</ul>
<p>&emsp;&emsp;算法流程如下：</p>
<ol>
<li>整理训练网络的数据。</li>
<li>建立卷积神经网络。</li>
<li>将数据代入进行训练。</li>
<li>保存训练好的模型。</li>
<li>把数据代入模型获得特征向量。</li>
<li>用特征向量送入<code>SVM</code>进行训练。</li>
<li>使用<code>SVM</code>进行预测，获得结果。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">​</span><br><span class="line">start = time.clock()</span><br><span class="line">​</span><br><span class="line">right0 = <span class="number">0.0</span>  <span class="comment"># 记录预测为1，且实际为1的结果数</span></span><br><span class="line">error0 = <span class="number">0</span>  <span class="comment"># 记录预测为1，但实际为0的结果数</span></span><br><span class="line">right1 = <span class="number">0.0</span>  <span class="comment"># 记录预测为0，且实际为0的结果数</span></span><br><span class="line">error1 = <span class="number">0</span>  <span class="comment"># 记录预测为0，但实际为1的结果数</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span>  <span class="comment"># 初始化权值向量</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span>  <span class="comment"># 初始化偏置向量</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span>  <span class="comment"># 二维卷积运算，步长为1，输出大小不变</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span>  <span class="comment"># 池化运算，将卷积特征缩小为“1/2”</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> file_num <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'testing NO.%d dataset.......'</span> % file_num)</span><br><span class="line">    ff = open(<span class="string">'digit_train_'</span> + file_num.__str__() + <span class="string">'.data'</span>)</span><br><span class="line">    rr = ff.readlines()</span><br><span class="line">    x_test2 = []</span><br><span class="line">    y_test2 = []</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rr)):</span><br><span class="line">        x_test2.append(list(map(int, map(float, rr[i].split(<span class="string">' '</span>)[:<span class="number">256</span>]))))</span><br><span class="line">        y_test2.append(list(map(int, rr[i].split(<span class="string">' '</span>)[<span class="number">256</span>:<span class="number">266</span>])))</span><br><span class="line"></span><br><span class="line">    ff.close()</span><br><span class="line">    <span class="comment"># 读出测试数据</span></span><br><span class="line">    ff2 = open(<span class="string">'digit_test_'</span> + file_num.__str__() + <span class="string">'.data'</span>)</span><br><span class="line">    rr2 = ff2.readlines()</span><br><span class="line">    x_test3 = []</span><br><span class="line">    y_test3 = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rr2)):</span><br><span class="line">        x_test3.append(list(map(int, map(float, rr2[i].split(<span class="string">' '</span>)[:<span class="number">256</span>]))))</span><br><span class="line">        y_test3.append(list(map(int, rr2[i].split(<span class="string">' '</span>)[<span class="number">256</span>:<span class="number">266</span>])))</span><br><span class="line"></span><br><span class="line">    ff2.close()</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 给x、y留出占位符，以便未来填充数据</span></span><br><span class="line">    x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">256</span>])</span><br><span class="line">    y_ = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 设置输入层的W和b</span></span><br><span class="line">    W = tf.Variable(tf.zeros([<span class="number">256</span>, <span class="number">10</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line">    <span class="comment"># 计算输出，采用的函数是softmax(输入的时候是“one hot”编码)</span></span><br><span class="line">    y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line">    <span class="comment"># 第一个卷积层，“5*5”的卷积核，输出向量是32维</span></span><br><span class="line">    w_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">    <span class="comment"># 图片大小是“16*16”，“-1”代表其他维数自适应</span></span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">1</span>])</span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)</span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 第二层卷积层，输入向量是32维，输出64维，还是“5*5”的卷积核</span></span><br><span class="line">    w_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)</span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 全连接层的w和b</span></span><br><span class="line">    w_fc1 = weight_variable([<span class="number">4</span> * <span class="number">4</span> * <span class="number">64</span>, <span class="number">256</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">256</span>])  <span class="comment"># 此时输出的维数是256维</span></span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">64</span>])</span><br><span class="line">    <span class="comment"># h_fc1是提取出的256维特征，后面就是用这个参数输入到SVM中</span></span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 设置dropout，否则很容易过拟合</span></span><br><span class="line">    keep_prob = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 输出层，本次实验只利用它的输出训练CNN</span></span><br><span class="line">    w_fc2 = weight_variable([<span class="number">256</span>, <span class="number">10</span>])</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">​</span><br><span class="line">    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)</span><br><span class="line">    <span class="comment"># 以交叉熵的形式设置误差代价</span></span><br><span class="line">    cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))</span><br><span class="line">    <span class="comment"># 用adma的优化算法优化目标函数</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):  <span class="comment"># 跑3000轮迭代，每次随机从训练样本中抽出50个进行训练</span></span><br><span class="line">            batch = ([], [])</span><br><span class="line">            p = random.sample(range(<span class="number">795</span>), <span class="number">50</span>)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> p:</span><br><span class="line">                batch[<span class="number">0</span>].append(x_test2[k])</span><br><span class="line">                batch[<span class="number">1</span>].append(y_test2[k])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                train_accuracy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>],</span><br><span class="line">                                                          y_: batch[<span class="number">1</span>],</span><br><span class="line">                                                          keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line"></span><br><span class="line">            train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.6</span>&#125;)</span><br><span class="line">​</span><br><span class="line">        print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;x: x_test3,</span><br><span class="line">                                                            y_: y_test3,</span><br><span class="line">                                                            keep_prob: <span class="number">1.0</span>&#125;))</span><br><span class="line">​</span><br><span class="line">        <span class="comment"># 以下两步都是为了将源数据的“one hot”编码改为1和0，数字“5”可以改为其他值</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(len(y_test2)):</span><br><span class="line">            <span class="keyword">if</span> np.argmax(y_test2[h]) == <span class="number">5</span>:</span><br><span class="line">                y_test2[h] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_test2[h] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(len(y_test3)):</span><br><span class="line">            <span class="keyword">if</span> np.argmax(y_test3[h]) == <span class="number">5</span>:</span><br><span class="line">                y_test3[h] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_test3[h] = <span class="number">0</span></span><br><span class="line">​</span><br><span class="line">        <span class="comment"># 将原来的x带入训练好的CNN中，计算出来全连接层的特征向量，将结果作为SVM中的特征向量</span></span><br><span class="line">        x_temp = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> x_test2:</span><br><span class="line">            x_temp.append(sess.run(h_fc1, feed_dict=&#123;x: np.array(g).reshape((<span class="number">1</span>, <span class="number">256</span>))&#125;)[<span class="number">0</span>])  </span><br><span class="line"></span><br><span class="line">        x_temp2 = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> x_test3:</span><br><span class="line">            x_temp2.append(sess.run(h_fc1, feed_dict=&#123;x: np.array(g).reshape((<span class="number">1</span>, <span class="number">256</span>))&#125;)[<span class="number">0</span>])</span><br><span class="line">​</span><br><span class="line">        clf = svm.SVC(C=<span class="number">0.9</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">        clf.fit(x_temp, y_test2)</span><br><span class="line">​</span><br><span class="line">        print(<span class="string">'svm testing accuracy:'</span>, clf.score(x_temp2, y_test3))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_temp2)):</span><br><span class="line">            <span class="comment"># 在验证时，对出现的四种情况分别使用四个变量进行存储</span></span><br><span class="line">            <span class="keyword">if</span> clf.predict(x_temp2[j].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>] == y_test3[j] == <span class="number">1</span>:</span><br><span class="line">                right0 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> clf.predict(x_temp2[j].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>] == y_test3[j] == <span class="number">0</span>:</span><br><span class="line">                right1 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> clf.predict(x_temp2[j].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">and</span> y_test3[j] == <span class="number">0</span>:</span><br><span class="line">                error0 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                error1 += <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">accuracy = right0 / (right0 + error0)  <span class="comment"># 准确率</span></span><br><span class="line">recall = right0 / (right0 + error1)  <span class="comment"># 召回率</span></span><br><span class="line">print(<span class="string">'svm right ratio:'</span>, (right0 + right1) / (right0 + right1 + error0 + error1))</span><br><span class="line">print(<span class="string">'accuracy:'</span>, accuracy)</span><br><span class="line">print(<span class="string">'recall:'</span>, recall)</span><br><span class="line">print(<span class="string">'F1 score:'</span>, <span class="number">2</span> * accuracy * recall / (accuracy + recall))  <span class="comment"># 计算F1值</span></span><br><span class="line">​</span><br><span class="line">end = time.clock()</span><br><span class="line">print(<span class="string">"time is:"</span>, end - start)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/Keras之回调函数/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/Keras之回调函数/" itemprop="url">Keras之回调函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T10:34:19+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;回调函数是一个函数的合集，会在训练的阶段中所使用。你可以使用回调函数来查看训练模型的内在状态和统计。你可以传递一个列表的回调函数(作为<code>callbacks</code>关键字参数)到<code>Sequential</code>或<code>Model</code>类型的<code>fit</code>方法。在训练时，相应的回调函数的方法就会被在各自的阶段被调用。<br>&emsp;&emsp;虽然我们称之为<code>回调函数</code>，但事实上<code>Keras</code>的回调函数是一个类，回调函数只是习惯性称呼。</p>
<h3 id="Callback"><a href="#Callback" class="headerlink" title="Callback"></a>Callback</h3><p>&emsp;&emsp;该函数用来组建新的回调函数的抽象基类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.Callback()</span><br></pre></td></tr></table></figure>
<p>类属性如下：</p>
<ul>
<li><code>params</code>：字典，训练参数(例如<code>verbosity</code>、<code>batch size</code>、<code>number of epochs</code>等)。</li>
<li><code>model</code>：<code>keras.models.Model</code>的实例，它是正在训练的模型的引用。</li>
</ul>
<p>被回调函数作为参数的<code>logs</code>字典，它会包含了一系列与当前<code>batch</code>或<code>epoch</code>相关的信息。目前，<code>Sequentia</code>模型类的<code>fit</code>方法会在传入到回调函数的<code>logs</code>里面包含以下的数据：</p>
<ul>
<li>在每个<code>epoch</code>的结尾处(<code>on_epoch_end</code>)：<code>logs</code>将包含训练的正确率和误差(<code>acc</code>和<code>loss</code>)，如果指定了验证集，还会包含验证集正确率和误差(<code>val_acc</code>和<code>val_loss</code>)，<code>val_acc</code>还额外需要在<code>compile</code>中启用<code>metrics = [&#39;accuracy&#39;]</code>。</li>
<li>在每个<code>batch</code>的开始处(<code>on_batch_begin</code>)：<code>logs</code>包含<code>size</code>，即当前<code>batch</code>的样本数。</li>
<li>在每个<code>batch</code>的结尾处(<code>on_batch_end</code>)：<code>logs</code>包含<code>loss</code>，若启用<code>accuracy</code>，则还包含<code>acc</code>。</li>
</ul>
<h3 id="BaseLogger"><a href="#BaseLogger" class="headerlink" title="BaseLogger"></a>BaseLogger</h3><p>&emsp;&emsp;该回调函数用来对每个<code>epoch</code>累加<code>metrics</code>指定的监视指标的<code>epoch</code>平均值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.BaseLogger()</span><br></pre></td></tr></table></figure>
<p>这个回调函数被自动应用到每一个<code>Keras</code>模型上面。</p>
<h3 id="TerminateOnNaN"><a href="#TerminateOnNaN" class="headerlink" title="TerminateOnNaN"></a>TerminateOnNaN</h3><p>&emsp;&emsp;该函数是当遇到<code>NaN</code>损失会停止训练的回调函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.TerminateOnNaN()</span><br></pre></td></tr></table></figure>
<h3 id="History"><a href="#History" class="headerlink" title="History"></a>History</h3><p>&emsp;&emsp;该函数是把所有事件都记录到<code>History</code>对象的回调函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.History()</span><br></pre></td></tr></table></figure>
<p>该回调函数在<code>Keras</code>模型上会被自动调用，<code>History</code>对象即为<code>fit</code>方法的返回值。</p>
<h3 id="ModelCheckpoint"><a href="#ModelCheckpoint" class="headerlink" title="ModelCheckpoint"></a>ModelCheckpoint</h3><p>&emsp;&emsp;该函数在每个训练期之后保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.ModelCheckpoint(</span><br><span class="line">    filepath, monitor=<span class="string">'val_loss'</span>, verbose=<span class="number">0</span>, save_best_only=<span class="keyword">False</span>,</span><br><span class="line">    save_weights_only=<span class="keyword">False</span>, mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><code>filepath</code>可以是格式化的字符串，里面的占位符将会被<code>epoch</code>值和传入<code>on_epoch_end</code>的<code>logs</code>关键字所填入。例如，如果<code>filepath</code>是<code>weights.{epoch:02d}-{val_loss:.2f}.hdf5</code>，那么会生成对应<code>epoch</code>和验证集<code>loss</code>的多个文件。</p>
<ul>
<li>filepath：字符串，保存模型的路径。</li>
<li>monitor：被监测的数据。</li>
<li><code>verbose</code>：详细信息模式，<code>0</code>或者<code>1</code>。</li>
<li><code>save_best_only</code>：当设置为<code>True</code>时，将只保存在验证集上性能最好的模型。</li>
<li><code>mode</code>：<code>auto</code>、<code>min</code>和<code>max</code>其中之一。在<code>save_best_only=True</code>时决定性能最佳模型的评判准则，例如当监测值为<code>val_acc</code>时，模式应为<code>max</code>；当检测值为<code>val_loss</code>时，模式应为<code>min</code>。在<code>auto</code>模式下，评价准则由被监测值的名字自动推断。</li>
<li><code>save_weights_only</code>：如果为<code>True</code>，那么只有模型的权重会被保存(<code>model.save_weights(filepath)</code>)；否则的话，整个模型会被保存(<code>model.save(filepath)</code>)。</li>
<li><code>period</code>：每个检查点之间的间隔(训练轮数)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">10</span>, input_dim=<span class="number">784</span>, kernel_initializer=<span class="string">'uniform'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line"><span class="string">""" 如果验证损失下降，那么在每个训练轮之后保存模型 """</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'/tmp/weights.hdf5'</span>,</span><br><span class="line">                               verbose=<span class="number">1</span>, save_best_only=<span class="keyword">True</span>)</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">20</span>, verbose=<span class="number">0</span>,</span><br><span class="line">          validation_data=(X_test, Y_test), callbacks=[checkpointer])</span><br></pre></td></tr></table></figure>
<h3 id="EarlyStopping"><a href="#EarlyStopping" class="headerlink" title="EarlyStopping"></a>EarlyStopping</h3><p>&emsp;&emsp;当监测值不再改善时，该回调函数将中止训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>, min_delta=<span class="number">0</span>, patience=<span class="number">0</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_delta</code>：在被监测的数据中被认为是提升的最小变化，例如小于<code>min_delta</code>的绝对变化会被认为没有提升。</li>
<li><code>patience</code>：当<code>early stop</code>被激活(例如发现<code>loss</code>相比上一个<code>epoch</code>训练没有下降)，则经过<code>patience</code>个<code>epoch</code>后停止训练。</li>
<li><code>mode</code>：<code>auto</code>、<code>min</code>和<code>max</code>其中之一。在<code>min</code>模式中，当被监测的数据停止下降，训练就会停止；在<code>max</code>模式中，当被监测的数据停止上升，训练就会停止；在<code>auto</code>模式中，方向会自动从被监测的数据的名字中判断出来。</li>
</ul>
<h3 id="LearningRateScheduler"><a href="#LearningRateScheduler" class="headerlink" title="LearningRateScheduler"></a>LearningRateScheduler</h3><p>&emsp;&emsp;学习速率定时器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.LearningRateScheduler(schedule, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>schedule</code>是一个函数，该函数以<code>epoch</code>号为参数(从<code>0</code>算起的整数)，返回一个新学习率(浮点数)。</p>
<h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><p>&emsp;&emsp;该函数用于<code>Tensorboard</code>基本可视化。<code>TensorBoard</code>是由<code>Tensorflow</code>提供的一个可视化工具。这个回调函数为<code>Tensorboard</code>编写一个日志，使得你可以动态地观察训练和测试指标的图像以及不同层的激活值直方图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.TensorBoard(</span><br><span class="line">    log_dir=<span class="string">'./logs'</span>, histogram_freq=<span class="number">0</span>, batch_size=<span class="number">32</span>, write_graph=<span class="keyword">True</span>,</span><br><span class="line">    write_grads=<span class="keyword">False</span>, write_images=<span class="keyword">False</span>, embeddings_freq=<span class="number">0</span>,</span><br><span class="line">    embeddings_layer_names=<span class="keyword">None</span>, embeddings_metadata=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>log_dir</code>：用来保存被<code>TensorBoard</code>分析的日志文件的文件名。</li>
<li><code>histogram_freq</code>：计算各个层激活值直方图的频率(每多少个<code>epoch</code>计算一次)，如果设置为<code>0</code>则不计算。</li>
<li><code>write_graph</code>：是否在<code>TensorBoard</code>中可视化图像。如果<code>write_graph</code>被设置为<code>True</code>，日志文件会变得非常大。</li>
<li><code>write_grads</code>：是否在<code>TensorBoard</code>中可视化梯度值直方图。<code>histogram_freq</code>必须要大于<code>0</code>。</li>
<li><code>batch_size</code>：用以直方图计算的传入神经元网络输入批的大小。</li>
<li><code>write_images</code>：是否将模型权重以图片的形式可视化。</li>
<li><code>embeddings_freq</code>：依据该频率(以<code>epoch</code>为单位)筛选保存的<code>embedding</code>层。</li>
<li><code>embeddings_layer_names</code>：要观察的层名称的列表，若设置为<code>None</code>或空列表，则所有<code>embedding</code>层都将被观察。</li>
<li><code>embeddings_metadata</code>：一个字典，将层名称映射为包含该<code>embedding</code>层元数据的文件名。如果所有的<code>embedding</code>层都使用相同的元数据文件，则可传递字符串。</li>
</ul>
<h3 id="ReduceLROnPlateau"><a href="#ReduceLROnPlateau" class="headerlink" title="ReduceLROnPlateau"></a>ReduceLROnPlateau</h3><p>&emsp;&emsp;当评价指标不再提升时，减少学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">    monitor=<span class="string">'val_loss'</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>, verbose=<span class="number">0</span>,</span><br><span class="line">    mode=<span class="string">'auto'</span>, epsilon=<span class="number">0.0001</span>, cooldown=<span class="number">0</span>, min_lr=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>monitor</code>：被监测的数据。</li>
<li><code>factor</code>：每次减少学习率的因子，学习率将以<code>lr = lr * factor</code>的形式被减少。</li>
<li><code>patience</code>：当经历了<code>patience</code>个<code>epoch</code>，而模型性能不提升时，学习率减少的动作会被触发。</li>
<li><code>mode</code>：<code>auto</code>、<code>min</code>或<code>max</code>其中之一。如果是<code>min</code>模式，如果被监测的数据已经停止下降，学习速率会被降低；在<code>max</code>模式，如果被监测的数据已经停止上升，学习速率会被降低；在<code>auto</code>模式，方向会被从被监测的数据中自动推断出来。</li>
<li><code>epsilon</code>：阈值，用来确定是否进入检测值的<code>平原区</code>。</li>
<li><code>cooldown</code>：学习率减少后，会经过<code>cooldown</code>个<code>epoch</code>才重新进行正常操作。</li>
<li><code>min_lr</code>：学习率的下限。</li>
</ul>
<p>当学习停滞时，减少<code>2</code>倍或<code>10</code>倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在<code>patience</code>个<code>epoch</code>中看不到模型性能提升，则减少学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduce_lr = ReduceLROnPlateau(monitor=<span class="string">'val_loss'</span>, factor=<span class="number">0.2</span>, patience=<span class="number">5</span>, min_lr=<span class="number">0.001</span>)</span><br><span class="line">model.fit(X_train, Y_train, callbacks=[reduce_lr])</span><br></pre></td></tr></table></figure>
<h3 id="CSVLogger"><a href="#CSVLogger" class="headerlink" title="CSVLogger"></a>CSVLogger</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.CSVLogger(filename, separator=<span class="string">','</span>, append=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>filename</code>：<code>csv</code>文件的文件名。</li>
<li><code>separator</code>：用来隔离<code>csv</code>文件中元素的字符串。</li>
<li><code>append</code>：如果为<code>True</code>，则表示如果文件存在则增加(可以被用于继续训练)；如果为<code>False</code>，表示覆盖存在的文件。</li>
</ul>
<p>将<code>epoch</code>的训练结果保存在<code>csv</code>文件中，支持所有可被转换为<code>string</code>的值，包括<code>1D</code>的可迭代数值(例如<code>np.ndarray</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">csv_logger = CSVLogger(<span class="string">'training.log'</span>)</span><br><span class="line">model.fit(X_train, Y_train, callbacks=[csv_logger])</span><br></pre></td></tr></table></figure>
<h3 id="编写自己的回调函数"><a href="#编写自己的回调函数" class="headerlink" title="编写自己的回调函数"></a>编写自己的回调函数</h3><p>&emsp;&emsp;我们可以通过继承<code>keras.callbacks.Callback</code>编写自己的回调函数。如下示例可以保存每个<code>batch</code>的<code>loss</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LossHistory</span><span class="params">(keras.callbacks.Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.losses = []</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_end</span><span class="params">(self, batch, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.losses.append(logs.get(<span class="string">'loss'</span>))</span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">10</span>, input_dim=<span class="number">784</span>, kernel_initializer=<span class="string">'uniform'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line">​</span><br><span class="line">history = LossHistory()</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">20</span>, verbose=<span class="number">0</span>, callbacks=[history])</span><br><span class="line">print(history.losses)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/Keras之卷积层/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/Keras之卷积层/" itemprop="url">Keras之卷积层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T08:06:29+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Conv1D"><a href="#Conv1D" class="headerlink" title="Conv1D"></a>Conv1D</h3><p>&emsp;&emsp;该函数是<code>1D</code>卷积层(例如<code>时序卷积</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv1D(</span><br><span class="line">    filters, kernel_size, strides=<span class="number">1</span>, padding=<span class="string">'valid'</span>, dilation_rate=<span class="number">1</span>,</span><br><span class="line">    activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>一维卷积层(即<code>时域卷积</code>)用以在一维输入信号上进行邻域滤波。当使用该层作为首层时，需要提供关键字参数<code>input_shape</code>。例如(<code>10, 128</code>)代表一个长为<code>10</code>的序列，序列中每个信号为<code>128</code>向量，而(<code>None, 128</code>)代表变长的<code>128</code>维向量序列。该层生成将输入信号与卷积核按照单一的空域(或时域)方向进行卷积。如果<code>use_bias = True</code>，则还会加上一个偏置项，若<code>activation</code>不为<code>None</code>，则输出为经过激活函数的输出。</p>
<ul>
<li><code>filters</code>：卷积核的数目(即输出的维度)。</li>
<li><code>kernel_size</code>：一个整数，或者单个整数表示的元组或列表，指明卷积核的空域或时域窗长度。</li>
<li><code>strides</code>：一个整数，或者单个整数表示的元组或列表，指明卷积的步长。任何不为<code>1</code>的<code>strides</code>均与任何不为<code>1</code>的<code>dilation_rate</code>均不兼容。</li>
<li><code>padding</code>：<code>valid</code>、<code>causal</code>或<code>same</code>之一(大小写敏感)。<code>valid</code>表示<code>不填充</code>；<code>same</code>表示填充输入以使输出具有与原始输入相同的长度；<code>causal</code>表示因果(膨胀)卷积，例如<code>output[t]</code>不依赖于<code>input[t + 1:]</code>，当对不能违反时间顺序的时序信号建模时有用。</li>
<li><code>dilation_rate</code>：一个整数，或者单个整数表示的元组或列表，指定用于膨胀卷积的膨胀率。任何不为<code>1</code>的<code>dilation_rate</code>均与任何不为<code>1</code>的<code>strides</code>均不兼容。</li>
<li><code>activation</code>：要使用的激活函数。如果你不指定，则不使用激活函数(即线性激活<code>a(x) = x</code>)。</li>
<li><code>use_bias</code>：布尔值，该层是否使用偏置向量。</li>
<li><code>kernel_initializer</code>：<code>kernel</code>权值矩阵的初始化器。</li>
<li><code>bias_initializer</code>：偏置向量的初始化器。</li>
<li><code>kernel_regularizer</code>：运用到<code>kernel</code>权值矩阵的正则化函数。</li>
<li><code>bias_regularizer</code>：运用到偏置向量的正则化函数。</li>
<li><code>activity_regularizer</code>：运用到层的输出的正则化函数。</li>
<li><code>kernel_constraint</code>：运用到<code>kernel</code>权值矩阵的约束函数。</li>
<li><code>bias_constraint</code>：运用到偏置向量的约束函数。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch_size, steps, input_dim</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch_size, new_steps, filters</code>)。由于填充或窗口按步长滑动，<code>steps</code>值可能已更改。</p>
<h3 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D"></a>Conv2D</h3><p>&emsp;&emsp;该函数是<code>2D</code>卷积层(例如对图像的<code>空间卷积</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>,</span><br><span class="line">    kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>二维卷积层即是对图像的空域卷积，该层对二维输入进行滑动窗卷积。当使用该层作为第一层时，应提供<code>input_shape</code>参数，例如<code>input_shape = (128, 128, 3)</code>代表<code>128 * 128</code>的彩色<code>RGB</code>图像(<code>data_format = &#39;channels_last&#39;</code>)。</p>
<ul>
<li><code>filters</code>：卷积核的数目(即输出的维度)。</li>
<li><code>kernel_size</code>：一个整数，或者<code>2</code>个整数表示的元组或列表，指明<code>2D</code>卷积窗口的宽度和高度。如为单个整数，则表示在各个空间维度的相同长度。</li>
<li><code>strides</code>：一个整数，或者<code>2</code>个整数表示的元组或列表，指明卷积沿宽度和高度方向的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为<code>1</code>的<code>strides</code>均与任何不为<code>1</code>的<code>dilation_rate</code>均不兼容。</li>
<li><code>padding</code>：<code>valid</code>或<code>same</code>。<code>valid</code>代表只进行有效的卷积，即对边界数据不处理；<code>same</code>代表保留边界处的卷积结果，通常会导致输出<code>shape</code>与输入<code>shape</code>相同。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
<li><code>dilation_rate</code>：一个整数或<code>2</code>个整数的元组或列表，指定膨胀卷积的膨胀率。任何不为<code>1</code>的<code>dilation_rate</code>均与任何不为<code>1</code>的<code>strides</code>均不兼容。</li>
<li><code>activation</code>：要使用的激活函数。如果你不指定，则不使用激活函数(即线性激活<code>a(x) = x</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸(注意这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>samples, channels, rows, cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>samples, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(输出的行列数可能会因为填充方法而改变)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>samples, filters, new_rows, new_cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>samples, new_rows, new_cols, filters</code>)。</li>
</ul>
<h3 id="SeparableConv2D"><a href="#SeparableConv2D" class="headerlink" title="SeparableConv2D"></a>SeparableConv2D</h3><p>&emsp;&emsp;该函数在深度方向的可分离<code>2D</code>卷积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SeparableConv2D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    depth_multiplier=<span class="number">1</span>, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    depthwise_initializer=<span class="string">'glorot_uniform'</span>, pointwise_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, depthwise_regularizer=<span class="keyword">None</span>, pointwise_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, depthwise_constraint=<span class="keyword">None</span>,</span><br><span class="line">    pointwise_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>可分离卷积首先按深度方向进行卷积(对每个输入通道分别卷积)，然后逐点进行卷积，将上一步的卷积结果混合到输出通道中。参数<code>depth_multiplier</code>控制了在<code>depthwise</code>卷积(第一步)的过程中，每个输入通道信号产生多少个输出通道。</p>
<ul>
<li><code>depth_multiplier</code>：在按深度卷积的步骤中，每个输入通道使用多少个输出通道。</li>
<li><code>depthwise_initializer</code>：运用到深度方向的核矩阵的初始化器。</li>
<li><code>pointwise_initializer</code>：运用到逐点核矩阵的初始化器。</li>
<li><code>depthwise_regularizer</code>：运用到深度方向的核矩阵的正则化函数。</li>
<li><code>pointwise_regularizer</code>：运用到逐点核矩阵的正则化函数。</li>
<li><code>depthwise_constraint</code>：运用到深度方向的核矩阵的约束函数。</li>
<li><code>pointwise_constraint</code>：运用到逐点核矩阵的约束函数。</li>
</ul>
<p>&emsp;&emsp;输入尺寸(注意这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(输出的行列数可能会因为填充方法而改变)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, filters, new_rows, new_cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, new_rows, new_cols, filters</code>)。</li>
</ul>
<h3 id="Conv2DTranspose"><a href="#Conv2DTranspose" class="headerlink" title="Conv2DTranspose"></a>Conv2DTranspose</h3><p>&emsp;&emsp;该函数转置卷积层(有时被称为<code>反卷积</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2DTranspose(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>需要反卷积的情况通常发生在用户想要对一个普通卷积的结果做反方向的变换，例如将具有该卷积层输出<code>shape</code>的<code>tensor</code>转换为具有该卷积层输入<code>shape</code>的<code>tensor</code>，同时保留与卷积层兼容的连接模式。当使用该层作为第一层时，应提供<code>input_shape</code>参数。例如<code>input_shape = (3, 128, 128)</code>代表<code>128 * 128</code>的彩色<code>RGB</code>图像。</p>
<p>&emsp;&emsp;输入尺寸(注意这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(输出的行列数可能会因为填充方法而改变)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, filters, new_rows, new_cols</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, new_rows, new_cols, filters</code>)。</li>
</ul>
<h3 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h3><p>&emsp;&emsp;该函数是<code>3D</code>卷积层(例如立体空间卷积)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv3D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>,</span><br><span class="line">    kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>三维卷积对三维的输入进行滑动窗卷积，当使用该层作为第一层时，应提供<code>input_shape</code>参数。例如<code>input_shape = (3, 10, 128, 128)</code>代表对<code>10</code>帧<code>128 * 128</code>的彩色<code>RGB</code>图像进行卷积。数据的通道位置仍然由<code>data_format</code>参数指定。</p>
<ul>
<li><code>filters</code>：整数，输出空间的维度(即卷积中滤波器的输出数量)。</li>
<li><code>kernel_size</code>：一个整数，或者<code>3</code>个整数表示的元组或列表，指明<code>3D</code>卷积窗口的深度、高度和宽度。如为单个整数，则表示在各个空间维度的相同长度。</li>
<li><code>strides</code>：一个整数，或者<code>3</code>个整数表示的元组或列表，指明卷积沿每一个空间维度的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为<code>1</code>的<code>strides</code>均与任何不为<code>1</code>的<code>dilation_rate</code>均不兼容。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
<li><code>dilation_rate</code>：一个整数或<code>3</code>个整数的元组或列表，指定<code>dilated convolution</code>中的膨胀比例。任何不为<code>1</code>的<code>dilation_rate</code>均与任何不为<code>1</code>的<code>strides</code>均不兼容。</li>
</ul>
<p>&emsp;&emsp;输入尺寸(这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输入<code>5D</code>张量，尺寸为(<code>samples, channels, conv_dim1, conv_dim2, conv_dim3</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输入<code>5D</code>张量，尺寸为(<code>samples, conv_dim1, conv_dim2, conv_dim3, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(由于填充的原因，<code>new_conv_dim1</code>、<code>new_conv_dim2</code>和<code>new_conv_dim3</code>值可能已更改)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输出<code>5D</code>张量，尺寸为(<code>samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输出<code>5D</code>张量，尺寸为(<code>samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters</code>)。</li>
</ul>
<h3 id="Cropping1D"><a href="#Cropping1D" class="headerlink" title="Cropping1D"></a>Cropping1D</h3><p>&emsp;&emsp;该函数是<code>1D</code>输入的裁剪层(例如时间序列)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Cropping1D(cropping=(<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>它沿着时间维度(第<code>1</code>个轴)对输入进行裁剪。参数<code>cropping</code>是整数或整数元组(长度为<code>2</code>)，决定在裁剪维度(第<code>1</code>个轴)的开始和结束位置应该裁剪多少个单位。如果只提供了一个整数，那么这两个位置将使用相同的值。</p>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch, axis_to_crop, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch, cropped_axis, features</code>)。</p>
<h3 id="Cropping2D"><a href="#Cropping2D" class="headerlink" title="Cropping2D"></a>Cropping2D</h3><p>&emsp;&emsp;该函数是<code>2D</code>输入的裁剪层(例如图像)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Cropping2D(cropping=((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>)), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>它对<code>2D</code>输入(图像)进行裁剪，将在空域维度(即宽和高的方向上)裁剪。</p>
<ul>
<li><code>cropping</code>：整数，或<code>2</code>个整数的元组，或<code>2</code>个整数的<code>2</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对宽度和高度应用相同的对称裁剪。</li>
<li>如果为<code>2</code>个整数的元组：解释为对高度和宽度使用两个不同的裁剪值(<code>symmetric_height_crop, symmetric_width_crop</code>)。</li>
<li>如果为<code>2</code>个整数的<code>2</code>个元组：解释为(<code>(top_crop, bottom_crop), (left_crop, right_crop)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, cropped_rows, cropped_cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, channels, cropped_rows, cropped_cols</code>)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 24, 20, 3)”</span></span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">4</span>)),input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>))</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 20, 16. 64)”</span></span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))))</span><br></pre></td></tr></table></figure>
<h3 id="Cropping3D"><a href="#Cropping3D" class="headerlink" title="Cropping3D"></a>Cropping3D</h3><p>&emsp;&emsp;该函数是<code>3D</code>数据的裁剪层(例如空间或时空)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Cropping3D(cropping=((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>)), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cropping</code>：整数，或<code>3</code>个整数的元组，或<code>2</code>个整数的<code>3</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对深度、高度和宽度应用相同的对称裁剪。</li>
<li>如果为<code>3</code>个整数的元组：解释为对深度、高度和宽度的<code>3</code>个不同的对称裁剪值(<code>symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop</code>)。</li>
<li>如果为<code>2</code>个整数的<code>3</code>个元组：解释为(<code>(left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis</code>)。</li>
</ul>
<h3 id="UpSampling1D"><a href="#UpSampling1D" class="headerlink" title="UpSampling1D"></a>UpSampling1D</h3><p>&emsp;&emsp;该函数是<code>1D</code>输入的上采样层：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.UpSampling1D(size=2)</span><br></pre></td></tr></table></figure>
<p>沿着时间轴重复每个时间步<code>size</code>次。参数<code>size</code>是整数，上采样因子。<br>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch, steps, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch, upsampled_steps, features</code>)。</p>
<h3 id="UpSampling2D"><a href="#UpSampling2D" class="headerlink" title="UpSampling2D"></a>UpSampling2D</h3><p>&emsp;&emsp;该函数是<code>2D</code>输入的上采样层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.UpSampling2D(size=(<span class="number">2</span>, <span class="number">2</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>沿着数据的行和列分别重复<code>size[0]</code>和<code>size[1]</code>次。</p>
<ul>
<li><code>size</code>：整数，或<code>2</code>个整数的元组，分别是行和列的上采样因子。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, upsampled_rows, upsampled_cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, channels, upsampled_rows, upsampled_cols</code>)。</li>
</ul>
<h3 id="UpSampling3D"><a href="#UpSampling3D" class="headerlink" title="UpSampling3D"></a>UpSampling3D</h3><p>&emsp;&emsp;该函数是<code>3D</code>输入的上采样层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.UpSampling3D(size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>沿着数据的第<code>1</code>、<code>2</code>、<code>3</code>维度分别重复<code>size[0]</code>、<code>size[1]</code>和<code>size[2]</code>次。</p>
<ul>
<li><code>size</code>：整数，或<code>3</code>个整数的元组，代表<code>dim1</code>、<code>dim2</code>和<code>dim3</code>的上采样因子。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, dim1, dim2, dim3, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, channels, dim1, dim2, dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3</code>)。</li>
</ul>
<h3 id="ZeroPadding1D"><a href="#ZeroPadding1D" class="headerlink" title="ZeroPadding1D"></a>ZeroPadding1D</h3><p>&emsp;&emsp;对<code>1D</code>输入的首尾端(如时域序列)填充<code>0</code>，以控制卷积以后向量的长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ZeroPadding1D(padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>padding</code>是整数，或长度为<code>2</code>的整数元组。</p>
<ul>
<li>整数：在填充维度(第一个轴)的开始和结束处添加多少个零。</li>
<li>长度为<code>2</code>的整数元组：在填充维度的开始和结尾处添加多少个零(<code>(left_pad, right_pad)</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch, axis_to_pad, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch, padded_axis, features</code>)。</p>
<h3 id="ZeroPadding2D"><a href="#ZeroPadding2D" class="headerlink" title="ZeroPadding2D"></a>ZeroPadding2D</h3><p>&emsp;&emsp;对<code>2D</code>输入(如图片)的边界填充<code>0</code>，以控制卷积以后特征图的大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>padding</code>：整数，或<code>2</code>个整数的元组，或<code>2</code>个整数的<code>2</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对宽度和高度运用相同的对称填充。</li>
<li>如果为<code>2</code>个整数的元组：解释为高度和宽度的2个不同的对称裁剪值(<code>symmetric_height_pad, symmetric_width_pad</code>)。</li>
<li>如果为<code>2</code>个整数的<code>2</code>个元组：解释为(<code>(top_pad, bottom_pad), (left_pad, right_pad)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, padded_rows, padded_cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, channels, padded_rows, padded_cols</code>)。</li>
</ul>
<h3 id="ZeroPadding3D"><a href="#ZeroPadding3D" class="headerlink" title="ZeroPadding3D"></a>ZeroPadding3D</h3><p>&emsp;&emsp;将数据的三个维度上填充<code>0</code>，本层目前只能在使用<code>Theano</code>为后端时可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ZeroPadding3D(padding=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>padding</code>：整数，或<code>3</code>个整数的元组，或<code>2</code>个整数的<code>3</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对深度、高度和宽度运用相同的对称填充。</li>
<li>如果为<code>3</code>个整数的元组：解释为深度、高度和宽度的三个不同的对称填充值(<code>symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad</code>)。</li>
<li>如果为<code>2</code>个整数的<code>3</code>个元组：解释为(<code>(left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，输入<code>5D</code>张量，尺寸为(<code>batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，输入<code>5D</code>张量，尺寸为(<code>batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，输出<code>5D</code>张量，尺寸为(<code>batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，输出<code>5D</code>张量，尺寸为(<code>batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad</code>)。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/15/深度学习/Keras之图像预处理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/15/深度学习/Keras之图像预处理/" itemprop="url">Keras之图像预处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-15T18:04:55+08:00">
                2019-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="ImageDataGenerator类"><a href="#ImageDataGenerator类" class="headerlink" title="ImageDataGenerator类"></a>ImageDataGenerator类</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.ImageDataGenerator(</span><br><span class="line">    featurewise_center=<span class="keyword">False</span>, samplewise_center=<span class="keyword">False</span>, featurewise_std_normalization=<span class="keyword">False</span>,</span><br><span class="line">    samplewise_std_normalization=<span class="keyword">False</span>, zca_whitening=<span class="keyword">False</span>, zca_epsilon=<span class="number">1e-06</span>, rotation_range=<span class="number">0.0</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.0</span>, height_shift_range=<span class="number">0.0</span>, brightness_range=<span class="keyword">None</span>, shear_range=<span class="number">0.0</span>,</span><br><span class="line">    zoom_range=<span class="number">0.0</span>, channel_shift_range=<span class="number">0.0</span>, fill_mode=<span class="string">'nearest'</span>, cval=<span class="number">0.0</span>, horizontal_flip=<span class="keyword">False</span>,</span><br><span class="line">    vertical_flip=<span class="keyword">False</span>, rescale=<span class="keyword">None</span>, preprocessing_function=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, validation_split=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>通过实时数据增强生成张量图像数据批次，数据将不断循环。</p>
<ul>
<li><code>featurewise_center</code>：布尔值，将输入数据的均值设置为<code>0</code>，逐特征进行。</li>
<li><code>samplewise_center</code>：布尔值，将每个样本的均值设置为<code>0</code>。</li>
<li><code>featurewise_std_normalization</code>：布尔值，将输入除以数据标准差，逐特征进行。</li>
<li><code>samplewise_std_normalization</code>：布尔值，将每个输入除以其标准差。</li>
<li><code>zca_epsilon</code>：<code>ZCA</code>白化的<code>epsilon</code>值，默认为<code>1e-6</code>。</li>
<li><code>zca_whitening</code>：布尔值，是否应用<code>ZCA</code>白化。</li>
<li><code>rotation_range</code>：整数，随机旋转的度数范围。</li>
<li><code>width_shift_range</code>：浮点数、一维数组或整数：</li>
</ul>
<ol>
<li><code>float</code>：如果小于<code>1</code>，则是除以总宽度的值；如果大于等于<code>1</code>，则为像素值。</li>
<li><code>一维数组</code>：数组中的随机元素。</li>
<li><code>int</code>：来自间隔(<code>-width_shift_range, +width_shift_range</code>)之间的整数个像素。</li>
</ol>
<p><code>width_shift_range</code>为<code>2</code>时，可能值是整数<code>[-1, 0, +1]</code>，与<code>width_shift_range = [-1, 0, +1]</code>相同；而<code>width_shift_range</code>为<code>1.0</code>时，可能值是<code>[-1.0, +1.0)</code>之间的浮点数。</p>
<ul>
<li><code>height_shift_range</code>：浮点数、一维数组或整数：</li>
</ul>
<ol>
<li><code>float</code>：如果小于<code>1</code>，则是除以总宽度的值；如果大于等于<code>1</code>，则为像素值。</li>
<li><code>一维数组</code>：数组中的随机元素。</li>
<li><code>int</code>：来自间隔<code>(-height_shift_range, +height_shift_range)</code>之间的整数个像素。</li>
</ol>
<p><code>height_shift_range</code>为<code>2</code>时，可能值是整数<code>[-1, 0, +1]</code>，与<code>height_shift_range = [-1, 0, +1]</code>相同；而<code>height_shift_range</code>为<code>1.0</code>时，可能值是<code>[-1.0, +1.0)</code>之间的浮点数。</p>
<ul>
<li><code>shear_range</code>：浮点数，剪切强度(以弧度逆时针方向剪切角度)。</li>
<li><code>zoom_range</code>：浮点数或<code>[lower, upper]</code>，随机缩放范围。如果是浮点数，<code>[lower, upper] = [1 - zoom_range, 1 + zoom_range]</code>。</li>
<li><code>channel_shift_range</code>：浮点数，随机通道转换的范围。</li>
<li><code>fill_mode</code>：<code>constant</code>、<code>nearest</code>、<code>reflect</code>和<code>wrap</code>之一。输入边界以外的点根据给定的模式填充：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>constant</code></td>
<td><code>kkkkkkkk&#124;abcd&#124;kkkkkkkk(cval = k)</code></td>
</tr>
<tr>
<td><code>nearest</code></td>
<td><code>aaaaaaaa&#124;abcd&#124;dddddddd</code></td>
</tr>
<tr>
<td><code>reflect</code></td>
<td><code>abcddcba&#124;abcd&#124;dcbaabcd</code></td>
</tr>
<tr>
<td><code>wrap</code></td>
<td><code>abcdabcd&#124;abcd&#124;abcdabcd</code></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><code>cval</code>：浮点数或整数，用于边界之外的点的值。</li>
<li><code>horizontal_flip</code>：布尔值，随机水平翻转。</li>
<li><code>vertical_flip</code>：布尔值，随机垂直翻转。</li>
<li><code>rescale</code>：重缩放因子，默认为<code>None</code>。如果是<code>None</code>或<code>0</code>，则不进行缩放，否则将数据乘以所提供的值(在应用任何其他转换之前)。</li>
<li><code>preprocessing_function</code>：应用于每个输入的函数，这个函数会在任何其他改变之前运行。这个函数需要一个参数：一张图像(秩为<code>3</code>的<code>Numpy</code>张量)，并且应该输出一个同尺寸的<code>Numpy</code>张量。</li>
<li><code>data_format</code>：图像数据格式，<code>channels_first</code>或<code>channels_last</code>。<code>channels_last</code>模式表示图像输入尺寸应该为(<code>samples, height, width, channels</code>)；<code>channels_first</code>模式表示输入尺寸应该为(<code>samples, channels, height, width</code>)。默认为在<code>Keras</code>配置文件<code>~/.keras/keras.json</code>中的<code>image_data_format</code>值。如果你从未设置它，那它就是<code>channels_last</code>。</li>
<li><code>validation_split</code>：浮点数，保留用于验证的图像的比例(严格在<code>0</code>和<code>1</code>之间)。</li>
</ul>
<p>&emsp;&emsp;使用<code>flow</code>的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes)</span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">    featurewise_center=<span class="keyword">True</span>, featurewise_std_normalization=<span class="keyword">True</span>, rotation_range=<span class="number">20</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>, height_shift_range=<span class="number">0.2</span>, horizontal_flip=<span class="keyword">True</span>)</span><br><span class="line">datagen.fit(x_train)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 使用实时数据增益的批数据对模型进行拟合</span></span><br><span class="line">model.fit_generator(datagen.flow(x_train, y_train, batch_size=<span class="number">32</span>),</span><br><span class="line">                    steps_per_epoch=len(x_train) / <span class="number">32</span>, epochs=epochs)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):  <span class="comment"># 这里有一个更“手动”的例子</span></span><br><span class="line">    print(<span class="string">'Epoch'</span>, e)</span><br><span class="line">    batches = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_batch, y_batch <span class="keyword">in</span> datagen.flow(x_train, y_train, batch_size=<span class="number">32</span>):</span><br><span class="line">        model.fit(x_batch, y_batch)</span><br><span class="line">        batches += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> batches &gt;= len(x_train) / <span class="number">32</span>:</span><br><span class="line">            <span class="keyword">break</span>  <span class="comment"># 我们需要手动打破循环，因为生成器会无限循环</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;使用<code>flow_from_directory</code>的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span> / <span class="number">255</span>, shear_range=<span class="number">0.2</span>,</span><br><span class="line">                                   zoom_range=<span class="number">0.2</span>, horizontal_flip=<span class="keyword">True</span>)</span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span> / <span class="number">255</span>)</span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">    <span class="string">'data/train'</span>, target_size=(<span class="number">150</span>, <span class="number">150</span>), batch_size=<span class="number">32</span>, class_mode=<span class="string">'binary'</span>)</span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">    <span class="string">'data/validation'</span>, target_size=(<span class="number">150</span>, <span class="number">150</span>), batch_size=<span class="number">32</span>, class_mode=<span class="string">'binary'</span>)</span><br><span class="line">model.fit_generator(train_generator, steps_per_epoch=<span class="number">2000</span>, epochs=<span class="number">50</span>,</span><br><span class="line">                    validation_data=validation_generator, validation_steps=<span class="number">800</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;同时转换图像和蒙版(<code>mask</code>)的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data_gen_args = dict(featurewise_center=<span class="keyword">True</span>, featurewise_std_normalization=<span class="keyword">True</span>,</span><br><span class="line">                     rotation_range=<span class="number">90.</span>, width_shift_range=<span class="number">0.1</span>,</span><br><span class="line">                     height_shift_range=<span class="number">0.1</span>, zoom_range=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 创建两个相同参数的实例</span></span><br><span class="line">image_datagen = ImageDataGenerator(**data_gen_args)</span><br><span class="line">mask_datagen = ImageDataGenerator(**data_gen_args)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 为fit和flow函数提供相同的种子和关键字参数</span></span><br><span class="line">seed = <span class="number">1</span></span><br><span class="line">image_datagen.fit(images, augment=<span class="keyword">True</span>, seed=seed)</span><br><span class="line">mask_datagen.fit(masks, augment=<span class="keyword">True</span>, seed=seed)</span><br><span class="line">​</span><br><span class="line">image_generator = image_datagen.flow_from_directory(<span class="string">'data/images'</span>, class_mode=<span class="keyword">None</span>, seed=seed)</span><br><span class="line">mask_generator = mask_datagen.flow_from_directory(<span class="string">'data/masks'</span>, class_mode=<span class="keyword">None</span>, seed=seed)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 将生成器组合成一个产生图像和蒙版(mask)的生成器</span></span><br><span class="line">train_generator = zip(image_generator, mask_generator)</span><br><span class="line">model.fit_generator(train_generator, steps_per_epoch=<span class="number">2000</span>, epochs=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h3 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h3><p>&emsp;&emsp;将数据生成器用于某些示例数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.fit(x, augment=<span class="keyword">False</span>, rounds=<span class="number">1</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>它基于一组样本数据，计算与数据转换相关的内部数据统计。当且仅当<code>featurewise_center</code>、<code>featurewise_std_normalization</code>或<code>zca_whitening</code>设置为<code>True</code>时才需要。</p>
<ul>
<li><code>x</code>：样本数据，秩应该为<code>4</code>。对于灰度数据，通道轴的值应该为<code>1</code>；对于<code>RGB</code>数据，值应该为<code>3</code>。</li>
<li><code>augment</code>：布尔值，是否使用随机样本扩张。</li>
<li><code>rounds</code>：整数。如果使用数据增强(<code>augment=True</code>)，表明在数据上进行多少次增强。</li>
<li><code>seed</code>：整数，随机种子。</li>
</ul>
<h3 id="flow"><a href="#flow" class="headerlink" title="flow"></a>flow</h3><p>&emsp;&emsp;采集数据和标签数组，生成批量增强数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.flow(</span><br><span class="line">    x, y=<span class="keyword">None</span>, batch_size=<span class="number">32</span>, shuffle=<span class="keyword">True</span>, sample_weight=<span class="keyword">None</span>, seed=<span class="keyword">None</span>,</span><br><span class="line">    save_to_dir=<span class="keyword">None</span>, save_prefix=<span class="string">''</span>, save_format=<span class="string">'png'</span>, subset=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：输入数据，秩为<code>4</code>的<code>Numpy</code>矩阵或元组。如果是元组，第一个元素应该包含图像，第二个元素是另一个<code>Numpy</code>数组或一列<code>Numpy</code>数组，它们不经过任何修改就传递给输出。可用于将模型杂项数据与图像一起输入。对于灰度数据，图像数组的通道轴的值应该为<code>1</code>，而对于<code>RGB</code>数据，其值应该为<code>3</code>。</li>
<li><code>y</code>：标签。</li>
<li><code>batch_size</code>：整数。</li>
<li><code>shuffle</code>：布尔值。</li>
<li><code>sample_weight</code>：样本权重。</li>
<li><code>seed</code>：整数。</li>
<li><code>save_to_dir</code>：<code>None</code>或字符串。这使您可以选择指定要保存的正在生成的增强图片的目录(用于可视化您正在执行的操作)。</li>
<li><code>save_prefix</code>：字符串，保存图片的文件名前缀(仅当<code>save_to_dir</code>设置时可用)。</li>
<li><code>save_format</code>：<code>png</code>和<code>jpeg</code>之一(仅当<code>save_to_dir</code>设置时可用)。</li>
<li><code>subset</code>：数据子集(<code>training</code>或<code>validation</code>)，如果在<code>ImageDataGenerator</code>中设置了<code>validation_split</code>。</li>
</ul>
<p>该函数返回一个生成元组(<code>x, y</code>)的<code>Iterator</code>，其中<code>x</code>是图像数据的<code>Numpy</code>数组(在单张图像输入时)，或<code>Numpy</code>数组列表(在多张图像输入时)，<code>y</code>是对应标签的<code>Numpy</code>数组。如果<code>sample_weight</code>不是<code>None</code>，生成的元组形式为(<code>x, y, sample_weight</code>)。如果<code>y</code>是<code>None</code>，只有<code>Numpy</code>数组<code>x</code>被返回。</p>
<h3 id="flow-from-directory"><a href="#flow-from-directory" class="headerlink" title="flow_from_directory"></a>flow_from_directory</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.flow_from_directory(</span><br><span class="line">    directory, target_size=(<span class="number">256</span>, <span class="number">256</span>), color_mode=<span class="string">'rgb'</span>, classes=<span class="keyword">None</span>,</span><br><span class="line">    class_mode=<span class="string">'categorical'</span>, batch_size=<span class="number">32</span>, shuffle=<span class="keyword">True</span>, seed=<span class="keyword">None</span>,</span><br><span class="line">    save_to_dir=<span class="keyword">None</span>, save_prefix=<span class="string">''</span>, save_format=<span class="string">'png'</span>,</span><br><span class="line">    follow_links=<span class="keyword">False</span>, subset=<span class="keyword">None</span>, interpolation=<span class="string">'nearest'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>directory</code>：目标目录的路径，每个类应该包含一个子目录。任何在子目录树下的<code>PNG</code>、<code>JPG</code>、<code>BMP</code>、<code>PPM</code>或<code>TIF</code>图像，都将被包含在生成器中。</li>
<li><code>target_size</code>：整数元组(<code>height, width</code>)，所有的图像将被调整到的尺寸。</li>
<li><code>color_mode</code>：<code>grayscale</code>和<code>rbg</code>之一，图像是否被转换成<code>1</code>或<code>3</code>个颜色通道。</li>
<li><code>classes</code>：可选的类的子目录列表(例如<code>[&#39;dogs&#39;, &#39;cats&#39;]</code>)。如果未提供，类的列表将自动从<code>directory</code>下的子目录名称或结构中推断出来，其中每个子目录都将被作为不同的类(类名将按字典序映射到标签的索引)。包含从类名到类索引的映射的字典可以通过<code>class_indices</code>属性获得。</li>
<li><p><code>class_mode</code>：<code>categorical</code>、<code>binary</code>、<code>sparse</code>、<code>input</code>或<code>None</code>之一，决定返回的标签数组的类型：<code>categorical</code>是<code>2</code>维<code>one-hot</code>编码标签，<code>binary</code>是一维二进制标签，<code>sparse</code>是一维整数标签，<code>input</code>是与输入图像相同的图像(主要用于自动编码器)。如果为<code>None</code>，则不返回标签(生成器将只产生批量的图像数据，对于<code>model.predict_generator</code>、<code>model.evaluate_generator</code>等很有用)。请注意，如果<code>class_mode</code>为<code>None</code>，那么数据仍然需要驻留在<code>directory</code>的子目录中才能正常工作。</p>
</li>
<li><p><code>batch_size</code>：一批数据的大小。</p>
</li>
<li><code>shuffle</code>：是否混洗数据。</li>
<li><code>seed</code>：随机种子，用于混洗和转换。</li>
<li><code>save_to_dir</code>：<code>None</code>或字符串。这使你可以最佳地指定正在生成的增强图片要保存的目录(用于可视化你在做什么)。</li>
<li><code>save_prefix</code>：字符串，保存图片的文件名前缀(仅当<code>save_to_dir</code>设置时可用)。</li>
<li><code>save_format</code>：<code>png</code>和<code>jpeg</code>之一(仅当<code>save_to_dir</code>设置时可用)。</li>
<li><code>follow_links</code>：是否跟踪类子目录中的符号链接(默认为<code>False</code>)。</li>
<li><code>subset</code>：数据子集(<code>training</code>或<code>validation</code>)，如果在<code>ImageDataGenerator</code>中设置了<code>validation_split</code>。</li>
<li><code>interpolation</code>：如果目标尺寸与加载图像的尺寸不同，则使用插值方法重新采样图像。支持的方法有<code>nearest</code>、<code>bilinear</code>和<code>bicubic</code>。如果安装了<code>1.1.3</code>以上版本的<code>PIL</code>，还支持<code>lanczos</code>。如果安装了<code>3.4.0</code>以上版本的<code>PIL</code>，还支持<code>box</code>和<code>hamming</code>。</li>
</ul>
<p>该函数返回一个生成(<code>x, y</code>)元组的<code>DirectoryIterator</code>，其中<code>x</code>是一个包含一批尺寸为(<code>batch_size, *target_size, channels</code>)的图像的<code>Numpy</code>数组，<code>y</code>是对应标签的<code>Numpy</code>数组。</p>
<h3 id="get-random-transform"><a href="#get-random-transform" class="headerlink" title="get_random_transform"></a>get_random_transform</h3><p>&emsp;&emsp;为转换生成随机参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.get_random_transform(img_shape, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>seed</code>是随机种子；<code>img_shape</code>是整数元组，被转换的图像的尺寸。</p>
<h3 id="random-transform"><a href="#random-transform" class="headerlink" title="random_transform"></a>random_transform</h3><p>&emsp;&emsp;将随机变换应用于图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.random_transform(x, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>x</code>是<code>3D</code>张量，即单张图像；参数<code>seed</code>是随机种子。</p>
<h3 id="standardize"><a href="#standardize" class="headerlink" title="standardize"></a>standardize</h3><p>&emsp;&emsp;将标准化配置应用于一批输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.preprocessing.image.standardize(x)</span><br></pre></td></tr></table></figure>
<p>参数<code>x</code>是需要标准化的一批输入。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/15/深度学习/Keras之循环层/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/15/深度学习/Keras之循环层/" itemprop="url">Keras之循环层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-15T17:19:11+08:00">
                2019-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>&emsp;&emsp;该函数是循环神经网络层基类，请使用它的子类<code>LSTM</code>、<code>GRU</code>或<code>SimpleRNN</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.recurrent.Recurrent(</span><br><span class="line">    return_sequences=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>,</span><br><span class="line">    stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>, implementation=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>所有的循环层(<code>LSTM</code>、<code>GRU</code>和<code>SimpleRNN</code>)都继承本层，因此下面的参数可以在任何循环层中使用：</p>
<ul>
<li><code>return_sequences</code>：布尔值，决定是返回输出序列中的最后一个输出，还是全部序列(<code>True</code>)。</li>
<li><code>go_backwards</code>：布尔值，如果为<code>True</code>，则逆向处理输入序列，并返回逆序后的序列。</li>
<li><code>stateful</code>：布尔值，如果为<code>True</code>，则一个<code>batch</code>中下标为<code>i</code>的样本的最终状态将会用作下一个<code>batch</code>同样下标的样本的初始状态。</li>
<li><code>unroll</code>：布尔值，如果为<code>True</code>，则网络将展开，否则就使用符号化的循环。当使用<code>TensorFlow</code>为后端时，循环网络本来就是展开的，因此该层不做任何事情。层展开会占用更多的内存，但会加速<code>RNN</code>的运算。层展开只适用于短序列。</li>
<li><code>implementation</code>：<code>0</code>、<code>1</code>或<code>2</code>。若为<code>0</code>，则<code>RNN</code>将以更少但是更大的矩阵乘法实现，因此在<code>CPU</code>上运行更快，但消耗更多的内存。如果设为<code>1</code>，则<code>RNN</code>将以更多但更小的矩阵乘法实现，因此在<code>CPU</code>上运行更慢，在<code>GPU</code>上运行更快，并且消耗更少的内存。如果设为<code>2</code>(仅<code>LSTM</code>和<code>GRU</code>可以设为<code>2</code>)，则<code>RNN</code>将把输入门、遗忘门和输出门合并为单个矩阵，以获得更加在<code>GPU</code>上更加高效的实现。注意，<code>RNN dropout</code>必须在所有门上共享，并导致正则效果性能微弱降低。</li>
<li><code>input_dim</code>：输入的维度(整数)，将此层用作模型中的第一层时，此参数(或者等价的指定<code>input_shape</code>)是必需的。</li>
<li><code>input_length</code>：当输入序列的长度固定时，该参数为输入序列的长度。当需要在该层后连接<code>Flatten</code>层，然后又要连接<code>Dense</code>层时，需要指定该参数，否则全连接的输出无法计算出来。注意，如果循环层不是网络的第一层，你需要在网络的第一层中指定序列的长度(通过<code>input_shape</code>指定)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch_size, timesteps, input_dim</code>)。<br>&emsp;&emsp;输出尺寸：如果<code>return_state</code>为<code>True</code>，则返回张量列表。第一个张量为输出，剩余的张量为最后的状态，每个张量的尺寸为(<code>batch_size, units</code>)；否则返回尺寸为(<code>batch_size, units</code>)的<code>2D</code>张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># now model.output_shape == (None, 32)</span></span><br><span class="line">model.add(LSTM(<span class="number">32</span>, input_shape=(<span class="number">10</span>, <span class="number">64</span>)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># the following is identical</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">32</span>, input_dim=<span class="number">64</span>, input_length=<span class="number">10</span>))</span><br><span class="line"><span class="comment"># for subsequent layers, no need to specify the input size</span></span><br><span class="line">model.add(LSTM(<span class="number">16</span>))</span><br><span class="line"><span class="comment"># to stack recurrent layers, you must use return_sequences=True</span></span><br><span class="line"><span class="comment"># on any recurrent layer that feeds into another recurrent layer.</span></span><br><span class="line"><span class="comment"># note that you only need to specify the input size on the first layer.</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">64</span>, input_dim=<span class="number">64</span>, input_length=<span class="number">10</span>, return_sequences=<span class="keyword">True</span>))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>, return_sequences=<span class="keyword">True</span>))</span><br><span class="line">model.add(LSTM(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h4 id="屏蔽覆盖"><a href="#屏蔽覆盖" class="headerlink" title="屏蔽覆盖"></a>屏蔽覆盖</h4><p>&emsp;&emsp;循环层支持通过时间步变量对输入数据进行<code>Masking</code>，如果想将输入数据的一部分屏蔽掉，请使用<code>Embedding</code>层并将参数<code>mask_zero</code>设为<code>True</code>。</p>
<h4 id="关于在RNN中使用状态的注意事项"><a href="#关于在RNN中使用状态的注意事项" class="headerlink" title="关于在RNN中使用状态的注意事项"></a>关于在RNN中使用状态的注意事项</h4><p>&emsp;&emsp;你可以将<code>RNN</code>层设置为<code>stateful</code>(有状态的)，这意味着针对一批中的样本计算的状态将被重新用作下一批样品的初始状态。这假定在不同连续批次的样品之间有一对一的映射。<br>&emsp;&emsp;为了启用状态<code>RNN</code>，请在实例化层对象时指定参数<code>stateful = True</code>，并在<code>Sequential</code>模型使用固定大小的<code>batch</code>：通过在模型的第一层传入<code>batch_size = (...)</code>和<code>input_shape</code>来实现。在函数式模型中，对所有的输入都要指定相同的<code>batch_size</code>。<br>&emsp;&emsp;如果要将循环层的状态重置，请调用<code>reset_states</code>。对模型调用将重置模型中所有状态<code>RNN</code>的状态；对单个层调用则只重置该层的状态。</p>
<h4 id="关于指定RNN初始状态的注意事项"><a href="#关于指定RNN初始状态的注意事项" class="headerlink" title="关于指定RNN初始状态的注意事项"></a>关于指定RNN初始状态的注意事项</h4><p>&emsp;&emsp;可以通过设置<code>initial_state</code>用符号式的方式指定<code>RNN</code>层的初始状态，<code>initial_stat</code>的值应该为一个<code>tensor</code>或一个<code>tensor</code>列表，代表<code>RNN</code>层的初始状态。<br>&emsp;&emsp;也可以通过设置<code>reset_states</code>参数用数值的方法设置<code>RNN</code>的初始状态，状态的值应该为<code>numpy</code>数组或<code>numpy</code>数组的列表，代表<code>RNN</code>层的初始状态。</p>
<h3 id="SimpleRNN"><a href="#SimpleRNN" class="headerlink" title="SimpleRNN"></a>SimpleRNN</h3><p>&emsp;&emsp;该函数是完全连接的<code>RNN</code>，其输出将被反馈到输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>, bias_initializer=<span class="string">'zeros'</span>,</span><br><span class="line">    kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>,</span><br><span class="line">    bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, return_sequences=<span class="keyword">False</span>,</span><br><span class="line">    return_state=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>units</code>：正整数，输出空间的维度。</li>
<li><code>activation</code>：要使用的激活函数。如果传入<code>None</code>，则不使用激活函数(即线性激活<code>a(x) = x</code>)。</li>
<li><code>use_bias</code>：布尔值，该层是否使用偏置向量。</li>
<li><code>kernel_initializer</code>：<code>kernel</code>权值矩阵的初始化器，用于输入的线性转换。</li>
<li><code>recurrent_initializer</code>：<code>recurrent_kernel</code>权值矩阵的初始化器，用于循环层状态的线性转换。</li>
<li><code>bias_initializer</code>：偏置向量的初始化器。</li>
<li><code>kernel_regularizer</code>：运用到<code>kernel</code>权值矩阵的正则化函数。</li>
<li><code>recurrent_regularizer</code>：运用到<code>recurrent_kernel</code>权值矩阵的正则化函数。</li>
<li><code>bias_regularizer</code>：运用到偏置向量的正则化函数。</li>
<li><code>activity_regularizer</code>：运用到层输出的正则化函数。</li>
<li><code>kernel_constraint</code>：运用到<code>kernel</code>权值矩阵的约束函数。</li>
<li><code>recurrent_constraint</code>：运用到<code>recurrent_kernel</code>权值矩阵的约束函数。</li>
<li><code>bias_constraint</code>：运用到偏置向量的约束函数。</li>
<li><code>dropout</code>：在<code>0</code>和<code>1</code>之间的浮点数。单元的丢弃比例，用于输入的线性转换。</li>
<li><code>recurrent_dropout</code>：在<code>0</code>和<code>1</code>之间的浮点数，控制输入线性变换的神经元断开比例。</li>
</ul>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>&emsp;&emsp;该函数是门限循环单元网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.GRU(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">    implementation=<span class="number">1</span>, return_sequences=<span class="keyword">False</span>, return_state=<span class="keyword">False</span>,</span><br><span class="line">    go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>&emsp;&emsp;该函数是长短期记忆网络层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, implementation=<span class="number">1</span>, return_sequences=<span class="keyword">False</span>,</span><br><span class="line">    return_state=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ConvLSTM2D"><a href="#ConvLSTM2D" class="headerlink" title="ConvLSTM2D"></a>ConvLSTM2D</h3><p>&emsp;&emsp;该函数是卷积<code>LSTM</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ConvLSTM2D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'hard_sigmoid'</span>,</span><br><span class="line">    use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">    return_sequences=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>它类似于<code>LSTM</code>层，但输入变换和循环变换都是卷积的。</p>
<ul>
<li><code>filters</code>：整数，输出空间的维度(即卷积中滤波器的输出数量)。</li>
<li><code>kernel_size</code>：一个整数，或者单个整数表示的元组或列表，指明<code>1D</code>卷积窗口的长度。</li>
<li><code>strides</code>：一个整数，或者单个整数表示的元组或列表，指明卷积的步长。当不等于<code>1</code>时，无法使用<code>dilation</code>功能，即<code>dialation_rate</code>必须为<code>1</code>。</li>
<li><code>padding</code>：<code>valid</code>或<code>same</code>之一。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
<li><code>dilation_rate</code>：一个整数，或<code>n</code>个整数的元组/列表，指定用于膨胀卷积的膨胀率。</li>
<li><code>recurrent_activation</code>：用在<code>recurrent</code>部分的激活函数，为预定义的激活函数名(参考激活函数)，或逐元素(<code>element-wise</code>)的<code>Theano</code>函数。如果不指定该参数，将不会使用任何激活函数(即使用线性激活函数<code>“a(x) = x”</code>)。</li>
<li><code>unit_forget_bias</code>：布尔值。如果为<code>True</code>，则初始化时，将忘记门的偏置加<code>1</code>。将其设置为<code>True</code>同时还会强制<code>bias_initializer=&quot;zeros&quot;</code>。。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，返回<code>5D</code>张量，尺寸为(<code>samples, time, channels, rows, cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，返回<code>5D</code>张量，尺寸为(<code>samples, time, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：如果<code>return_sequences</code>为<code>True</code>：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，返回<code>5D</code>张量，尺寸为(<code>samples, time, filters, output_row, output_col</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，返回<code>5D</code>张量，尺寸为(<code>samples, time, output_row, output_col, filters</code>)。</li>
</ul>
<p>否则：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，返回<code>4D</code>张量，尺寸为(<code>samples, filters, output_row, output_col</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，返回<code>4D</code>张量，尺寸为(<code>samples, output_row, output_col, filters</code>)。<code>o_row</code>和<code>o_col</code>依赖于过滤器的尺寸和填充。</li>
</ul>
<h3 id="SimpleRNNCell"><a href="#SimpleRNNCell" class="headerlink" title="SimpleRNNCell"></a>SimpleRNNCell</h3><p>&emsp;&emsp;该函数是<code>SimpleRNN</code>的单元类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNNCell(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>, bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="GRUCell"><a href="#GRUCell" class="headerlink" title="GRUCell"></a>GRUCell</h3><p>&emsp;&emsp;该函数是<code>GRU</code>层的单元类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.GRUCell(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>,</span><br><span class="line">    bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, implementation=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="LSTMCell"><a href="#LSTMCell" class="headerlink" title="LSTMCell"></a>LSTMCell</h3><p>&emsp;&emsp;该函数是<code>LSTM</code>层的单元类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTMCell(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>,</span><br><span class="line">    recurrent_dropout=<span class="number">0.0</span>, implementation=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="StackedRNNCells"><a href="#StackedRNNCells" class="headerlink" title="StackedRNNCells"></a>StackedRNNCells</h3><p>&emsp;&emsp;该函数允许将一堆<code>RNN</code>单元包装为一个单元的封装器，用于实现高效堆叠的<code>RNN</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.StackedRNNCells(cells)</span><br></pre></td></tr></table></figure>
<p>参数<code>cells</code>是<code>RNN</code>单元实例的列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cells = [keras.layers.LSTMCell(output_dim),</span><br><span class="line">         keras.layers.LSTMCell(output_dim),</span><br><span class="line">         keras.layers.LSTMCell(output_dim),]</span><br><span class="line">inputs = keras.Input((timesteps, input_dim))</span><br><span class="line">x = keras.layers.RNN(cells)(inputs)</span><br></pre></td></tr></table></figure>
<h3 id="CuDNNGRU"><a href="#CuDNNGRU" class="headerlink" title="CuDNNGRU"></a>CuDNNGRU</h3><p>&emsp;&emsp;该函数是由<code>CuDNN</code>支持的快速<code>GRU</code>实现，只能以<code>TensorFlow</code>后端运行在<code>GPU</code>上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.CuDNNGRU(</span><br><span class="line">    units, kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, return_sequences=<span class="keyword">False</span>,</span><br><span class="line">    return_state=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="CuDNNLSTM"><a href="#CuDNNLSTM" class="headerlink" title="CuDNNLSTM"></a>CuDNNLSTM</h3><p>&emsp;&emsp;该函数是由<code>CuDNN</code>支持的快速<code>LSTM</code>实现，只能以<code>TensorFlow</code>后端运行在<code>GPU</code>上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.CuDNNLSTM(</span><br><span class="line">    units, kernel_initializer=<span class="string">'glorot_uniform'</span>, recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">    return_sequences=<span class="keyword">False</span>, return_state=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/15/深度学习/Kears之核心网络层/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/15/深度学习/Kears之核心网络层/" itemprop="url">Kears之核心网络层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-15T14:59:09+08:00">
                2019-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Dense"><a href="#Dense" class="headerlink" title="Dense"></a>Dense</h3><p>&emsp;&emsp;该函数就是常用的的全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(</span><br><span class="line">    units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p><code>Dense</code>实现以下操作：<code>output = activation(dot(input, kernel) + bias)</code>，其中<code>activation</code>是按逐个元素计算的激活函数，<code>kernel</code>是由网络层创建的权值矩阵，以及<code>bias</code>是其创建的偏置向量(只在<code>use_bias</code>为<code>True</code>时才有用)。如果本层的输入数据的维度大于<code>2</code>，则会先被压为与<code>kernel</code>相匹配的大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在模型就会以尺寸为(*, 16)的数组作为输入，其输出数组的尺寸为(*, 32)</span></span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_shape=(<span class="number">16</span>,)))</span><br><span class="line">model.add(Dense(<span class="number">32</span>))  <span class="comment"># 在第一层之后，你就不再需要指定输入的尺寸了</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>units</code>：正整数，输出空间维度。</li>
<li><code>activation</code>：激活函数。若不指定，则不使用激活函数(即线性激活<code>a(x) = x</code>)。</li>
<li><code>use_bias</code>：布尔值，该层是否使用偏置向量。</li>
<li><code>kernel_initializer</code>：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。</li>
<li><code>bias_initializer</code>：偏置向量初始化方法，为预定义初始化方法名的字符串，或用于初始化偏置向量的初始化器。</li>
<li><code>kernel_regularizer</code>：运用到<code>kernel</code>权值矩阵的正则化函数。</li>
<li><code>bias_regularizer</code>：运用到偏置向量的的正则化函数。</li>
<li><code>activity_regularizer</code>：运用到层的输出的正则化函数。</li>
<li><code>kernel_constraint</code>：运用到<code>kernel</code>权值矩阵的约束函数。</li>
<li><code>bias_constraint</code>：运用到偏置向量的约束函数。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：<code>nD</code>张量，尺寸为(<code>batch_size, ..., input_dim</code>)。最常见的情况是一个尺寸为(<code>batch_size, input_dim</code>)的<code>2D</code>输入。<br>&emsp;&emsp;输出尺寸：<code>nD</code>张量，尺寸为(<code>batch_size, ..., units</code>)。例如，对于尺寸为(<code>batch_size, input_dim</code>)的<code>2D</code>输入，输出的尺寸为(<code>batch_size, units</code>)。</p>
<h3 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h3><p>&emsp;&emsp;该函数将激活函数应用于输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Activation(activation)</span><br></pre></td></tr></table></figure>
<p>参数<code>activation</code>是要使用的激活函数的名称，或者选择一个<code>Theano</code>或<code>TensorFlow</code>操作。<br>&emsp;&emsp;输入尺寸：任意尺寸。当使用此层作为模型中的第一层时，使用参数<code>input_shape</code>(整数元组，不包括样本数的轴)。<br>&emsp;&emsp;输出尺寸：与输入相同。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>&emsp;&emsp;该函数将<code>Dropout</code>应用于输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p><code>Dropout</code>将在训练过程中每次更新参数时，按一定概率(<code>rate</code>)随机断开输入神经元，用于防止过拟合。</p>
<ul>
<li><code>rate</code>：在<code>0</code>和<code>1</code>之间浮动，控制需要断开的神经元的比例。</li>
<li><code>noise_shape</code>：<code>1D</code>整数张量，为将要应用在输入上的二值<code>Dropout mask</code>的<code>shape</code>，例如你的输入为(<code>batch_size, timesteps, features</code>)，并且你希望在各个时间步上的<code>Dropout mask</code>都相同，则可传入<code>noise_shape = (batch_size, 1, features)</code>。</li>
<li><code>seed</code>：一个作为随机种子的<code>Python</code>整数。</li>
</ul>
<h3 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h3><p>&emsp;&emsp;<code>Flatten</code>层用来将输入<code>压平</code>，也就是把多维的输入一维化，常用在从卷积层到全连接层的过渡，不影响<code>batch</code>的大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Flatten()</span><br></pre></td></tr></table></figure>
<p>使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 64, 32, 32)”</span></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, input_shape=(<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)))</span><br><span class="line">model.add(Flatten())  <span class="comment"># 现在“model.output_shape == (None, 65536)”</span></span><br></pre></td></tr></table></figure>
<h3 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a>Reshape</h3><p>&emsp;&emsp;该函数将输入重新调整为特定的尺寸：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Reshape(target_shape)</span><br></pre></td></tr></table></figure>
<p>参数<code>target_shape</code>是目标尺寸，为整数元组，不包含样本数目的维度(即<code>batch</code>大小)。<br>&emsp;&emsp;输入尺寸：任意，尽管输入尺寸中的所有维度必须是固定的。当使用此层作为模型中的第一层时，使用参数<code>input_shape</code>(整数元组，不包括样本数的轴)。<br>&emsp;&emsp;输出尺寸：<code>(batch_size,) + target_shape</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 3, 4)”，None是批表示的维度</span></span><br><span class="line">model.add(Reshape((<span class="number">3</span>, <span class="number">4</span>), input_shape=(<span class="number">12</span>,)))</span><br><span class="line">model.add(Reshape((<span class="number">6</span>, <span class="number">2</span>)))  <span class="comment"># 现在“model.output_shape == (None, 6, 2)”</span></span><br><span class="line"><span class="comment"># 还支持使用“-1”表示维度的尺寸推断，现在“model.output_shape == (None, 3, 2, 2)”</span></span><br><span class="line">model.add(Reshape((<span class="number">-1</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="Permute"><a href="#Permute" class="headerlink" title="Permute"></a>Permute</h3><p>&emsp;&emsp;<code>Permute</code>层将输入的维度按照给定模式进行重排，例如当需要将<code>RNN</code>和<code>CNN</code>网络连接时，可能会用到该层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Permute(dims)</span><br></pre></td></tr></table></figure>
<p>参数<code>dims</code>是整数元组，指定重排的模式，不包含样本数的维度。重排模式的下标从<code>1</code>开始，例如(<code>2, 1</code>)代表将输入的第二个维度重排到输出的第一个维度，而将输入的第一个维度重排到第二个维度。<br>&emsp;&emsp;输入尺寸：任意。当使用此层作为模型中的第一层时，使用参数<code>input_shape</code>(整数元组，不包括样本数的轴)。<br>&emsp;&emsp;输出尺寸：与输入尺寸相同，但是维度根据指定的模式重新排列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 64, 10)”，None是批表示的维度</span></span><br><span class="line">model.add(Permute((<span class="number">2</span>, <span class="number">1</span>), input_shape=(<span class="number">10</span>, <span class="number">64</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="RepeatVector"><a href="#RepeatVector" class="headerlink" title="RepeatVector"></a>RepeatVector</h3><p>&emsp;&emsp;该函数将输入重复<code>n</code>次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.RepeatVector(n)</span><br></pre></td></tr></table></figure>
<p>参数<code>n</code>是整数，即重复次数。<br>&emsp;&emsp;输入尺寸：<code>2D</code>张量，尺寸为(<code>num_samples, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>num_samples, n, features</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 32)”，None是批表示的维度</span></span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_dim=<span class="number">32</span>))</span><br><span class="line">model.add(RepeatVector(<span class="number">3</span>))  <span class="comment"># 现在“model.output_shape == (None, 3, 32)”</span></span><br></pre></td></tr></table></figure>
<h3 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h3><p>&emsp;&emsp;该函数将上一层的输出封装为<code>Layer</code>对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>function</code>：需要封装的函数，将输入张量作为第一个参数，即上一层的输出。</li>
<li><code>output_shape</code>：预期的函数输出尺寸，只在使用<code>Theano</code>时有意义，可以是一个<code>tuple</code>，也可以是一个根据输入<code>shape</code>计算输出<code>shape</code>的函数。</li>
<li><code>arguments</code>：需要传递给函数的关键字参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model.add(Lambda(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>))  <span class="comment"># 添加一个“x -&gt; x^2”层</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">antirectifier</span><span class="params">(x)</span>:</span>  <span class="comment"># 添加一个网络层，返回输入的正数部分与负数部分的反面的连接</span></span><br><span class="line">    x -= K.mean(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    x = K.l2_normalize(x, axis=<span class="number">1</span>)</span><br><span class="line">    pos = K.relu(x)</span><br><span class="line">    neg = K.relu(-x)</span><br><span class="line">    <span class="keyword">return</span> K.concatenate([pos, neg], axis=<span class="number">1</span>)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">antirectifier_output_shape</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    shape = list(input_shape)</span><br><span class="line">    <span class="keyword">assert</span> len(shape) == <span class="number">2</span>  <span class="comment"># only valid for 2D tensors</span></span><br><span class="line">    shape[<span class="number">-1</span>] *= <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> tuple(shape)</span><br><span class="line">​</span><br><span class="line">model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;输入尺寸：任意，当使用此层作为模型中的第一层时，使用参数<code>input_shape</code>(整数元组，不包括样本数的轴)。<br>&emsp;&emsp;输出尺寸：由<code>output_shape</code>参数指定(或者在使用<code>TensorFlow</code>时，自动推理得到)。</p>
<h3 id="ActivityRegularization"><a href="#ActivityRegularization" class="headerlink" title="ActivityRegularization"></a>ActivityRegularization</h3><p>&emsp;&emsp;经过本层的数据不会有任何变化，但会基于其激活值更新损失函数值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ActivityRegularization(l1=<span class="number">0.0</span>, l2=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>l1</code>是<code>L1</code>范数正则化因子(正数浮点型)；<code>l2</code>是<code>L2</code>范数正则化因子(正数浮点型)。<br>&emsp;&emsp;输入尺寸：任意，当使用此层作为模型中的第一层时，使用参数<code>input_shape</code>(整数元组，不包括样本数的轴)。<br>&emsp;&emsp;输出尺寸：与输入相同。</p>
<h3 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h3><p>&emsp;&emsp;使用给定的值对输入的序列信号进行<code>屏蔽</code>，用以定位需要跳过的时间步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Masking(mask_value=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>对于输入张量的时间步，即输入张量的第<code>1</code>维度(维度从<code>0</code>开始算)，如果输入张量在该时间步上都等于<code>mask_value</code>，则该时间步将在模型接下来的所有层(只要支持<code>masking</code>)被跳过(即<code>屏蔽</code>)。<br>&emsp;&emsp;考虑将要喂入一个<code>LSTM</code>层的<code>Numpy</code>矩阵<code>x</code>，尺寸为(<code>samples, timesteps, features</code>)，现将其送入<code>LSTM</code>层。因为你缺少时间步为<code>3</code>和<code>5</code>的信号，所以你希望将其掩盖，这时候应该：</p>
<ul>
<li>设置<code>x[:, 3, :] = 0</code>以及<code>x[:, 5, :] = 0</code>。</li>
<li>在<code>LSTM</code>层之前，插入一个<code>mask_value = 0</code>的<code>Masking</code>层：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Masking(mask_value=<span class="number">0.</span>, input_shape=(timesteps, features)))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/15/深度学习/Keras之MNIST识别/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/15/深度学习/Keras之MNIST识别/" itemprop="url">Keras之MNIST识别</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-15T14:38:35+08:00">
                2019-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>&emsp;&emsp;<code>Keras</code>自身就有<code>MNIST</code>这个数据包，再分成训练集和测试集。<code>x</code>是一张张图片，<code>y</code>是每张图片对应的标签，即它是哪个数字。<br>&emsp;&emsp;输入的<code>x</code>变成(<code>60000 * 784</code>)的数据(即训练集有<code>6</code>万张图片，每张图片的大小是<code>28 * 28</code>)，然后除以<code>255</code>进行标准化，因为每个像素都是在<code>0</code>到<code>255</code>之间，标准化之后就变成了<code>0</code>到<code>1</code>之间。<br>&emsp;&emsp;对于<code>y</code>，要用到<code>Keras</code>改造的<code>numpy</code>的一个函数<code>np_utils.to_categorical</code>，把y变成了<code>one-hot</code>的形式，即之前<code>y</code>是一个数值(在<code>0</code>至<code>9</code>之间)，现在是一个大小为<code>10</code>的向量，它属于哪个数字，就在哪个位置为<code>1</code>，其他位置都是<code>0</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line">​</span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><br><span class="line"><span class="comment"># data pre-processing</span></span><br><span class="line">X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">-1</span>) / <span class="number">255.</span>  <span class="comment"># normalize</span></span><br><span class="line">X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">-1</span>) / <span class="number">255.</span>  <span class="comment"># normalize</span></span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">10</span>)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">10</span>)</span><br><span class="line">​</span><br><span class="line">print(X_train[<span class="number">1</span>].shape)</span><br><span class="line">print(y_train[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>&emsp;&emsp;模型的第一段就是加入<code>Dense</code>神经层，<code>32</code>是输出的维度，<code>784</code>是输入的维度。第一层传出的数据有<code>32</code>个<code>feature</code>，传给激励单元，激励函数用到的是<code>relu</code>函数。经过激励函数之后，就变成了非线性的数据。然后再把这个数据传给下一个神经层，对于这个<code>Dense</code>，我们定义它有<code>10</code>个输出的<code>feature</code>。同样的，此处不需要再定义输入的维度，因为它接收的是上一层的输出。接下来再输入给下面的<code>softmax</code>函数，用来分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">    Dense(<span class="number">32</span>, input_dim=<span class="number">784</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">10</span>),</span><br><span class="line">    Activation(<span class="string">'softmax'</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>接下来用<code>RMSprop</code>作为优化器，它的参数包括学习率等，可以通过修改这些参数来看一下模型的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmsprop = RMSprop(lr=<span class="number">0.001</span>, rho=<span class="number">0.9</span>, epsilon=<span class="number">1e-08</span>, decay=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="激活模型"><a href="#激活模型" class="headerlink" title="激活模型"></a>激活模型</h3><p>&emsp;&emsp;接下来用<code>model.compile</code>激励神经网络。优化器可以选择默认的，也可以是我们在上一步定义的。对于损失函数，分类和回归问题的不一样，用的是交叉熵。<code>metrics</code>里面可以放入需要计算的<code>cost</code>、<code>accuracy</code>、<code>score</code>等：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We add metrics to get more results you want to see</span></span><br><span class="line">model.compile(optimizer=rmsprop, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<h3 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h3><p>&emsp;&emsp;这里用到的是<code>fit</code>函数，<code>nb_epoch</code>表示把整个数据训练多少次，<code>batch_size</code>显示每批处理<code>32</code>个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Training ------------'</span>)</span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">2</span>, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h3><p>&emsp;&emsp;接下来就是用测试集来检验一下模型，方法和回归网络中是一样的。运行代码之后，可以输出<code>accuracy</code>和<code>loss</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line"><span class="comment"># Evaluate the model with the metrics we defined earlier</span></span><br><span class="line">loss, accuracy = model.evaluate(X_test, y_test)</span><br><span class="line">print(<span class="string">'test loss: '</span>, loss)</span><br><span class="line">print(<span class="string">'test accuracy: '</span>, accuracy)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="CNN的MNIST实验"><a href="#CNN的MNIST实验" class="headerlink" title="CNN的MNIST实验"></a>CNN的MNIST实验</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution2D, MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line">​</span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line">epochs = <span class="number">12</span></span><br><span class="line">img_rows, img_cols = <span class="number">28</span>, <span class="number">28</span>  <span class="comment"># input image dimensions</span></span><br><span class="line">nb_filters = <span class="number">32</span>  <span class="comment"># number of convolutional filters to use</span></span><br><span class="line">pool_size = (<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># size of pooling area for max pooling</span></span><br><span class="line">kernel_size = (<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># convolution kernel size</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># the data, shuffled and split between train and test sets</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> K.image_dim_ordering() == <span class="string">'th'</span>:  <span class="comment"># 根据不同的backend，决定不同的格式</span></span><br><span class="line">    X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</span><br><span class="line">    X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</span><br><span class="line">    input_shape = (<span class="number">1</span>, img_rows, img_cols)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], img_rows, img_cols, <span class="number">1</span>)</span><br><span class="line">    X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], img_rows, img_cols, <span class="number">1</span>)</span><br><span class="line">    input_shape = (img_rows, img_cols, <span class="number">1</span>)</span><br><span class="line">​</span><br><span class="line">X_train = X_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">X_test = X_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">X_train /= <span class="number">255</span></span><br><span class="line">X_test /= <span class="number">255</span></span><br><span class="line">print(<span class="string">'X_train shape:'</span>, X_train.shape)</span><br><span class="line">print(X_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</span><br><span class="line">print(X_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 转换为one_hot类型  </span></span><br><span class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</span><br><span class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</span><br><span class="line">​</span><br><span class="line">model = Sequential()  <span class="comment"># 构建模型</span></span><br><span class="line">model.add(  <span class="comment"># 卷积层1</span></span><br><span class="line">    Convolution2D(nb_filters, (kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>]),</span><br><span class="line">                  padding=<span class="string">'same'</span>, input_shape=input_shape)</span><br><span class="line">)</span><br><span class="line">​</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))  <span class="comment"># 激活层</span></span><br><span class="line">model.add(Convolution2D(nb_filters, (kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>])))  <span class="comment"># 卷积层2</span></span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))  <span class="comment"># 激活层</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=pool_size))  <span class="comment"># 池化层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))  <span class="comment"># 神经元随机失活</span></span><br><span class="line">model.add(Flatten())  <span class="comment"># 拉成一维数据</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>))  <span class="comment"># 全连接层1</span></span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))  <span class="comment"># 激活层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))  <span class="comment"># 随机失活</span></span><br><span class="line">model.add(Dense(nb_classes))  <span class="comment"># 全连接层2</span></span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))  <span class="comment"># Softmax评分</span></span><br><span class="line">​</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              optimizer=<span class="string">'adadelta'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,</span><br><span class="line">          verbose=<span class="number">1</span>, validation_data=(X_test, Y_test))</span><br><span class="line">score = model.evaluate(X_test, Y_test, verbose=<span class="number">0</span>)  <span class="comment"># 评估模型</span></span><br><span class="line">print(<span class="string">'Test score:'</span>, score[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/15/深度学习/TensorFlow的运行方式/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/15/深度学习/TensorFlow的运行方式/" itemprop="url">TensorFlow的运行方式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-15T14:05:11+08:00">
                2019-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>TensorFlow</code>的运行方式分如下<code>4</code>步：</p>
<ul>
<li>加载数据以及定义超参数。</li>
<li>构建网络。</li>
<li>训练模型。</li>
<li>评估模型和进行预测。</li>
</ul>
<p>下面以一个神经网络为例，讲解<code>TensorFlow</code>的运行方式。在这个例子中，我们构造一个满足一元二次函数<code>y = ax^2 + b</code>的原始数据，然后构造一个最简单的神经网络(仅包含一个输入层、一个隐藏层和一个输出层)。通过<code>TensorFlow</code>将隐藏层和输出层的<code>weights</code>和<code>biases</code>的值学习出来，看看随着训练的增加，损失值是不是在不断地减小。</p>
<h3 id="生成以及加载数据"><a href="#生成以及加载数据" class="headerlink" title="生成以及加载数据"></a>生成以及加载数据</h3><p>&emsp;&emsp;首先来生成输入数据，假设最后要学习的方程为<code>y = x^2 - 0.5</code>，我们来构造满足这个方程的一堆<code>x</code>和<code>y</code>，同时加上一些不满足方程的噪声点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 为了使点更密集一些，我们构建了300个点，分布在“-1”到1区间，直接采用np生成</span></span><br><span class="line"><span class="comment"># 等差数列的方法，并将结果为300个点的一维数组，转换为“300 * 1”的二维数组</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:, np.newaxis]</span><br><span class="line"><span class="comment"># 加入一些噪声点，使它与x_data的维度一致，并且均为均值为0，方差为0.05的正态分布</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise  <span class="comment"># y = x^2 - 0.5 + 噪声</span></span><br></pre></td></tr></table></figure>
<p>接下来定义<code>x</code>和<code>y</code>的占位符来作为将要输入神经网络的变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="构建网络模型"><a href="#构建网络模型" class="headerlink" title="构建网络模型"></a>构建网络模型</h3><p>&emsp;&emsp;我们这里需要构建一个隐藏层和一个输出层。作为神经网络中的层，输入的参数应该有<code>4</code>个变量：输入数据、输入数据的维度、输出数据的维度和激活函数。每一层经过向量化(<code>y = weights * x + biases</code>)的处理，并且经过激活函数的非线性处理后，最终得到输出数据。<br>&emsp;&emsp;接下来定义隐藏层和输出层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    <span class="comment"># 构建权重：“in_size * out_size”大小的矩阵</span></span><br><span class="line">    weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    <span class="comment"># 构建偏置：“1 * out_size”的矩阵</span></span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, weights) + biases  <span class="comment"># 矩阵相乘</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        out_puts = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out_puts = activation_function(Wx_plus_b)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">return</span> out_puts  <span class="comment"># 输出得到的数据</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 构建隐藏层，假设隐藏层有20个神经元</span></span><br><span class="line">h1 = add_layer(xs, <span class="number">1</span>, <span class="number">20</span>, activation_function=tf.nn.relu)</span><br><span class="line"><span class="comment"># 构建输出层，假设输出层和输入层一样，有一个神经元</span></span><br><span class="line">prediction = add_layer(h1, <span class="number">20</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;接下来需要构建损失函数：计算输出层的预测值和真实值间的误差，对二者差的平方求和，然后再取平均，得到损失函数。运用梯度下降法，以<code>0.1</code>的效率最小化损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 计算预测值和真实值间的误差</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>&emsp;&emsp;我们让<code>TensorFlow</code>训练<code>1000</code>次，每<code>50</code>次输出训练的损失值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:  <span class="comment"># 每50次打印出一次损失值</span></span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;以上就是最简单的利用<code>TensorFlow</code>的神经网络训练一个模型的过程，目标就是要训练出权重值来使模型拟合<code>y = x^2 - 0.5</code>的系数<code>1</code>和<code>-0.5</code>。通过损失值越来越小的现象，可以看出训练的参数越来越逼近目标结果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/54/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/54/">54</a><span class="page-number current">55</span><a class="page-number" href="/page/56/">56</a><span class="space">&hellip;</span><a class="page-number" href="/page/93/">93</a><a class="extend next" rel="next" href="/page/56/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">930</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
