<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="泥腿子出身">
<meta property="og:url" content="http://fukangwei.gitee.io/page/16/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="泥腿子出身">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/16/">





  <title>泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/自编码和解码/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/自编码和解码/" itemprop="url">自编码和解码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T14:00:44+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h3><p>&emsp;&emsp;有一个神经网络，它所做的事情是接收一张图片，然后给它打码，最后再从打码后的图片中还原，具体过程如下：</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/1.png" height="229" width="832"></p>
<p>&emsp;&emsp;可以看出图片其实是经过了压缩再解压的这一道工序，当压缩的时候，原有的图片质量被缩减，解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片。为什么要这样做呢？</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/2.png" height="200" width="394"></p>
<p>&emsp;&emsp;原来有时神经网络要接受大量的输入信息，比如输入信息是高清图片时，输入信息量可能达到上千万，让神经网络直接从上千万个信息源中学习是一件很吃力的工作。所以需要压缩一下，提取出原图片中的最具代表性的信息，缩减输入信息量，再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了，自编码就能在这时发挥作用。通过将原数据白色的<code>X</code>压缩并解压成黑色的<code>X</code>，然后通过对比黑白<code>X</code>，求出预测误差，进行反向传递，逐步提升自编码的准确性。训练好的自编码中间这一部分就是能总结原数据的精髓。可以看出，从头到尾我们只用到了输入数据<code>X</code>，并没有用到<code>X</code>对应的数据标签，所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。</p>
<h3 id="编码器Encoder"><a href="#编码器Encoder" class="headerlink" title="编码器Encoder"></a>编码器Encoder</h3><p>&emsp;&emsp;这部分也叫作<code>encoder</code>编码器，编码器能得到原数据的精髓，然后我们只需要再创建一个小的神经网络学习这个精髓的数据，不仅减少了神经网络的负担，而且同样能达到很好的效果。</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/3.png" height="262" width="682"></p>
<p>&emsp;&emsp;右图是一个通过自编码整理出来的数据，它能从原数据中总结出每种类型数据的特征，如果把这些特征类型都放在一张二维的图片上，每种类型都已经被很好的用原数据的精髓区分开来。如果你了解<code>PCA</code>主成分分析，在提取主要特征时，自编码和它一样，甚至超越了<code>PCA</code>。换句话说，自编码可以像<code>PCA</code>一样给特征属性降维。</p>
<hr>
<p>&emsp;&emsp;今天的代码会运用两个类型：</p>
<ul>
<li>通过<code>Feature</code>的压缩并解压，并将结果与原始数据进行对比，观察处理过后的数据是不是如预期跟原始数据很相像。</li>
<li>只看<code>encoder</code>压缩的过程，使用它将一个数据集压缩到只有两个<code>Feature</code>时，将数据放入一个二维坐标系内。</li>
</ul>
<h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><p>&emsp;&emsp;基本设置代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">training_epochs = <span class="number">20</span>  <span class="comment"># 训练的轮数</span></span><br><span class="line">batch_size = <span class="number">256</span>  <span class="comment"># 每次训练的数据多少</span></span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># 每隔多少轮显示一次训练结果</span></span><br><span class="line">examples_to_show = <span class="number">10</span>  <span class="comment"># 从测试集中中选取10张图片去验证自动编码器的结果</span></span><br><span class="line">n_input = <span class="number">784</span>  <span class="comment"># 输入数据的特征值个数，MNIST data input (img shape: 28 * 28)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>压缩环节：我们要把这个<code>Features</code>不断压缩，经过第一个隐藏层压缩至<code>256</code>个<code>Features</code>，再经过第二个隐藏层压缩至<code>128</code>个。</li>
<li>解压环节：我们将<code>128</code>个<code>Features</code>还原至<code>256</code>个，再经过一步还原至<code>784</code>个。</li>
<li>对比环节：比较原始数据与还原后的拥有<code>784</code>个<code>Features</code>的数据进行<code>cost</code>的对比，根据<code>cost</code>来提升<code>Autoencoder</code>的准确率。</li>
</ul>
<p>下面是两个隐藏层的<code>weights</code>和<code>biases</code>定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">n_hidden_1 = <span class="number">256</span>  <span class="comment"># (第一个隐藏层神经元的个数，也是特征值的个数)1st layer num features</span></span><br><span class="line">n_hidden_2 = <span class="number">128</span>  <span class="comment"># (第二个隐藏层神经元的个数，也是特征值的个数)2nd layer num features</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 然后定义输入数据，这里是无监督学习，所以只要输入图片数据，不需要标记数据</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 初始化每一层的权重和偏置</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'encoder_h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_h1'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_input])),</span><br><span class="line">&#125;</span><br><span class="line">​</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'encoder_b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_b2'</span>: tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;下面来定义<code>Encoder</code>和<code>Decoder</code>，使用的<code>Activation function</code>是<code>sigmoid</code>，压缩之后的值应该在<code>[0, 1]</code>这个范围内。在<code>decoder</code>过程中，通常使用对应于<code>encoder</code>的<code>Activation function</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span>  <span class="comment"># (定义压缩函数)Building the encoder</span></span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'encoder_h1'</span>]), biases[<span class="string">'encoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'encoder_h2'</span>]), biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span>  <span class="comment"># (定义解压缩函数)Building the decoder</span></span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'decoder_h1'</span>]), biases[<span class="string">'decoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'decoder_h2'</span>]), biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br></pre></td></tr></table></figure>
<p>来实现<code>Encoder</code>和<code>Decoder</code>输出的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (构建模型)Construct model</span></span><br><span class="line">encoder_op = encoder(X)  <span class="comment"># 128 Features</span></span><br><span class="line">decoder_op = decoder(encoder_op)  <span class="comment"># 784 Features</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># (得出预测值)Prediction</span></span><br><span class="line">y_pred = decoder_op  <span class="comment"># After</span></span><br><span class="line"><span class="comment"># (得出真实值，即输入值)Targets (Labels) are the input data</span></span><br><span class="line">y_true = X  <span class="comment"># Before</span></span><br></pre></td></tr></table></figure>
<p>再通过非监督学习进行对照，即对<code>原始的有784个Features的数据集</code>和<code>通过Prediction得出的有784个Features的数据集</code>进行最小二乘法的计算，并且使<code>cost</code>最小化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (定义损失)Define loss and optimizer, minimize the squared error</span></span><br><span class="line">cost = tf.reduce_mean(tf.pow(y_true - y_pred, <span class="number">2</span>))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure>
<p>最后通过<code>Matplotlib</code>的<code>pyplot</code>模块将结果显示出来，注意在输出时<code>MNIST</code>数据集经过压缩之后<code>x</code>的最大值是<code>1</code>，而非<code>255</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  <span class="comment"># Launch the graph</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):  <span class="comment"># Training cycle</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):  <span class="comment"># Loop over all batches</span></span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  <span class="comment"># max(x) = 1, min(x) = 0</span></span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_xs&#125;)</span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:  <span class="comment"># Display logs per epoch step</span></span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost ="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(c))</span><br><span class="line">​</span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    <span class="comment"># Applying encode and decode over test set</span></span><br><span class="line">    encode_decode = sess.run(y_pred, feed_dict=&#123;X: mnist.test.images[:examples_to_show]&#125;)</span><br><span class="line">    <span class="comment"># Compare original images with their reconstructions</span></span><br><span class="line">    f, a = plt.subplots(<span class="number">2</span>, <span class="number">10</span>, figsize=(<span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(examples_to_show):</span><br><span class="line">        a[<span class="number">0</span>][i].imshow(np.reshape(mnist.test.images[i], (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">        a[<span class="number">1</span>][i].imshow(np.reshape(encode_decode[i], (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>通过<code>20</code>个<code>Epoch</code>的训练，结果如下所示，上面一行是真实数据，下面一行是经过<code>encoder</code>和<code>decoder</code>之后的数据。如果继续进行训练，效果会更好。</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/4.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/全卷积网络FCN/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/全卷积网络FCN/" itemprop="url">全卷积网络FCN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T12:21:42+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;神经网络大神<code>Jonathan Long</code>发表了论文<code>Fully Convolutional Networks for Semantic Segmentation</code>，使得图像语义分割成为了现实。<br>&emsp;&emsp;通常<code>CNN</code>网络在卷积层之后会接上若干个全连接层，将卷积层产生的特征图(<code>feature map</code>)映射成一个固定长度的特征向量。以<code>AlexNet</code>为代表的经典<code>CNN</code>结构适合于图像级的分类和回归任务，因为它们最后都期望得到整个输入图像的一个数值描述(概率)，比如<code>AlexNet</code>的<code>ImageNet</code>模型输出一个<code>1000</code>维的向量表示输入图像属于每一类的概率(<code>softmax</code>归一化)。<br>&emsp;&emsp;对于下图中的猫，使用<code>AlexNet</code>网络将得到一个长为<code>1000</code>的输出向量，表示输入图像属于每一类的概率，其中在<code>tabby cat</code>这一类统计概率最高：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/1.jpg" height="165" width="563"></p>
<p>&emsp;&emsp;<code>FCN</code>对图像进行像素级的分类，从而解决了语义级别的图像分割(<code>semantic segmentation</code>)问题。与经典<code>CNN</code>在卷积层之后使用全连接层得到固定长度的特征向量进行分类(<code>全连接层 + softmax输出</code>)不同，<code>FCN</code>可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的<code>feature map</code>进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测，同时保留了原始输入图像中的空间信息，最后在上采样的特征图上进行逐像素分类。最终逐个像素计算<code>softmax</code>分类的损失，相当于每一个像素对应一个训练样本。下图是用于语义分割所采用的全卷积网络(<code>FCN</code>)的结构示意图：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/2.jpg" height="214" width="403"></p>
<p>&emsp;&emsp;简单的来说，<code>FCN</code>与<code>CNN</code>的区别在于把<code>CNN</code>最后的全连接层换成卷积层，输出的是一张已经<code>Label</code>好的图片：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/3.png" height="267" width="465"></p>
<p>&emsp;&emsp;实际上，<code>CNN</code>的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征：较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。这些抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高。下图<code>CNN</code>分类网络的示意图：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/4.png" height="151" width="578"></p>
<p>&emsp;&emsp;这些抽象的特征对分类很有帮助，可以很好地判断出一幅图像中包含什么类别的物体。但是因为丢失了一些物体的细节，不能很好地给出物体的具体轮廓、指出每个像素具体属于哪个物体，因此做到精确的分割就很有难度。<br>&emsp;&emsp;传统的基于<code>CNN</code>的分割方法：为了对一个像素分类，使用该像素周围的一个图像块作为<code>CNN</code>的输入用于训练和预测。这种方法有几个缺点：一是存储开销很大，例如对每个像素使用的图像块的大小为<code>15 * 15</code>，然后不断滑动窗口，每次滑动的窗口给<code>CNN</code>进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升；二是计算效率低下，相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复；三是像素块大小的限制了感知区域的大小，通常像素块的大小比整幅图像的大小要小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。<br>&emsp;&emsp;而全卷积网络则是从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。全连接层和卷积层之间唯一的不同，就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在这两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。<br>&emsp;&emsp;相反，任何全连接层都可以被转化为卷积层。比如，一个<code>K = 4096</code>的全连接层，输入数据体的尺寸是<code>7 * 7 * 512</code>，这个全连接层可以被等效地看做一个<code>F = 7</code>、<code>P = 0</code>、<code>S = 1</code>以及<code>K = 4096</code>的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成<code>1 * 1 * 4096</code>，这个结果就和使用初始的那个全连接层一样了。<br>&emsp;&emsp;在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是<code>224 * 224 * 3</code>的图像，一系列的卷积层和下采样层将图像数据变为尺寸为<code>7 * 7 * 512</code>的激活数据体。<code>AlexNet</code>使用了两个尺寸为<code>4096</code>的全连接层，最后一个有<code>1000</code>个神经元的全连接层用于计算分类评分。我们可以将这<code>3</code>个全连接层中的任意一个转化为卷积层：</p>
<ul>
<li>针对第一个连接区域是<code>7 * 7 * 512</code>的全连接层，令其滤波器尺寸为<code>F = 7</code>，这样输出数据体就为<code>1 * 1 * 4096</code>了。</li>
<li>针对第二个全连接层，令其滤波器尺寸为<code>F = 1</code>，这样输出数据体为<code>1 * 1 * 4096</code>。</li>
<li>对最后一个全连接层也做类似的处理，令其<code>F = 1</code>，最终输出为<code>1 * 1 * 1000</code>。</li>
</ul>
<p>&emsp;&emsp;实际操作中，每次这样的变换都需要把全连接层的权重<code>W</code>重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动，得到多个输出(可以理解为一个<code>label map</code>)，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。<br>&emsp;&emsp;举个例子，如果我们想让<code>224 * 224</code>尺寸的浮窗，以步长为<code>32</code>在<code>384 * 384</code>的图片上滑动，把每个经停的位置都带入卷积网络，最后得到<code>6 * 6</code>个位置的类别得分。将上述的过程把全连接层转换成卷积层则会更简便，如果<code>224 * 224</code>的输入图片经过卷积层和下采样层之后得到了<code>7 * 7 * 512</code>的数组，那么<code>384 * 384</code>的大图片直接经过同样的卷积层和下采样层之后会得到<code>12 * 12 * 512</code>的数组。然后再经过上面由<code>3</code>个全连接层转化得到的<code>3</code>个卷积层，最终得到<code>6 * 6 * 1000</code>的输出(<code>(12 - 7) / 1 + 1 = 6</code>)，这个结果正是浮窗在原图经停的<code>6 * 6</code>个位置的得分！面对<code>384 * 384</code>的图像，让(含全连接层)的初始卷积神经网络以<code>32</code>像素的步长独立对图像中的<code>224 * 224</code>块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。<br>&emsp;&emsp;如下图所示，<code>FCN</code>将传统<code>CNN</code>中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层。<code>FCN</code>将这<code>3</code>层表示为卷积层，卷积核的大小<code>(通道数, 宽, 高)</code>分别为<code>(4096, 1, 1)</code>、<code>(4096, 1, 1)</code>和<code>(1000, 1, 1)</code>。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前<code>CNN</code>已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。因此<code>FCN</code>网络中所有的层都是卷积层，故称为全卷积网络。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/5.png" height="201" width="447"></p>
<p>&emsp;&emsp;下图是一个全卷积层，与上图不一样的是图像对应的大小下标，<code>CNN</code>中输入的图像大小是同意固定<code>resize</code>成<code>227 * 227</code>大小的图像，第一层<code>pooling</code>后为<code>55 * 55</code>，第二层<code>pooling</code>后图像大小为<code>27 * 27</code>，第五层<code>pooling</code>后的图像大小为<code>13 * 13</code>。而<code>FCN</code>输入的图像是<code>H * W</code>大小，第一层<code>pooling</code>后变为原图大小的<code>1/4</code>，第二层变为原图大小的<code>1/8</code>，第五层变为原图大小的<code>1/16</code>，第八层变为原图大小的<code>1/32</code>(实际上代码当中第一层是<code>1/2</code>，以此类推)。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/6.png" height="165" width="590"></p>
<p>&emsp;&emsp;经过多次卷积和<code>pooling</code>以后，得到的图像越来越小，分辨率越来越低。其中图像到<code>H/32 * W/32</code>的时候图片是最小的一层时，所产生图叫做<code>heatmap</code>热图，热图就是我们最重要的高维特征图。得到高维特征的<code>heatmap</code>之后就是最重要的一步，也是最后的一步，对原图像进行<code>upsampling</code>，把图像逐渐放大，直到原图像的大小。最后的输出是<code>1000</code>张<code>heatmap</code>经过<code>upsampling</code>变为原图大小的图片：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/7.png"></p>
<p>&emsp;&emsp;为了对每个像素进行分类预测，并<code>label</code>成最后已经进行语义分割的图像，这里有一个小<code>trick</code>，就是最后通过逐个像素地求其在<code>1000</code>张图像该像素位置的最大数值描述(概率)作为该像素的分类。因此产生了一张已经分类好的图片，如下图右侧有狗狗和猫猫的图：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/8.png" height="206" width="456"></p>
<p>&emsp;&emsp;相较于使用被转化前的原始卷积神经网络对所有<code>36</code>个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为<code>36</code>次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。例如通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。<br>&emsp;&emsp;如果我们想用步长小于<code>32</code>的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为<code>16</code>的浮窗，那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移<code>16</code>个像素，然后把这些平移之后的图分别带入卷积网络。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/9.jpg" height="220" width="197"></p>
<p>&emsp;&emsp;如下图所示，当图片在网络中经过处理后变成越小的图片，其特征也越明显，就像图像中颜色所示。当然最后一层的图片不再是一个<code>1</code>个像素的图片，而是原图像<code>H/32 * W/32</code>大小的图，这里为了简化而画成一个像素而已：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/10.png" height="224" width="450"></p>
<p>&emsp;&emsp;如下图所示，对原图像进行卷积<code>conv1</code>、<code>pool1</code>后原图像缩小为<code>1/2</code>；之后对图像进行第二次<code>conv2</code>、<code>pool2</code>后图像缩小为<code>1/4</code>；接着继续对图像进行第三次卷积操作<code>conv3</code>、<code>pool3</code>缩小为原图像的<code>1/8</code>，此时保留<code>pool3</code>的<code>featureMap</code>；接着继续对图像进行第四次卷积操作<code>conv4</code>、<code>pool4</code>，缩小为原图像的<code>1/16</code>，保留<code>pool4</code>的<code>featureMap</code>；最后对图像进行第五次卷积操作<code>conv5</code>、<code>pool5</code>，缩小为原图像的<code>1/32</code>，然后把原来<code>CNN</code>操作中的全连接变成卷积操作<code>conv6</code>、<code>conv7</code>，图像的<code>featureMap</code>数量改变但是图像大小依然为原图的<code>1/32</code>，此时图像不再叫<code>featureMap</code>而是叫<code>heatMap</code>。<br>&emsp;&emsp;现在我们有<code>1/32</code>尺寸的<code>heatMap</code>，<code>1/16</code>尺寸的<code>featureMap</code>和<code>1/8</code>尺寸的<code>featureMap</code>，<code>1/32</code>尺寸的<code>heatMap</code>进行<code>upsampling</code>操作之后，因为这样的操作还原的图片仅仅是<code>conv5</code>中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把<code>conv4</code>中的卷积核对上一次<code>upsampling</code>之后的图进行反卷积补充细节(相当于一个差值过程)，最后把<code>conv3</code>中的卷积核对刚才<code>upsampling</code>之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/11.png" height="246" width="454"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/TensorFlow处理图片/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/TensorFlow处理图片/" itemprop="url">TensorFlow处理图片</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T08:45:16+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="TensorFlow与OpenCV"><a href="#TensorFlow与OpenCV" class="headerlink" title="TensorFlow与OpenCV"></a>TensorFlow与OpenCV</h3><p>&emsp;&emsp;<code>OpenCV</code>读入图片，使用<code>tf.Variable</code>初始化为<code>tensor</code>，加载到<code>tensorflow</code>对图片进行转置操作，然后<code>opencv</code>显示转置后的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">filename = <span class="string">"index.jpg"</span></span><br><span class="line">image = cv2.imread(filename, <span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'image'</span>, image)</span><br><span class="line">​</span><br><span class="line">x = tf.Variable(image, name=<span class="string">'x'</span>)</span><br><span class="line">​</span><br><span class="line">model = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    x = tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">    session.run(model)</span><br><span class="line">    result = session.run(x)</span><br><span class="line">​</span><br><span class="line">cv2.imshow(<span class="string">'result'</span>, result)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/1.png" height="220" width="419"></p>
<p>&emsp;&emsp;<code>OpenCV</code>读入图片，使用<code>tf.placeholder</code>符号变量加载到<code>tensorflow</code>里，然后<code>tensorflow</code>对图片进行剪切操作，最后<code>opencv</code>显示处理后的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">filename = <span class="string">"index.jpg"</span></span><br><span class="line">raw_image_data = cv2.imread(filename)</span><br><span class="line">cv2.imshow(<span class="string">'image'</span>, raw_image_data)</span><br><span class="line">image = tf.placeholder(<span class="string">"uint8"</span>, [<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">3</span>])</span><br><span class="line">slice = tf.slice(image, [<span class="number">100</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">200</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(slice, feed_dict=&#123;image: raw_image_data&#125;)</span><br><span class="line">    print(result.shape)</span><br><span class="line">​</span><br><span class="line">cv2.imshow(<span class="string">'result'</span>, result)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/2.png" height="218" width="418"></p>
<p>&emsp;&emsp;<code>TensorFlow</code>解码<code>png</code>格式文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">file_name = <span class="string">"call.1.png"</span></span><br><span class="line">file_contents = tf.read_file(file_name)</span><br><span class="line">​</span><br><span class="line">image = tf.image.decode_png(file_contents)  <span class="comment"># 解码png格式</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    img = sess.run(image)</span><br><span class="line">    print(img.shape)</span><br><span class="line">    cv2.imshow(<span class="string">"show"</span>, img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/3.png" height="239" width="359"></p>
<hr>
<h3 id="image类图像操作Ops之编码"><a href="#image类图像操作Ops之编码" class="headerlink" title="image类图像操作Ops之编码"></a>image类图像操作Ops之编码</h3><p>&emsp;&emsp;在源码路径<code>tensorflow/tensorflow/python/ops</code>下存在很多使用<code>python</code>实现的<code>ops</code>，其中<code>tensorflow</code>的<code>image</code>类操作就是在该目录下定义实现的，对应的文件是<code>image_ops_impl.py</code>。</p>
<h4 id="编码-decode"><a href="#编码-decode" class="headerlink" title="编码(decode_*)"></a>编码(decode_*)</h4><p>&emsp;&emsp;我们读进一张图片后，往往得到的是字符串张量，如下图所示，并非是常见的张量形式。我们是没办法直接使用图片来训练的，于是<code>decode_*</code>的作用就是要把字符串张量转为特定格式的张量。</p>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/4.png" height="156" width="400"></p>
<h4 id="解码-encode"><a href="#解码-encode" class="headerlink" title="解码(encode_*)"></a>解码(encode_*)</h4><p>&emsp;&emsp;解码操作可以把一个张量编码成为某种图片格式，得到的是一个字符串张量，它是某种图片格式数据。</p>
<h4 id="tf-image-decode-gif"><a href="#tf-image-decode-gif" class="headerlink" title="tf.image.decode_gif"></a>tf.image.decode_gif</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_gif(contents, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：<code>gif</code>图片字符串<code>0-D</code>张量。</li>
<li><code>name</code>：指定的<code>OP</code>操作名字(可选项)。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>uint8</code>的<code>4-D</code>张量，其<code>shape</code>为<code>[num_frames, height, width, 3]</code>。</p>
<h4 id="tf-image-decode-jpeg"><a href="#tf-image-decode-jpeg" class="headerlink" title="tf.image.decode_jpeg"></a>tf.image.decode_jpeg</h4><p>&emsp;&emsp;<code>tf.image.decode_jpeg</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_jpeg(</span><br><span class="line">    contents, channels=<span class="number">0</span>, ratio=<span class="number">1</span>, fancy_upscaling=<span class="keyword">True</span>,</span><br><span class="line">    try_recover_truncated=<span class="keyword">False</span>, acceptable_fraction=<span class="number">1</span>,</span><br><span class="line">    dct_method=<span class="string">""</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：<code>jpeg</code>图片字符串<code>0-D</code>张量。</li>
<li><code>channels</code>：图片通道数，指定<code>decode</code>生成张量的<code>RGB</code>通道数。</li>
<li><code>ratio</code>：缩放系数，默认设定为<code>1</code>，即不做缩放，只能为<code>int</code>。</li>
<li><code>name</code>：指定的<code>OP</code>操作名字(可选项)。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>uint8</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</p>
<h4 id="tf-image-encode-jpeg"><a href="#tf-image-encode-jpeg" class="headerlink" title="tf.image.encode_jpeg"></a>tf.image.encode_jpeg</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.image.encode_jpeg(</span><br><span class="line">    image, format=<span class="string">""</span>, quality=<span class="number">95</span>, progressive=<span class="keyword">False</span>, optimize_size=<span class="keyword">False</span>, chroma_downsampling=<span class="keyword">True</span>,</span><br><span class="line">    density_unit=<span class="string">"in"</span>, x_density=<span class="number">300</span>, y_density=<span class="number">300</span>, xmp_metadata=<span class="string">""</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>image</code>：<code>dtype</code>为<code>uint8</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</li>
<li><code>format</code>：编码格式，可选择<code>grayscale</code>和<code>rgb</code>，默认为空(根据传入<code>image</code>的格式来决定输出格式)。</li>
<li><code>quality</code>：压缩品质，范围为<code>0</code>至<code>100</code>，值越高其压缩得到的图片品质越好。</li>
<li><code>chroma_downsampling</code>：色度抽样，这里默认压缩色度。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>string</code>的<code>0-D</code>张量，且其格式为<code>jpeg</code>。<br>&emsp;&emsp;下面是一个<code>encode_jpeg</code>设定<code>format</code>为<code>grayscale</code>，且缩放系数为<code>2</code>，然后再用<code>opencv</code>来<code>show</code>出的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">img_name = <span class="string">"girl.jpg"</span></span><br><span class="line">image_jpg = tf.read_file(img_name)</span><br><span class="line">imgage_decode_jpeg = tf.image.decode_jpeg(image_jpg, channels=<span class="number">1</span>, ratio=<span class="number">2</span>, name=<span class="string">"decode_jpeg_1"</span>)</span><br><span class="line">print(imgage_decode_jpeg.shape)</span><br><span class="line">print(imgage_decode_jpeg.dtype)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">imgage_encode_jpeg = tf.image.encode_jpeg(sess.run(imgage_decode_jpeg), format=<span class="string">'grayscale'</span>, name=<span class="string">"encode_jpeg"</span>)</span><br><span class="line">print(imgage_encode_jpeg.shape)</span><br><span class="line">print(imgage_encode_jpeg.dtype)</span><br><span class="line">img = tf.image.decode_jpeg(sess.run(imgage_encode_jpeg), ratio=<span class="number">2</span>, name=<span class="string">"decode_jpeg_2"</span>)</span><br><span class="line">​</span><br><span class="line">img = cv2.imshow(<span class="string">"img"</span>, sess.run(img))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf-image-decode-png"><a href="#tf-image-decode-png" class="headerlink" title="tf.image.decode_png"></a>tf.image.decode_png</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_png(contents, channels=<span class="number">0</span>, dtype=_dtypes.uint8, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：<code>dtype</code>为<code>string</code>的<code>0-D</code>张量，图像为<code>png</code>格式。</li>
<li><code>channels</code>：<code>decode</code>的输出通道数，可选<code>0</code>至<code>4</code>，默认根据所传图像来决定。</li>
</ul>
<p>该函数返回所传参数<code>dtype</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</p>
<h4 id="tf-image-encode-png"><a href="#tf-image-encode-png" class="headerlink" title="tf.image.encode_png"></a>tf.image.encode_png</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.encode_png(image, compression=<span class="number">-1</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>image</code>：<code>dtype</code>为<code>uint8</code>或<code>uint16</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</li>
<li><code>compression</code>：压缩级数，数值越高得到的图像质量越差。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>string</code>的<code>0-D</code>张量，且其格式为<code>png</code>。<br>&emsp;&emsp;下面的代码将图片从<code>jpg</code>格式转为<code>png</code>格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">img_name = <span class="string">"girl.jpg"</span></span><br><span class="line">image_jpg = tf.read_file(img_name)</span><br><span class="line">imgage_decode_jpeg = tf.image.decode_jpeg(image_jpg, channels=<span class="number">3</span>, ratio=<span class="number">2</span>, name=<span class="string">"decode_jpeg_1"</span>)</span><br><span class="line">print(imgage_decode_jpeg.shape)</span><br><span class="line">print(imgage_decode_jpeg.dtype)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">imgage_encode_png = tf.image.encode_png(sess.run(imgage_decode_jpeg), name=<span class="string">"encode_png"</span>)</span><br><span class="line">print(imgage_encode_png.shape)</span><br><span class="line">print(imgage_encode_png.dtype)</span><br><span class="line">​</span><br><span class="line">img = tf.image.decode_png(sess.run(imgage_encode_png), name=<span class="string">"decode_png"</span>)</span><br><span class="line">​</span><br><span class="line">img = cv2.imshow(<span class="string">"img"</span>, sess.run(img))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf-image-decode-image"><a href="#tf-image-decode-image" class="headerlink" title="tf.image.decode_image"></a>tf.image.decode_image</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_image(contents, channels=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：可传入任何格式的图像，包括<code>jpeg</code>、<code>png</code>、<code>gif</code>及<code>bmp</code>，<code>dtype</code>为<code>string</code>的<code>0-D</code>张量。</li>
<li><code>channels</code>：<code>decode</code>的输出通道数，可选<code>0</code>至<code>4</code>，默认根据所传图像来决定。</li>
</ul>
<p>如果图片的格式是<code>BMP</code>、<code>JPEG</code>或<code>PNG</code>时，返回<code>[height, width, num_channels]</code>，而<code>gif</code>返回<code>[num_frames, height, width, 3]</code>，且它们的<code>dtype</code>都为<code>uint8</code>，这会导致<code>dtype</code>为<code>uint16</code>的<code>png</code>格式图像失真。这是一个兼容<code>gif</code>、<code>jpeg</code>、<code>png</code>及<code>bmp</code>格式的<code>decode</code>接口，它将会根据所传的图像自动返回特定格式的张量。<br>&emsp;&emsp;以下是传入<code>jpg</code>格式图像到<code>decode_image</code>，然后再使用<code>opencv</code>来显示<code>jpeg</code>编解码器生成的图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">img_name = <span class="string">"apple.jpg"</span></span><br><span class="line">image_jpg = tf.read_file(img_name)</span><br><span class="line">imgage_decode = tf.image.decode_image(image_jpg, name=<span class="string">"decode_image"</span>)</span><br><span class="line">print(imgage_decode.shape)</span><br><span class="line">print(imgage_decode.dtype)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">imgage_encode_jpeg = tf.image.encode_jpeg(</span><br><span class="line">                        sess.run(imgage_decode), quality=<span class="number">100</span>, progressive=<span class="keyword">True</span>,</span><br><span class="line">                        chroma_downsampling=<span class="keyword">False</span>, optimize_size=<span class="keyword">True</span>, name=<span class="string">"encode_jpeg"</span>)</span><br><span class="line">print(imgage_encode_jpeg.shape)</span><br><span class="line">print(imgage_encode_jpeg.dtype)</span><br><span class="line">​</span><br><span class="line">img = tf.image.decode_png(sess.run(imgage_encode_jpeg), name=<span class="string">"decode_jpeg"</span>)</span><br><span class="line">​</span><br><span class="line">img = cv2.imshow(<span class="string">"img"</span>, sess.run(img))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="image类图像操作Ops之大小重调和图像标注框"><a href="#image类图像操作Ops之大小重调和图像标注框" class="headerlink" title="image类图像操作Ops之大小重调和图像标注框"></a>image类图像操作Ops之大小重调和图像标注框</h3><h4 id="resize-images"><a href="#resize-images" class="headerlink" title="resize_images"></a>resize_images</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resize_images(images, size, method=ResizeMethod.BILINEAR, align_corners=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>images</code>：<code>shape</code>为<code>[batch, height, width, channels]</code>的<code>4-D</code>图像张量或者<code>shape</code>为<code>[height, width, channels]</code>的<code>3-D</code>图像张量，如果传入图像张量不兼容，所制定规则会报错。</li>
<li><code>size</code>：一个<code>dtype</code>为<code>int32</code>拥有两个元素的<code>1-D</code>张量，格式为<code>[new_height, new_width]</code>。</li>
<li><code>method</code>：<code>resize</code>使用的方法，有四种方式：</li>
</ul>
<ol>
<li><code>ResizeMethod.BILINEAR</code>：双线性内插，其核心思想是在两个方向分别进行一次线性插值。</li>
<li><code>ResizeMethod.NEAREST_NEIGHBOR</code>：最近邻插值法，将变换后的图像中的原像素点最邻近像素的灰度值赋给原像素点的方法，返回图像张量<code>dtype</code>与所传入的相同。</li>
<li><code>ResizeMethod.BICUBIC</code>：双三次插值，双三次插值是一种更加复杂的插值方式，它能创造出比双线性插值更平滑的图像边缘。</li>
<li><code>ResizeMethod.AREA</code>：基于区域的图像插值算法，首先将原始低分辨率图像分割成不同区域，然后将插值点映射到低分辨率图像，判断其所属区域，最后根据插值点的邻域像素设计不同的插值公式，计算插值点的值。</li>
</ol>
<ul>
<li><code>align_corners</code>：精确对准输入输出图像的四个角，默认为<code>false</code>，不精确对准。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>float</code>的<code>3-D</code>或<code>4-D</code>图像张量，其<code>shape</code>分别为<code>[batch, new_height, new_width, channels]</code>和<code>[new_height, new_width, channels]</code>。<br>&emsp;&emsp;而其余四个接口则是具体的不同实现图像缩放处理的方法，它们的参数都形如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(images, size, align_corners=<span class="keyword">False</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>第一个参数要求其<code>shape</code>一定是形如<code>[batch, height, width, channels]</code>的<code>4-D</code>格式，中间两个参数如<code>resize_image</code>所解释，后一个<code>name</code>是操作的名称(可选项)。<br>&emsp;&emsp;下面是用<code>matplotlib</code>来显示四种不同缩放处理方法后的图像对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">img_name = [<span class="string">"apple.jpg"</span>]</span><br><span class="line">filename_queue = tf.train.string_input_producer(img_name)</span><br><span class="line">img_reader = tf.WholeFileReader()</span><br><span class="line">_, image_jpg = img_reader.read(filename_queue)</span><br><span class="line">​</span><br><span class="line">image_decode_jpeg = tf.image.decode_png(image_jpg)</span><br><span class="line">image_decode_jpeg = tf.image.convert_image_dtype(image_decode_jpeg, dtype=tf.float32)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">​</span><br><span class="line">image_bilinear = tf.image.resize_images(image_decode_jpeg, size=[<span class="number">1200</span>, <span class="number">1920</span>], method=tf.image.ResizeMethod.BILINEAR)</span><br><span class="line">image_nearest_neighbor = tf.image.resize_images(</span><br><span class="line">                            image_decode_jpeg, size=[<span class="number">728</span>, <span class="number">1280</span>],</span><br><span class="line">                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</span><br><span class="line">image_bicubic = tf.image.resize_images(image_decode_jpeg, size=[<span class="number">720</span>, <span class="number">1440</span>], method=tf.image.ResizeMethod.BICUBIC)</span><br><span class="line">image_area = tf.image.resize_images(image_decode_jpeg, size=[<span class="number">1080</span>, <span class="number">1920</span>], method=tf.image.ResizeMethod.AREA)</span><br><span class="line">​</span><br><span class="line">print(image_bicubic.shape)</span><br><span class="line">print(image_bicubic.dtype)</span><br><span class="line">​</span><br><span class="line">plt.figure()</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.imshow(sess.run(image_bilinear))</span><br><span class="line">plt.title(<span class="string">"bilinear interpolation"</span>)</span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.imshow(sess.run(image_nearest_neighbor))</span><br><span class="line">plt.title(<span class="string">"nearest neighbor interpolation"</span>)</span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.imshow(sess.run(tf.image.convert_image_dtype(image_bicubic, dtype=tf.uint8)))</span><br><span class="line">plt.title(<span class="string">"bicubic interpolation"</span>)</span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.imshow(sess.run(tf.image.convert_image_dtype(image_area, dtype=tf.float32)))</span><br><span class="line">plt.title(<span class="string">"area interpolation"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="tf-image-resize-bilinear"><a href="#tf-image-resize-bilinear" class="headerlink" title="tf.image.resize_bilinear"></a>tf.image.resize_bilinear</h4><p>&emsp;&emsp;该函数使用双线性插值调整<code>images</code>为<code>size</code>，输入图像可以是不同的类型，但输出图像总是浮点型的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.resize_bilinear(images, size, align_corners=<span class="keyword">False</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>images</code>：一个<code>Tensor</code>，必须是下列类型之一：<code>int8</code>、<code>uint8</code>、<code>int16</code>、<code>uint16</code>、<code>int32</code>、<code>int64</code>、<code>float16</code>、<code>float32</code>和<code>float64</code>，<code>4</code>维的并且具有形状<code>[batch, height, width, channels]</code>。</li>
<li><code>size</code>：<code>2</code>个元素<code>(new_height, new_width)</code>的<code>1</code>维<code>int32</code>张量，用来表示图像的新大小。</li>
<li><code>align_corners</code>：可选的<code>bool</code>。如果为<code>True</code>，则输入和输出张量的<code>4</code>个角像素的中心对齐，并且保留角落像素处的值。</li>
<li><code>name</code>：操作的名称(可选项)。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/词袋模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/词袋模型/" itemprop="url">词袋模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T18:12:34+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>BoW</code>(<code>Bag of Words</code>)词袋模型最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子(因为里面装的都是词汇，所以称为词袋)，然后看这个袋子里装的都是些什么词汇，将其分类。如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。举个例子，有如下两个文档：</p>
<ul>
<li>文档一：<code>Bob likes to play basketball, Jim likes too.</code></li>
<li>文档二：<code>Bob also likes to play football games.</code></li>
</ul>
<p>基于这两个文本文档，构造一个词典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dictionary = &#123;</span><br><span class="line">    <span class="number">1</span>: <span class="string">"Bob"</span>, <span class="number">2</span>: <span class="string">"like"</span>, <span class="number">3</span>: <span class="string">"to"</span>, <span class="number">4</span>: <span class="string">"play"</span>, <span class="number">5</span>: <span class="string">"basketball"</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">"also"</span>, <span class="number">7</span>: <span class="string">"football"</span>, <span class="number">8</span>: <span class="string">"games"</span>, <span class="number">9</span>: <span class="string">"Jim"</span>, <span class="number">10</span>: <span class="string">"too"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个词典一共包含<code>10</code>个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个<code>10</code>维向量表示(用整数数字<code>0</code>至<code>n</code>(<code>n</code>为正整数)表示某个单词在文档中出现的次数)：</p>
<ul>
<li>文档一：<code>[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]</code></li>
<li>文档二：<code>[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]</code></li>
</ul>
<p>向量中每个元素表示词典中相关元素在文档中出现的次数。不过，在构造文档向量的过程中可以看到，我们并没有表达单词在原来句子中出现的次序(这是词袋模型的缺点之一，不过瑕不掩瑜，甚至在此处无关紧要)。<br>&emsp;&emsp;<code>SIFT</code>特征虽然也能描述一幅图像，但是每个<code>SIFT</code>矢量都是<code>128</code>维的，而且一幅图像通常都包含成百上千个<code>SIFT</code>矢量。在进行相似度计算时，这个计算量是非常大的，通行的做法是用聚类算法对这些矢量数据进行聚类，然后用聚类中的一个簇代表<code>BoW</code>中的一个视觉词，将同一幅图像的<code>SIFT</code>矢量映射到视觉词序列生成码本，这样每一幅图像只用一个码本矢量来描述，这样计算相似度时效率就大大提高了。<br>&emsp;&emsp;现在想象在一个巨大的文档集合<code>D</code>，里面一共有<code>M</code>个文档，而文档里面的所有单词提取出来后，一起构成一个包含<code>N</code>个单词的词典。利用<code>BoW</code>模型，每个文档都可以被表示成为一个<code>N</code>维向量，计算机非常擅长于处理数值向量，这样就可以利用计算机来完成海量文档的分类过程。为了表示一幅图像，我们可以将图像看作文档，即若干个<code>视觉词汇</code>的集合，同样的，视觉词汇相互之间没有顺序。</p>
<p><img src="/2019/02/27/机器学习/词袋模型/1.png" height="265" width="355"></p>
<p>&emsp;&emsp;由于图像中的词汇不像文本文档中的那样是现成的，我们需要先从图像中提取出相互独立的视觉词汇。这通常需要经过三个步骤：特征检测，特征表示和单词本的生成。从图像中提取出相互独立的视觉词汇：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/2.png" height="139" width="542"></p>
<p>&emsp;&emsp;通过观察会发现，同一类目标的不同实例之间虽然存在差异，但我们仍然可以找到它们之间的一些共同的地方。比如说人脸，虽然说不同人的脸差别比较大，但眼睛、嘴、鼻子等一些比较细小的部位却观察不到太大差别，我们可以把这些不同实例之间共同的部位提取出来，作为识别这一类目标的视觉词汇。而<code>SIFT</code>算法是提取图像中局部不变特征的应用最广泛的算法，因此可以用<code>SIFT</code>算法从图像中提取不变特征点，作为视觉词汇，并构造单词表，用单词表中的单词表示一幅图像。<br>&emsp;&emsp;接下来通过上述图像展示如何通过<code>BoW</code>模型，将图像表示成数值向量。现在有三个目标类，分别是人脸、自行车和吉他。<code>BoW</code>模型的第一步是利用<code>SIFT</code>算法，从每类图像中提取视觉词汇，将所有的视觉词汇集合在一起：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/3.png"></p>
<p>&emsp;&emsp;第二步是利用<code>K-Means</code>算法构造单词表。<code>K-Means</code>算法是一种基于样本间相似性度量的间接聚类方法，此算法以<code>K</code>为参数，把<code>N</code>个对象分为<code>K</code>个簇，以使簇内具有较高的相似度，而簇间相似度较低。<code>SIFT</code>提取的视觉词汇向量之间根据距离的远近，可以利用<code>K-Means</code>算法将词义相近的词汇合并，作为单词表中的基础词汇。假定我们将<code>K</code>设为<code>4</code>，那么单词表的构造过程如下：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/4.png" height="252" width="401"></p>
<p>&emsp;&emsp;第三步是利用单词表的中词汇表示图像。利用<code>SIFT</code>算法，可以从每幅图像中提取很多个特征点，这些特征点都可以用单词表中的单词近似代替。通过统计单词表中每个单词在图像中出现的次数，可以将图像表示成为一个<code>K = 4</code>维数值向量：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/5.png" height="146" width="548"></p>
<p>&emsp;&emsp;我们从人脸、自行车和吉他三个目标类图像中提取出的不同视觉词汇，而构造的词汇表中，会把词义相近的视觉词汇合并为同一类。经过合并，词汇表中只包含了四个视觉单词，分别按索引值标记为<code>1</code>、<code>2</code>、<code>3</code>和<code>4</code>。通过观察可以看到，它们分别属于自行车、人脸、吉他、人脸类。统计这些词汇在不同目标类中出现的次数可以得到每幅图像的直方图表示(我们假定存在误差，实际情况亦不外如此)：</p>
<ul>
<li>人脸：<code>[3, 30, 3, 20]</code></li>
<li>自行车：<code>[20, 3, 3, 2]</code></li>
<li>吉他：<code>[8, 12, 32, 7]</code></li>
</ul>
<p>其实这个过程非常简单，就是针对人脸、自行车和吉他这三个文档，抽取出相似的部分(或者词义相近的视觉词汇合并为同一类)，构造一个词典，词典中包含<code>4</code>个视觉单词，即<code>Dictionary = {1:&quot;自行车&quot;, 2:&quot;人脸&quot;, 3:&quot;吉他&quot;, 4:&quot;人脸类&quot;}</code>，最终人脸、自行车和吉他这三个文档皆可以用一个<code>4</code>维向量表示，最后根据三个文档相应部分出现的次数画成了上面对应的直方图。需要说明的是，以上过程只是针对三个目标类非常简单的一个示例，实际应用中，为了达到较好的效果，单词表中的词汇数量<code>K</code>往往非常庞大，并且目标类数目越多，对应的<code>K</code>值也越大。一般情况下，<code>K</code>的取值在几百到上千，在这里取<code>K = 4</code>仅仅是为了方便说明。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/过拟合与欠拟合/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/过拟合与欠拟合/" itemprop="url">过拟合与欠拟合</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T17:13:20+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;机器学习主要有两个主要挑战：欠拟合(<code>underfitting</code>)和过拟合(<code>overfitting</code>)。欠拟合是指模型不能在训练集上获得足够低的误差，而过拟合是指训练误差和和测试误差之间的差距太大。</p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>&emsp;&emsp;过拟合其实就是机器学习模型于自信，已经到了自负的阶段了。自负的坏处就是在自己的小圈子里表现非凡，不过在现实的大圈子里却经常处处碰壁。<br>&emsp;&emsp;机器学习模型的自负又表现在哪些方面呢？下图是一些数据，如果要你画一条线来描述这些数据，大多数人都会采用蓝线的方式，这也是我们希望机器也能学出来的一条用来总结这些数据的线。假设此时蓝线与数据的总误差可能是<code>10</code>，可有时候机器过于纠结这误差值，它想把误差减到更小，来完成它对这一批数据的学习使命。最后学习的模型(虚线)几乎经过了每一个数据点，这样的误差值会更小。可是误差越小就真的好吗？当我拿这个模型运用在现实中的时候，它的自负就体现出来。蓝线的误差基本保持不变，而虚线的误差值突然飙高。自负的虚线再也骄傲不起来，因为它不能成功地表达除了训练数据以外的其他数据，这就叫做过拟合。</p>
<p><img src="/2019/02/27/机器学习/过拟合与欠拟合/1.png" height="146" width="251"></p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>&emsp;&emsp;方法一：增加数据量。大部分过拟合产生的原因是因为数据量太少了，如果有成千上万的数据，虚线也会慢慢被拉直，变得没那么扭曲。</p>
<p><img src="/2019/02/27/机器学习/过拟合与欠拟合/2.png" height="146" width="178"></p>
<p>&emsp;&emsp;方法二：运用正规化，例如<code>L1</code>、<code>L2</code>正规化。这些方法适用于大多数的机器学习(包括神经网络)。它们的做法大同小异，我们简化机器学习的关键公式为<code>y = Wx</code>，<code>W</code>为机器需要学习到的各种参数。在过拟合中，<code>W</code>的值往往变化得特别大或特别小，为了不让<code>W</code>变化太大，我们在计算误差上做些手脚。原始的<code>cost</code>误差计算方式为<code>cost = (预测值 - 真实值)^2</code>，如果<code>W</code>变得太大，我们就让<code>cost</code>也跟着变大，变成一种惩罚机制，如下图所示(这里的<code>abs</code>是绝对值)，这种形式的正规化叫做<code>L1</code>正规化。<code>L2</code>正规化和<code>L1</code>类似，只是绝对值换成了平方，<code>L3</code>、<code>L4</code>也都是换成了立方和<code>4</code>次方等。</p>
<p><img src="/2019/02/27/机器学习/过拟合与欠拟合/3.png" height="148" width="266"></p>
<p>&emsp;&emsp;方法三：还有一种专门用在神经网络的正规化的方法，叫作<code>dropout</code>。在训练的时候，随机忽略掉一些神经元和神经联结，使这个神经网络变得<code>不完整</code>，用一个不完整的神经网络训练一次。到第二次再随机忽略另一些，变成另一个不完整的神经网络。有了这些随机<code>drop</code>掉的规则，我们可以让每一次预测结果都不会依赖于其中某部分特定的神经元。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/手写数字识别sklearn/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/手写数字识别sklearn/" itemprop="url">手写数字识别sklearn</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T15:07:09+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;在<code>sklearn</code>的手写数字集中，一共有<code>1797</code>个样本数，每个样本有<code>64</code>个特征数，即<code>8 * 8</code>像素，每个像素由<code>0</code>到<code>16</code>之间的整数表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">print(digits.data.shape)  <span class="comment"># 输出数据集的样本数与特征数</span></span><br><span class="line">print(np.unique(digits.target))  <span class="comment"># 输出所有目标类别</span></span><br><span class="line">print(digits.data)  <span class="comment"># 输出数据集</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1797</span>, <span class="number">64</span>)</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]</span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">5.</span> ...  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span> ... <span class="number">10.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span> ... <span class="number">16.</span>  <span class="number">9.</span>  <span class="number">0.</span>]</span><br><span class="line"> ...</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span> ...  <span class="number">6.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">2.</span> ... <span class="number">12.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span> <span class="number">10.</span> ... <span class="number">12.</span>  <span class="number">1.</span>  <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>
<p>数据集可视化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager <span class="keyword">as</span> fm  <span class="comment"># 导入字体管理器，用于提供中文支持</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">font_set = fm.FontProperties(fname=<span class="string">'C:/Windows/Fonts/msyh.ttc'</span>, size=<span class="number">14</span>)</span><br><span class="line">images_and_labels = list(zip(digits.images, digits.target))  <span class="comment"># 将图像和目标标签合并到一个列表中</span></span><br><span class="line">​</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> index, (image, label) <span class="keyword">in</span> enumerate(images_and_labels[:<span class="number">4</span>]):  <span class="comment"># 打印数据集的前4个图像</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">4</span>, index + <span class="number">1</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">    plt.title(<span class="string">u'训练样本：'</span> + str(label), fontproperties=font_set)</span><br><span class="line">​</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/手写数字识别sklearn/1.png"></p>
<p>样本图片效果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.imshow(digits.images[<span class="number">0</span>], cmap=plt.cm.gray_r, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/手写数字识别sklearn/2.png" height="256" width="256"></p>
<h3 id="用PCA降维"><a href="#用PCA降维" class="headerlink" title="用PCA降维"></a>用PCA降维</h3><p>&emsp;&emsp;由于该数据集有<code>64</code>个特征值，也就是说有<code>64</code>个维度，因此没办法直观地看到数据的分布及其之间的关系。但是，实际起作用的维度可能比特征值的个数要少得多，我们可以通过主成分分析来降低数据集的维度，从而观察样本点之间的关系。<br>&emsp;&emsp;主成分分析<code>PCA</code>：找到两个变量的线性组合，尽可能保留大部分的信息，这个新的变量(主成分)就可以替代原来的变量。也就是说，<code>PCA</code>就是通过线性变换来产生新的变量，并最大化保留了数据的差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> *</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)  <span class="comment"># 创建一个PCA模型</span></span><br><span class="line">reduced_data_pca = pca.fit_transform(digits.data)  <span class="comment"># 将数据应用到模型上</span></span><br><span class="line">print(reduced_data_pca.shape)  <span class="comment"># 查看维度</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1797</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>绘制散点图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager <span class="keyword">as</span> fm</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> *</span><br><span class="line">​</span><br><span class="line">font_set = fm.FontProperties(fname=<span class="string">'C:/Windows/Fonts/msyh.ttc'</span>, size=<span class="number">14</span>)</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)  <span class="comment"># 创建一个PCA模型</span></span><br><span class="line">reduced_data_pca = pca.fit_transform(digits.data)</span><br><span class="line">colors = [<span class="string">'black'</span>, <span class="string">'blue'</span>, <span class="string">'purple'</span>, <span class="string">'yellow'</span>, <span class="string">'white'</span>, <span class="string">'red'</span>, <span class="string">'lime'</span>, <span class="string">'cyan'</span>, <span class="string">'orange'</span>, <span class="string">'gray'</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(colors)):</span><br><span class="line">    x = reduced_data_pca[:, <span class="number">0</span>][digits.target == i]</span><br><span class="line">    y = reduced_data_pca[:, <span class="number">1</span>][digits.target == i]</span><br><span class="line">    plt.scatter(x, y, c=colors[i])</span><br><span class="line">​</span><br><span class="line">plt.legend(digits.target_names, bbox_to_anchor=(<span class="number">1.05</span>, <span class="number">1</span>), loc=<span class="number">2</span>, borderaxespad=<span class="number">0.</span>)</span><br><span class="line">plt.xlabel(<span class="string">u'第一个主成分'</span>, fontproperties=font_set)</span><br><span class="line">plt.ylabel(<span class="string">u'第二个主成分'</span>, fontproperties=font_set)</span><br><span class="line">plt.title(<span class="string">u"PCA 散点图"</span>, fontproperties=font_set)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/手写数字识别sklearn/3.png" height="206" width="279"></p>
<h3 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">data = scale(digits.data)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">0.</span>         <span class="number">-0.33501649</span> <span class="number">-0.04308102</span> ... <span class="number">-1.14664746</span> <span class="number">-0.5056698</span> <span class="number">-0.19600752</span>]</span><br><span class="line"> [ <span class="number">0.</span>         <span class="number">-0.33501649</span> <span class="number">-1.09493684</span> ...  <span class="number">0.54856067</span> <span class="number">-0.5056698</span> <span class="number">-0.19600752</span>]</span><br><span class="line"> [ <span class="number">0.</span>         <span class="number">-0.33501649</span> <span class="number">-1.09493684</span> ...  <span class="number">1.56568555</span>  <span class="number">1.6951369</span> <span class="number">-0.19600752</span>]</span><br><span class="line"> ...</span><br><span class="line"> [ <span class="number">0.</span>         <span class="number">-0.33501649</span> <span class="number">-0.88456568</span> ... <span class="number">-0.12952258</span> <span class="number">-0.5056698</span> <span class="number">-0.19600752</span>]</span><br><span class="line"> [ <span class="number">0.</span>         <span class="number">-0.33501649</span> <span class="number">-0.67419451</span> ...  <span class="number">0.8876023</span>  <span class="number">-0.5056698</span> <span class="number">-0.19600752</span>]</span><br><span class="line"> [ <span class="number">0.</span>         <span class="number">-0.33501649</span>  <span class="number">1.00877481</span> ...  <span class="number">0.8876023</span>  <span class="number">-0.2611357</span> <span class="number">-0.19600752</span>]]</span><br></pre></td></tr></table></figure>
<p>将数据集拆分成训练集和测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">data = scale(digits.data)</span><br><span class="line">X_train, X_test, y_train, y_test, images_train, images_test = \</span><br><span class="line">    train_test_split(data, digits.target, digits.images, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span>)</span><br><span class="line">​</span><br><span class="line">print(<span class="string">"训练集"</span>, X_train.shape)</span><br><span class="line">print(<span class="string">"测试集"</span>, X_test.shape)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集 (<span class="number">1347</span>, <span class="number">64</span>)</span><br><span class="line">测试集 (<span class="number">450</span>, <span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<h3 id="使用SVM分类器"><a href="#使用SVM分类器" class="headerlink" title="使用SVM分类器"></a>使用SVM分类器</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">data = scale(digits.data)</span><br><span class="line">​</span><br><span class="line">X_train, X_test, y_train, y_test, images_train, images_test = \</span><br><span class="line">    train_test_split(data, digits.target, digits.images, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span>)</span><br><span class="line">svc_model = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100</span>, kernel=<span class="string">'linear'</span>)  <span class="comment"># 创建SVC模型</span></span><br><span class="line">svc_model.fit(X_train, y_train)  <span class="comment"># 将训练集应用到SVC模型上</span></span><br><span class="line">print(svc_model.score(X_test, y_test))  <span class="comment"># 评估模型的预测效果</span></span><br></pre></td></tr></table></figure>
<h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager <span class="keyword">as</span> fm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">font_set = fm.FontProperties(fname=<span class="string">'C:/Windows/Fonts/msyh.ttc'</span>, size=<span class="number">14</span>)</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">data = scale(digits.data)</span><br><span class="line">​</span><br><span class="line">X_train, X_test, y_train, y_test, images_train, images_test = \</span><br><span class="line">    train_test_split(data, digits.target, digits.images, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span>)</span><br><span class="line">svc_model = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100</span>, kernel=<span class="string">'linear'</span>)  <span class="comment"># 创建SVC模型</span></span><br><span class="line">svc_model.fit(X_train, y_train)  <span class="comment"># 将训练集应用到SVC模型上</span></span><br><span class="line">predicted = svc_model.predict(X_test)  <span class="comment"># 使用创建的SVC模型对测试集进行预测</span></span><br><span class="line"><span class="comment"># 将测试集的图像与预测的标签合并到一个列表中</span></span><br><span class="line">images_and_predictions = list(zip(images_test, predicted))</span><br><span class="line">​</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, (image, prediction) <span class="keyword">in</span> enumerate(images_and_predictions[:<span class="number">4</span>]):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">4</span>, index + <span class="number">1</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">    plt.title(<span class="string">u'预测结果: '</span> + str(prediction), fontproperties=font_set)</span><br><span class="line">​</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/手写数字识别sklearn/4.png"></p>
<h3 id="分析结果的准确性"><a href="#分析结果的准确性" class="headerlink" title="分析结果的准确性"></a>分析结果的准确性</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">​</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">data = scale(digits.data)</span><br><span class="line">​</span><br><span class="line">X_train, X_test, y_train, y_test, images_train, images_test = \</span><br><span class="line">    train_test_split(data, digits.target, digits.images, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span>)</span><br><span class="line">​</span><br><span class="line">svc_model = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100</span>, kernel=<span class="string">'linear'</span>)  <span class="comment"># 创建SVC模型</span></span><br><span class="line">svc_model.fit(X_train, y_train)  <span class="comment"># 将训练集应用到SVC模型上</span></span><br><span class="line">predicted = svc_model.predict(X_test)  <span class="comment"># 使用创建的SVC模型对测试集进行预测</span></span><br><span class="line">​</span><br><span class="line">X = np.arange(len(y_test))</span><br><span class="line"><span class="comment"># 生成比较列表，如果预测的结果正确，则对应位置为0，错误则为1</span></span><br><span class="line">comp = [<span class="number">0</span> <span class="keyword">if</span> y1 == y2 <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> y1, y2 <span class="keyword">in</span> zip(y_test, predicted)]</span><br><span class="line">​</span><br><span class="line">print(<span class="string">"测试集数量："</span>, len(y_test))</span><br><span class="line">print(<span class="string">"错误识别数："</span>, sum(comp))</span><br><span class="line">print(<span class="string">"识别准确率："</span>, <span class="number">1</span> - float(sum(comp)) / len(y_test))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">测试集数量：<span class="number">450</span></span><br><span class="line">错误识别数：<span class="number">10</span></span><br><span class="line">识别准确率：<span class="number">0.9777777777777777</span></span><br></pre></td></tr></table></figure>
<h3 id="错误识别样本分析"><a href="#错误识别样本分析" class="headerlink" title="错误识别样本分析"></a>错误识别样本分析</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager <span class="keyword">as</span> fm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">​</span><br><span class="line">font_set = fm.FontProperties(fname=<span class="string">'C:/Windows/Fonts/msyh.ttc'</span>, size=<span class="number">14</span>)</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">data = scale(digits.data)</span><br><span class="line">​</span><br><span class="line">X_train, X_test, y_train, y_test, images_train, images_test = \</span><br><span class="line">    train_test_split(data, digits.target, digits.images, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span>)</span><br><span class="line">​</span><br><span class="line">svc_model = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100</span>, kernel=<span class="string">'linear'</span>)  <span class="comment"># 创建SVC模型</span></span><br><span class="line">svc_model.fit(X_train, y_train)  <span class="comment"># 将训练集应用到SVC模型上</span></span><br><span class="line">predicted = svc_model.predict(X_test)  <span class="comment"># 使用创建的SVC模型对测试集进行预测</span></span><br><span class="line">​</span><br><span class="line">X = np.arange(len(y_test))</span><br><span class="line"><span class="comment"># 生成比较列表，如果预测的结果正确，则对应位置为0，错误则为1</span></span><br><span class="line">comp = [<span class="number">0</span> <span class="keyword">if</span> y1 == y2 <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> y1, y2 <span class="keyword">in</span> zip(y_test, predicted)]</span><br><span class="line">​</span><br><span class="line">wrong_index = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, value <span class="keyword">in</span> enumerate(comp):</span><br><span class="line">    <span class="keyword">if</span> value:</span><br><span class="line">        wrong_index.append(i)</span><br><span class="line">​</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> plot_index, image_index <span class="keyword">in</span> enumerate(wrong_index):  <span class="comment"># 输出错误识别的样本图像</span></span><br><span class="line">    image = images_test[image_index]</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">5</span>, plot_index + <span class="number">1</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">    <span class="comment"># “8-&gt;9”表示正确值为8，被错误地识别成了9</span></span><br><span class="line">    info = <span class="string">"&#123;right&#125;-&gt;&#123;wrong&#125;"</span>.format(right=y_test[image_index], wrong=predicted[image_index])</span><br><span class="line">    plt.title(info, fontsize=<span class="number">16</span>)</span><br><span class="line">​</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/手写数字识别sklearn/5.png" height="260" width="399"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/K-means算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/K-means算法/" itemprop="url">K-means算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T09:49:14+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;俗话说<code>物以类聚，人以群分</code>，而聚类算法就是体现这样的思想。聚类是一种非监督学习，与之前学习的分类和回归算法不同(监督学习)，监督学习是有有<code>label</code>标签的，而非监督学习没有。<br>&emsp;&emsp;聚类是把相似的对象归到同一簇中，有点像全自动分类。聚类的应用场景有很多，例如在电商行业，通过用户的购买历史进行聚类，针对不同的用户群体推送不同的广告。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>&emsp;&emsp;<code>K-Means</code>聚类首先随机确定<code>K</code>个初始点作为质心，这也是<code>K-Means</code>聚类的一个问题，<code>K</code>值的不合理选择会使得模型效果很差。然后将数据集中的每个点分配到一个簇中，具体来讲，就是为每个点找到距其最近的质心(这里算的是欧式距离，当然也可以使用其他距离)，并将其分配至该质心所对应的簇；这一步完成之后，每个簇的质心更新为该簇所有点的平均值。算法伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建k个点作为起始质心(通常是随机选择)</span><br><span class="line">当任意一个点的簇分配结果发生改变时(不改变时算法结束)</span><br><span class="line">    对数据集中的每个数据点</span><br><span class="line">        对每个质心</span><br><span class="line">            计算质心与数据点之间的距离</span><br><span class="line">        将数据点分配到距其最近的簇</span><br><span class="line">    对每一个簇，计算簇中所有点的均值并将均值作为质心</span><br></pre></td></tr></table></figure>
<p>数据集<code>testSet.txt</code>的部分数据如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.658985</span>    <span class="number">4.285136</span>  </span><br><span class="line"><span class="number">-3.453687</span>   <span class="number">3.424321</span>  </span><br><span class="line"><span class="number">4.838138</span>    <span class="number">1.151539</span>  </span><br><span class="line"><span class="number">-5.379713</span>   <span class="number">-3.362104</span>  </span><br><span class="line"><span class="number">0.972564</span>    <span class="number">2.924086</span>  </span><br><span class="line"><span class="number">-3.567919</span>   <span class="number">1.531611</span>  </span><br><span class="line"><span class="number">0.450614</span>    <span class="number">-3.302219</span>  </span><br><span class="line"><span class="number">-3.487105</span>   <span class="number">-1.724432</span>  </span><br><span class="line"><span class="number">2.668759</span>    <span class="number">1.594842</span>  </span><br><span class="line"><span class="number">-3.156485</span>   <span class="number">3.191137</span>  </span><br><span class="line"><span class="number">3.165506</span>    <span class="number">-3.999838</span>  </span><br><span class="line"><span class="number">-2.786837</span>   <span class="number">-3.099354</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>K</code>均值聚类函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(filename)</span>:</span>  <span class="comment"># 将文本文件导入到一个列表中</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = list(map(float, curLine))</span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span>  <span class="comment"># 计算两个向量的欧式距离</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 为给定数据集构建一个包含k个随机质心的集合。随机质心必须要在整个</span></span><br><span class="line"><span class="comment"># 数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]  <span class="comment"># 列的数量</span></span><br><span class="line">    centroids = mat(zeros((k, n)))  <span class="comment"># 创建k个质心矩阵</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):  <span class="comment"># 创建随机簇质心，并且在每一维的边界内</span></span><br><span class="line">        minJ = min(dataSet[:, j])  <span class="comment"># 最小值</span></span><br><span class="line">        rangeJ = float(max(dataSet[:, j]) - minJ)  <span class="comment"># 范围 = 最大值 - 最小值</span></span><br><span class="line">        centroids[:, j] = mat(minJ + rangeJ * random.rand(k, <span class="number">1</span>))  <span class="comment"># 随机生成</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line">​</span><br><span class="line"><span class="comment"># k-means聚类算法，该算法会创建k个质心，然后将每个点分配到最近的质心，</span></span><br><span class="line"><span class="comment"># 再重新计算质心。这个过程重复数次，直到数据点的簇分配结果不再改变位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]  <span class="comment"># 行数</span></span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))  <span class="comment"># 一列记录簇索引值，一列存储误差(指的是当前点到簇质心的距离)</span></span><br><span class="line">    centroids = createCent(dataSet, k)  <span class="comment"># 创建质心，随机k个质心</span></span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):  <span class="comment"># 循环每一个数据点，并分配到最近的质心中去</span></span><br><span class="line">            minDist = inf</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])  <span class="comment"># 计算数据点到质心的距离</span></span><br><span class="line">                <span class="comment"># 如果距离比minDist(最小距离)还小，更新minDist(最小距离)和最小质心的index(索引)</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="comment"># 在k个簇里面与第i个样本距离最小的的标号和距离保存在clusterAssment中</span></span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:  <span class="comment"># 簇分配结果改变</span></span><br><span class="line">                clusterChanged = <span class="keyword">True</span>  <span class="comment"># 簇改变</span></span><br><span class="line">                <span class="comment"># 更新簇分配结果为最小质心的index(索引)、minDist(最小距离)的平方</span></span><br><span class="line">                clusterAssment[i, :] = minIndex, minDist ** <span class="number">2</span></span><br><span class="line">        print(centroids)</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):  <span class="comment"># 更新质心</span></span><br><span class="line">            <span class="comment"># “clusterAssment[:,0].A == cent”是找出矩阵clusterAssment中第一列元素中等于cent的行的下标</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == cent)[<span class="number">0</span>]]  <span class="comment"># 将dataSet矩阵中相对应的样本提取出来</span></span><br><span class="line">            centroids[cent, :] = mean(ptsInClust, axis=<span class="number">0</span>)  <span class="comment"># 将质心修改为簇中所有点的平均值，mean就是求平均值</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(dataSet, k, centroids, clusterAssment)</span>:</span></span><br><span class="line">    numSamples, dim = dataSet.shape</span><br><span class="line">    <span class="keyword">if</span> dim != <span class="number">2</span>:</span><br><span class="line">        print(<span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">    mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</span><br><span class="line">    <span class="keyword">if</span> k &gt; len(mark):</span><br><span class="line">        print(<span class="string">"Sorry! Your k is too large! "</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  <span class="comment"># draw all samples</span></span><br><span class="line">        markIndex = int(clusterAssment[i, <span class="number">0</span>])  <span class="comment"># 为样本指定颜色</span></span><br><span class="line">        plt.plot(dataSet[i, <span class="number">0</span>], dataSet[i, <span class="number">1</span>], mark[markIndex])</span><br><span class="line">​</span><br><span class="line">    mark = [<span class="string">'Dr'</span>, <span class="string">'Db'</span>, <span class="string">'Dg'</span>, <span class="string">'Dk'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  <span class="comment"># draw the centroids</span></span><br><span class="line">        plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize=<span class="number">12</span>)</span><br><span class="line">​</span><br><span class="line">    plt.show()</span><br><span class="line">​</span><br><span class="line">datMAt = mat(loadDataSet(<span class="string">"testSet.txt"</span>))</span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">centroids, clusterAssment = kMeans(datMAt, k)</span><br><span class="line">showCluster(datMAt, k, centroids, clusterAssment)</span><br></pre></td></tr></table></figure>
<p>不同的类用不同的颜色来表示，其中的大菱形是对应类的均值质心点：</p>
<p><img src="/2019/02/27/机器学习/K-means算法/1.png" height="242" width="319"></p>
<p>&emsp;&emsp;<code>K-means</code>算法比较简单，但也有几个比较大的缺点：</p>
<ul>
<li><code>k</code>值的选择是用户指定的，不同的<code>k</code>得到的结果会有很大的不同，如下图所示。左边是<code>k = 3</code>的结果，这个就太稀疏了，蓝色的那个簇其实是可以再划分成两个簇的；而右图是<code>k = 5</code>的结果，可以看到红色菱形和蓝色菱形这两个簇应该是可以合并成一个簇的：</li>
</ul>
<p><img src="/2019/02/27/机器学习/K-means算法/2.jpg" height="240" width="609"></p>
<ul>
<li>对<code>k</code>个初始质心的选择比较敏感，容易陷入局部最小值。例如，我们在运行上面的算法时，有可能会得到不同的结果，如下面这两种情况。<code>K-means</code>也是收敛了，只是收敛到了局部最小值：</li>
</ul>
<p><img src="/2019/02/27/机器学习/K-means算法/3.jpg" height="239" width="602"></p>
<ul>
<li>存在局限性，例如下面这种非球状的数据分布就搞不定了：</li>
</ul>
<p><img src="/2019/02/27/机器学习/K-means算法/4.jpg" height="264" width="606"></p>
<p>&emsp;&emsp;<code>K-means</code>算法有个比较大的缺点，就是对初始<code>k</code>个质心点的选取比较敏感。而二分<code>k</code>均值(<code>bisecting k-means</code>)的出现就是为了一定情况下解决这个问题的，也就是说它对初始的<code>k</code>个质心的选择不太敏感。<br>&emsp;&emsp;该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低<code>SSE</code>(平方和误差)的值。以此进行下去，直到簇的数目等于用户给定的数目<code>k</code>为止。<br>&emsp;&emsp;二分<code>k</code>均值算法的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">将所有数据点看成一个簇</span><br><span class="line">当簇数目小于k时</span><br><span class="line">    对每一个簇</span><br><span class="line">        计算总误差</span><br><span class="line">        在给定的簇上面进行k-均值聚类(k = <span class="number">2</span>)</span><br><span class="line">        计算将该簇一分为二后的总误差</span><br><span class="line">    选择使得误差最小的那个簇进行划分操作</span><br></pre></td></tr></table></figure>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(filename)</span>:</span>  <span class="comment"># 将文本文件导入到一个列表中</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = list(map(float, curLine))</span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span>  <span class="comment"># 计算两个向量的欧式距离</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 为给定数据集构建一个包含k个随机质心的集合。随机质心必须要在</span></span><br><span class="line"><span class="comment"># 整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]  <span class="comment"># 列的数量</span></span><br><span class="line">    centroids = mat(zeros((k, n)))  <span class="comment"># 创建k个质心矩阵</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):  <span class="comment"># 创建随机簇质心，并且在每一维的边界内</span></span><br><span class="line">        minJ = min(dataSet[:, j])  <span class="comment"># 最小值</span></span><br><span class="line">        rangeJ = float(max(dataSet[:, j]) - minJ)  <span class="comment"># 范围 = 最大值 - 最小值</span></span><br><span class="line">        centroids[:, j] = mat(minJ + rangeJ * random.rand(k, <span class="number">1</span>))  <span class="comment"># 随机生成</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line">​</span><br><span class="line"><span class="comment"># k-means聚类算法，该算法会创建k个质心，然后将每个点分配到最近的质心，</span></span><br><span class="line"><span class="comment"># 再重新计算质心。这个过程重复数次，直到数据点的簇分配结果不再改变位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]  <span class="comment"># 行数</span></span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))  <span class="comment"># 一列记录簇索引值，一列存储误差(指的是当前点到簇质心的距离)</span></span><br><span class="line">    centroids = createCent(dataSet, k)  <span class="comment"># 创建质心，随机k个质心</span></span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):  <span class="comment"># 循环每一个数据点，并分配到最近的质心中去</span></span><br><span class="line">            minDist = inf</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])  <span class="comment"># 计算数据点到质心的距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:  <span class="comment"># 如果距离比minDist(最小距离)还小，更新minDist(最小距离)和最小质心的index(索引)</span></span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="comment"># 在k个簇里面与第i个样本距离最小的的标号和距离保存在clusterAssment中</span></span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:  <span class="comment"># 簇分配结果改变</span></span><br><span class="line">                clusterChanged = <span class="keyword">True</span>  <span class="comment"># 簇改变</span></span><br><span class="line">                <span class="comment"># 更新簇分配结果为最小质心的index(索引)、minDist(最小距离)的平方</span></span><br><span class="line">                clusterAssment[i, :] = minIndex, minDist ** <span class="number">2</span></span><br><span class="line">        print(centroids)</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):  <span class="comment"># 更新质心</span></span><br><span class="line">            <span class="comment"># “clusterAssment[:,0].A == cent”是找出矩阵clusterAssment中第一列元素中等于cent的行的下标</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == cent)[<span class="number">0</span>]]  <span class="comment"># 将dataSet矩阵中相对应的样本提取出来</span></span><br><span class="line">            centroids[cent, :] = mean(ptsInClust, axis=<span class="number">0</span>)  <span class="comment"># 将质心修改为簇中所有点的平均值，mean就是求平均值</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKMeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span>  <span class="comment"># 二分KMeans聚类算法，基于kMeans基础之上的优化，以避免陷入局部最小值</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))  <span class="comment"># 保存每个数据点的簇分配结果和平方误差</span></span><br><span class="line">    centroid0 = mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]  <span class="comment"># 质心初始化为所有数据点的均值</span></span><br><span class="line">    centList = [centroid0]  <span class="comment"># 初始化只有1个质心的list</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):  <span class="comment"># 计算所有数据点到初始质心的距离平方误差</span></span><br><span class="line">        clusterAssment[j, <span class="number">1</span>] = distMeas(mat(centroid0), dataSet[j, :]) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k):  <span class="comment"># 当质心数量小于k时</span></span><br><span class="line">        lowestSSE = inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):  <span class="comment"># 对每一个质心</span></span><br><span class="line">            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == i)[<span class="number">0</span>], :]  <span class="comment"># 获取当前簇i下的所有数据点</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas)  <span class="comment"># 将当前簇i进行二分kMeans处理</span></span><br><span class="line">            sseSplit = sum(splitClustAss[:, <span class="number">1</span>])  <span class="comment"># 将二分kMeans结果中的平方和的距离进行求和</span></span><br><span class="line">            <span class="comment"># 将未参与二分kMeans分配结果中的平方和的距离进行求和</span></span><br><span class="line">            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, <span class="number">0</span>].A != i)[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">            print(<span class="string">"sseSplit, and notSplit: "</span>, sseSplit, sseNotSplit)</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE:</span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        <span class="comment"># 找出最好的簇分配结果</span></span><br><span class="line">        <span class="comment"># 调用二分kMeans的结果，默认簇是0和1，当然也可以改成其它的数字</span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>], <span class="number">0</span>] = len(centList)</span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>], <span class="number">0</span>] = bestCentToSplit  <span class="comment"># 更新为最佳质心</span></span><br><span class="line">        print(<span class="string">'the bestCentToSplit is: '</span>, bestCentToSplit)</span><br><span class="line">        print(<span class="string">'the len of bestClustAss is: '</span>, len(bestClustAss))</span><br><span class="line">        <span class="comment"># 更新质心列表</span></span><br><span class="line">        <span class="comment"># 更新原质心list中的第i个质心为使用二分kMeans后，bestNewCents的第一个质心</span></span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>, :].tolist()[<span class="number">0</span>]</span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>, :].tolist()[<span class="number">0</span>])  <span class="comment"># 添加bestNewCents的第二个质心</span></span><br><span class="line">        <span class="comment"># 重新分配最好簇下的数据(质心)以及SSE</span></span><br><span class="line">        clusterAssment[nonzero(clusterAssment[:, <span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>], :] = bestClustAss</span><br><span class="line">    <span class="keyword">return</span> mat(centList), clusterAssment</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(dataSet, k, centroids, clusterAssment)</span>:</span></span><br><span class="line">    numSamples, dim = dataSet.shape</span><br><span class="line">    <span class="keyword">if</span> dim != <span class="number">2</span>:</span><br><span class="line">        print(<span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">    mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</span><br><span class="line">    <span class="keyword">if</span> k &gt; len(mark):</span><br><span class="line">        print(<span class="string">"Sorry! Your k is too large! "</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):  <span class="comment"># draw all samples</span></span><br><span class="line">        markIndex = int(clusterAssment[i, <span class="number">0</span>])  <span class="comment"># 为样本指定颜色</span></span><br><span class="line">        plt.plot(dataSet[i, <span class="number">0</span>], dataSet[i, <span class="number">1</span>], mark[markIndex])</span><br><span class="line">​</span><br><span class="line">    mark = [<span class="string">'Dr'</span>, <span class="string">'Db'</span>, <span class="string">'Dg'</span>, <span class="string">'Dk'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  <span class="comment"># draw the centroids</span></span><br><span class="line">        plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize=<span class="number">12</span>)</span><br><span class="line">​</span><br><span class="line">    plt.show()</span><br><span class="line">​</span><br><span class="line">datMAt = mat(loadDataSet(<span class="string">"testSet.txt"</span>))</span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">centroids, clusterAssment = biKMeans(datMAt, k)  </span><br><span class="line">showCluster(datMAt, k, centroids, clusterAssment)</span><br></pre></td></tr></table></figure>
<p>上述函数可以运行多次，聚类会收敛到全局最小值，而原始的<code>kMeans</code>函数偶尔会陷入局部最小值：</p>
<p><img src="/2019/02/27/机器学习/K-means算法/5.png" height="242" width="317"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/sklearn之pipeline/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/sklearn之pipeline/" itemprop="url">sklearn之pipeline</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T09:27:08+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;我们经常需要在训练集和测试集上同时进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vect = CountVectorizer()</span><br><span class="line">tfidf = TfidfTransformer()</span><br><span class="line">clf = SGDClassifier()</span><br><span class="line">​</span><br><span class="line">vX = vect.fit_transform(Xtrain)</span><br><span class="line">tfidfX = tfidf.fit_transform(vX)</span><br><span class="line">predicted = clf.fit_predict(tfidfX)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Now evaluate all steps on test set</span></span><br><span class="line">vX = vect.fit_transform(Xtest)</span><br><span class="line">tfidfX = tfidf.fit_transform(vX)</span><br><span class="line">predicted = clf.fit_predict(tfidfX)</span><br></pre></td></tr></table></figure>
<p>很容易看出训练测试集重复了代码。而<code>pipeline</code>不仅减少了代码量，同时也让机器学习的流程变得更加直观：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'tfidf'</span>, TfidfTransformer()),</span><br><span class="line">    (<span class="string">'clf'</span>, SGDClassifier()),</span><br><span class="line">])</span><br><span class="line">​</span><br><span class="line">predicted = pipeline.fit(Xtrain).predict(Xtrain)</span><br><span class="line">predicted = pipeline.predict(Xtest)  <span class="comment"># Now evaluate all steps on test set</span></span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/sklearn之pipeline/1.jpg" height="215" width="295"></p>
<h3 id="使用pipeline做cross-validation"><a href="#使用pipeline做cross-validation" class="headerlink" title="使用pipeline做cross validation"></a>使用pipeline做cross validation</h3><p>&emsp;&emsp;下面的代码先对手写数字的数据进行<code>PCA</code>降维，再通过逻辑回归预测标签。其中我们通过<code>pipeline</code>对<code>PCA</code>的降维维数<code>n_components</code>和逻辑回归的正则项<code>C</code>大小做交叉验证，主要步骤有：</p>
<ol>
<li>依次实例化各成分对象，例如<code>pca = decomposition.PCA()</code>。</li>
<li>以<code>(name, object)</code>的<code>tuble</code>为元素组装<code>pipeline</code>，例如<code>Pipeline(steps=[(&#39;pca&#39;, pca), (&#39;logistic&#39;, logistic)])</code>。</li>
<li>初始化<code>CV</code>参数，例如<code>n_components = [20, 40, 64]</code>。</li>
<li>实例化<code>CV</code>对象，例如<code>estimator = GridSearchCV(pipe, dict(pca__n_components=n_components, logistic__C=Cs))</code>，其中注意参数的传递方式，即<code>key</code>为<code>pipeline</code>元素名加上函数参数。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, decomposition, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">​</span><br><span class="line">logistic = linear_model.LogisticRegression()</span><br><span class="line">pca = decomposition.PCA()</span><br><span class="line">pipe = Pipeline(steps=[(<span class="string">'pca'</span>, pca), (<span class="string">'logistic'</span>, logistic)])</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X_digits = digits.data</span><br><span class="line">y_digits = digits.target</span><br><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">n_components = [<span class="number">20</span>, <span class="number">40</span>, <span class="number">64</span>]</span><br><span class="line">Cs = np.logspace(<span class="number">-4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">pca.fit(X_digits)</span><br><span class="line">estimator = GridSearchCV(pipe, dict(pca__n_components=n_components, logistic__C=Cs))</span><br><span class="line">estimator.fit(X_digits, y_digits)</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.clf()</span><br><span class="line">plt.axes([<span class="number">.2</span>, <span class="number">.2</span>, <span class="number">.7</span>, <span class="number">.7</span>])</span><br><span class="line">plt.plot(pca.explained_variance_, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'n_components'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'explained_variance_'</span>)</span><br><span class="line">plt.axvline(estimator.best_estimator_.named_steps[<span class="string">'pca'</span>].n_components, linestyle=<span class="string">':'</span>, label=<span class="string">'n_components chosen'</span>)</span><br><span class="line">plt.legend(prop=dict(size=<span class="number">12</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/27/机器学习/sklearn之pipeline/2.jpg" height="254" width="328"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/决策树sklearn/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/决策树sklearn/" itemprop="url">决策树sklearn</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T08:43:33+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>sklearn.tree</code>模块提供了决策树模型，用于解决分类问题和回归问题。方法如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tree.DecisionTreeClassifier([criterion, ...])</code></td>
<td>A decision tree classifier.</td>
</tr>
<tr>
<td><code>tree.DecisionTreeRegressor([criterion, ...])</code></td>
<td>A decision tree regressor.</td>
</tr>
<tr>
<td><code>tree.ExtraTreeClassifier([criterion, ...])</code></td>
<td>An extremely randomized tree classifier.</td>
</tr>
<tr>
<td><code>tree.ExtraTreeRegressor([criterion, ...])</code></td>
<td>An extremely randomized tree regressor.</td>
</tr>
<tr>
<td><code>tree.export_graphviz(decision_tree[, ...])</code></td>
<td>Export a decision tree in <code>DOT</code> format.</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;<code>DecisionTreeClassifier</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeClassifier</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    criterion=<span class="string">'gini'</span>, splitter=<span class="string">'best'</span>, max_depth=None, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=None, random_state=None, max_leaf_nodes=None,</span></span></span><br><span class="line"><span class="class"><span class="params">    min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=None, class_weight=None, presort=False)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>criterion</code>：特征选择标准，可选参数，可以设置为<code>entropy</code>或<code>gini</code>。<code>gini</code>是基尼不纯度，是将来自集合的某种结果随机应用于某一数据项的预期误差率，是一种基于统计的思想。<code>entropy</code>是香农熵，是一种基于信息论的思想。<code>ID3</code>算法使用的是<code>entropy</code>，<code>CART</code>算法使用的则是<code>gini</code>。</li>
<li><code>splitter</code>：特征划分点选择标准，可选参数，可以设置为<code>random</code>或<code>best</code>。每个结点的选择策略。<code>best</code>参数是根据算法选择最佳的切分特征，例如<code>gini</code>、<code>entropy</code>。<code>random</code>随机的在部分划分点中找局部最优的划分点。默认的<code>best</code>适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐<code>random</code>。</li>
<li><code>max_features</code>：划分时考虑的最大特征数，可选参数。寻找最佳切分时考虑的最大特征数(<code>n_features</code>为总共的特征数)，有如下<code>6</code>种情况：</li>
</ul>
<ol>
<li>如果<code>max_features</code>是整型的数，则考虑<code>max_features</code>个特征。</li>
<li>如果<code>max_features</code>是浮点型的数，则考虑<code>int(max_features * n_features)</code>个特征。</li>
<li>如果<code>max_features</code>设为<code>auto</code>，那么<code>max_features = sqrt(n_features)</code>。</li>
<li>如果<code>max_features</code>设为<code>sqrt</code>，那么<code>max_featrues = sqrt(n_features)</code>，跟<code>auto</code>一样。</li>
<li>如果<code>max_features</code>设为<code>log2</code>，那么<code>max_features = log2(n_features)</code>。</li>
<li>如果<code>max_features</code>设为<code>None</code>，那么<code>max_features = n_features</code>，也就是所有特征都用。</li>
</ol>
<p>一般来说，如果样本特征数不多(比如小于<code>50</code>)，我们用默认的<code>None</code>就可以了；如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p>
<ul>
<li><code>max_depth</code>：决策树的最大层数，可选参数。如果这个参数设置为<code>None</code>，那么决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。或者如果设置了<code>min_samples_slipt</code>参数，那么直到少于<code>min_smaples_split</code>个样本为止。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值为<code>10</code>至<code>100</code>之间。</li>
<li><code>min_samples_split</code>：内部节点再划分所需最小样本数，可选参数。这个值限制了子树继续划分的条件。如果<code>min_samples_split</code>为整数，那么在切分内部结点的时候，<code>min_samples_split</code>作为最小的样本数，也就是说，如果样本已经少于<code>min_samples_split</code>个样本，则停止继续切分。如果<code>min_samples_split</code>为浮点数，那么<code>min_samples_split</code>就是一个百分比，<code>ceil(min_samples_split * n_samples)</code>，数是向上取整的。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</li>
<li><code>min_weight_fraction_leaf</code>：叶子节点最小的样本权重和，可选参数。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</li>
<li><code>max_leaf_nodes</code>：最大叶子节点数，可选参数。通过限制最大叶子节点数，可以防止过拟合。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</li>
<li><code>class_weight</code>：类别权重，可选参数，默认是<code>None</code>，也可以是字典、字典列表、<code>balanced</code>。指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。类别的权重可以通过<code>{class_label: weight}</code>这样的格式给出，这里可以自己指定各个样本的权重。如果使用<code>balanced</code>，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的<code>None</code>。</li>
<li><code>random_state</code>：可选参数，随机数种子。如果是整数，那么<code>random_state</code>会作为随机数生成器的随机数种子。如果没有设置随机数，随机出来的数与当前系统时间有关，每个时刻都是不同的。如果设置了随机数种子，那么相同随机数种子在不同时刻产生的随机数也是相同的。如果是<code>RandomState instance</code>，那么<code>random_state</code>是随机数生成器；如果为<code>None</code>，则随机数生成器使用<code>np.random</code>。</li>
<li><code>min_impurity_split</code>：节点划分最小不纯度，可选参数。这是一个阈值，它限制了决策树的增长。如果某节点的不纯度(基尼系数、信息增益、均方差、绝对差)小于这个阈值，则该节点不再生成子节点，即成为叶子节点。</li>
<li><code>presort</code>：数据是否预排序，可选参数，默认是<code>False</code>，即不排序。一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为<code>true</code>可以让划分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。</li>
</ul>
<p>&emsp;&emsp;除了这些参数要注意以外，其他在调参时的注意点有：</p>
<ul>
<li>当样本数量少但是样本特征非常多的时候，决策树很容易过拟合。一般来说，样本数比特征数多一些会比较容易建立健壮的模型。</li>
<li>如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析<code>PCA</code>、特征选择<code>Losso</code>或者独立成分分析<code>ICA</code>。这样特征的维度会大大减小，再来拟合决策树模型效果会好。</li>
<li>推荐多用决策树的可视化，同时先限制决策树的深度，这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。</li>
<li>在训练模型时，注意观察样本的类别情况(主要指分类树)，如果类别分布非常不均匀，就要考虑用<code>class_weight</code>来限制模型过于偏向样本多的类别。</li>
<li>决策树的数组使用的是<code>numpy.float32</code>类型，如果训练数据不是这样的格式，算法会先做<code>copy</code>再运行。</li>
<li>如果输入的样本矩阵是稀疏的，推荐在拟合前调用<code>csc_matrix</code>稀疏化，在预测前调用<code>csr_matrix</code>稀疏化。</li>
</ul>
<p>&emsp;&emsp;<code>sklearn.tree.DecisionTreeClassifier</code>提供了一些方法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>apply(X[, check_input])</code></td>
<td>Returns the index of the leaf that each sample is predicted as.</td>
</tr>
<tr>
<td><code>decision_path(X[, check_input])</code></td>
<td>Return the decision path in the tree.</td>
</tr>
<tr>
<td><code>fit(X, y[, sample_weight, check_input, ...])</code></td>
<td>Build a decision tree classifier from the training <code>set (X, y)</code>.</td>
</tr>
<tr>
<td><code>get_params([deep])</code></td>
<td>Get parameters for this estimator.</td>
</tr>
<tr>
<td><code>predict(X[, check_input])</code></td>
<td>Predict class or regression value for <code>X</code>.</td>
</tr>
<tr>
<td><code>predict_log_proba(X)</code></td>
<td>Predict class <code>log-probabilities</code> of the input samples <code>X</code>.</td>
</tr>
<tr>
<td><code>predict_proba(X[, check_input])</code></td>
<td>Predict class probabilities of the input samples <code>X</code>.</td>
</tr>
<tr>
<td><code>score(X, y[, sample_weight])</code></td>
<td>Returns the mean accuracy on the given test data and labels.</td>
</tr>
<tr>
<td><code>set_params(**params)</code></td>
<td>Set the parameters of this estimator.</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;训练数据如下：</p>
<p><img src="/2019/02/27/机器学习/决策树sklearn/1.png" height="191" width="426"></p>
<p>&emsp;&emsp;首先导入一些可能用到的库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br></pre></td></tr></table></figure>
<p>读取<code>data.csv</code>中的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">allElectronicsData = open(<span class="string">r'test.csv'</span>, <span class="string">'rt'</span>)</span><br><span class="line">reader = csv.reader(allElectronicsData)</span><br><span class="line">headers = next(reader)</span><br></pre></td></tr></table></figure>
<p>到此我们已经拿到数据了，但是这些数据是不能直接使用的，因为计算机不认识字符，它只认识数字，所以要把字符串转换成数组。这里怎样进行转换呢？例如我们把<code>age</code>分为了三类，即<code>youth</code>、<code>middle_aged</code>和<code>senior</code>，如果第一条数据是<code>youth</code>，就转换为<code>0 0 1</code>。当然还有另外的转换方法，也可以使用<code>1</code>、<code>2</code>、<code>3</code>来分别表示三类数据。下面使用<code>sklearn</code>自带的工具进行转换，它就使用第一种转换方式，即<code>youth = 0 0 1</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">featureList = [] <span class="comment"># save feature Dict</span></span><br><span class="line">labelList = [] <span class="comment"># save label</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">    labelList.append(row[len(row) - <span class="number">1</span>])  <span class="comment"># 将每一行的label标记存到labellist</span></span><br><span class="line">    rowDict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(row) - <span class="number">1</span>):</span><br><span class="line">        rowDict[headers[i]] = row[i]</span><br><span class="line"></span><br><span class="line">    featureList.append(rowDict)  <span class="comment"># 将每一行的特征向量存到featurelist</span></span><br></pre></td></tr></table></figure>
<p>转换数据集如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vec = DictVectorizer()  <span class="comment"># 将数据集转换为0和1的格式</span></span><br><span class="line">dummyX = vec.fit_transform(featureList).toarray()  <span class="comment"># 对特征向量进行转换</span></span><br><span class="line">print(<span class="string">"dummyX: "</span> + str(dummyX))</span><br><span class="line">print(vec.get_feature_names())</span><br><span class="line">print(<span class="string">"labelList: "</span> + str(labelList))</span><br><span class="line">lb = preprocessing.LabelBinarizer()  <span class="comment"># 对标记进行转换</span></span><br><span class="line">dummyY = lb.fit_transform(labelList)</span><br><span class="line">print(<span class="string">"dummyY: "</span> + str(dummyY))</span><br></pre></td></tr></table></figure>
<p>到此为止我们的数据已经准好了，下面就可以开始训练数据，然后建立模型了。建立决策树模型的方法非常简单，直接调用函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)  <span class="comment"># entropy(使用信息熵)</span></span><br><span class="line">clf = clf.fit(dummyX, dummyY)  <span class="comment"># 建模</span></span><br><span class="line">print(<span class="string">"clf: "</span> + str(clf))</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"allElectronicInformationGainOri.dot"</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:  <span class="comment"># save clf to dot</span></span><br><span class="line">    f = tree.export_graphviz(clf, feature_names=vec.get_feature_names(), out_file=f)</span><br></pre></td></tr></table></figure>
<p>上面的代码执行之后，决策树模型就建立好了，并且已经保存在<code>allElectronicInformationGainOri.dot</code>文件中了。怎么来进行预测呢？代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">oneRowX = dummyX[<span class="number">0</span>, :]</span><br><span class="line">print(<span class="string">"oneRowX: "</span> + str(oneRowX))</span><br><span class="line">newRowX = oneRowX</span><br><span class="line">newRowX[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">newRowX[<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">print(<span class="string">"newRowX: "</span> + str(newRowX))</span><br><span class="line">predictedY = clf.predict(newRowX.reshape(<span class="number">1</span>, <span class="number">-1</span>))  <span class="comment"># 开始预测</span></span><br><span class="line">print(<span class="string">"predictedY: "</span> + str(predictedY))  <span class="comment"># 打印预测结果</span></span><br></pre></td></tr></table></figure>
<p>到此为止，基本的决策树应用就算完成了。其实在建模部分，有一些参数是需要调整的，要根据正确率，不断地进行调整。如果特征值比较多的话，还有可能牵扯到降维，那就比较复杂了。<br>&emsp;&emsp;完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Read in the csv file and put features into list of dict and list of class label</span></span><br><span class="line">allElectronicsData = open(<span class="string">r'test.csv'</span>, <span class="string">'rt'</span>)</span><br><span class="line">reader = csv.reader(allElectronicsData)</span><br><span class="line">headers = next(reader)</span><br><span class="line">print(headers)</span><br><span class="line">​</span><br><span class="line">featureList = []</span><br><span class="line">labelList = []</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">    labelList.append(row[len(row) - <span class="number">1</span>])</span><br><span class="line">    rowDict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(row) - <span class="number">1</span>):</span><br><span class="line">        rowDict[headers[i]] = row[i]</span><br><span class="line">    featureList.append(rowDict)</span><br><span class="line">​</span><br><span class="line">print(featureList)</span><br><span class="line">​</span><br><span class="line">vec = DictVectorizer()  <span class="comment"># Vetorize features</span></span><br><span class="line">dummyX = vec.fit_transform(featureList).toarray()</span><br><span class="line">​</span><br><span class="line">print(<span class="string">"dummyX: "</span> + str(dummyX))</span><br><span class="line">print(vec.get_feature_names())</span><br><span class="line">​</span><br><span class="line">print(<span class="string">"labelList: "</span> + str(labelList))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># vectorize class labels</span></span><br><span class="line">lb = preprocessing.LabelBinarizer()</span><br><span class="line">dummyY = lb.fit_transform(labelList)</span><br><span class="line">print(<span class="string">"dummyY: "</span> + str(dummyY))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Using decision tree for classification</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line">clf = clf.fit(dummyX, dummyY)</span><br><span class="line">print(<span class="string">"clf: "</span> + str(clf))</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"allElectronicInformationGainOri.dot"</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:  <span class="comment"># Visualize model</span></span><br><span class="line">    f = tree.export_graphviz(clf, feature_names=vec.get_feature_names(), out_file=f)</span><br><span class="line">​</span><br><span class="line">oneRowX = dummyX[<span class="number">0</span>, :]</span><br><span class="line">print(<span class="string">"oneRowX: "</span> + str(oneRowX))</span><br><span class="line">​</span><br><span class="line">newRowX = oneRowX</span><br><span class="line">newRowX[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">newRowX[<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">print(<span class="string">"newRowX: "</span> + str(newRowX))</span><br><span class="line">​</span><br><span class="line">predictedY = clf.predict(newRowX.reshape(<span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">print(<span class="string">"predictedY: "</span> + str(predictedY))</span><br></pre></td></tr></table></figure>
<p>使用<code>Graphviz</code>将生成的<code>allElectronicInformationGainOri.dot</code>文件转换为<code>pdf</code>文件：</p>
<p><img src="/2019/02/27/机器学习/决策树sklearn/2.png" height="247" width="319"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/25/CC2530和zigbee笔记/事件与消息/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/25/CC2530和zigbee笔记/事件与消息/" itemprop="url">事件与消息</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-25T18:11:31+08:00">
                2019-02-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>zigbee</code>事件有两类，即系统定义事件和用户定义事件。系统类事件是协议栈已定义好的，用户类事件是我们用户层面来定义的。<br>&emsp;&emsp;事件类号采用一个<code>16bit</code>的常量，使用独热码编码，独热码是只有一个<code>bit</code>为<code>1</code>，其他全为<code>0</code>的一种码制。采用独热码的优点是操作方便，可以使用简单的位操作指令实现，例如提取系统类事件用<code>events &amp; SYS_EVENT_MSG</code>，清除系统类事件用<code>events ^ SYS_EVENT_MSG</code>。由于事件类号是<code>16bit</code>，也就是说<code>zigbee</code>事件类只有<code>16</code>个，系统类事件<code>SYS_EVENT_MSG</code>使用<code>0x8000</code>，故自定义事件类只有<code>15</code>个，可采用<code>0x4000</code>至<code>0x0001</code>。<br>&emsp;&emsp;事件处理函数为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app_ProcessEvent ( byte task_id, UINT16 events );</span><br></pre></td></tr></table></figure>
<p>可见调用此函数将传递两个参数，即<code>task_id</code>任务号和<code>events</code>事件类号。事件处理函数首先根据事件类号来判断是何种类型事件，然后根据任务号得到消息指针<code>pMsg</code>，最后根据消息指针结构里的事件号<code>pMsg-&gt;event</code>来具体处理事件。<code>event</code>为<code>8bit</code>的常量，系统<code>event</code>在<code>ZComDef.h</code>里定义。<br>&emsp;&emsp;在<code>ZigBee</code>协议栈中，任何通信数据都是利用帧的格式来组织的，协议栈的每一层都有特定的帧结构。<code>AF</code>定义了两种帧类型，键值对(<code>Key Value Pair</code>，<code>KVP</code>)和报文(<code>Message</code>，<code>MSG</code>)。<code>KVP</code>消息主要用于传输一些较为简单的变量格式，但由于<code>Zigbee</code>的很多应用领域中的消息较为复杂并不适用<code>KVP</code>格式，因此<code>Zigbee</code>协议规划定义了<code>MSG</code>类型。<code>MSG</code>对数据格式不作要求，适合任何格式的数据传输，因此可以用于传送数据量大的消息。<br>&emsp;&emsp;下面对系统<code>event</code>说明(其中<code>AF_</code>代表应用框架，<code>ZDO_</code>代表设备对象)：</p>
<ul>
<li><code>AF_DATA_CONFIRM_CMD</code>：<code>Data confirmation</code>数据收到确认；<code>A</code>设备发送数据，<code>B</code>设备收到数据后将返回应答<code>ack</code>给<code>A</code>，<code>A</code>收到此<code>ack</code>将触发<code>AF_DATA_CONFIRM_CMD</code>事件。<code>AF_DataRequest</code>函数参数<code>options</code>如果设为<code>AF_ACK_REQUEST</code>，则要求对方设备回复<code>ACK</code>。</li>
<li><code>AF_INCOMING_MSG_CMD</code>：<code>Incoming MSG type message</code>收到报文类型的消息；<code>A</code>设备用<code>AF_DataRequest</code>函数发出报文消息，<code>B</code>设备收到报文消息将触发<code>AF_INCOMING_MSG_CMD</code>事件。</li>
<li><code>AF_INCOMING_KVP_CMD</code>：<code>Incoming KVP type message</code>收到键值对类型的消息。</li>
<li><code>AF_INCOMING_GRP_KVP_CMD</code>：<code>Incoming Group KVP type message</code>收到群键值对类型的消息。</li>
<li><code>KEY_CHANGE</code>：<code>Key Events</code>按键触发事件。</li>
<li><code>ZDO_NEW_DSTADDR</code>：<code>ZDO</code>终端获得新地址；匹配描述符请求(<code>Match Descriptor Request</code>)响应指示(例如自动匹配)；如果<code>A</code>设备加入绑定后，<code>A</code>设备触发<code>ZDO_NEW_DSTADDR</code>事件。</li>
<li><code>ZDO_STATE_CHANGE</code>：<code>ZDO</code>网络状态改变；当<code>A</code>设备的网络状态改变时，<code>A</code>就会触发此消息，例如<code>SampleApp_NwkState = DEV_INIT</code>也会触发。</li>
<li><code>ZDO_MATCH_DESC_RSP_SENT</code>：<code>ZDO</code>描述符匹配响应发送；<code>A</code>设备发送<code>ZDP_MatchDescReq</code>函数请求描述符匹配绑定，<code>B</code>设备收到后发送<code>ZDP_MatchDescRsp</code>函数响应，<code>B</code>设备发送响应函数后触发<code>ZDO_MATCH_DESC_RSP_SENT</code>事件。</li>
<li><code>ZDO_CB_MSG</code>：收到<code>ZDO</code>反馈消息；<code>A</code>设备用函数<code>ZDO_RegisterForZDOMsg</code>注册了特定消息，<code>A</code>设备才能用<code>ZDO_CB_MSG</code>消息接收解析此特定消息。应用绑定例子：<code>A</code>设备发送绑定请求，B设备收到后发送匹配响应，<code>A</code>设备收到响应触发<code>ZDO_CB_MSG</code>事件。</li>
</ul>
<hr>
<h3 id="ZStack的OSAL的事件-event-与消息-message"><a href="#ZStack的OSAL的事件-event-与消息-message" class="headerlink" title="ZStack的OSAL的事件(event)与消息(message)"></a>ZStack的OSAL的事件(event)与消息(message)</h3><p>&emsp;&emsp;在<code>zstack</code>中，有两种方式在<code>OSAL</code>的任务(<code>task</code>)中添加自定义的功能：事件(<code>event</code>)和消息(<code>message</code>)。</p>
<h3 id="事件"><a href="#事件" class="headerlink" title="事件"></a>事件</h3><p>&emsp;&emsp;事件是驱动任务去执行某些操作的条件，当系统产生了一个事件，将这个触发传递给相应的任务后，任务才能执行一个相应的操作。<code>OSAL</code>通过一个<code>16</code>位宽度的数组来管理事件，意味着<code>OSAL</code>最多可以支持<code>16</code>个事件，其中最高位(<code>0x08000</code>，<code>SYS_EVENT_MSG</code>)系统保留，用户可以使用的事件有<code>15</code>个。事件的使用很简单：</p>
<ol>
<li>需要找个地方定义事件的<code>ID</code>，实际上是指定该事件占用事件数组的哪个位。例如<code>#define MY_EVENT 0x02</code>，占用<code>bit1</code>。</li>
<li>在需要触发事件的地方调用函数<code>osal_set_event(task_id, event_flag)</code>。这个函数有两个参数，一个是接收事件任务的<code>ID</code>，另一个参数指定事件<code>ID</code>。</li>
<li>在相应任务的处理函数，检查该事件执行相应代码即可。</li>
<li>清除事件标识。</li>
</ol>
<p>&emsp;&emsp;一个<code>event</code>被调用的过程：</p>
<ol>
<li><code>main -&gt; osal_start_system</code>(在<code>ZMain.c</code>中)</li>
<li><code>osal_start_system -&gt; XX_event</code>(在<code>OSAL.c</code>中)</li>
</ol>
<p>&emsp;&emsp;第一步简单易懂，重要的是第二步的实现。在<code>osal_start_system</code>函数中有如下代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 调用该idx个任务的事件处理函数(函数指针指向的函数) */</span></span><br><span class="line">events = ( tasksArr[idx] ) ( idx, events );</span><br></pre></td></tr></table></figure>
<p>这一步并不是直接调用的，而是根据不同的参数(<code>idx</code>，<code>events</code>)来动态调用的。这里涉及到一个很重要的结构体<code>tasksArr</code>数组：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> pTaskEventHandlerFn tasksArr[] = &#123;</span><br><span class="line">    macEventLoop,</span><br><span class="line">    nwk_event_loop,</span><br><span class="line">    Hal_ProcessEvent,</span><br><span class="line">#<span class="keyword">if</span> defined( MT_TASK )</span><br><span class="line">    MT_ProcessEvent,</span><br><span class="line">#endif</span><br><span class="line">    APS_event_loop,</span><br><span class="line">    ZDApp_event_loop,</span><br><span class="line">    ControlWifi_ProcessEvent,</span><br><span class="line">    GenericApp_ProcessEvent</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这时候回到之前的那个语句<code>events = (tasksArr[idx])(idx, events)</code>。可以看出，这个语句是调用<code>tasksArr</code>中第<code>idx</code>个函数，然后把返回值赋给<code>events</code>(<code>events</code>是若干个标志位，每个标志位代表一个事件的发生与否)。至于调用的是哪个函数，就跟参数(<code>idx</code>，<code>events</code>)有关了。<br>&emsp;&emsp;<code>events</code>里对应的标志位决定着相应的事件处理函数被调用，只要某一个事件的标志位变成了<code>1</code>，处理函数就被调用。那么什么时候这个标志位会从<code>0</code>变成<code>1</code>呢？以下代码显示了这些标志位都被初始化为<code>0</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">osalInitTasks</span> <span class="params">( <span class="keyword">void</span> )</span> </span>&#123;</span><br><span class="line">    uint8 taskID = <span class="number">0</span>;</span><br><span class="line">    tasksEvents = ( uint16 * ) osal_mem_alloc ( <span class="keyword">sizeof</span> ( uint16 ) * tasksCnt );</span><br><span class="line">    osal_memset ( tasksEvents, <span class="number">0</span>, ( <span class="keyword">sizeof</span> ( uint16 ) * tasksCnt ) );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从<code>0</code>变为<code>1</code>是由<code>osal_set_event</code>完成的，你想要在哪些动作完成后触发这个事件，就调用这个函数，将相应的标志位置<code>1</code>，那么在之后协议栈运行的过程中就会根据需要调用这个事件的处理函数了。如果是消息的话，使用<code>osal_msg_send</code>可以完成类似的功能。</p>
<h3 id="消息"><a href="#消息" class="headerlink" title="消息"></a>消息</h3><p>&emsp;&emsp;消息可以理解为带有附加信息的事件。最典型的一类便是按键消息，它同时产生了一个哪个按键被按下了附加信息。所以在<code>OnBoard_SendKeys</code>这个函数中，不仅向<code>GenericApp</code>发送了事件，还通过调用<code>osal_msg_send</code>函数向<code>GenericApp</code>发送了一个消息，这个消息记录了这个事件的附加信息。一般来说，一个消息总是和一个事件对应。当协议栈接收到消息后，在<code>ProcessEvent</code>函数中有以下语句：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ( events &amp;SYS_EVENT_MSG ) &#123;</span><br><span class="line">    MSGpkt = ( afIncomingMSGPacket_t * ) osal_msg_receive ( GenericApp_TaskID );</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">while</span> ( MSGpkt ) &#123;</span><br><span class="line">        <span class="keyword">switch</span> ( MSGpkt-&gt;hdr.event ) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="comment">/* 消息 */</span></span><br></pre></td></tr></table></figure>
<p>可以看出，消息是被当作系统事件接收的，接收到之后会找到消息对应的<code>event</code>，之后进行相应的处理。消息的使用与事件类似，但是使用了不同的函数触发：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">byte <span class="title">osal_msg_send</span> <span class="params">( byte destination_task, byte *msg_ptr )</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数有两个参数，分别为接收事件任务的<code>ID</code>，另一个是指向消息的指针。它的功能是向一个任务发送命令或数据消息，此外这个函数也会触发目标任务的<code>SYS_EVENT_MSG</code>(系统消息任务)。<br>&emsp;&emsp;首先我们需要明确几个概念，<code>msg</code>是用来干什么的？<code>msg</code>和<code>event</code>有什么不同？我们已经知道，<code>event</code>是一个事件，当这个事件发生以后，会触发相应的事件处理函数。即<code>event</code>是事先定义好的，但不知道会在哪个确定时间点被触发。而消息不同，顾名思义，消息是用来传递信息的，即有两个主体(如下图中的<code>task1</code>和<code>task2</code>)，在这两个主体想要通信的时候，就会用到消息：</p>
<p><img src="/2019/02/25/CC2530和zigbee笔记/事件与消息/1.png" height="186" width="252"></p>
<h3 id="osal-msg-allocate"><a href="#osal-msg-allocate" class="headerlink" title="osal_msg_allocate"></a>osal_msg_allocate</h3><p>&emsp;&emsp;可以看出，<code>task1</code>想要给<code>task2</code>发送消息(这两个任务可能属于一个设备，也可能属于不同的设备，这一点稍后再说)，于是<code>task1</code>就得先产生一个<code>msg</code>，这时候就要用到<code>osal_msg_allocation</code>给这个消息分配一个缓存：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">uint8 *<span class="title">osal_msg_allocate</span> <span class="params">( uint16 len )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">osal_msg_hdr_t</span> *hdr;</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( len == <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="keyword">return</span> ( <span class="literal">NULL</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    hdr = ( <span class="keyword">osal_msg_hdr_t</span> * ) osal_mem_alloc ( ( <span class="keyword">short</span> ) ( len + <span class="keyword">sizeof</span> ( <span class="keyword">osal_msg_hdr_t</span> ) ) );</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( hdr ) &#123;</span><br><span class="line">        hdr-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">        hdr-&gt;len = len;</span><br><span class="line">        hdr-&gt;dest_id = TASK_NO_TASK;</span><br><span class="line">        <span class="keyword">return</span> ( ( uint8 * ) ( hdr + <span class="number">1</span> ) );</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ( <span class="literal">NULL</span> );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的参数<code>len</code>是<code>msg</code>的长度，那<code>osal_msg_hdr_t *hdr</code>又是什么呢？看一下<code>osal_msg_hdr_t</code>这个数据结构的定义：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="keyword">void</span>   *next;</span><br><span class="line">    uint16 len;</span><br><span class="line">    uint8  dest_id;</span><br><span class="line">&#125; <span class="keyword">osal_msg_hdr_t</span>;</span><br></pre></td></tr></table></figure>
<p>这个实际上是消息的头部。再看下一句：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdr = ( <span class="keyword">osal_msg_hdr_t</span> * ) osal_mem_alloc ( ( <span class="keyword">short</span> ) ( len + <span class="keyword">sizeof</span> ( <span class="keyword">osal_msg_hdr_t</span> ) ) );</span><br></pre></td></tr></table></figure>
<p>这句代码是给消息分配缓存区的，而缓存区的大小是<code>len + sizeof(osal_msg_hdr_t)</code>，也就是消息的大小加上消息头的大小。<br>&emsp;&emsp;到这里我们得出结论：一个消息应该是由两部分组成，即消息头和消息的实际内容，消息头是协议栈定义好的，而消息的内容则应该是我们自己添加的。再关注一下<code>osal_msg_allocate</code>的代码，可以看出除了分配缓存以外，它还赋给了消息头一个初始值，但是却没有对消息本身做什么处理。因为消息是要留给大家自己定义的，所以<code>osal_msg_allocate</code>将消息头<code>hdr</code>下一位的指针做为函数的返回值，以便添加自己的消息代码。<br>&emsp;&emsp;至于消息的具体定义，可以分为两种：系统消息和用户消息。系统消息就拜托协议栈来完成，而用户消息就需要大家根据实际要求动手来写。在接下来的内容中，完整的消息被认为是已经定义好的，直接拿来用就可以了。</p>
<h3 id="osal-msg-send"><a href="#osal-msg-send" class="headerlink" title="osal_msg_send"></a>osal_msg_send</h3><p>&emsp;&emsp;<code>send</code>函数有两个参数，一个是目标<code>task</code>的地址，另一个是指向消息的指针。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">uint8 <span class="title">osal_msg_send</span> <span class="params">( uint8 destination_task, uint8 *msg_ptr )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ( msg_ptr == <span class="literal">NULL</span> ) &#123;</span><br><span class="line">        <span class="keyword">return</span> ( INVALID_MSG_POINTER );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( destination_task &gt;= tasksCnt ) &#123;</span><br><span class="line">        osal_msg_deallocate ( msg_ptr );</span><br><span class="line">        <span class="keyword">return</span> ( INVALID_TASK );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* Check the message header */</span></span><br><span class="line">    <span class="keyword">if</span> ( OSAL_MSG_NEXT ( msg_ptr ) != <span class="literal">NULL</span> ||</span><br><span class="line">         OSAL_MSG_ID ( msg_ptr ) != TASK_NO_TASK ) &#123;</span><br><span class="line">        osal_msg_deallocate ( msg_ptr );</span><br><span class="line">        <span class="keyword">return</span> ( INVALID_MSG_POINTER );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    OSAL_MSG_ID ( msg_ptr ) = destination_task;</span><br><span class="line">    osal_msg_enqueue ( &amp;osal_qHead, msg_ptr ); <span class="comment">/* queue message */</span></span><br><span class="line">    <span class="comment">/* Signal the task that a message is waiting */</span></span><br><span class="line">    osal_set_event ( destination_task, SYS_EVENT_MSG );</span><br><span class="line">    <span class="keyword">return</span> ( SUCCESS );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>osal_msg_enqueue</code>这句看起来是入栈操作，其实就是这样的。协议栈在内部维护着一个消息队列，也就是说消息是按照队列的方式被操作的，所以就有了入栈出栈的内容。这里只要明白发送的消息被放到(应该是目标任务的)消息队列里就好了。再看一下<code>osal_set_event( destination_task, SYS_EVENT_MSG );</code>，它告诉我们，在发送了一个消息之后，会触发目标协议栈的系统事件<code>SYS_EVENT_MSG</code>。也就是说，虽然消息的处理方式与事件类似，但是有关消息的事件是系统事件。</p>
<h3 id="osal-msg-receive"><a href="#osal-msg-receive" class="headerlink" title="osal_msg_receive"></a>osal_msg_receive</h3><p>&emsp;&emsp;<code>osal_msg_receive</code>又是一个重量级的<code>API</code>，这个<code>API</code>关系到我们怎么取出消息。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">uint16 <span class="title">SampleApp_ProcessEvent</span> <span class="params">( uint8 task_id, uint16 events )</span> </span>&#123;</span><br><span class="line">    afIncomingMSGPacket_t *MSGpkt;</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( events &amp; SYS_EVENT_MSG ) &#123;</span><br><span class="line">        MSGpkt = ( afIncomingMSGPacket_t * ) osal_msg_receive ( SampleApp_TaskID );</span><br><span class="line">​</span><br><span class="line">        <span class="keyword">while</span> ( MSGpkt ) &#123;</span><br><span class="line">            <span class="keyword">switch</span> ( MSGpkt-&gt;hdr.event ) &#123;</span><br><span class="line">                <span class="keyword">case</span> KEY_CHANGE: <span class="comment">/* Received when a key is pressed */</span></span><br><span class="line">                    SampleApp_HandleKeys ( ( ( keyChange_t * ) MSGpkt )-&gt;state, ( ( keyChange_t * ) MSGpkt )-&gt;keys );</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">​</span><br><span class="line">                <span class="keyword">case</span> AF_INCOMING_MSG_CMD: <span class="comment">/* Received when a messages is received (OTA) for this endpoint */</span></span><br><span class="line">                    SampleApp_MessageMSGCB ( MSGpkt );</span><br><span class="line">                    <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>在上面的函数中，首先定义了一个数据结构<code>afIncomingMSGPacket_t *MSGpkt</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="keyword">osal_event_hdr_t</span> hdr;     <span class="comment">/* OSAL Message header */</span></span><br><span class="line">    uint16 groupId;           <span class="comment">/* Message's group ID - 0 if not set */</span></span><br><span class="line">    uint16 clusterId;         <span class="comment">/* Message's cluster ID */</span></span><br><span class="line">    <span class="comment">/* Source Address, if endpoint is STUBAPS_INTER_PAN_EP,</span></span><br><span class="line"><span class="comment">       it's an InterPAN message */</span></span><br><span class="line">    afAddrType_t srcAddr;</span><br><span class="line">    uint16 macDestAddr;       <span class="comment">/* MAC header destination short address */</span></span><br><span class="line">    uint8 endPoint;           <span class="comment">/* destination endpoint */</span></span><br><span class="line">    uint8 wasBroadcast;       <span class="comment">/* TRUE if network destination was a broadcast address */</span></span><br><span class="line">    uint8 LinkQuality;        <span class="comment">/* The link quality of the received data frame */</span></span><br><span class="line">    uint8 correlation;        <span class="comment">/* The raw correlation value of the received data frame */</span></span><br><span class="line">    int8  rssi;               <span class="comment">/* The received RF power in units dBm */</span></span><br><span class="line">    uint8 SecurityUse;        <span class="comment">/* deprecated */</span></span><br><span class="line">    uint32 timestamp;         <span class="comment">/* receipt timestamp from MAC */</span></span><br><span class="line">    afMSGCommandFormat_t cmd; <span class="comment">/* Application Data */</span></span><br><span class="line">&#125; afIncomingMSGPacket_t;</span><br></pre></td></tr></table></figure>
<p>该结构体以<code>af_</code>开头，说明这个数据是经过无线传输后收到的，也就是由另一个设备上的<code>task</code>发送给本设备的。再来看一下<code>osal_event_hdr_t</code>结构体：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    uint8 event;</span><br><span class="line">    uint8 status;</span><br><span class="line">&#125; <span class="keyword">osal_event_hdr_t</span>;</span><br></pre></td></tr></table></figure>
<p>只包括了一个事件发生的标志(<code>event</code>)和状态(<code>status</code>)，它是用来找到事件对应的消息的。<br>&emsp;&emsp;前面说过，在发送一个消息之后<code>osal_mem_send</code>，会触发一个系统事件。我们现在遇到的情况是，有了这个事件之后，怎么找到它对应的消息？这时就需要上面介绍的机制了，定义一个指向<code>osal_event_hdr_t</code>类型的指针，然后用<code>osal_msg_receive</code>找到这个消息并让前面的指针<code>MSGpkt</code>指向它。在代码给出的例子中，指向消息的指针类型是<code>afIncomingMSGPacket_t</code>。但是从刚才的分析可以看出，如果是在同一个设备中的两个<code>task</code>通信的话，也就是不需要无线传输的时候，用<code>osal_event_hdr_t</code>类型的指针就可以满足要求了。<br>&emsp;&emsp;现在我们既有事件又有消息，接下来就愉快地进入处理带有消息的事件部分。以<code>case AF_INCOMING_MSG_CMD</code>为例，只要以<code>MSGpkt</code>为参数，调用相应的处理函数就好了，即<code>SampleApp_MessageMSGCB(MSGpkt);</code>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/15/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span><a class="page-number" href="/page/17/">17</a><span class="space">&hellip;</span><a class="page-number" href="/page/96/">96</a><a class="extend next" rel="next" href="/page/17/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">958</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
