<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="泥腿子出身">
<meta property="og:url" content="http://fukangwei.gitee.io/page/23/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="泥腿子出身">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/23/">





  <title>泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/软件与硬件问题/ubuntu服务器/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/软件与硬件问题/ubuntu服务器/" itemprop="url">ubuntu服务器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T16:37:15+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="FTP服务器"><a href="#FTP服务器" class="headerlink" title="FTP服务器"></a>FTP服务器</h3><p>&emsp;&emsp;在<code>Linux</code>系统中，<code>ftp</code>服务器的全名叫<code>vsftpd</code>。我们需要利用相关命令来安装<code>ftp</code>服务器，然后在<code>vsftpd.conf</code>中进行相关配置。<br>&emsp;&emsp;1. 首先使用如下命令进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vsftpd</span><br></pre></td></tr></table></figure>
<p>安装完成后，输入<code>vsftpd -version</code>查看是否安装成功。<br>&emsp;&emsp;2. 新建一个用于<code>FTP</code>的工作目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/ftp</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. 新建<code>FTP</code>用户，并设置密码以及工作目录，其中<code>ftpname</code>是创建的用户名：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -d /home/ftp -s /bin/bash ftpname</span><br></pre></td></tr></table></figure>
<p>为新建的用户设置密码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd ftpname</span><br></pre></td></tr></table></figure>
<p>可以使用<code>cat /etc/passwd</code>查看当前系统用户。<br>&emsp;&emsp;4. 修改<code>vsftpd</code>配置文件，该文件位于<code>/etc</code>目录下，相关的配置如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">listen=YES                     <span class="comment"># 服务器监听</span></span><br><span class="line">anonymous_enable=NO            <span class="comment"># 匿名访问允许。此选项很危险，默认不要开启</span></span><br><span class="line">local_enable=YES               <span class="comment"># 本地主机访问允许</span></span><br><span class="line">write_enable=YES               <span class="comment"># 写允许</span></span><br><span class="line"><span class="comment"># anon_upload_enable=YES       # 匿名上传允许</span></span><br><span class="line"><span class="comment"># anon_mkdir_write_enable=YES  # 匿名创建文件夹允许</span></span><br><span class="line">dirmessage_enable=YES          <span class="comment"># 进入文件夹允许</span></span><br><span class="line">xferlog_enable=YES             <span class="comment"># ftp日志记录允许</span></span><br><span class="line">connect_from_port_20=YES       <span class="comment"># 允许使用端口号20作为数据传送的端口</span></span><br><span class="line">secure_chroot_dir=/var/run/vsftpd/empty</span><br><span class="line">pam_service_name=vsftpd</span><br><span class="line">rsa_cert_file=/etc/ssl/private/vsftpd.pem</span><br><span class="line">local_root=/home/ftp           <span class="comment"># 设置ftp的根目录为“/home/ftp”</span></span><br></pre></td></tr></table></figure>
<p>设置好之后，保存退出，然后使用如下命令重启<code>ftp</code>服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service vsftpd restart</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;5. 在浏览器中的地址栏输入<code>ftp://ftp服务器地址</code>，然后输入账号和密码即可实现登录。</p>
<h4 id="vsftp配置参数说明"><a href="#vsftp配置参数说明" class="headerlink" title="vsftp配置参数说明"></a>vsftp配置参数说明</h4><p>&emsp;&emsp;<code>vsftp</code>的各配置参数如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>anonymous_enable=YES</code></td>
<td>是否开启匿名用户</td>
</tr>
<tr>
<td><code>no_anon_password=YES</code></td>
<td>匿名用户<code>login</code>时不询问用户名和口令</td>
</tr>
<tr>
<td><code>anon_umask=077</code></td>
<td>匿名用户上传的文件权限</td>
</tr>
<tr>
<td><code>anon_upload_enable=YES</code></td>
<td>是否允许匿名用户上传文件</td>
</tr>
<tr>
<td><code>anon_mkdir_write_enable=YES</code></td>
<td>是否允许匿名用户建立目录</td>
</tr>
<tr>
<td><code>anon_other_write_enable=YES</code></td>
<td>是否允许匿名用户具有建立目录、上传之外的权限，如重命名、删除</td>
</tr>
<tr>
<td><code>anon_world_readable_only=YES</code></td>
<td>匿名登入者是否能下载可阅读的档案</td>
</tr>
<tr>
<td><code>anon_max_rate=80000</code></td>
<td>匿名用户的下载速度为<code>80KB/s</code></td>
</tr>
<tr>
<td><code>anon_root=(none)</code></td>
<td>匿名用户的宿主目录</td>
</tr>
<tr>
<td><code>allow_anon_ssl=NO</code></td>
<td>匿名用户是否允许使用安全的<code>SSL</code>连接服务器</td>
</tr>
<tr>
<td><code>ftp_username=FTP</code></td>
<td>定义匿名使用者登录的使用者名称(默认为<code>FTP</code>)</td>
</tr>
<tr>
<td><code>banned_email_file=/etc/vsftpd.banned_emails</code></td>
<td>禁止使用的匿名用户登陆时作为密码的电子邮件地址使用表的位置</td>
</tr>
<tr>
<td><code>deny_email_enable=NO</code></td>
<td>禁止使用的匿名用户登陆时作为密码的电子邮件地址</td>
</tr>
<tr>
<td><code>secure_email_list_enable</code></td>
<td>如果你想只接受以指定<code>E-MAIL</code>地址登录的匿名用户的话，启用它</td>
</tr>
<tr>
<td><code>local_enable=YES</code></td>
<td>本地用户是否可以登录</td>
</tr>
<tr>
<td><code>local_umask=022</code></td>
<td>设置本地用户的文件生成掩码</td>
</tr>
<tr>
<td><code>file_open_mode=0666</code></td>
<td>上传文件的权限配合<code>umask</code>使用</td>
</tr>
<tr>
<td><code>local_root=(none)</code></td>
<td>指定所有本地用户登陆后的目录，如果不设置此项，用户都会登陆于自己的主目录</td>
</tr>
<tr>
<td><code>local_max_rate=500000</code></td>
<td>本地用户的下载速度为<code>500KB/s</code></td>
</tr>
<tr>
<td><code>chroot_local_user=YES</code></td>
<td>是否允许用户离开其宿主目录</td>
</tr>
<tr>
<td><code>chroot_list_enable=NO</code></td>
<td>登录用户名字若在<code>/etc/vsftpd.chroot_list</code>内，则会启用<code>chroot</code>机制，将这个用户限制在其<code>home</code>目录下</td>
</tr>
<tr>
<td><code>guest_enable=YES</code></td>
<td>是否开启虚拟用户</td>
</tr>
<tr>
<td><code>guest_username=vsftpd</code></td>
<td>指定虚拟用户名</td>
</tr>
<tr>
<td><code>virtual_use_local_privs=YES</code></td>
<td>虚拟用户和本地用户权限是否相同</td>
</tr>
<tr>
<td><code>userlist_enable=YES</code></td>
<td>是否根据<code>user_list</code>实行访问控制(若启用此选项，<code>userlist_deny</code>选项才被启动)</td>
</tr>
<tr>
<td><code>userlist_deny=NO</code></td>
<td>若为<code>YES</code>，则<code>userlist_file</code>中的用户将不能登录；为<code>NO</code>，则只有<code>userlist_file</code>的用户可以登录</td>
</tr>
<tr>
<td><code>write_enable=YES</code></td>
<td>用户是否具有写的权限</td>
</tr>
<tr>
<td><code>download_enable=YES</code></td>
<td>是否允许下载</td>
</tr>
<tr>
<td><code>chmod_enable=YES</code></td>
<td>是否可以修改文件权限</td>
</tr>
<tr>
<td><code>nopriv_user= nobody</code></td>
<td>设定服务执行者为<code>nobody</code>，<code>vsftpd</code>推荐使用一个权限很低的用户，最好是没有家目录(<code>/dev/null</code>)，没有登陆<code>shell</code>(<code>/sbin/nologin</code>)，系统会更安全</td>
</tr>
<tr>
<td><code>dirmessage_enable=YES</code></td>
<td>当切换到<code>FTP</code>服务器的某个目录时，是否显示该目录下的<code>.message</code>信息</td>
</tr>
<tr>
<td><code>dirlist_enable=YES</code></td>
<td>是否启用通俗命令(如果设置为<code>NO</code>，那么只能使用<code>unix/linux</code>的命令)</td>
</tr>
<tr>
<td><code>xferlog_enable=YES</code></td>
<td>是否启用上传和下载日志</td>
</tr>
<tr>
<td><code>xferlog_std_format=YES</code></td>
<td>是否使用标准的<code>ftpd-xferlog</code>日志格式</td>
</tr>
<tr>
<td><code>xferlog_file=/var/log/vsftpd.log</code></td>
<td>将上传下载日志记录到<code>/var/log/vsftpd.log</code>中</td>
</tr>
<tr>
<td><code>log_ftp_protocol=NO</code></td>
<td>当<code>xferlog_std_format</code>关闭且本选项开启时，记录所有<code>ftp</code>请求和回复，当调试时比较有用</td>
</tr>
<tr>
<td><code>dual_log_enable=NO</code></td>
<td>如果启用，两个<code>LOG</code>文件会各自产生，默认的是<code>/var/log/xferlog</code>和<code>/var/log/vsftpd.log</code></td>
</tr>
<tr>
<td><code>vsftpd_log_file=/var/log/vsftpd.log</code></td>
<td>这是被生成的<code>vsftpd</code>格式的<code>log</code>文件的名字(只有<code>xferlog_enable</code>被设置，而<code>xferlog_std_format</code>没有被设置时，此项才生效)</td>
</tr>
<tr>
<td><code>syslog_enable=NO</code></td>
<td>如果启用，系统<code>log</code>将取代<code>vsftpd</code>的<code>log</code>输出到<code>/var/log/vsftpd.log</code>，<code>FTPD</code>的<code>log</code>工具将不工作</td>
</tr>
<tr>
<td><code>connect_from_port_20=YES</code></td>
<td>是否启用<code>FTP</code>数据端口的连接请求</td>
</tr>
<tr>
<td><code>listen=YES</code></td>
<td>是否使用<code>standalone</code>启动<code>vsftpd</code>，而不是<code>super daemon(xinetd)</code>控制它(<code>vsftpd</code>推荐使用<code>standalone</code>方式)</td>
</tr>
<tr>
<td><code>listen_ipv6=NO</code></td>
<td>与<code>listen</code>功能相同，但此项监听<code>IPV6</code>(两个只能设置一个)</td>
</tr>
<tr>
<td><code>pam_service_name=vsftpd</code></td>
<td><code>PAM</code>认证服务配置文件名称，保存在<code>/etc/pam.d</code>目录下</td>
</tr>
<tr>
<td><code>userlist_enable=YES</code></td>
<td>是否检查<code>userlist_file</code>设置文件</td>
</tr>
<tr>
<td><code>tcp_wrappers=YES</code></td>
<td>是否使用<code>tcp_wrappers</code>作为主机访问控制方式(<code>tcp_wrappers</code>的两个配置文件<code>/etc/hosts.allow</code>允许访问的主机和<code>/etc/hosts.deny</code>拒绝访问的主机</td>
</tr>
<tr>
<td><code>ftpd_banner=Welcome to yayi.biz FTP Service</code></td>
<td><code>FTP</code>欢迎信息(如果设置了<code>banner_file</code>，则此设置无效)</td>
</tr>
<tr>
<td><code>banner_file=/etc/vsftpd/banner</code></td>
<td>定义登录信息文件的位置</td>
</tr>
<tr>
<td><code>check_shell=NO</code></td>
<td>是否检测<code>SHELL</code></td>
</tr>
<tr>
<td><code>chown_uploads=YES</code></td>
<td>是否开启匿名上传用户切换(如果开启，上传用户则变为<code>chown_username=daemon</code>指定的用户)</td>
</tr>
<tr>
<td><code>chown_username=daemon</code></td>
<td>匿名上传文件的属主</td>
</tr>
<tr>
<td><code>file_open_mode=0666</code></td>
<td>对于上传的文件设定权限</td>
</tr>
<tr>
<td><code>idle_session_timeout=600</code></td>
<td>客户端超过<code>600s</code>没有动作则视为超时</td>
</tr>
<tr>
<td><code>data_connection_timeout=300</code></td>
<td>数据传输时超过<code>300s</code>没有动作则视为超时</td>
</tr>
<tr>
<td><code>connect_timeout=60</code></td>
<td>连接超时时间</td>
</tr>
<tr>
<td><code>pasv_enable=YES</code></td>
<td>是否允许使用<code>PASV</code>模式</td>
</tr>
<tr>
<td><code>pasv_promiscuous+NO</code></td>
<td>是否关闭<code>PASV</code>安全检查(删除<code>+NO</code>则开启)</td>
</tr>
<tr>
<td><code>pasv_address=(none)</code></td>
<td>使<code>vsftpd</code>在<code>pasv</code>命令回复时跳转到指定的<code>IP</code>地址</td>
</tr>
<tr>
<td><code>port_enable=YES</code></td>
<td>是否允许使用<code>PORT</code>模式</td>
</tr>
<tr>
<td><code>prot_promiscuous</code></td>
<td>是否开启安全<code>PORT</code>检查(<code>+NO</code>则不开启)</td>
</tr>
<tr>
<td><code>pasv_max_port=0</code></td>
<td>指定为被动模式数据连接分配的最大端口(<code>0</code>为任何)</td>
</tr>
<tr>
<td><code>pasv_min_port=0</code></td>
<td>指定为被动模式数据连接分配的最小端口(<code>0</code>为任何)</td>
</tr>
<tr>
<td><code>ACCEPT_TIMEOUT=60</code></td>
<td><code>PAVS</code>请求<code>60s</code>无响应则视为超时</td>
</tr>
<tr>
<td><code>ascii_upload_enable=YES</code></td>
<td>是否可用<code>ASCII</code>模式上传(默认为<code>NO</code>)</td>
</tr>
<tr>
<td><code>ascii_download_enable=YES</code></td>
<td>是否可用<code>ASCII</code>模式下载(默认为<code>NO</code>)</td>
</tr>
<tr>
<td><code>secure_chroot_dir=/usr/share/empty</code></td>
<td>这个选项必须指定一个空的数据夹，且任何登入者都不能有写入的权限，当<code>vsftpd</code>不需要<code>file system</code>的权限时，就会将使用者限制在此数据夹中，默认值为<code>/usr/share/empty</code></td>
</tr>
<tr>
<td><code>one_process_model=YES</code></td>
<td>是否使用单进程模式</td>
</tr>
<tr>
<td><code>text_userdb_names=NO</code></td>
<td>是否可以查看文件拥有者的<code>UID</code></td>
</tr>
<tr>
<td><code>use_localtime=NO</code></td>
<td>显示目录清单时是用本地时间还是<code>GMT</code>时间，可以通过<code>mdtm</code>命令来达到一样的效果</td>
</tr>
<tr>
<td><code>use_sendfile=YES</code></td>
<td>是否测试平台优化</td>
</tr>
<tr>
<td><code>setproctitle_enable=YES</code></td>
<td>是否显示状态会话信息</td>
</tr>
<tr>
<td><code>user_config_dir=/etc/vsftpd/userconf</code></td>
<td>定义用户配置文件的目录</td>
</tr>
<tr>
<td><code>local_root=xxx</code></td>
<td>定义本地用户登陆的根目录，注意定义根目录可以是相对路径也可以是绝对路径。相对路径是针对用户家目录来说的</td>
</tr>
<tr>
<td><code>max_clients=0</code></td>
<td>可接受的最大<code>client</code>数目(<code>0</code>为不限制)</td>
</tr>
<tr>
<td><code>max_per_ip=0</code></td>
<td>每个<code>ip</code>的最大<code>client</code>数目(<code>0</code>为不限制)</td>
</tr>
<tr>
<td><code>connect_from_port_20=YES</code></td>
<td>是否启用<code>FTP</code>数据端口的数据连接</td>
</tr>
<tr>
<td><code>ftp_data_port=20</code></td>
<td>设定<code>PORT</code>模式下的连接端口(只要<code>connect_from_port_20</code>被激活)</td>
</tr>
<tr>
<td><code>listen_address=192.168.0.2</code></td>
<td>绑定<code>FTP</code>的<code>IP</code>地址(在多网卡或者多<code>IP</code>地址的机器上使用)</td>
</tr>
<tr>
<td><code>listen_port=2121</code></td>
<td>绑定<code>FTP</code>使用使用端口</td>
</tr>
<tr>
<td><code>ftp_data_port=2020</code></td>
<td>绑定<code>FTP</code>数据传输端口</td>
</tr>
<tr>
<td><code>background=NO</code></td>
<td>启用时，<code>VSFTPD</code>将把监听进程置于后台；但访问<code>VSFTPD</code>时，控制台将立即被返回到<code>SHELL</code></td>
</tr>
<tr>
<td><code>force_dot_files=NO</code></td>
<td>如果激活，以<code>.</code>开始的文件和目录在目录列取的时候将会被显示，即使客户端没有使用<code>a</code>标识。这不包括<code>.</code>和<code>..</code>目录</td>
</tr>
<tr>
<td><code>ssl_enable=NO</code></td>
<td>是否启用<code>SSL</code></td>
</tr>
<tr>
<td><code>force_local_data_ssl=YES</code></td>
<td>是否要求非匿名用户使用安全的<code>SSL</code>在数据线路上收发数据</td>
</tr>
<tr>
<td><code>force_local_logins_ssl=YES</code></td>
<td>是否要求非匿名用户使用安全的<code>SSL</code>登录以发送密码</td>
</tr>
<tr>
<td><code>ssl_tlsv1=YES</code></td>
<td>是否允许以<code>TLS V1</code>协议的连接，<code>TLS V1</code>连接将是首选</td>
</tr>
<tr>
<td><code>ssl_sslv2=NO</code></td>
<td>是否允许以<code>SSL V2</code>协议的连接，<code>TLS V1</code>连接将是首选</td>
</tr>
<tr>
<td><code>ssl_sslv3=NO</code></td>
<td>是否允许以<code>SSL V3</code>协议的连接，<code>TLS V1</code>连接将是首选</td>
</tr>
<tr>
<td><code>session_support=NO</code></td>
<td>是否让<code>VSFTPD</code>去尝试管理登录会话</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;<strong>补充说明</strong>：<br>&emsp;&emsp;1. <code>anonymous_enable=NO</code>必须要打开，不能注释。<br>&emsp;&emsp;2. <code>ftp</code>服务器的根目录可以是多级目录，如<code>/home/fukangwei/ftp</code>。<br>&emsp;&emsp;3. 要设置好<code>ftp</code>服务器的根目录权限，否则上传文件时会出现<code>553 Could not create file</code>错误。</p>
<hr>
<h3 id="tftp服务器"><a href="#tftp服务器" class="headerlink" title="tftp服务器"></a>tftp服务器</h3><p>&emsp;&emsp;1. 安装<code>xinetd</code>、<code>tftp-hpa</code>和<code>tftpd-hpa</code>(<code>tftp-hpa</code>是客户端，<code>tftpd-hpa</code>是服务器端)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install xinetd</span><br><span class="line">sudo apt-get install tftp-hpa tftpd-hpa</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;2. 配置<code>TFTP</code>服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/default/tftpd-hpa</span><br></pre></td></tr></table></figure>
<p>将原来的内容改为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TFTP_USERNAME=<span class="string">"tftp"</span></span><br><span class="line">TFTP_ADDRESS=<span class="string">"0.0.0.0:69"</span></span><br><span class="line">TFTP_DIRECTORY=<span class="string">"tftp根目录"</span>  <span class="comment"># 服务器目录，需要设置权限为777</span></span><br><span class="line">TFTP_OPTIONS=<span class="string">"-l -c -s"</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. 重新启动<code>TFTP</code>服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service tftpd-hpa restart</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="ssh服务器"><a href="#ssh服务器" class="headerlink" title="ssh服务器"></a>ssh服务器</h3><p>&emsp;&emsp;1. 安装<code>ssh-server</code>和<code>ssh-client</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br><span class="line">sudo apt-get install openssh-client</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;2. 确认<code>sshserver</code>是否安装好：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -e | grep sshd</span><br></pre></td></tr></table></figure>
<p>如果看到<code>sshd</code>，那说明<code>ssh-server</code>已经启动了；如果只有<code>ssh-agent</code>，说明<code>ssh-server</code>还没有启动，需要执行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh start</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. <code>SSH</code>默认服务端口为<code>22</code>，用户可以自定义成其他端口，需要修改的配置文件为<code>/etc/ssh/sshd_config</code>，把里面的<code>Port</code>参数修改成其他数组即可，然后重启<code>SSH</code>服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh restart</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="VNC服务器"><a href="#VNC服务器" class="headerlink" title="VNC服务器"></a>VNC服务器</h3><p>&emsp;&emsp;在<code>ubuntu</code>上安装并运行<code>VNC</code>服务器，推荐使用<code>tightvnc</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install tightvncserver</span><br></pre></td></tr></table></figure>
<p>安装完成之后，使用如下命令运行<code>tightvnc</code>服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vncserver :1</span><br></pre></td></tr></table></figure>
<p>第一次运行时需要你设置一个密码，该密码和系统用户密码无关。<br>&emsp;&emsp;在<code>Windows</code>上安装<code>vnc viewer</code>，下载地址为<code>http://www.realvnc.com/</code>。启动<code>vnc viewer</code>，在<code>VNC Server</code>上填入<code>VNC服务器IP地址:1</code>，点击<code>Connect</code>，输入<code>vnc</code>密码即可。</p>
<hr>
<h3 id="sftp服务器"><a href="#sftp服务器" class="headerlink" title="sftp服务器"></a>sftp服务器</h3><p>&emsp;&emsp;<code>sftp</code>服务器使用如下命令进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>sftp</code>是<code>Secure File Transfer Protocol</code>的缩写，意思是安全文件传送协议，可以为传输文件提供一种安全的加密方法，与<code>ftp</code>有着几乎一样的语法和功能。<code>SFTP</code>本身没有单独的守护进程，它必须使用<code>sshd</code>守护进程(端口号默认是<code>22</code>)来完成相应的连接操作。所以从某种意义上来说，<code>SFTP</code>并不像一个服务器程序，而更像是一个客户端程序。<code>SFTP</code>使用加密传输认证信息和数据，所以是非常安全的。但由于这种传输方式使用了加密和解密技术，所以传输效率比普通的FTP要低得多。常用的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sftp user@host                      <span class="comment"># 登陆远程主机</span></span><br><span class="line">lcd或lpwd                           <span class="comment"># 针对本机的命令都加上“l”</span></span><br><span class="line">put filename.txt directory          <span class="comment"># 将本机文件上传到远程</span></span><br><span class="line">mput *.*                            <span class="comment"># 将当前文件夹下的文件上传到远程</span></span><br><span class="line">get filename.file directory         <span class="comment"># 下载远程文件到本地</span></span><br><span class="line">mget *.* directory                  <span class="comment"># 下载目录下所有远程文件到本地</span></span><br><span class="line">?                                   <span class="comment"># 帮助</span></span><br><span class="line"><span class="built_in">bye</span>、<span class="built_in">exit</span>或quit                     <span class="comment"># 退出</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/深度学习/TensorFlow之nn的API/" itemprop="url">TensorFlow之nn的API</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T11:02:43+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>&emsp;&emsp;在神经网络中，我们有很多的非线性函数来作为激活函数，比如连续的平滑非线性函数(<code>sigmoid</code>、<code>tanh</code>和<code>softplus</code>)，连续但不平滑的非线性函数(<code>relu</code>、<code>relu6</code>和<code>relu_x</code>)和随机正则化函数(<code>dropout</code>)。所有的激活函数都是单独应用在每个元素上面的，并且输出张量的维度和输入张量的维度一样。<br>&emsp;&emsp;<code>relu</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>relu</code>，即<code>max(features, 0)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.relu(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[0. 2.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>和<code>int8</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>relu6</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu6(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>relu6</code>，即<code>min(max(features, 0), 6)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">12.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.relu6(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[0. 6.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是float、double、int32、int64、uint8、int16或者int8。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>softplus</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softplus(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>softplus</code>，即<code>log(exp(features) + 1)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">12.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.softplus(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[ 0.31326166 12.000006 ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>或者<code>int8</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>dropout</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(x, keep_prob, noise_shape = <span class="keyword">None</span>, seed = <span class="keyword">None</span>, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算神经网络层的<code>dropout</code>。<br>&emsp;&emsp;一个神经元将以概率<code>keep_prob</code>决定是否放电，如果不放电，那么该神经元的输出将是<code>0</code>，如果该神经元放电，那么该神经元的输出值将被放大到原来的<code>1 / keep_prob</code>倍。这里的放大操作是为了保持神经元输出总个数不变。比如，神经元的值为<code>[1, 2]</code>，<code>keep_prob</code>的值是<code>0.5</code>，并且是第一个神经元是放电的，第二个神经元不放电，那么神经元输出的结果是<code>[2, 0]</code>，也就是相当于，第一个神经元被当做了<code>1 / keep_prob</code>个输出，即<code>2</code>个。这样保证了总和<code>2</code>个神经元保持不变。<br>&emsp;&emsp;默认情况下，每个神经元是否放电是相互独立的。但是，如果<code>noise_shape</code>被修改了，那么它对于变量<code>x</code>就是一个广播形式，而且当且仅当<code>noise_shape[i] == shape(x)[i]</code>，<code>x</code>中的元素是相互独立的。比如，如果<code>shape(x) = [k, l, m, n], noise_shape = [k, 1, 1, n]</code>，那么每个批和通道都是相互独立的，但是每行和每列的数据都是关联的，即要不都为<code>0</code>，要不都还是原来的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[[-2.  0.  6.  0.]]</span></span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape=[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[[-2.  4.  6.  8.]]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>keep_prob</code>：一个<code>Python</code>的<code>float</code>类型，表示元素是否放电的概率。</li>
<li><code>noise_shape</code>：一个一维的<code>Tensor</code>，数据类型是<code>int32</code>，代表元素是否独立的标志。</li>
<li><code>seed</code>：一个<code>Python</code>的整数类型，设置随机种子。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和<code>x</code>相同。<br>&emsp;&emsp;<code>bias_add</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.bias_add(value, bias, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是将偏差项<code>bias</code>加到<code>value</code>上面。这个操作你可以看做是<code>tf.add</code>的一个特例，其中<code>bias</code>必须是一维的。该<code>API</code>支持广播形式，因此<code>value</code>可以有任何维度。但是，该<code>API</code>又不像<code>tf.add</code>，可以让<code>bias</code>的维度和<code>value</code>的最后一维不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">1.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">1.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.nn.bias_add(a, b)))</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line"><span class="comment"># 因为a最后一维的维度是2，但是c的维度是1，所以以下语句将发生错误</span></span><br><span class="line"><span class="comment"># print(sess.run(tf.nn.bias_add(a, c)))</span></span><br><span class="line">print(sess.run(tf.add(a, c)))  <span class="comment"># 但是tf.add可以正确运行</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int64</code>、<code>int32</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>或者<code>complex64</code>。</li>
<li><code>bias</code>：一个一维的<code>Tensor</code>，数据维度和<code>value</code>的最后一维相同，数据类型必须和<code>value</code>相同。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>sigmoid</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sigmoid(x, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的<code>sigmoid</code>函数，具体计算公式为<code>y = 1 / (1 + exp(-x))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.sigmoid(a)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7310586</span> <span class="number">0.880797</span> ]</span><br><span class="line"> [<span class="number">0.7310586</span> <span class="number">0.880797</span> ]</span><br><span class="line"> [<span class="number">0.7310586</span> <span class="number">0.880797</span> ]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int32</code>、<code>complex64</code>、<code>int64</code>或者<code>qint32</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，如果<code>x.dtype != qint32</code>，那么返回的数据类型和<code>x</code>相同，否则返回的数据类型是<code>quint8</code>。<br>&emsp;&emsp;<code>tanh</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.tanh(x, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的<code>tanh</code>函数。具体计算公式为<code>(exp(x) - exp(-x))/(exp(x) + exp(-x))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.tanh(a)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7615942</span> <span class="number">0.9640276</span>]</span><br><span class="line"> [<span class="number">0.7615942</span> <span class="number">0.9640276</span>]</span><br><span class="line"> [<span class="number">0.7615942</span> <span class="number">0.9640276</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int32</code>、<code>complex64</code>、<code>int64</code>或者<code>qint32</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，如果<code>x.dtype != qint32</code>，那么返回的数据类型和<code>x</code>相同，否则返回的数据类型是<code>quint8</code>。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>&emsp;&emsp;卷积操作是使用一个二维的卷积核在一个批处理的图片上进行不断扫描。具体操作是将一个卷积核在每张图片上按照一个合适的尺寸在每个通道上面进行扫描。为了达到好的卷积效率，需要在不同的通道和不同的卷积核之间进行权衡。</p>
<ul>
<li><code>conv2d</code>：任意的卷积核，能同时在不同的通道上面进行卷积操作。</li>
<li><code>depthwise_conv2d</code>：卷积核能相互独立的在自己的通道上面进行卷积操作。</li>
<li><code>separable_conv2d</code>：在纵深卷积(<code>depthwise filter</code>)之后进行逐点卷积(<code>separable filter</code>)。</li>
</ul>
<p>注意，虽然这些操作被称之为<code>卷积</code>，但是严格的说，他们只是互相关，因为卷积核没有做一个逆向的卷积过程。<br>&emsp;&emsp;卷积核的卷积过程是按照<code>strides</code>参数来确定的，比如<code>strides = [1, 1, 1, 1]</code>表示卷积核对每个像素点进行卷积，即在二维屏幕上面，两个轴方向的步长都是<code>1</code>。<code>strides = [1, 2, 2, 1]</code>表示卷积核对每隔一个像素点进行卷积，即在二维屏幕上面，两个轴方向的步长都是<code>2</code>。<br>&emsp;&emsp;<code>conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是对一个四维的输入数据<code>input</code>和四维的卷积核<code>filter</code>进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果。<br>&emsp;&emsp;给定的输入张量的维度是<code>[batch, in_height, in_width, in_channels]</code>，卷积核张量的维度是<code>[filter_height, filter_width, in_channels, out_channels]</code>，具体卷积操作如下：</p>
<ol>
<li>将卷积核的维度转换成一个二维的矩阵形状<code>[filter_height * filter_width * in_channels, output_channels]</code>。</li>
<li>对于每个批处理的图片，我们将输入张量转换成一个临时的数据维度<code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>。</li>
<li>对于每个批处理的图片，我们右乘以卷积核，得到最后的输出结果。</li>
</ol>
<p>注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>或者<code>float64</code>。</li>
<li><code>filter</code>：一个<code>Tensor</code>，数据类型必须是<code>input</code>相同。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>use_cudnn_on_gpu</code>：一个可选布尔值，默认情况下是<code>True</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型是<code>input</code>相同。<br>&emsp;&emsp;<code>depthwise_conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.depthwise_conv2d(input, filter, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数也是一个卷积操作。给定一个输入张量，数据维度是<code>[batch, in_height, in_width, in_channels]</code>，一个卷积核的维度是<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>，在通道<code>in_channels</code>上面的卷积深度是<code>1</code>(我的理解是在每个通道上单独进行卷积)，<code>depthwise_conv2d</code>函数将不同的卷积核独立的应用在<code>in_channels</code>的每个通道上(从通道<code>1</code>到通道<code>channel_multiplier</code>)，然后把所以的结果进行汇总。最后输出通道的总数是<code>in_channels * channel_multiplier</code>。<br>&emsp;&emsp;注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维<code>[batch, in_height, in_width, in_channels]</code>。</li>
<li><code>filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如，<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>use_cudnn_on_gpu</code>：一个可选布尔值，默认情况下是<code>True</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个四维的<code>Tensor</code>，数据维度为<code>[batch, out_height, out_width, in_channels * channel_multiplier]</code>。<br>&emsp;&emsp;<code>separable_conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用几个分离的卷积核去做卷积，下图是常规卷积和分离卷积的区别：</p>
<p><img src="/2019/02/14/深度学习/TensorFlow之nn的API/1.png" height="210" width="560"></p>
<p>这个卷积是为了避免卷积核在全通道的情况下进行卷积，这样非常浪费时间。使用这个<code>API</code>，你将应用一个二维的卷积核，在每个通道上，以深度<code>channel_multiplier</code>进行卷积。其实如上图<code>Separable Convolution</code>中，就是先利用<code>depthwise_filter</code>，将<code>ID</code>的通道数映射到<code>ID * DM</code>的通道数上面，之后从<code>ID * DM</code>的通道数映射到<code>OD</code>的通道数上面，这也就是上面说的深度<code>channel_multiplier</code>对应于<code>DM</code>。<br>&emsp;&emsp;<code>strides</code>只是仅仅控制<code>depthwise convolution</code>的卷积步长，因为<code>pointwise convolution</code>的卷积步长是确定的<code>[1, 1, 1, 1]</code>。注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">depthwise_filter = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">pointwise_filter = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">20</span>), dtype=np.float32)</span><br><span class="line"><span class="comment"># out_channels &gt;= channel_multiplier * in_channels</span></span><br><span class="line">y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维<code>[batch, in_height, in_width, in_channels]</code>。</li>
<li><code>depthwise_filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>。其中，<code>in_channels</code>的卷积深度是<code>1</code>。</li>
<li><code>pointwise_filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[1, 1, channel_multiplier * in_channels, out_channels]</code>。其中，<code>pointwise_filter</code>是在<code>depthwise_filter</code>卷积之后的混合卷积。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如，<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个四维的<code>Tensor</code>，数据维度为<code>[batch, out_height, out_width, out_channels]</code>。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>&emsp;&emsp;池化操作是利用一个矩阵窗口在输入张量上进行扫描，并且将每个矩阵窗口中的值通过取最大值，平均值或者<code>XXXX</code>来减少元素个数。每个池化操作的矩阵窗口大小是由<code>ksize</code>来指定的，并且根据步长参数<code>strides</code>来决定移动步长。比如，如果<code>strides</code>中的值都是<code>1</code>，那么每个矩阵窗口都将被使用。如果<code>strides</code>中的值都是<code>2</code>，那么每一维度上的矩阵窗口都是每隔一个被使用。以此类推。更具体的输出结果是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output[i] = reduce(value[strides * i: strides * i + ksize])</span><br></pre></td></tr></table></figure>
<p>输出数据维度是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shape(output) = (shape(value) - ksize + <span class="number">1</span>) / strides</span><br></pre></td></tr></table></figure>
<p>其中，取舍方向取决于参数<code>padding</code>：</p>
<ul>
<li><code>padding = &#39;SAME&#39;</code>：向下取舍，仅适用于全尺寸操作，即输入数据维度和输出数据维度相同。</li>
<li><code>padding = &#39;VALID</code>：向上取舍，适用于部分窗口，即输入数据维度和输出数据维度不同。</li>
</ul>
<p>&emsp;&emsp;<code>avg_pool</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.avg_pool(value, ksize, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.avg_pool(value=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>、<code>float64</code>、<code>qint8</code>、<code>quint8</code>和<code>qint32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>max_pool</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool(value, ksize, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.max_pool(value=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>、<code>float64</code>、<code>qint8</code>、<code>quint8</code>或<code>qint32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>max_pool_with_argmax</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax = <span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置。因为在计算位置<code>argmax</code>的时候，我们将<code>input</code>铺平了进行计算，所以如果<code>input = [b, y, x, c]</code>，那么索引位置是<code>((b * height + y) * width + x) * channels + c</code>。<br>&emsp;&emsp;使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output, argmax = tf.nn.max_pool_with_argmax(input=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>Targmax</code>：一个可选的数据类型，即<code>tf.int32</code>或者<code>tf.int64</code>。默认情况下是<code>tf.int64</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个元祖张量<code>(output, argmax)</code>：</p>
<ul>
<li><code>output</code>：一个<code>Tensor</code>，数据类型是<code>float32</code>，表示池化区域的最大值。</li>
<li><code>argmax</code>：一个<code>Tensor</code>，数据类型是<code>Targmax</code>，数据维度是四维的。</li>
</ul>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>&emsp;&emsp;标准化是能防止模型过拟合的好方法，特别是在大数据的情况下。<br>&emsp;&emsp;<code>l2_normalize</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_normalize(x, dim, epsilon=<span class="number">1e-12</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用<code>L2</code>范数对指定维度<code>dim</code>进行标准化。比如，对于一个一维的张量，指定维度<code>dim = 0</code>，那么计算结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = x / sqrt(max(sum(x ** <span class="number">2</span>), epsilon))</span><br></pre></td></tr></table></figure>
<p>假设<code>x</code>是多维度的，那么标准化只会独立的对维度<code>dim</code>进行，不会影响到别的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.l2_normalize(input_data, dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>dim</code>：需要标准化的维度。</li>
<li><code>epsilon</code>：一个很小的值，确定标准化的下边界。如果<code>norm &lt; sqrt(epsilon)</code>，那么我们将使用<code>sqrt(epsilon)</code>进行标准化。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和x相同。<br>&emsp;&emsp;<code>local_response_normalization</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.local_response_normalization(input, depth_radius=<span class="keyword">None</span>, bias=<span class="keyword">None</span>, alpha=<span class="keyword">None</span>, beta=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算局部数据标准化。输入的数据<code>input</code>是一个四维的张量，但该张量被看做是一个一维的向量(<code>input</code>的最后一维作为向量)，向量中的每一个元素都是一个三维的数组(对应<code>input</code>的前三维)。向量的每一个元素都是独立的被标准化的。具体数学形式如下：<br>&emsp;&emsp;使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.local_response_normalization(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维的，数据类型是<code>float32</code>。</li>
<li><code>depth_radius</code>：可选项，一个整型，默认情况下是<code>5</code>。</li>
<li><code>bias</code>：可选项，一个浮点型，默认情况下是<code>1</code>。一个偏移项，为了避免除<code>0</code>，一般情况下取正值。</li>
<li><code>alpha</code>：可选项，一个浮点型，默认情况下是<code>1</code>。一个比例因子，一般情况下取正值。</li>
<li><code>beta</code>：可选项，一个浮点型，默认情况下是0.5。一个指数。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型是<code>float32</code>。<br>&emsp;&emsp;<code>moments</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.moments(x, axes, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的均值和方差。沿着<code>axes</code>维度，计算<code>x</code>的均值和方差。如果<code>x</code>是一维的，并且<code>axes = [0]</code>，那么就是计算整个向量的均值和方差。如果我们取<code>axes = [0, 1, 2]</code>(<code>batch, height, width</code>)，那么我们就是计算卷积的全局标准化。如果只是计算批处理的标准化，那么我们取<code>axes = [0]</code>(<code>batch</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">mean, variance = tf.nn.moments(input_data, [<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(mean))</span><br><span class="line">    print(sess.run(tf.shape(mean)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>axes</code>：一个整型的数组，确定计算均值和方差的维度。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出两个<code>Tensor</code>，分别是均值<code>mean</code>和方差<code>variance</code>。</p>
<h3 id="误差值"><a href="#误差值" class="headerlink" title="误差值"></a>误差值</h3><p>&emsp;&emsp;度量两个张量或者一个张量和零之间的损失误差，这个可用于在一个回归任务或者用于正则的目的(权重衰减)。<br>&emsp;&emsp;<code>l2_loss</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_loss(t, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用<code>L2</code>范数来计算张量的误差值，但是没有开方并且只取<code>L2</code>范数的值的一半：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = sum(t ** <span class="number">2</span>) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.l2_loss(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.03965811</span> <span class="number">0.9202959</span>  <span class="number">0.83564</span>   ]</span><br><span class="line"> [<span class="number">0.23268144</span> <span class="number">0.77983814</span> <span class="number">0.8602118</span> ]]</span><br><span class="line"><span class="number">1.474532</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>t</code>：一个<code>Tensor</code>。数据类型必须是以下之一：<code>float32</code>、<code>float64</code>、<code>int64</code>、<code>int32</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>、<code>complex64</code>、<code>qint8</code>、<code>quint8</code>或<code>qint32</code>。虽然一般情况下，数据维度是二维的。但是，数据维度可以取任意维度。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>t</code>相同，是一个标量。</p>
<h3 id="分类操作"><a href="#分类操作" class="headerlink" title="分类操作"></a>分类操作</h3><p>&emsp;&emsp;<code>sigmoid_cross_entropy_with_logits</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>logits</code>经<code>sigmoid</code>函数激活之后的交叉熵。对于一个不相互独立的离散分类任务，这个函数作用是去度量概率误差。比如在一张图片中，同时包含多个分类目标(大象和狗)，那么就可以使用这个函数。<br>&emsp;&emsp;为了描述简洁，我们规定<code>x = logits</code>，<code>z = targets</code>，那么<code>Logistic</code>损失值为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x - x * z + log(<span class="number">1</span> + exp(-x))</span><br></pre></td></tr></table></figure>
<p>为了确保计算稳定，避免溢出，真实的计算实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max(x, <span class="number">0</span>) - x * z + log(<span class="number">1</span> + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<p><code>logits</code>和<code>targets</code>必须有相同的数据类型和数据维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.random.rand传入一个shape，返回一个在[0,1)区间符合均匀分布的array</span></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.sigmoid_cross_entropy_with_logits(logits=input_data, labels=[[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))  <span class="comment"># 结果为[[0.3589966 1.1557628 0.9552358]]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个<code>Tensor</code>，数据类型是<code>float32</code>或者<code>float64</code>之一。</li>
<li><code>targets</code>：一个<code>Tensor</code>，数据类型和数据维度都和<code>logits</code>相同。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和<code>logits</code>相同。<br>&emsp;&emsp;<code>softmax</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(logits, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>softmax</code>激活函数。对于每个批<code>i</code>和分类<code>j</code>，我们可以得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))</span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable([[<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.9</span>]], dtype=tf.float32)</span><br><span class="line">output = tf.nn.softmax(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.2</span> <span class="number">0.1</span> <span class="number">0.9</span>]]</span><br><span class="line">[[<span class="number">0.25519383</span> <span class="number">0.23090893</span> <span class="number">0.51389724</span>]]</span><br><span class="line">[<span class="number">1</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个<code>Tensor</code>。数据类型是<code>float32</code>或者<code>float64</code>之一。数据维度是二维<code>[batch_size, num_classes]</code>。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和数据类型都和<code>logits</code>相同。<br>&emsp;&emsp;<code>softmax_cross_entropy_with_logits</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>logits</code>经<code>softmax</code>函数激活之后的交叉熵。对于每个独立的分类任务，这个函数是去度量概率误差。比如，在<code>CIFAR-10</code>数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车，但绝对不可能两者都在一张图中(这也是和<code>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)</code>这个<code>API</code>的区别)。<br>&emsp;&emsp;输入<code>API</code>的数据<code>logits</code>不能进行缩放，因为在这个<code>API</code>的执行中会进行<code>softmax</code>计算，如果<code>logits</code>进行了缩放，那么会影响计算正确率。不要调用这个<code>API</code>去计算<code>softmax</code>的值，因为这个<code>API</code>最终输出的结果并不是经过<code>softmax</code>函数的值。<br>&emsp;&emsp;<code>logits</code>和<code>labels</code>必须有相同的数据维度<code>[batch_size, num_classes]</code>，和相同的数据类型(<code>float32</code>或者<code>float64</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable([[<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.9</span>]], dtype=tf.float32)</span><br><span class="line">output = tf.nn.softmax_cross_entropy_with_logits(logits=input_data, labels=[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.2</span> <span class="number">0.1</span> <span class="number">0.9</span>]]</span><br><span class="line">[<span class="number">1.365732</span>]</span><br><span class="line">[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个没有缩放的对数张量。</li>
<li><code>labels</code>：每一行<code>labels[i]</code>必须是一个有效的概率分布值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度是一维的，长度是<code>batch_size</code>，数据类型都和<code>logits</code>相同。</p>
<h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h3><p>&emsp;&emsp;<code>Tensorflow</code>提供了从张量中嵌入查找的库。<br>&emsp;&emsp;`embedding_lookup函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(params, ids, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是查询<code>params</code>中索引是<code>ids</code>的值。这个操作是<code>tf.gather</code>的一个泛化，但它可以被并行计算处理，其中<code>params</code>被认为是一个大型的张量库，<code>ids</code>中的值对应于各个分区。如果<code>len(params) &gt; 1</code>，<code>ids</code>中每个元素<code>id</code>对应于<code>params</code>中每个分区<code>p</code>，即<code>p = id % len(params)</code>。那么，我们得到的每个切片是<code>params[p][id // len(params), ...]</code>。最后得到的切片结果被重新连接成一个稠密张量，最后返回的张量维度是<code>shape(ids) + shape(params)[1:]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">params = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">ids = tf.constant([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">output = tf.nn.embedding_lookup(params, ids)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">'params:'</span>, sess.run(params))</span><br><span class="line">    print(<span class="string">'-------------------------------'</span>)</span><br><span class="line">    print(<span class="string">'输出第0行和第2行:'</span>, sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params: [[<span class="number">0.59815351</span> <span class="number">0.65806083</span> <span class="number">0.1297678</span>  <span class="number">0.8921319</span> ]</span><br><span class="line"> [<span class="number">0.29017073</span> <span class="number">0.32162826</span> <span class="number">0.12856839</span> <span class="number">0.14271805</span>]</span><br><span class="line"> [<span class="number">0.3123268</span>  <span class="number">0.8437347</span>  <span class="number">0.49344986</span> <span class="number">0.29818202</span>]]</span><br><span class="line">-------------------------------</span><br><span class="line">输出第<span class="number">0</span>行和第<span class="number">2</span>行: [[<span class="number">0.59815351</span> <span class="number">0.65806083</span> <span class="number">0.1297678</span>  <span class="number">0.8921319</span> ]</span><br><span class="line"> [<span class="number">0.3123268</span>  <span class="number">0.8437347</span>  <span class="number">0.49344986</span> <span class="number">0.29818202</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>params</code>：一个拥有相同数据维度和数据类型的张量。</li>
<li><code>ids</code>：一个张量，数据类型是<code>int32</code>。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>params</code>相同。</p>
<h3 id="评估操作"><a href="#评估操作" class="headerlink" title="评估操作"></a>评估操作</h3><p>&emsp;&emsp;评估操作对于测量网络的性能是有用的。由于它们是不可微分的，所以它们通常只是被用在评估阶段。<br>&emsp;&emsp;<code>top_k</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.top_k(input, k, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是返回<code>input</code>中每行最大的k个数，并且返回它们所在位置的索引。<code>value(i, j)</code>表示输入数据<code>input(i)</code>的第<code>j</code>大的元素。<code>indices(i, j)</code>给出对应元素的列索引，即<code>input(i, indices(i, j)) = values(i, j)</code>。如果遇到两个相等的元素，那么我们先取索引小的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">k = <span class="number">2</span></span><br><span class="line">output = tf.nn.top_k(input, k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(input))</span><br><span class="line">    print(<span class="string">'--------------------'</span>)</span><br><span class="line">    print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.9522081</span>  <span class="number">0.59109382</span> <span class="number">0.44528462</span> <span class="number">0.34926363</span>]</span><br><span class="line"> [<span class="number">0.46024959</span> <span class="number">0.59693116</span> <span class="number">0.36231259</span> <span class="number">0.55662777</span>]</span><br><span class="line"> [<span class="number">0.37875119</span> <span class="number">0.4205928</span>  <span class="number">0.43581744</span> <span class="number">0.489631</span>  ]]</span><br><span class="line">--------------------</span><br><span class="line">TopKV2(values=array([[<span class="number">0.9522081</span> , <span class="number">0.59109382</span>],</span><br><span class="line">       [<span class="number">0.59693116</span>, <span class="number">0.55662777</span>],</span><br><span class="line">       [<span class="number">0.489631</span>  , <span class="number">0.43581744</span>]]), indices=array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">2</span>]]))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个张量，数据类型必须是以下之一：<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>。数据维度是<code>batch_size</code>乘上<code>x</code>个类别。</li>
<li><code>k</code>：一个整型，必须大于等于<code>1</code>。在每行中，查找最大的<code>k</code>个值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个元组<code>Tensor</code>，数据元素是<code>(values, indices)</code>：</p>
<ul>
<li><code>values</code>：一个张量，数据类型和<code>input</code>相同。数据维度是<code>batch_size</code>乘上<code>k</code>个最大值。</li>
<li><code>indices</code>：一个张量，数据类型是<code>int32</code>。每个最大值在<code>input</code>中的索引位置。</li>
</ul>
<p>&emsp;&emsp;<code>in_top_k</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.in_top_k(predictions, targets, k, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是返回一个布尔向量，说明目标值是否存在于预测值之中。输出数据是一个<code>batch_size</code>长度的布尔向量，如果目标值存在于预测值之中，那么<code>out[i] = true</code>。注意，<code>targets</code>是<code>predictions</code>中的索引位，并不是<code>predictions</code>中具体的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>), tf.float32)</span><br><span class="line">k = <span class="number">2</span></span><br><span class="line">output = tf.nn.in_top_k(input, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(input))</span><br><span class="line">    print(<span class="string">'--------------------'</span>)</span><br><span class="line">    print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.34494555</span> <span class="number">0.9111343</span>  <span class="number">0.93085057</span> <span class="number">0.419403</span>  ]</span><br><span class="line"> [<span class="number">0.14929643</span> <span class="number">0.46961188</span> <span class="number">0.72727567</span> <span class="number">0.04639981</span>]</span><br><span class="line"> [<span class="number">0.13701458</span> <span class="number">0.83330005</span> <span class="number">0.33437857</span> <span class="number">0.1281736</span> ]]</span><br><span class="line">--------------------</span><br><span class="line">[<span class="keyword">False</span> <span class="keyword">False</span> <span class="keyword">False</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>predictions</code>：一个张量，数据类型是<code>float32</code>。数据维度是<code>batch_size</code>乘上<code>x</code>个类别。</li>
<li><code>targets</code>：一个张量，数据类型是<code>int32</code>。一个长度是<code>batch_size</code>的向量，里面的元素是目标<code>class ID</code>。</li>
<li><code>k</code>：一个整型。在每行中，查找最大的<code>k</code>个值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个张量，数据类型是<code>bool</code>，判断是否预测正确。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之函数总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之函数总结/" itemprop="url">TensorFlow之函数总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T19:48:39+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="get-shape函数"><a href="#get-shape函数" class="headerlink" title="get_shape函数"></a>get_shape函数</h3><p>&emsp;&emsp;<code>get_shape</code>函数主要用于获取一个张量的维度，并且输出张量每个维度上面的值。如果是二维矩阵，也就是输出行和列的值，使用非常方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    A = tf.random_normal(shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    print(A.get_shape())</span><br><span class="line">    print(A.get_shape)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">&lt;bound method Tensor.get_shape of &lt;tf.Tensor <span class="string">'random_normal:0'</span> shape=(<span class="number">3</span>, <span class="number">4</span>) dtype=float32&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>第一个输出是一个元祖，就是数值，而第二输出就是一个张量的对象，里面包含更多的东西。如果你需要输出某一个维度上面的值，那就用下面的这种方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.get_shape()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这就表示第一个维度。该函数经常和<code>as_list</code>一起使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">varX = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">varX_shape = tf.shape(varX)</span><br><span class="line">print(sess.run(varX_shape))  <span class="comment"># 输出“[3 3]”</span></span><br><span class="line">varX_shape = varX.get_shape().as_list()</span><br><span class="line">print(varX_shape)  <span class="comment"># 输出“[3, 3]”</span></span><br></pre></td></tr></table></figure>
<h3 id="random-normal函数"><a href="#random-normal函数" class="headerlink" title="random_normal函数"></a>random_normal函数</h3><p>&emsp;&emsp;<code>tf.random_normal</code>函数用于从服从指定正态分布的数值中取出指定个数的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random_normal(shape, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32, seed=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>shape</code>：输出张量的形状。</li>
<li><code>mean</code>：正态分布的均值。</li>
<li><code>stddev</code>是正态分布的标准差。</li>
<li><code>dtype</code>：输出的类型。</li>
<li><code>seed</code>：随机数种子，是一个整数。</li>
<li><code>name</code>：操作的名称。</li>
</ul>
<p>&emsp;&emsp;以下程序定义一个<code>w1</code>变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(w1)</span><br><span class="line">    print(sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable <span class="string">'Variable:0'</span> shape=(<span class="number">2</span>, <span class="number">3</span>) dtype=float32_ref&gt;</span><br><span class="line">[[<span class="number">-0.8113182</span>   <span class="number">1.4845988</span>   <span class="number">0.06532937</span>]</span><br><span class="line"> [<span class="number">-2.4427042</span>   <span class="number">0.0992484</span>   <span class="number">0.5912243</span> ]]</span><br></pre></td></tr></table></figure>
<p>变量<code>w1</code>声明之后并没有被赋值，需要在<code>Session</code>中调用<code>run(tf.global_variables_initializer())</code>方法初始化之后才会被具体赋值。<code>tf</code>中张量与常规向量不同的是，执行<code>print(w1)</code>输出的是<code>w1</code>的形状和数据类型等属性信息，获取<code>w1</code>的值需要调用<code>sess.run(w1)</code>方法。</p>
<h3 id="TensorFlow的算术操作"><a href="#TensorFlow的算术操作" class="headerlink" title="TensorFlow的算术操作"></a>TensorFlow的算术操作</h3><p>&emsp;&emsp;<code>TensorFlow</code>的算术操作如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tf.add(x, y, name=None)</code></td>
<td>求和</td>
</tr>
<tr>
<td><code>tf.multiply(x, y, name=None)</code></td>
<td>减法</td>
</tr>
<tr>
<td><code>tf.multiply(x, y, name=None)</code></td>
<td>乘法</td>
</tr>
<tr>
<td><code>tf.div(x, y, name=None)</code>，推荐使用<code>tf.divide</code></td>
<td>除法</td>
</tr>
<tr>
<td><code>tf.mod(x, y, name=None)</code></td>
<td>取模</td>
</tr>
<tr>
<td><code>tf.abs(x, name=None)</code></td>
<td>求绝对值</td>
</tr>
<tr>
<td><code>tf.negative(x, name=None)</code></td>
<td>取负(<code>y = -x</code>)</td>
</tr>
<tr>
<td><code>tf.sign(x, name=None)</code></td>
<td>返回符号</td>
</tr>
<tr>
<td><code>tf.inv(x, name=None)</code></td>
<td>取反</td>
</tr>
<tr>
<td><code>tf.square(x, name=None)</code></td>
<td>计算平方</td>
</tr>
<tr>
<td><code>tf.round(x, name=None)</code></td>
<td>四舍五入最接近的整数</td>
</tr>
<tr>
<td><code>tf.sqrt(x, name=None)</code></td>
<td>开根号</td>
</tr>
<tr>
<td><code>tf.pow(x, y, name=None)</code></td>
<td>幂次方</td>
</tr>
<tr>
<td><code>tf.exp(x, name=None)</code></td>
<td>计算<code>e</code>的次方</td>
</tr>
<tr>
<td><code>tf.log(x, name=None)</code></td>
<td>计算<code>log</code></td>
</tr>
<tr>
<td><code>tf.maximum(x, y, name=None)</code></td>
<td>返回最大值</td>
</tr>
<tr>
<td><code>tf.minimum(x, y, name=None)</code></td>
<td>返回最小值</td>
</tr>
<tr>
<td><code>tf.cos(x, name=None)</code></td>
<td>三角函数<code>cos</code></td>
</tr>
<tr>
<td><code>tf.sin(x, name=None)</code></td>
<td>三角函数<code>sin</code></td>
</tr>
<tr>
<td><code>tf.tan(x, name=None)</code></td>
<td>三角函数<code>tan</code></td>
</tr>
<tr>
<td><code>tf.atan(x, name=None)</code></td>
<td>三角函数<code>atan</code></td>
</tr>
<tr>
<td><code>tf.sign(x, name=None)</code></td>
<td><code>y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0</code></td>
</tr>
<tr>
<td><code>tf.round(x, name=None)</code></td>
<td><code>a is [0.9, 2.5, 2.3, -4.4], tf.round(a) is [1.0, 3.0, 2.0, -4.0]</code></td>
</tr>
<tr>
<td><code>tf.pow(x, y, name=None)</code></td>
<td><code>x is [[2, 2], [3, 3]], y is [[8, 16], [2, 3]], tf.pow(x, y) is [[256, 65536], [9, 27]]</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="assign函数"><a href="#assign函数" class="headerlink" title="assign函数"></a>assign函数</h3><p>&emsp;&emsp;<code>tf.assign(A, new_number)</code>的功能是把<code>A</code>的值变为<code>new_number</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)  <span class="comment"># 定义常量one</span></span><br><span class="line">new_value = tf.add(state, one)  <span class="comment"># 定义加法步骤(注意，此步并没有直接计算)</span></span><br><span class="line">update = tf.assign(state, new_value)  <span class="comment"># 将State更新成new_value</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>assign</code>函数也可以用于给图变量赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">In [<span class="number">2</span>]: v = tf.Variable(<span class="number">3</span>, name=<span class="string">'v'</span>)</span><br><span class="line">In [<span class="number">3</span>]: v2 = v.assign(<span class="number">5</span>)</span><br><span class="line">In [<span class="number">4</span>]: sess = tf.InteractiveSession()</span><br><span class="line">In [<span class="number">5</span>]: sess.run(v.initializer)</span><br><span class="line">In [<span class="number">6</span>]: sess.run(v)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">3</span></span><br><span class="line">In [<span class="number">7</span>]: sess.run(v2)</span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-contrib-layers-separable-conv2d"><a href="#tf-contrib-layers-separable-conv2d" class="headerlink" title="tf.contrib.layers.separable_conv2d"></a>tf.contrib.layers.separable_conv2d</h3><p>&emsp;&emsp;Aliases:</p>
<ul>
<li>tf.contrib.layers.separable_conv2d</li>
<li>tf.contrib.layers.separable_convolution2d</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.separable_conv2d(</span><br><span class="line">    inputs, num_outputs, kernel_size, depth_multiplier, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=DATA_FORMAT_NHWC, rate=<span class="number">1</span>, activation_fn=tf.nn.relu, normalizer_fn=<span class="keyword">None</span>,</span><br><span class="line">    normalizer_params=<span class="keyword">None</span>, weights_initializer=initializers.xavier_initializer(),</span><br><span class="line">    weights_regularizer=<span class="keyword">None</span>, biases_initializer=tf.zeros_initializer(), biases_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    reuse=<span class="keyword">None</span>, variables_collections=<span class="keyword">None</span>, outputs_collections=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/layers.py</code>. Adds a <code>depth-separable</code> <code>2D</code> convolution with optional <code>batch_norm</code> layer.<br>&emsp;&emsp;This op first performs a depthwise convolution that acts separately on channels, creating a variable called <code>depthwise_weights</code>. If <code>num_outputs</code> is not <code>None</code>, it adds a pointwise convolution that mixes channels, creating a variable called <code>pointwise_weights</code>. Then, if <code>normalizer_fn</code> is <code>None</code>, it adds bias to the result, creating a variable called <code>biases</code>, otherwise, <code>the normalizer_fn</code> is applied. It finally applies an activation function to produce the end result.</p>
<ul>
<li><code>inputs</code>: A tensor of size <code>[batch_size, height, width, channels]</code>.</li>
<li><code>num_outputs</code>: The number of pointwise convolution output filters. If <code>num_outputs</code> is <code>None</code>, then we skip the pointwise convolution stage.</li>
<li><code>kernel_size</code>: A list of length <code>2</code>: <code>[kernel_height, kernel_width]</code> of the filters. Can be an <code>int</code> if both values are the same.</li>
<li><code>depth_multiplier</code>: The number of depthwise convolution output channels for each input channel.</li>
<li><code>stride</code>: A list of length <code>2</code>: <code>[stride_height, stride_width]</code>, specifying the depthwise convolution stride. Can be an <code>int</code> if both strides are the same.</li>
<li><code>padding</code>: One of <code>VALID</code> or <code>SAME</code>.</li>
<li><code>data_format</code>: A string. <code>NHWC</code> (default) and <code>NCHW</code> are supported.</li>
<li><code>rate</code>: A list of length <code>2</code>: <code>[rate_height, rate_width]</code>, specifying the dilation rates for atrous convolution. Can be an <code>int</code> if both rates are the same. If any value is larger than one, then both stride values need to be one.</li>
<li><code>activation_fn</code>: Activation function. The default value is a <code>ReLU</code> function. Explicitly set it to <code>None</code> to skip it and maintain a linear activation.</li>
<li><code>normalizer_fn</code>: Normalization function to use instead of biases. If <code>normalizer_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and biases are not created nor added. default set to <code>None</code> for no normalizer function</li>
<li><code>normalizer_params</code>: Normalization function parameters.</li>
<li><code>weights_initializer</code>: An initializer for the weights.</li>
<li><code>weights_regularizer</code>: Optional regularizer for the weights.</li>
<li><code>biases_initializer</code>: An initializer for the biases. If <code>None</code>, skip biases.</li>
<li><code>biases_regularizer</code>: Optional regularizer for the biases.</li>
<li><code>reuse</code>: Whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li>
<li><code>variables_collections</code>: Optional list of collections for all the variables or a dictionary containing a different list of collection per variable.</li>
<li><code>outputs_collections</code>: Collection to add the outputs.</li>
<li><code>trainable</code>: Whether or not the variables should be trainable or not.</li>
<li><code>scope</code>: Optional scope for <code>variable_scope</code>.</li>
</ul>
<p>Returns a <code>Tensor</code> representing the output of the operation.</p>
<h3 id="tf-contrib-layers-xavier-initializer"><a href="#tf-contrib-layers-xavier-initializer" class="headerlink" title="tf.contrib.layers.xavier_initializer"></a>tf.contrib.layers.xavier_initializer</h3><p>&emsp;&emsp;Aliases:</p>
<ul>
<li>tf.contrib.layers.xavier_initializer</li>
<li>tf.contrib.layers.xavier_initializer_conv2d</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.xavier_initializer(uniform=<span class="keyword">True</span>, seed=<span class="keyword">None</span>, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/initializers.py</code>. Returns an initializer performing <code>Xavier</code> initialization for weights. This function implements the weight initialization from <code>Xavier Glorot</code> and <code>Yoshua Bengio</code> (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.<br>&emsp;&emsp;This initializer is designed to keep the scale of the gradients roughly the same in all layers.</p>
<ul>
<li><code>uniform</code>: Whether to use uniform or normal distributed random initialization.</li>
<li><code>seed</code>: A <code>Python</code> integer. Used to create random seeds. See <code>tf.set_random_seed</code> for behavior.</li>
<li><code>dtype</code>: The data type. Only floating point types are supported.</li>
</ul>
<h3 id="tf-contrib-layers-l2-regularizer"><a href="#tf-contrib-layers-l2-regularizer" class="headerlink" title="tf.contrib.layers.l2_regularizer"></a>tf.contrib.layers.l2_regularizer</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.l2_regularizer(scale, scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/regularizers.py</code>. Returns a function that can be used to apply <code>L2</code> regularization to weights. Small values of <code>L2</code> can help prevent overfitting the training data.</p>
<ul>
<li><code>scale</code>: A scalar multiplier <code>Tensor</code>. <code>0.0</code> disables the regularizer.</li>
<li><code>scope</code>: An optional scope name.</li>
</ul>
<h3 id="tf-placeholder-with-default"><a href="#tf-placeholder-with-default" class="headerlink" title="tf.placeholder_with_default"></a>tf.placeholder_with_default</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder_with_default(input, shape, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/gen_array_ops.py</code>. A placeholder op that passes through input when its output is not fed.</p>
<ul>
<li><code>input</code>: A <code>Tensor</code>. The default value to produce when output is not fed.</li>
<li><code>shape</code>: A <code>tf.TensorShape</code> or list of <code>ints</code>. The (possibly partial) shape of the tensor.</li>
<li><code>name</code>: A name for the operation (optional).</li>
</ul>
<p>Returns a <code>Tensor</code>. Has the same type as input.</p>
<h3 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split"></a>tf.split</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.split(value, num_or_size_splits, axis=<span class="number">0</span>, num=<span class="keyword">None</span>, name=<span class="string">'split'</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Splits a tensor into sub tensors.<br>&emsp;&emsp;If <code>num_or_size_splits</code> is an integer type, <code>num_split</code>, then splits <code>value</code> along dimension <code>axis</code> into <code>num_split</code> smaller tensors. Requires that <code>num_split</code> evenly divides <code>value.shape[axis]</code>.<br>&emsp;&emsp;If <code>num_or_size_splits</code> is not an integer type, it is presumed to be a <code>Tensor</code> <code>size_splits</code>, then splits <code>value</code> into <code>len(size_splits)</code> pieces. The shape of the <code>i-th</code> piece has the same size as the <code>value</code> except along dimension <code>axis</code> where the size is <code>size_splits[i]</code>.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 'value' is a tensor with shape [5, 30]. Split 'value' into 3 tensors</span></span><br><span class="line"><span class="comment"># with sizes [4, 15, 11] along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, [<span class="number">4</span>, <span class="number">15</span>, <span class="number">11</span>], <span class="number">1</span>)</span><br><span class="line">tf.shape(split0)  <span class="comment"># [5, 4]</span></span><br><span class="line">tf.shape(split1)  <span class="comment"># [5, 15]</span></span><br><span class="line">tf.shape(split2)  <span class="comment"># [5, 11]</span></span><br><span class="line"><span class="comment"># Split 'value' into 3 tensors along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, num_or_size_splits=<span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">tf.shape(split0)  <span class="comment"># [5, 10]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>: The <code>Tensor</code> to split.</li>
<li><code>num_or_size_splits</code>: Either a <code>0-D</code> integer <code>Tensor</code> indicating the number of splits along <code>split_dim</code> or a <code>1-D</code> integer <code>Tensor</code> containing the sizes of each output tensor along <code>split_dim</code>. If a scalar then it must evenly divide <code>value.shape[axis]</code>; otherwise the sum of sizes along the split dimension must match that of the value.</li>
<li><code>axis</code>: A <code>0-D</code> <code>int32</code> <code>Tensor</code>. The dimension along which to split. Must be in the range <code>[-rank(value), rank(value))</code>.</li>
<li><code>num</code>: Optional, used to specify the number of outputs when it cannot be inferred from the shape of <code>size_splits</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>&emsp;&emsp;Returns: if <code>num_or_size_splits</code> is a scalar returns <code>num_or_size_splits</code> <code>Tensor</code> objects; if <code>num_or_size_splits</code> is a <code>1-D</code> <code>Tensor</code> returns <code>num_or_size_splits.get_shape[0]</code> <code>Tensor</code> objects resulting from splitting value.</p>
<h3 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape"></a>tf.reshape</h3><p>&emsp;&emsp;<code>tf.reshape(tensor, shape, name=None)</code>的作用是将<code>tensor</code>变换为参数<code>shape</code>的形式。其中<code>shape</code>为一个列表形式，特殊的一点是列表中可以存在<code>-1</code>，<code>-1</code>代表的含义是不用我们自己指定这一维的大小，函数会自动计算，但列表中只能存在一个<code>-1</code>(当然如果存在多个<code>-1</code>，就是一个存在多解的方程了)。<br>&emsp;&emsp;<code>TensorFlow</code>根据<code>shape</code>变换矩阵的方式为<code>reshape(t, shape) =&gt; reshape(t, [-1]) =&gt; reshape(t, shape)</code>，首先将矩阵t变为一维矩阵，然后再对矩阵的形式进行更改。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]. tensor 't' has shape [9]</span></span><br><span class="line">reshape(t, [3, 3]) ==&gt; [[1, 2, 3],</span><br><span class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></span><br><span class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</span><br><span class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></span><br><span class="line"><span class="comment">#                 [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3, 3],</span></span><br><span class="line"><span class="comment">#                 [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#                [[5, 5, 5],</span></span><br><span class="line"><span class="comment">#                 [6, 6, 6]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]. pass '[-1]' to flatten 't'</span></span><br><span class="line">reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</span><br><span class="line"><span class="comment"># -1 can also be used to infer the shape</span></span><br><span class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],  # -1 is inferred to be 9</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line">reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],  # -1 is inferred to be 2</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line">reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],  # -1 is inferred to be 3</span><br><span class="line">                              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                              [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">                             [[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">                              [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                              [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line">reshape(t, []) ==&gt; 7  # tensor 't' is [7]. shape `[]` reshapes to a scalar</span><br></pre></td></tr></table></figure>
<h3 id="tf-shape"><a href="#tf-shape" class="headerlink" title="tf.shape"></a>tf.shape</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.shape(input, name=<span class="keyword">None</span>, out_type=tf.int32)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Returns the shape of a tensor. This operation returns a <code>1-D</code> integer tensor representing the shape of input.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]], [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]])</span><br><span class="line">tf.shape(t)  <span class="comment"># [2, 2, 3]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li>
<li><code>name</code>: A name for the operation (optional).</li>
<li><code>out_type</code>: (Optional) The specified output type of the operation (<code>int32</code> or <code>int64</code>).</li>
</ul>
<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat"></a>tf.concat</h3><p>&emsp;&emsp;该函数用于连接两个矩阵的操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(concat_dim, values, name=<span class="string">'concat'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>concat_dim</code>：<code>tensor</code>连接的方向(维度)，<code>cancat_dim</code>维度可以不一样，其他维度的尺寸必须一样。</li>
<li><code>values</code>：两个或者一组待连接的<code>tensor</code>。</li>
<li><code>name</code>：指定该操作的<code>name</code>。</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">tf.concat(0, [t1, t2]) =&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</span><br><span class="line">tf.concat(1, [t1, t2]) =&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span><br></pre></td></tr></table></figure>
<p>这里要注意的是，如果是两个向量，它们是无法调用<code>tf.concat(1, [t1, t2])</code>来连接的，因为它们对应的<code>shape</code>只有一个维度，当然不能在第二维上连了，虽然实际中两个向量可以在行上连，但是放在程序里是会报错的。如果要连，必须要调用<code>tf.expand_dims</code>来扩维：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t1 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t2 = tf.constant([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># concated = tf.concat(1, [t1, t2])  # 这样会报错</span></span><br><span class="line">t1 = tf.expand_dims(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]),<span class="number">1</span>)</span><br><span class="line">t2 = tf.expand_dims(tf.constant([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]),<span class="number">1</span>)</span><br><span class="line">concated = tf.concat(<span class="number">1</span>, [t1, t2])  <span class="comment"># 这样就是正确的</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-add-n"><a href="#tf-add-n" class="headerlink" title="tf.add_n"></a>tf.add_n</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add_n(inputs, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/math_ops.py</code>. Adds all input tensors <code>element-wise</code>.</p>
<ul>
<li><code>inputs</code>: A list of <code>Tensor</code> objects, each with same shape and type.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>&emsp;&emsp;Returns a <code>Tensor</code> of same shape and type as the elements of <code>inputs</code>.</p>
<h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze"></a>tf.squeeze</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.squeeze(input, squeeze_dims=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Removes dimensions of size <code>1</code> from the shape of a tensor.<br>&emsp;&emsp;Given a tensor <code>input</code>, this operation returns a tensor of the same type with all dimensions of size <code>1</code> removed. If you don’t want to remove all size <code>1</code> dimensions, you can remove specific size <code>1</code> dimensions by specifying axis.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]</span></span><br><span class="line">tf.shape(tf.squeeze(t))  <span class="comment"># [2, 3]</span></span><br><span class="line">Or, to remove specific size <span class="number">1</span> dimensions:</span><br><span class="line"><span class="comment"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]</span></span><br><span class="line">tf.shape(tf.squeeze(t, [<span class="number">2</span>, <span class="number">4</span>]))  <span class="comment"># [1, 2, 3, 1]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>: A <code>Tensor</code>. The input to squeeze.</li>
<li><code>axis</code>: An optional list of <code>ints</code>. If specified, only squeezes the dimensions listed. The dimension index starts at <code>0</code>. It is an error to squeeze a dimension that is not <code>1</code>. Must be in the range <code>[-rank(input), rank(input))</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>squeeze_dims</code>: Deprecated keyword argument that is now <code>axis</code>.</li>
</ul>
<p>&emsp;&emsp;Return a <code>Tensor</code>. Has the same type as <code>input</code>. Contains the same data as <code>input</code>, but has one or more dimensions of size <code>1</code> removed.</p>
<h3 id="tf-expand-dims"><a href="#tf-expand-dims" class="headerlink" title="tf.expand_dims"></a>tf.expand_dims</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(input, axis=<span class="keyword">None</span>, name=<span class="keyword">None</span>, dim=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Inserts a dimension of <code>1</code> into a tensor’s shape.<br>&emsp;&emsp;Given a tensor <code>input</code>, this operation inserts a dimension of <code>1</code> at the dimension index <code>axis</code> of <code>input&#39;s</code> shape. The dimension index <code>axis</code> starts at <code>zero</code>; if you specify a negative number for <code>axis</code> it is counted backward from the end.<br>&emsp;&emsp;This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape <code>[height, width, channels]</code>, you can make it a batch of <code>1</code> image with <code>expand_dims(image, 0)</code>, which will make the shape <code>[1, height, width, channels]</code>.<br>&emsp;&emsp;Other examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"><span class="comment"># 't2' is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure>
<p>This operation requires that:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-1</span> - input.dims() &lt;= dim &lt;= input.dims()</span><br></pre></td></tr></table></figure>
<p>This operation is related to <code>squeeze()</code>, which removes dimensions of size <code>1</code>.</p>
<ul>
<li><code>input</code>: A <code>Tensor</code>.</li>
<li><code>axis</code>: <code>0-D</code> (scalar). Specifies the dimension index at which to expand the shape of <code>input</code>. Must be in the range <code>[-rank(input) - 1, rank(input)]</code>.</li>
<li><code>name</code>: The <code>name</code> of the output <code>Tensor</code>.</li>
<li><code>dim</code>: <code>0-D</code> (scalar). Equivalent to <code>axis</code>, to be deprecated.</li>
</ul>
<p>&emsp;&emsp;Return a <code>Tensor</code> with the same data as <code>input</code>, but its shape has an additional dimension of size <code>1</code> added.</p>
<h3 id="tf-Session-as-default"><a href="#tf-Session-as-default" class="headerlink" title="tf.Session.as_default"></a>tf.Session.as_default</h3><p>&emsp;&emsp;如果使用关键字<code>with</code>来指定会话，可以在会话中执行<code>Operation.run</code>或<code>Tensor.eval</code>，以得到运行的<code>tensor</code>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant(..)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="keyword">assert</span> tf.get_default_session() <span class="keyword">is</span> sess</span><br><span class="line">    print(c.eval())</span><br></pre></td></tr></table></figure>
<p>使用函数<code>tf.get_default_session</code>来得到当前默认的会话。需要注意的是，退出<code>as_default</code>上下文管理器时，并没有关闭该会话(<code>session</code>)，所以你必须明确地关闭会话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant(...)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line">sess.close()  <span class="comment"># 关闭会话</span></span><br></pre></td></tr></table></figure>
<p>而使用<code>with tf.Session()</code>的方式可以创建并自动关闭会话。</p>
<h3 id="错误类-Error-classes"><a href="#错误类-Error-classes" class="headerlink" title="错误类(Error classes)"></a>错误类(Error classes)</h3><p>&emsp;&emsp;错误类如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>class tf.OpError</code></td>
<td>一个基本的错误类型，在当TF执行失败时候报错</td>
</tr>
<tr>
<td><code>tf.OpError.op</code></td>
<td>返回执行失败的操作节点，有的操作(如<code>Send</code>或<code>Recv</code>)可能不会返回，那就要用用到<code>node_def</code>方法</td>
</tr>
<tr>
<td><code>tf.OpError.node_def</code></td>
<td>以<code>NodeDef proto</code>形式表示失败的<code>op</code></td>
</tr>
<tr>
<td><code>tf.OpError.error_code</code></td>
<td>描述该错误的整数错误代码</td>
</tr>
<tr>
<td><code>tf.OpError.message</code></td>
<td>返回错误信息</td>
</tr>
<tr>
<td><code>class tf.errors.CancelledError</code></td>
<td>当操作或者阶段被取消时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.UnknownError</code></td>
<td>未知错误类型</td>
</tr>
<tr>
<td><code>class tf.errors.InvalidArgumentError</code></td>
<td>在接收到非法参数时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.NotFoundError</code></td>
<td>当发现不存在所请求的一个实体时候，比如文件或目录</td>
</tr>
<tr>
<td><code>class tf.errors.AlreadyExistsError</code></td>
<td>当创建的实体已经存在的时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.PermissionDeniedError</code></td>
<td>没有执行权限做某操作的时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.FailedPreconditionError</code></td>
<td>系统没有条件执行某个行为时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.AbortedError</code></td>
<td>操作中止时报错，常常发生在并发情形</td>
</tr>
<tr>
<td><code>class tf.errors.OutOfRangeError</code></td>
<td>超出范围报错</td>
</tr>
<tr>
<td><code>class tf.errors.UnimplementedError</code></td>
<td>某个操作没有执行时报错</td>
</tr>
<tr>
<td><code>class tf.errors.InternalError</code></td>
<td>当系统经历了一个内部错误时报出</td>
</tr>
<tr>
<td><code>class tf.errors.ResourceExhaustedError</code></td>
<td>资源耗尽时报错</td>
</tr>
<tr>
<td><code>class tf.errors.DataLossError</code></td>
<td>当出现不可恢复的错误，例如在运行<code>tf.WholeFileReader.read</code>读取整个文件的同时文件被删减</td>
</tr>
<tr>
<td><code>tf.errors.XXXXX.__init__(node_def, op, message)</code></td>
<td>使用该形式方法创建以上各种错误类</td>
</tr>
</tbody>
</table>
</div>
<h3 id="tf-one-hot"><a href="#tf-one-hot" class="headerlink" title="tf.one_hot"></a>tf.one_hot</h3><p>&emsp;&emsp;该函数用于将输入转换成<code>one-hot</code>形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(indices, depth, on_value, off_value, axis)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>indices</code>：非负整数表示的标签列表，<code>len(indices)</code>就是分类的类别数。<code>tf.one_hot</code>返回的张量的阶数为<code>indeces</code>的阶数加上<code>1</code>。当<code>indices</code>的某个分量取<code>-1</code>时，即对应的向量没有独热值。</li>
<li><code>depth</code>：每个独热向量的维度。</li>
<li><code>on_value</code>：独热值。</li>
<li><code>off_value</code>：非独热值。</li>
<li><code>axis</code>：指定第几阶为<code>depth</code>维独热向量，默认为<code>-1</code>，即指定张量的最后一维为独热向量。例如对于一个<code>2</code>阶张量而言，<code>axis = 0</code>时，每个列向量是一个独热的<code>depth</code>维向量；<code>axis = 1</code>时，每个行向量是一个独热的<code>depth</code>维向量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=[<span class="number">10</span>])</span><br><span class="line">y = tf.one_hot(z, <span class="number">10</span>, on_value=<span class="number">1</span>, off_value=<span class="keyword">None</span>, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session()<span class="keyword">as</span> sess:</span><br><span class="line">    print(z)</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4</span> <span class="number">4</span> <span class="number">5</span> <span class="number">8</span> <span class="number">9</span> <span class="number">0</span> <span class="number">5</span> <span class="number">0</span> <span class="number">3</span> <span class="number">1</span>]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="tf-sparse-to-dense"><a href="#tf-sparse-to-dense" class="headerlink" title="tf.sparse_to_dense"></a>tf.sparse_to_dense</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是将一个稀疏表示转换成一个密集张量。具体将稀疏张量<code>sparse</code>转换成密集张量<code>dense</code>的步骤如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If sparse_indices is scalar</span></span><br><span class="line">dense[i] = (i == sparse_indices ? sparse_values : default_value)</span><br><span class="line"><span class="comment"># If sparse_indices is a vector, then for each i</span></span><br><span class="line">dense[sparse_indices[i]] = sparse_values[i]</span><br><span class="line"><span class="comment"># If sparse_indices is an n by d matrix, then for each i in [0, n)</span></span><br><span class="line">dense[sparse_indices[i][<span class="number">0</span>], ..., sparse_indices[i][d<span class="number">-1</span>]] = sparse_values[i]</span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>dense</code>中的填充值<code>default_value</code>都是<code>0</code>，除非该值被设置成一个标量。</p>
<ul>
<li><code>sparse_indices</code>是稀疏矩阵中那些个别元素对应的索引值，有三种情况：</li>
</ul>
<ol>
<li>如果<code>sparse_indices</code>是个数，那么它只能指定一维矩阵的某一个元素。</li>
<li>如果<code>sparse_indices</code>是个向量，那么它可以指定一维矩阵的多个元素。</li>
<li>如果<code>sparse_indices</code>是个矩阵，那么它可以指定二维矩阵的多个元素。</li>
</ol>
<ul>
<li><code>output_shape</code>是输出的稀疏矩阵的<code>shape</code>。</li>
<li><code>sparse_values</code>是个别元素的值，分为两种情况：</li>
</ul>
<ol>
<li>如果sparse_values是个数，则所有索引指定的位置都用这个数。</li>
<li>如果sparse_values是个向量，则输出矩阵的某一行向量里某一行对应的数(所以这里向量的长度应该和输出矩阵的行数对应，不然报错)。</li>
</ol>
<ul>
<li><code>default_value</code>是未指定元素的默认值，一般如果是稀疏矩阵，就是0了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span></span><br><span class="line"><span class="comment"># 真实标签，shape为[5, 1]</span></span><br><span class="line">label = tf.expand_dims(tf.constant([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]), <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 真实标签的索引，shape为[5, 1]</span></span><br><span class="line">index = tf.expand_dims(tf.range(<span class="number">0</span>, BATCH_SIZE), <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 将标签和索引tensor在第二个维度上连接起来，新的concated的shape为[5, 2]</span></span><br><span class="line">concated = tf.concat([index, label], <span class="number">1</span>)</span><br><span class="line"><span class="comment"># onehot_labels的shape为[5, 10]</span></span><br><span class="line">onehot_labels = tf.sparse_to_dense(concated, [BATCH_SIZE, <span class="number">10</span>], <span class="number">1.0</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(concated))</span><br><span class="line">    print(<span class="string">"----------------"</span>)</span><br><span class="line">    onehot1 = sess.run(onehot_labels)</span><br><span class="line">    print(onehot1)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">9</span>]]</span><br><span class="line">----------------</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;如果<code>output_shape</code>是一个行向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">predicted_class = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line">one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    onehot1 = sess.run(one_hot)</span><br><span class="line">    print(onehot1)  <span class="comment"># 输出“[0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]”</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-nn-in-top-k"><a href="#tf-nn-in-top-k" class="headerlink" title="tf.nn.in_top_k"></a>tf.nn.in_top_k</h3><p>&emsp;&emsp;<code>tf.nn.in_top_k</code>用于计算预测的结果和实际结果的是否相等，并返回一个<code>bool</code>类型的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.in_top_k(prediction, target, K)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>prediction</code>：预测的结果，大小就是预测样本的数量乘以输出的维度。</li>
<li><code>target</code>：实际样本类别的标签，大小就是样本数量的个数。</li>
<li><code>K</code>：每个样本的预测结果的前<code>k</code>个最大的数里面是否包含<code>targets</code>预测中的标签，一般都是取<code>1</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.3</span>], [<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.4</span>]]</span><br><span class="line">B = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">out = tf.nn.in_top_k(A, B, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(out))  <span class="comment"># 输出“[False  True]”</span></span><br></pre></td></tr></table></figure>
<p>因为<code>A</code>张量里面的第一个元素的最大值的标签是<code>0</code>，第二个元素的最大值的标签是<code>1</code>，但实际上是<code>1</code>和<code>1</code>，所以输出就是<code>False</code>和<code>True</code>。如果把<code>K</code>改成<code>2</code>，那么第一个元素的前面<code>2</code>个最大元素的位置是<code>0</code>和<code>1</code>，第二个元素的就是<code>1</code>和<code>2</code>。而<code>B</code>是<code>[1, 1]</code>，包含在里面，所以输出结果就是<code>True</code>和<code>True</code>。</p>
<h3 id="initialized-value"><a href="#initialized-value" class="headerlink" title="initialized_value"></a>initialized_value</h3><p>&emsp;&emsp;你有时候会需要用另一个变量的初始化值给当前变量初始化。由于<code>tf.initialize_all_variables</code>是并行地初始化所有变量，所以在有这种需求的情况下需要小心。<br>&emsp;&emsp;用其它变量的值初始化一个新的变量时，使用其它变量的<code>initialized_value</code>属性。你可以直接把已初始化的值作为新变量的初始值，或者把它当做<code>tensor</code>计算得到一个值赋予新变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a variable with a random value</span></span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=<span class="string">"weights"</span>)</span><br><span class="line"><span class="comment"># Create another variable with the same value as 'weights'</span></span><br><span class="line">w2 = tf.Variable(weights.initialized_value(), name=<span class="string">"w2"</span>)</span><br><span class="line"><span class="comment"># Create another variable with twice the value of 'weights'</span></span><br><span class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">0.2</span>, name=<span class="string">"w_twice"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tf-py-func"><a href="#tf-py-func" class="headerlink" title="tf.py_func"></a>tf.py_func</h3><p>&emsp;&emsp;这是一个可以把<code>TensorFlow</code>和<code>Python</code>原生代码无缝衔接起来的函数，有了它，你就可以在<code>TensorFlow</code>里面自由的实现你想要的功能，而不用考虑<code>TensorFlow</code>有没有实现它的<code>API</code>，并且可以帮助我们实现自由地检查该功能模块的输入输出是否正确，而不受到<code>TensorFlow</code>的先构造计算图再运行导致的不能单独检测单一模块的功能的限制。<br>&emsp;&emsp;它的具体功能描述是包装一个普通的<code>Python</code>函数，这个函数接受<code>numpy</code>的数组作为输入和输出，让这个函数可以作为<code>TensorFlow</code>计算图上的计算节点<code>OP</code>来使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">py_func(func, inp, Tout, stateful=<span class="keyword">True</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>func</code>：一个<code>Python</code>函数，它接受<code>NumPy</code>数组作为输入和输出，并且数组的类型和大小必须和输入和输出用来衔接的<code>Tensor</code>大小和数据类型相匹配。</li>
<li><code>inp</code>：输入的<code>Tensor</code>列表。</li>
<li><code>Tout</code>：输出<code>Tensor</code>数据类型的列表或元祖。</li>
<li><code>stateful</code>：状态，布尔值。</li>
<li><code>name</code>：节点<code>OP</code>的名称。</li>
</ul>
<p>&emsp;&emsp;<code>operation</code>分为有状态的与无状态的<code>operation</code>：无状态的<code>operation</code>主要进行数学计算，比如矩阵乘法、加法等，如果给该<code>OP</code>同一个输入，那么将会得到同一个输出；有状态的<code>operation</code>(<code>stateful operation</code>)分为<code>variable</code>以及<code>queue</code>，<code>variable</code>负责保存机器学习模型的模型参数，<code>queue</code>提供更加复杂的模型架构，给定同一个输入，可能会得到不同的输出。<code>common subexpression elimination</code>(<code>CSE</code>)公共子表达式消除只会在无状态的节点<code>OP</code>上执行。<br>&emsp;&emsp;简单代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sinh(x)</span><br><span class="line"></span><br><span class="line">inp = tf.placeholder(tf.float32)</span><br><span class="line">y = tf.py_func(my_func, [inp], tf.float32)</span><br></pre></td></tr></table></figure>
<p>缺点如下：</p>
<ul>
<li>这个被包装过的的计算函数的内部部分不会被序列化到<code>GraphDef</code>里面去，所以，如果你要序列化存储和恢复模型，就不能使用该函数。</li>
<li>这个被包装的计算节点<code>OP</code>与调用它的<code>Python</code>程序必须运行在同一个物理设备上，也就是说，如果使用分布式<code>TensorFlow</code>，必须使用<code>tf.train.Server</code>和<code>with tf.device</code>来保证二者在同一个服务器内。</li>
</ul>
<h3 id="tf-trainable-variables"><a href="#tf-trainable-variables" class="headerlink" title="tf.trainable_variables"></a>tf.trainable_variables</h3><p>&emsp;&emsp;在创造变量(<code>tf.Variable</code>、<code>tf.get_variable</code>等)时，都会有一个<code>trainable</code>的选项，表示该变量是否可训练，这个函数会返回图中所有<code>trainable=True</code>的变量。<code>tf.get_variable</code>和<code>tf.Variable</code>的默认选项是<code>True</code>，而<code>tf.constant</code>只能是<code>False</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.get_variable(<span class="string">'a'</span>, shape=[<span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>, shape=[<span class="number">2</span>, <span class="number">5</span>], trainable=<span class="keyword">False</span>)</span><br><span class="line">c = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.int32, shape=[<span class="number">8</span>], name=<span class="string">'c'</span>)</span><br><span class="line">d = tf.Variable(tf.random_uniform(shape=[<span class="number">3</span>, <span class="number">3</span>]), name=<span class="string">'d'</span>)</span><br><span class="line">tvar = tf.trainable_variables()</span><br><span class="line">tvar_name = [x.name <span class="keyword">for</span> x <span class="keyword">in</span> tvar]</span><br><span class="line">print(tvar)</span><br><span class="line">print(tvar_name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable <span class="string">'a:0'</span> shape=(<span class="number">5</span>, <span class="number">2</span>) dtype=float32_ref&gt;, &lt;tf.Variable <span class="string">'d:0'</span> shape=(<span class="number">3</span>, <span class="number">3</span>) dtype=float32_ref&gt;]</span><br><span class="line">[<span class="string">'a:0'</span>, <span class="string">'d:0'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a>tf.train.ExponentialMovingAverage</h3><p>&emsp;&emsp;该函数用于更新参数，就是采用滑动平均的方法更新参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.ExponentialMovingAverage(decay, steps)</span><br></pre></td></tr></table></figure>
<p>这个函数初始化需要提供一个衰减速率(<code>decay</code>)，用于控制模型的更新速度。这个函数还会维护一个影子变量，也就是更新参数后的参数值，这个影子变量的初始值就是这个变量的初始值，影子变量值的更新方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shadow_variable = decay * shadow_variable + (<span class="number">1</span> - decay) * variable</span><br></pre></td></tr></table></figure>
<p><code>shadow_variable</code>是影子变量；<code>variable</code>表示待更新的变量，也就是变量被赋予的值；<code>decay</code>为衰减速率，一般设为接近于<code>1</code>的数(<code>0.99</code>或<code>0.999</code>)，其越大模型越稳定，因为<code>decay</code>越大，参数更新的速度就越慢，趋于稳定。<br>&emsp;&emsp;<code>tf.train.ExponentialMovingAverage</code>这个函数还提供了自己动更新<code>decay</code>的计算方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decay = min(decay, (<span class="number">1</span> + steps) / (<span class="number">10</span> + steps))</span><br></pre></td></tr></table></figure>
<p><code>steps</code>是迭代的次数，可以自己设定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line">step = tf.Variable(tf.constant(<span class="number">0</span>))</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>, step)</span><br><span class="line">maintain_average = ema.apply([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))  <span class="comment"># 初始的值都为0</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">5</span>))  <span class="comment"># 把v1变为5</span></span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, 1/10) = 0.1”，“v1 = 0.1 * 0 + 0.9 * 5 = 4.5”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    sess.run(tf.assign(step, <span class="number">10000</span>))  <span class="comment"># steps = 10000</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">10</span>))  <span class="comment"># v1 = 10</span></span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, (1 + 10000)/(10 + 10000)) = 0.99”，“v1 = 0.99 * 4.5 + 0.01 * 10 = 4.555”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, (1 + 10000)/(10 + 10000)) = 0.99”，“v1 = 0.99 * 4.555 + 0.01 * 10 = 4.6”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure>
<h3 id="tf-moving-average-variables-scope-None"><a href="#tf-moving-average-variables-scope-None" class="headerlink" title="tf.moving_average_variables(scope=None)"></a>tf.moving_average_variables(scope=None)</h3><p>&emsp;&emsp;Returns all variables that maintain their moving averages. If an <code>ExponentialMovingAverage</code> object is created and <code>the apply()</code> method is called on a list of variables, these variables will be added to the <code>GraphKeys.MOVING_AVERAGE_VARIABLES</code> collection. This convenience function returns the contents of that collection.<br>&emsp;&emsp;<code>scope</code> (optional) is a string. If supplied, the resulting list is filtered to include only items whose name attribute matches <code>scope</code> using <code>re.match</code>. Items without a name attribute are never returned if a <code>scope</code> is supplied. The choice of <code>re.match</code> means that a s<code>cope</code> without special tokens filters by prefix.<br>&emsp;&emsp;使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">variable_averages = tf.train.ExponentialMovingAverage(decay, global_step)</span><br><span class="line">variables_to_average = (tf.trainable_variables() + tf.moving_average_variables())</span><br><span class="line">variables_averages_op = variable_averages.apply(variables_to_average)</span><br><span class="line">train_op = tf.group(opt, variables_averages_op)</span><br></pre></td></tr></table></figure>
<h3 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a>tf.control_dependencies</h3><p>&emsp;&emsp;在有些机器学习程序中，我们想要指定某些操作执行的依赖关系，这时可以使用<code>tf.control_dependencies(control_inputs)</code>来实现。该函数返回一个控制依赖的上下文管理器，使用<code>with</code>关键字可以让在这个上下文环境中的操作都在<code>control_inputs</code>执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</span><br><span class="line">    <span class="comment"># 'd' and 'e' will only run after 'a', 'b', and 'c' have executed</span></span><br><span class="line">    d = ...</span><br><span class="line">    e = ...</span><br></pre></td></tr></table></figure>
<p>可以嵌套<code>control_dependencies</code>使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after 'a' and 'b'</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">        <span class="comment"># Ops constructed here run after 'a', 'b', 'c', and 'd'</span></span><br></pre></td></tr></table></figure>
<p>可以传入<code>None</code>来消除依赖：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after 'a' and 'b'</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</span><br><span class="line">        <span class="comment"># Ops constructed here run normally, not waiting for either 'a' or 'b'</span></span><br><span class="line">        <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">            <span class="comment"># Ops constructed here run after 'c' and 'd',</span></span><br><span class="line">            <span class="comment"># also not waiting for either 'a' or 'b'</span></span><br></pre></td></tr></table></figure>
<p>注意，控制依赖只对那些在上下文环境中建立的操作有效，仅仅在<code>context</code>中使用一个操作或张量是没用的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span>  <span class="comment"># WRONG</span></span><br><span class="line">    t = tf.matmul(tensor, tensor)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">        <span class="comment"># The matmul op is created outside the context,</span></span><br><span class="line">        <span class="comment"># so no control dependency will be added</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span>  <span class="comment"># RIGHT</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">        <span class="comment"># The matmul op is created in the context,</span></span><br><span class="line">        <span class="comment"># so a control dependency will be added</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure>
<h3 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.group(*inputs, **kwargs)</span><br></pre></td></tr></table></figure>
<p>Create an op that groups multiple operations. When this op finishes, all ops in <code>inputs</code> have finished. This op has no output.</p>
<ul>
<li><code>*inputs</code>: Zero or more tensors to group.</li>
<li><code>name</code>: A name for this operation (optional).</li>
</ul>
<p>Returns an Operation that executes all its <code>inputs</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">b = tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>], minval=<span class="number">10</span>, maxval=<span class="number">11</span>)</span><br><span class="line">w = tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>], minval=<span class="number">10</span>, maxval=<span class="number">11</span>)</span><br><span class="line">mul = tf.multiply(w, <span class="number">2</span>)</span><br><span class="line">add = tf.add(w, <span class="number">2</span>)</span><br><span class="line">group = tf.group(mul, add)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(group))</span><br></pre></td></tr></table></figure>
<h3 id="tf-add-to-collection和tf-get-collection"><a href="#tf-add-to-collection和tf-get-collection" class="headerlink" title="tf.add_to_collection和tf.get_collection"></a>tf.add_to_collection和tf.get_collection</h3><p>&emsp;&emsp;<code>tf.add_to_collection</code>是把变量放入一个集合，把很多变量变成一个列表；<code>tf.get_collection</code>是从一个集合中取出全部变量，返回值是一个列表；<code>tf.add_n</code>是把一个列表的东西都依次加起来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v1 = tf.get_variable(name=<span class="string">'v1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'loss'</span>, v1)</span><br><span class="line">v2 = tf.get_variable(name=<span class="string">'v2'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'loss'</span>, v2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(tf.get_collection(<span class="string">'loss'</span>))</span><br><span class="line">    print(sess.run(tf.add_n(tf.get_collection(<span class="string">'loss'</span>))))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable <span class="string">'v1:0'</span> shape=(<span class="number">1</span>,) dtype=float32_ref&gt;, &lt;tf.Variable <span class="string">'v2:0'</span> shape=(<span class="number">1</span>,) dtype=float32_ref&gt;]</span><br><span class="line">[<span class="number">2.</span>]</span><br></pre></td></tr></table></figure>
<h3 id="tf-stack"><a href="#tf-stack" class="headerlink" title="tf.stack"></a>tf.stack</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack(values, axis=<span class="number">0</span>, name=<span class="string">'stack'</span>)</span><br></pre></td></tr></table></figure>
<p>Stacks a list of <code>rank-R</code> tensors into one <code>rank-(R+1)</code> tensor.</p>
<ul>
<li><code>values</code>: A list of <code>Tensor</code> objects with the same shape and type.</li>
<li><code>axis</code>: An <code>int</code>. The <code>axis</code> to stack along. Defaults to the first dimension. Negative <code>values</code> wrap around, so the valid range is <code>[-(R+1), R+1)</code>.</li>
<li><code>name</code>: A <code>name</code> for this operation (optional).</li>
</ul>
<p>Return a stacked <code>Tensor</code> with the same type as <code>values</code>.<br>&emsp;&emsp;Packs the list of tensors in <code>values</code> into a tensor with rank one higher than each tensor in <code>values</code>, by packing them along the <code>axis</code> dimension.<br>&emsp;&emsp;Given a list of length <code>N</code> of tensors of shape <code>(A, B, C)</code>: if <code>axis == 0</code> then the output tensor will have the shape <code>(N, A, B, C)</code>; if <code>axis == 1</code> then the output tensor will have the shape <code>(A, N, B, C)</code>. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">y = tf.constant([<span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line">z = tf.constant([<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.stack([x, y, z])  <span class="comment"># [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)</span></span><br><span class="line">tf.stack([x, y, z], axis=<span class="number">1</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6]]</span></span><br></pre></td></tr></table></figure>
<p>This is the opposite of <code>unstack</code>. The <code>numpy</code> equivalent is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([x, y, z]) = np.stack([x, y, z])</span><br></pre></td></tr></table></figure>
<h3 id="tf-unstack"><a href="#tf-unstack" class="headerlink" title="tf.unstack"></a>tf.unstack</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.unstack(value, num=<span class="keyword">None</span>, axis=<span class="number">0</span>, name=<span class="string">'unstack'</span>)</span><br></pre></td></tr></table></figure>
<p>Unpacks the given dimension of a <code>rank-R</code> tensor into <code>rank-(R-1)</code> tensors.<br>&emsp;&emsp;Unpacks <code>num</code> tensors from <code>value</code> by chipping it along the <code>axis</code> dimension. If <code>num</code> is not specified (the default), it is inferred from <code>value&#39;s</code> shape. If <code>value.shape[axis]</code> is not known, <code>ValueError</code> is raised.<br>&emsp;&emsp;For example, given a tensor of shape <code>(A, B, C, D)</code>:</p>
<ul>
<li>If <code>axis == 0</code> then the <code>i&#39;th</code> tensor in output is the slice <code>value[i, :, :, :]</code> and each tensor in output will have shape <code>(B, C, D)</code>. Note that the dimension unpacked along is gone, unlike split.</li>
<li>If <code>axis == 1</code> then the <code>i&#39;th</code> tensor in output is the slice <code>value[:, i, :, :]</code> and each tensor in output will have shape <code>(A, C, D)</code>.<br>&emsp;&emsp;Code example:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">d = tf.unstack(c, axis=<span class="number">0</span>)</span><br><span class="line">e = tf.unstack(c, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(d))  <span class="comment"># 输出“[array([1, 2, 3]), array([4, 5, 6])]”</span></span><br><span class="line">    print(sess.run(e))  <span class="comment"># 输出“[array([1, 4]), array([2, 5]), array([3, 6])]”</span></span><br></pre></td></tr></table></figure>
<p>This is the opposite of <code>stack</code>.</p>
<ul>
<li><code>value</code>: A rank <code>R &gt; 0</code> <code>Tensor</code> to be unstacked.</li>
<li><code>num</code>: An <code>int</code>. The length of the dimension <code>axis</code>. Automatically inferred if <code>None</code> (the default).</li>
<li><code>axis</code>: An <code>int</code>. The <code>axis</code> to <code>unstack</code> along. Defaults to the first dimension. Negative values wrap around, so the valid range is <code>[-R, R)</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>Returns the list of <code>Tensor</code> objects unstacked from <code>value</code>.</p>
<h3 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose"></a>tf.transpose</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.transpose(a, perm=<span class="keyword">None</span>, name=<span class="string">'transpose'</span>, conjugate=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>Transposes <code>a</code>. Permutes the dimensions according to <code>perm</code>. The returned tensor’s dimension <code>i</code> will correspond to the input dimension <code>perm[i]</code>. If <code>perm</code> is not given, it is set to <code>(n-1...0)</code>, where <code>n</code> is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on <code>2-D</code> input <code>Tensors</code>. If <code>conjugate</code> is <code>True</code> and <code>a.dtype</code> is either <code>complex64</code> or <code>complex128</code> then the values of <code>a</code> are conjugated and transposed.</p>
<ul>
<li><code>a</code>: A <code>Tensor</code>.</li>
<li><code>perm</code>: A permutation of the dimensions of <code>a</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>conjugate</code>: Optional <code>bool</code>. Setting it to <code>True</code> is mathematically equivalent to <code>tf.conj(tf.transpose(input))</code>.</li>
</ul>
<p>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tf.transpose(x)</span><br><span class="line"><span class="comment"># [[1, 4]</span></span><br><span class="line"><span class="comment">#  [2, 5]</span></span><br><span class="line"><span class="comment">#  [3, 6]]</span></span><br><span class="line">tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># Equivalently</span></span><br><span class="line"><span class="comment"># [[1, 4]</span></span><br><span class="line"><span class="comment">#  [2, 5]</span></span><br><span class="line"><span class="comment">#  [3, 6]]</span></span><br><span class="line"><span class="comment"># If x is complex, setting conjugate=True gives the conjugate transpose</span></span><br><span class="line">x = tf.constant([[<span class="number">1</span> + <span class="number">1j</span>, <span class="number">2</span> + <span class="number">2j</span>, <span class="number">3</span> + <span class="number">3j</span>], [<span class="number">4</span> + <span class="number">4j</span>, <span class="number">5</span> + <span class="number">5j</span>, <span class="number">6</span> + <span class="number">6j</span>]])</span><br><span class="line">tf.transpose(x, conjugate=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># [[1 - 1j, 4 - 4j],</span></span><br><span class="line"><span class="comment">#  [2 - 2j, 5 - 5j],</span></span><br><span class="line"><span class="comment">#  [3 - 3j, 6 - 6j]]</span></span><br><span class="line"><span class="comment"># "perm" is more useful for n-dimensional tensors, for n &gt; 2</span></span><br><span class="line">x = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">                 [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]])</span><br><span class="line"><span class="comment"># Take the transpose of the matrices in dimension-0(this common operation has a shorthand "matrix_transpose")</span></span><br><span class="line">tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># [[[1,  4],</span></span><br><span class="line"><span class="comment">#   [2,  5],</span></span><br><span class="line"><span class="comment">#   [3,  6]],</span></span><br><span class="line"><span class="comment">#  [[7, 10],</span></span><br><span class="line"><span class="comment">#   [8, 11],</span></span><br><span class="line"><span class="comment">#   [9, 12]]]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;In <code>numpy</code>, transposes are <code>memory-efficient</code> constant time operations as they simply return a new view of the same data with adjusted strides. <code>TensorFlow</code> does not support strides, so transpose returns a new tensor with the items permuted.</p>
<h3 id="tf-set-random-seed"><a href="#tf-set-random-seed" class="headerlink" title="tf.set_random_seed"></a>tf.set_random_seed</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.set_random_seed(seed)</span><br></pre></td></tr></table></figure>
<p>Sets the <code>graph-level</code> random <code>seed</code>. Operations that rely on a random <code>seed</code> actually derive it from two seeds: the <code>graph-level</code> and <code>operation-level</code> seeds. This sets the <code>graph-level</code> seed.<br>&emsp;&emsp;Its interactions with <code>operation-level</code> seeds is as follows:</p>
<ul>
<li>If neither the <code>graph-level</code> nor the operation <code>seed</code> is set: A random <code>seed</code> is used for this op.</li>
<li>If the <code>graph-level</code> <code>seed</code> is set, but the operation <code>seed</code> is not: The system deterministically picks an operation <code>seed</code> in conjunction with the <code>graph-level</code> <code>seed</code> so that it gets a unique random sequence.</li>
<li>If the <code>graph-level</code> <code>seed</code> is not set, but the operation <code>seed</code> is set: A default <code>graph-level</code> <code>seed</code> and the specified operation <code>seed</code> are used to determine the random sequence.</li>
<li>If both the <code>graph-level</code> and the operation <code>seed</code> are set: Both seeds are used in conjunction to determine the random sequence.</li>
</ul>
<p>&emsp;&emsp;To generate different sequences across sessions, set neither <code>graph-level</code> nor <code>op-level</code> seeds:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>])</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A3'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A4'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B3'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B4'</span></span><br></pre></td></tr></table></figure>
<p>To generate the same repeatable sequence for an op across sessions, set the <code>seed</code> for the op:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>], seed=<span class="number">1</span>)</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Repeatedly running this block with the same graph will generate the same</span></span><br><span class="line"><span class="comment"># sequence of values for 'a', but different sequences of values for 'b'.</span></span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B3'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B4'</span></span><br></pre></td></tr></table></figure>
<p>To make the random sequences generated by all ops be repeatable across sessions, set a graph-level seed:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.set_random_seed(<span class="number">1234</span>)</span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>])</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Repeatedly running this block with the same graph</span></span><br><span class="line"><span class="comment"># will generate the same sequences of 'a' and 'b'.</span></span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B2'</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-identity"><a href="#tf-identity" class="headerlink" title="tf.identity"></a>tf.identity</h3><p>&emsp;&emsp;下面的程序想要执行<code>5</code>次循环，每次循环给<code>x</code>加<code>1</code>并赋值给<code>y</code>，然后打印出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)  <span class="comment"># 返回一个op，表示给变量x加1的操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>打印出的结果都是<code>0.0</code>，也就是说没有达到预期的效果。这是因为<code>y</code>只是复制了<code>x</code>变量内容，并未和<code>tensorflow</code>图上的节点相联系，不能执行节点上的操作。进行如下修改就能实现需要的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = tf.identity(x)  <span class="comment"># 修改部分</span></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>也就是说，<code>tf.identity</code>返回了一个和<code>x</code>相同的的新<code>tensor</code>。</p>
<h3 id="TensorFlow之命令行参数"><a href="#TensorFlow之命令行参数" class="headerlink" title="TensorFlow之命令行参数"></a>TensorFlow之命令行参数</h3><p>&emsp;&emsp;<code>TensorFlow</code>定义了<code>tf.app.flags</code>(也可以用它的别名<code>tf.flags</code>)，用于支持接受命令行传递参数，相当于接收<code>argv</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是参数名称，第二个参数是默认值，第三个是参数描述</span></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'str_name'</span>, <span class="string">'def_v_1'</span>, <span class="string">"descript1"</span>)</span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">'int_name'</span>, <span class="number">10</span>, <span class="string">"descript2"</span>)</span><br><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">'bool_name'</span>, <span class="keyword">False</span>, <span class="string">"descript3"</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    print(FLAGS.str_name)</span><br><span class="line">    print(FLAGS.int_name)</span><br><span class="line">    print(FLAGS.bool_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()  <span class="comment"># run main function</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python test.py --str_name <span class="string">"hello"</span> --int_name <span class="number">12</span> --bool_name <span class="keyword">True</span></span><br><span class="line">hello</span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<h3 id="Session-run和Tensor-eval"><a href="#Session-run和Tensor-eval" class="headerlink" title="Session.run和Tensor.eval"></a>Session.run和Tensor.eval</h3><p>&emsp;&emsp;<code>TensorFlow</code>运行代码时，在会话中需要运行节点，可能会碰到两种方式，即<code>Session.run</code>和<code>Tensor.eval</code>，两者之间的差异如下。<br>&emsp;&emsp;如果你有一个<code>Tensor t</code>，在使用<code>t.eval</code>时，等价于<code>tf.get_default_session().run(t)</code>，实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant(<span class="number">42.0</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():  <span class="comment"># or `with sess:` to close on exit</span></span><br><span class="line">    <span class="keyword">assert</span> sess <span class="keyword">is</span> tf.get_default_session()</span><br><span class="line">    <span class="keyword">assert</span> t.eval() == sess.run(t)</span><br></pre></td></tr></table></figure>
<p>这其中最主要的区别就在于，你可以使用<code>sess.run</code>在同一步获取多个<code>tensor</code>中的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant(<span class="number">42.0</span>)</span><br><span class="line">u = tf.constant(<span class="number">37.0</span>)</span><br><span class="line">tu = tf.mul(t, u)</span><br><span class="line">ut = tf.mul(u, t)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    tu.eval()  <span class="comment"># runs one step</span></span><br><span class="line">    ut.eval()  <span class="comment"># runs one step</span></span><br><span class="line">    sess.run([tu, ut])  <span class="comment"># evaluates both tensors in a single step</span></span><br></pre></td></tr></table></figure>
<p>注意到，每次使用<code>eval</code>和<code>run</code>时，都会执行整个计算图，为了获取计算的结果，将它分配给<code>tf.Variable</code>，然后获取。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/The Ultimate Guide To Speech Recognition With Python/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/The Ultimate Guide To Speech Recognition With Python/" itemprop="url">The Ultimate Guide To Speech Recognition With Python</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T16:11:08+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;Have you ever wondered how to add speech recognition to your <code>Python</code> project? If so, then keep reading! It’s easier than you might think.<br>&emsp;&emsp;Far from a being a fad, the overwhelming success of <code>speech-enabled</code> products like <code>Amazon Alexa</code> has proven that some degree of speech support will be an essential aspect of household tech for the foreseeable future. If you think about it, the reasons why are pretty obvious. Incorporating speech recognition into your <code>Python</code> application offers a level of interactivity and accessibility that few technologies can match.<br>&emsp;&emsp;The accessibility improvements alone are worth considering. Speech recognition allows the elderly and the physically and visually impaired to interact with <code>state-of-the-art</code> products and services quickly and naturally.<br>&emsp;&emsp;Best of all, including speech recognition in a <code>Python</code> project is really simple. In this guide, you’ll find out how. You’ll learn:</p>
<ul>
<li>How speech recognition works?</li>
<li>What packages are available on <code>PyPI</code>?</li>
<li>How to install and use the <code>SpeechRecognition</code> package - a <code>full-featured</code> and <code>easy-to-use</code> <code>Python</code> speech recognition library.</li>
</ul>
<p>&emsp;&emsp;In the end, you’ll apply what you’ve learned to a simple <code>Guess the Word</code> game and see how it all comes together.</p>
<h3 id="How-Speech-Recognition-Works-An-Overview"><a href="#How-Speech-Recognition-Works-An-Overview" class="headerlink" title="How Speech Recognition Works - An Overview"></a>How Speech Recognition Works - An Overview</h3><p>&emsp;&emsp;Before we get to the <code>nitty-gritty</code> of doing speech recognition in <code>Python</code>, let’s take a moment to talk about how speech recognition works. A full discussion would fill a book, so I won’t bore you with all of the technical details here. In fact, this section is not <code>pre-requisite</code> to the rest of the tutorial. If you’d like to get straight to the point, then feel free to skip ahead.<br>&emsp;&emsp;Speech recognition has its roots in research done at <code>Bell Labs</code> in the early <code>1950s</code>. Early systems were limited to a single speaker and had limited vocabularies of about a dozen words. Modern speech recognition systems have come a long way since their ancient counterparts. They can recognize speech from multiple speakers and have enormous vocabularies in numerous languages.<br>&emsp;&emsp;The first component of speech recognition is, of course, speech. Speech must be converted from physical sound to an electrical signal with a microphone, and then to digital data with an <code>analog-to-digital</code> converter. Once digitized, several models can be used to transcribe the audio to text.<br>&emsp;&emsp;Most modern speech recognition systems rely on what is known as a <code>Hidden Markov Model</code> (<code>HMM</code>). This approach works on the assumption that a speech signal, when viewed on a short enough timescale (say, ten milliseconds), can be reasonably approximated as a stationary process - that is, a process in which statistical properties do not change over time.<br>&emsp;&emsp;In a typical <code>HMM</code>, the speech signal is divided into <code>10-millisecond</code> fragments. The power spectrum of each fragment, which is essentially a plot of the signal’s power as a function of frequency, is mapped to a vector of real numbers known as cepstral coefficients. The dimension of this vector is usually small - sometimes as low as <code>10</code>, although more accurate systems may have dimension <code>32</code> or more. The final output of the <code>HMM</code> is a sequence of these vectors.<br>&emsp;&emsp;To decode the speech into text, groups of vectors are matched to one or more phonemes - a fundamental unit of speech. This calculation requires training, since the sound of a phoneme varies from speaker to speaker, and even varies from one utterance to another by the same speaker. A special algorithm is then applied to determine the most likely word (or words) that produce the given sequence of phonemes.<br>&emsp;&emsp;One can imagine that this whole process may be computationally expensive. In many modern speech recognition systems, neural networks are used to simplify the speech signal using techniques for feature transformation and dimensionality reduction before <code>HMM</code> recognition. <code>Voice activity detectors</code> (<code>VADs</code>) are also used to reduce an audio signal to only the portions that are likely to contain speech. This prevents the recognizer from wasting time analyzing unnecessary parts of the signal.<br>&emsp;&emsp;Fortunately, as a <code>Python</code> programmer, you don’t have to worry about any of this. A number of speech recognition services are available for use online through an <code>API</code>, and many of these services offer <code>Python SDKs</code>.</p>
<h3 id="Picking-a-Python-Speech-Recognition-Package"><a href="#Picking-a-Python-Speech-Recognition-Package" class="headerlink" title="Picking a Python Speech Recognition Package"></a>Picking a Python Speech Recognition Package</h3><p>&emsp;&emsp;A handful of packages for speech recognition exist on <code>PyPI</code>. A few of them include: <code>apiai</code>, <code>assemblyai</code>, <code>google-cloud-speech</code>, <code>pocketsphinx</code>, <code>SpeechRecognition</code>, <code>watson-developer-cloud</code>, <code>wit</code>.<br>&emsp;&emsp;Some of these packages - such as <code>wit</code> and <code>apiai</code> - offer <code>built-in</code> features, like natural language processing for identifying a speaker’s intent, which go beyond basic speech recognition. Others, like <code>google-cloud-speech</code>, focus solely on <code>speech-to-text</code> conversion.<br>&emsp;&emsp;There is one package that stands out in terms of <code>ease-of-use</code>: <code>SpeechRecognition</code>. Recognizing speech requires audio input, and <code>SpeechRecognition</code> makes retrieving this input really easy. Instead of having to build scripts for accessing microphones and processing audio files from scratch, <code>SpeechRecognition</code> will have you up and running in just a few minutes.<br>&emsp;&emsp;The <code>SpeechRecognition</code> library acts as a wrapper for several popular speech <code>APIs</code> and is thus extremely flexible. One of these - the <code>Google Web Speech API</code> - supports a default <code>API</code> key that is <code>hard-coded</code> into the <code>SpeechRecognition</code> library. That means you can get off your feet without having to sign up for a service.<br>&emsp;&emsp;The flexibility and <code>ease-of-use</code> of the <code>SpeechRecognition</code> package make it an excellent choice for any <code>Python</code> project. However, support for every feature of each <code>API</code> it wraps is not guaranteed. You will need to spend some time researching the available options to find out if <code>SpeechRecognition</code> will work in your particular case.<br>&emsp;&emsp;So, now that you’re convinced you should try out <code>SpeechRecognition</code>, the next step is getting it installed in your environment.</p>
<h3 id="Installing-SpeechRecognition"><a href="#Installing-SpeechRecognition" class="headerlink" title="Installing SpeechRecognition"></a>Installing SpeechRecognition</h3><p>&emsp;&emsp;<code>SpeechRecognition</code> is compatible with <code>Python 2.6</code>, <code>2.7</code> and <code>3.3+</code>, but requires some additional installation steps for <code>Python 2</code>. For this tutorial, I’ll assume you are using <code>Python 3.3+</code>.<br>&emsp;&emsp;You can install <code>SpeechRecognition</code> from a terminal with <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install SpeechRecognition</span><br></pre></td></tr></table></figure>
<p>Once installed, you should verify the installation by opening an interpreter session and typing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sr.__version__</span><br><span class="line"><span class="string">'3.8.1'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Go ahead and keep this session open. You’ll start to work with it in just a bit.<br>&emsp;&emsp;<code>SpeechRecognition</code> will work out of the box if all you need to do is work with existing audio files. Specific use cases, however, require a few dependencies. Notably, the <code>PyAudio</code> package is needed for capturing microphone input.<br>&emsp;&emsp;You’ll see which dependencies you need as you read further. For now, let’s dive in and explore the basics of the package.</p>
<h3 id="The-Recognizer-Class"><a href="#The-Recognizer-Class" class="headerlink" title="The Recognizer Class"></a>The Recognizer Class</h3><p>&emsp;&emsp;All of the magic in <code>SpeechRecognition</code> happens with the <code>Recognizer</code> class. The primary purpose of a <code>Recognizer</code> instance is, of course, to recognize speech. Each instance comes with a variety of settings and functionality for recognizing speech from an audio source.<br>&emsp;&emsp;Creating a <code>Recognizer</code> instance is easy. In your current interpreter session, just type:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = sr.Recognizer()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Each Recognizer instance has seven methods for recognizing speech from an audio source using various <code>APIs</code>. These are:</p>
<ul>
<li><code>recognize_bing()</code>: <code>Microsoft Bing Speech</code></li>
<li><code>recognize_google()</code>: <code>Google Web Speech API</code></li>
<li><code>recognize_google_cloud()</code>: <code>Google Cloud Speech</code> - requires installation of the <code>google-cloud-speech</code> package</li>
<li><code>recognize_houndify()</code>: <code>Houndify by SoundHound</code></li>
<li><code>recognize_ibm()</code>: <code>IBM Speech to Text</code></li>
<li><code>recognize_sphinx()</code>: <code>CMU Sphinx</code> - requires installing <code>PocketSphinx</code></li>
<li><code>recognize_wit()</code>: <code>Wit.ai</code></li>
</ul>
<p>Of the seven, only <code>recognize_sphinx()</code> works offline with the <code>CMU Sphinx</code> engine. The other six all require an internet connection.<br>&emsp;&emsp;A full discussion of the features and benefits of each <code>API</code> is beyond the scope of this tutorial. Since <code>SpeechRecognition</code> ships with a default <code>API</code> key for the <code>Google Web Speech API</code>, you can get started with it right away. For this reason, we’ll use the <code>Web Speech API</code> in this guide. The other six <code>APIs</code> all require authentication with either an <code>API</code> key or a <code>username/password</code> combination. For more information, consult the <code>SpeechRecognition</code> docs.<br>&emsp;&emsp;Caution: The default key provided by <code>SpeechRecognition</code> is for testing purposes only, and <code>Google</code> may revoke it at any time. It is not a good idea to use the <code>Google Web Speech API</code> in production. Even with a valid <code>API</code> key, you’ll be limited to only <code>50</code> requests per day, and there is no way to raise this quota. Fortunately, <code>SpeechRecognition&#39;s</code> interface is nearly identical for each <code>API</code>, so what you learn today will be easy to translate to a <code>real-world</code> project.<br>&emsp;&emsp;Each <code>recognize_*()</code> method will throw a <code>RequestError</code> exception if the <code>API</code> is unreachable. For <code>recognize_sphinx()</code>, this could happen as the result of a missing, corrupt or incompatible <code>Sphinx</code> installation. For the other six methods, <code>RequestError</code> may be thrown if quota limits are met, the server is unavailable, or there is no internet connection.<br>&emsp;&emsp;Let’s get our hands dirty. Go ahead and try to call <code>recognize_google()</code> in your interpreter session.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google()</span><br></pre></td></tr></table></figure>
<p>What happened? You probably got something that looks like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: recognize_google() missing <span class="number">1</span> required positional argument: <span class="string">'audio_data'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;You might have guessed this would happen. How could something be recognized from nothing? All seven <code>recognize_*()</code> methods of the <code>Recognizer</code> class require an <code>audio_data</code> argument. In each case, <code>audio_data</code> must be an instance of <code>SpeechRecognition&#39;s</code> <code>AudioData</code> class.<br>&emsp;&emsp;There are two ways to create an <code>AudioData</code> instance: from an audio file or audio recorded by a microphone. Audio files are a little easier to get started with, so let’s take a look at that first.</p>
<h3 id="Working-With-Audio-Files"><a href="#Working-With-Audio-Files" class="headerlink" title="Working With Audio Files"></a>Working With Audio Files</h3><p>&emsp;&emsp;Before you continue, you’ll need to download an audio file. The one I used to get started, <code>harvard.wav</code> can be found here (<code>https://github.com/realpython/python-speech-recognition</code>). Make sure you save it to the same directory in which your <code>Python</code> interpreter session is running.<br>&emsp;&emsp;<code>SpeechRecognition</code> makes working with audio files easy thanks to its handy <code>AudioFile</code> class. This class can be initialized with the path to an audio file and provides a context manager interface for reading and working with the file’s contents.</p>
<h4 id="Supported-File-Types"><a href="#Supported-File-Types" class="headerlink" title="Supported File Types"></a>Supported File Types</h4><p>&emsp;&emsp;Currently, <code>SpeechRecognition</code> supports the following file formats: <code>WAV</code>: must be in <code>PCM/LPCM</code> format, <code>AIFF</code>, <code>AIFF-C</code>, <code>FLAC</code>: must be native <code>FLAC</code> format; <code>OGG-FLAC</code> is not supported.<br>&emsp;&emsp;If you are working on <code>X86</code> based <code>Linux</code>, <code>macOS</code> or <code>Windows</code>, you should be able to work with <code>FLAC</code> files without a problem. On other platforms, you will need to install a <code>FLAC</code> encoder and ensure you have access to the flac command line tool. You can find more information here if this applies to you.</p>
<h4 id="Using-record-to-Capture-Data-From-a-File"><a href="#Using-record-to-Capture-Data-From-a-File" class="headerlink" title="Using record() to Capture Data From a File"></a>Using record() to Capture Data From a File</h4><p>&emsp;&emsp;Type the following into your interpreter session to process the contents of the <code>harvard.wav</code> file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>harvard = sr.AudioFile(<span class="string">'harvard.wav'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>   audio = r.record(source)</span><br></pre></td></tr></table></figure>
<p>The context manager opens the file and reads its contents, storing the data in an <code>AudioFile</code> instance called source. Then the <code>record()</code> method records the data from the entire file into an <code>AudioData</code> instance. You can confirm this by checking the type of audio:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(audio)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">speech_recognition</span>.<span class="title">AudioData</span>'&gt;</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;You can now invoke <code>recognize_google()</code> to attempt to recognize any speech in the audio. Depending on your internet connection speed, you may have to wait several seconds before seeing the result.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the stale smell of old beer lingers it takes heat to bring out</span></span><br><span class="line"><span class="string"> the odor a cold dip restores health and zest a salt pickle taste</span></span><br><span class="line"><span class="string"> fine with ham tacos al Pastore are my favorite a zestful food is the hot cross bun'</span></span><br></pre></td></tr></table></figure>
<p>Congratulations! You’ve just transcribed your first audio file!<br>&emsp;&emsp;If you’re wondering where the phrases in the <code>harvard.wav</code> file come from, they are examples of <code>Harvard Sentences</code>. These phrases were published by the <code>IEEE</code> in <code>1965</code> for use in speech intelligibility testing of telephone lines. They are still used in <code>VoIP</code> and cellular testing today.<br>&emsp;&emsp;The <code>Harvard Sentences</code> are comprised of <code>72</code> lists of ten phrases. You can find freely available recordings of these phrases on the <code>Open Speech Repository</code> website. Recordings are available in <code>English</code>, <code>Mandarin Chinese</code>, <code>French</code>, and <code>Hindi</code>. They provide an excellent source of free material for testing your code.</p>
<h4 id="Capturing-Segments-With-offset-and-duration"><a href="#Capturing-Segments-With-offset-and-duration" class="headerlink" title="Capturing Segments With offset and duration"></a>Capturing Segments With offset and duration</h4><p>&emsp;&emsp;What if you only want to capture a portion of the speech in a file? The <code>record()</code> method accepts a duration keyword argument that stops the recording after a specified number of seconds.<br>&emsp;&emsp;For example, the following captures any speech in the first four seconds of the file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source, duration=<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the stale smell of old beer lingers'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The <code>record()</code> method, when used inside a with block, always moves ahead in the file stream. This means that if you record once for four seconds and then record again for four seconds, the second time returns the four seconds of audio after the first four seconds.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio1 = r.record(source, duration=<span class="number">4</span>)</span><br><span class="line"><span class="meta">... </span>    audio2 = r.record(source, duration=<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio1)</span><br><span class="line"><span class="string">'the stale smell of old beer lingers'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio2)</span><br><span class="line"><span class="string">'it takes heat to bring out the odor a cold dip'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Notice that <code>audio2</code> contains a portion of the third phrase in the file. When specifying a duration, the recording might stop <code>mid-phrase</code> or even <code>mid-word-which</code> can hurt the accuracy of the transcription. More on this in a bit.<br>&emsp;&emsp;In addition to specifying a recording duration, the <code>record()</code> method can be given a specific starting point using the offset keyword argument. This value represents the number of seconds from the beginning of the file to ignore before starting to record.<br>&emsp;&emsp;To capture only the second phrase in the file, you could start with an offset of four seconds and record for, say, three seconds.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source, offset=<span class="number">4</span>, duration=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recognizer.recognize_google(audio)</span><br><span class="line"><span class="string">'it takes heat to bring out the odor'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The offset and duration keyword arguments are useful for segmenting an audio file if you have prior knowledge of the structure of the speech in the file. However, using them hastily can result in poor transcriptions. To see this effect, try the following in your interpreter:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source, offset=<span class="number">4.7</span>, duration=<span class="number">2.8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recognizer.recognize_google(audio)</span><br><span class="line"><span class="string">'Mesquite to bring out the odor Aiko'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;By starting the recording at <code>4.7</code> seconds, you miss the <code>it t</code> portion a the beginning of the phrase <code>it takes heat to bring out the odor</code>, so the <code>API</code> only got <code>akes heat</code> which it matched to <code>Mesquite</code>.<br>&emsp;&emsp;Similarly, at the end of the recording, you captured <code>a co</code>, which is the beginning of the third phrase <code>a cold dip restores health and zest</code>. This was matched to <code>Aiko</code> by the <code>API</code>.<br>&emsp;&emsp;There is another reason you may get inaccurate transcriptions. Noise! The above examples worked well because the audio file is reasonably clean. In the real world, unless you have the opportunity to process audio files beforehand, you can not expect the audio to be <code>noise-free</code>.</p>
<h4 id="The-Effect-of-Noise-on-Speech-Recognition"><a href="#The-Effect-of-Noise-on-Speech-Recognition" class="headerlink" title="The Effect of Noise on Speech Recognition"></a>The Effect of Noise on Speech Recognition</h4><p>&emsp;&emsp;Noise is a fact of life. All audio recordings have some degree of noise in them, and <code>un-handled</code> noise can wreck the accuracy of speech recognition apps.<br>&emsp;&emsp;To get a feel for how noise can affect speech recognition, download the <code>jackhammer.wav</code> file here (<code>https://github.com/realpython/python-speech-recognition</code>). As always, make sure you save this to your interpreter session’s working directory.<br>&emsp;&emsp;This file has the phrase <code>the stale smell of old beer lingers</code> spoken with a loud jackhammer in the background. What happens when you try to transcribe this file?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>jackhammer = sr.AudioFile(<span class="string">'jackhammer.wav'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> jackhammer <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the snail smell of old gear vendors'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;So how do you deal with this? One thing you can try is using the <code>adjust_for_ambient_noise()</code> method of the Recognizer class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> jackhammer <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    r.adjust_for_ambient_noise(source)</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'still smell of old beer vendors'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;That got you a little closer to the actual phrase, but it still isn’t perfect. Also, <code>the</code> is missing from the beginning of the phrase. Why is that?<br>&emsp;&emsp;The <code>adjust_for_ambient_noise()</code> method reads the first second of the file stream and calibrates the recognizer to the noise level of the audio. Hence, that portion of the stream is consumed before you call <code>record()</code> to capture the data.<br>&emsp;&emsp;You can adjust the <code>time-frame</code> that <code>adjust_for_ambient_noise()</code> uses for analysis with the duration keyword argument. This argument takes a numerical value in seconds and is set to <code>1</code> by default. Try lowering this value to <code>0.5</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> jackhammer <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    r.adjust_for_ambient_noise(source, duration=<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the snail smell like old Beer Mongers'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Well, that got you <code>the</code> at the beginning of the phrase, but now you have some new issues! Sometimes it isn’t possible to remove the effect of the noise - the signal is just too noisy to be dealt with successfully. That’s the case with this file.<br>&emsp;&emsp;If you find yourself running up against these issues frequently, you may have to resort to some <code>pre-processing</code> of the audio. This can be done with audio editing software or a <code>Python</code> package (such as <code>SciPy</code>) that can apply filters to the files. A detailed discussion of this is beyond the scope of this tutorial - check out <code>Allen Downey&#39;s Think DSP</code> book if you are interested. For now, just be aware that ambient noise in an audio file can cause problems and must be addressed in order to maximize the accuracy of speech recognition.<br>&emsp;&emsp;When working with noisy files, it can be helpful to see the actual <code>API</code> response. Most <code>APIs</code> return a <code>JSON</code> string containing many possible transcriptions. The <code>recognize_google()</code> method will always return the most likely transcription unless you force it to give you the full response.<br>&emsp;&emsp;You can do this by setting the <code>show_all</code> keyword argument of the <code>recognize_google()</code> method to <code>True</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio, show_all=<span class="keyword">True</span>)</span><br><span class="line">&#123;<span class="string">'alternative'</span>: [</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the snail smell like old Beer Mongers'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the snail smell like old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the stale smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the snail smell like old beermongers'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'destihl smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell like old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'bastille smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell like old beermongers'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell of old beer venders'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smelling old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'musty smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell of old beer vendor'</span>&#125;</span><br><span class="line">], <span class="string">'final'</span>: <span class="keyword">True</span>&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;As you can see, <code>recognize_google()</code> returns a dictionary with the key <code>alternative</code> that points to a list of possible transcripts. The structure of this response may vary from <code>API</code> to <code>API</code> and is mainly useful for debugging.<br>&emsp;&emsp;By now, you have a pretty good idea of the basics of the <code>SpeechRecognition</code> package. You’ve seen how to create an <code>AudioFile</code> instance from an audio file and use the <code>record()</code> method to capture data from the file. You learned how record segments of a file using the offset and duration keyword arguments of <code>record()</code>, and you experienced the detrimental effect noise can have on transcription accuracy.<br>&emsp;&emsp;Now for the fun part. Let’s transition from transcribing static audio files to making your project interactive by accepting input from a microphone.</p>
<h3 id="Working-With-Microphones"><a href="#Working-With-Microphones" class="headerlink" title="Working With Microphones"></a>Working With Microphones</h3><p>&emsp;&emsp;To access your microphone with <code>SpeechRecognizer</code>, you’ll have to install the <code>PyAudio</code> package. Go ahead and close your current interpreter session, and let’s do that.</p>
<h4 id="Installing-PyAudio"><a href="#Installing-PyAudio" class="headerlink" title="Installing PyAudio"></a>Installing PyAudio</h4><p>&emsp;&emsp;The process for installing <code>PyAudio</code> will vary depending on your operating system.<br>&emsp;&emsp;<code>Debian Linux</code>: If you’re on <code>Debian-based Linux</code> (like <code>Ubuntu</code>), you can install <code>PyAudio</code> with apt:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python-pyaudio python3-pyaudio</span><br></pre></td></tr></table></figure>
<p>Once installed, you may still need to run pip install <code>pyaudio</code>, especially if you are working in a virtual environment.<br>&emsp;&emsp;<code>macOS</code>: For <code>macOS</code>, first you will need to install <code>PortAudio</code> with <code>Homebrew</code>, and then install <code>PyAudio</code> with <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew install portaudio</span><br><span class="line">pip install pyaudio</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>Windows</code>: On <code>Windows</code>, you can install <code>PyAudio</code> with <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyaudio</span><br></pre></td></tr></table></figure>
<h4 id="Testing-the-Installation"><a href="#Testing-the-Installation" class="headerlink" title="Testing the Installation"></a>Testing the Installation</h4><p>&emsp;&emsp;Once you’ve got <code>PyAudio</code> installed, you can test the installation from the console.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m speech_recognition</span><br></pre></td></tr></table></figure>
<p>Make sure your default microphone is on and unmuted. If the installation worked, you should see something like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A moment of silence, please...</span><br><span class="line">Set minimum energy threshold to <span class="number">600.4452854381937</span></span><br><span class="line">Say something!</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Go ahead and play around with it a little bit by speaking into your microphone and seeing how well <code>SpeechRecognition</code> transcribes your speech.<br>&emsp;&emsp;<strong>Note</strong>: If you are on <code>Ubuntu</code> and get some funky output like <code>ALSA lib ... Unknown PCM</code>, refer to this page for tips on suppressing these messages. This output comes from the <code>ALSA</code> package installed with <code>Ubuntu</code> - not <code>SpeechRecognition</code> or <code>PyAudio</code>. In all reality, these messages may indicate a problem with your <code>ALSA</code> configuration, but in my experience, they do not impact the functionality of your code. They are mostly a nuisance.</p>
<h4 id="The-Microphone-Class"><a href="#The-Microphone-Class" class="headerlink" title="The Microphone Class"></a>The Microphone Class</h4><p>&emsp;&emsp;Open up another interpreter session and create an instance of the recognizer class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = sr.Recognizer()</span><br></pre></td></tr></table></figure>
<p>Now, instead of using an audio file as the source, you will use the default system microphone. You can access this by creating an instance of the <code>Microphone</code> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mic = sr.Microphone()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;If your system has no default microphone (such as on a <code>RaspberryPi</code>), or you want to use a microphone other than the default, you will need to specify which one to use by supplying a device index. You can get a list of microphone names by calling the <code>list_microphone_names()</code> static method of the <code>Microphone</code> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sr.Microphone.list_microphone_names()</span><br><span class="line">[<span class="string">'HDA Intel PCH: ALC272 Analog (hw:0,0)'</span>,</span><br><span class="line"> <span class="string">'HDA Intel PCH: HDMI 0 (hw:0,3)'</span>,</span><br><span class="line"> <span class="string">'sysdefault'</span>,</span><br><span class="line"> <span class="string">'front'</span>,</span><br><span class="line"> <span class="string">'surround40'</span>,</span><br><span class="line"> <span class="string">'surround51'</span>,</span><br><span class="line"> <span class="string">'surround71'</span>,</span><br><span class="line"> <span class="string">'hdmi'</span>,</span><br><span class="line"> <span class="string">'pulse'</span>,</span><br><span class="line"> <span class="string">'dmix'</span>,</span><br><span class="line"> <span class="string">'default'</span>]</span><br></pre></td></tr></table></figure>
<p>Note that your output may differ from the above example. The device index of the microphone is the index of its name in the list returned by <code>list_microphone_names()</code>. For example, given the above output, if you want to use the microphone called <code>front</code>, which has index <code>3</code> in the list, you would create a microphone instance like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is just an example, do not run</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mic = sr.Microphone(device_index=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>For most projects, though, you’ll probably want to use the default system microphone.</p>
<h4 id="Using-listen-to-Capture-Microphone-Input"><a href="#Using-listen-to-Capture-Microphone-Input" class="headerlink" title="Using listen() to Capture Microphone Input"></a>Using listen() to Capture Microphone Input</h4><p>&emsp;&emsp;Now that you’ve got a <code>Microphone</code> instance ready to go, it’s time to capture some input.<br>&emsp;&emsp;Just like the <code>AudioFile</code> class, <code>Microphone</code> is a context manager. You can capture input from the microphone using the <code>listen()</code> method of the <code>Recognizer</code> class inside of the with block. This method takes an audio source as its first argument and records input from the source until silence is detected.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> mic <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.listen(source)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Once you execute the with block, try speaking <code>hello</code> into your microphone. Wait a moment for the interpreter prompt to display again. Once the <code>&gt;&gt;&gt;</code> prompt returns, you’re ready to recognize the speech.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'hello'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;If the prompt never returns, your microphone is most likely picking up too much ambient noise. You can interrupt the process with <code>ctrl + c</code> to get your prompt back.<br>&emsp;&emsp;To handle ambient noise, you’ll need to use the <code>adjust_for_ambient_noise()</code> method of the <code>Recognizer</code> class, just like you did when trying to make sense of the noisy audio file. Since input from a microphone is far less predictable than input from an audio file, it is a good idea to do this anytime you listen for microphone input.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> mic <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    r.adjust_for_ambient_noise(source)</span><br><span class="line"><span class="meta">... </span>    audio = r.listen(source)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;After running the above code, wait a second for <code>adjust_for_ambient_noise()</code> to do its thing, then try speaking <code>hello</code> into the microphone. Again, you will have to wait a moment for the interpreter prompt to return before trying to recognize the speech.<br>&emsp;&emsp;<code>Recall that adjust_for_ambient_noise()</code> analyzes the audio source for one second. If this seems too long to you, feel free to adjust this with the duration keyword argument.<br>&emsp;&emsp;The <code>SpeechRecognition</code> documentation recommends using a duration no less than <code>0.5</code> seconds. In some cases, you may find that durations longer than the default of one second generate better results. The minimum value you need depends on the microphone’s ambient environment. Unfortunately, this information is typically unknown during development. In my experience, the default duration of one second is adequate for most applications.</p>
<h4 id="Handling-Unrecognizable-Speech"><a href="#Handling-Unrecognizable-Speech" class="headerlink" title="Handling Unrecognizable Speech"></a>Handling Unrecognizable Speech</h4><p>&emsp;&emsp;Try typing the previous code example in to the interpeter and making some unintelligible noises into the microphone. You should get something like this in response:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">"/home/david/real_python/speech_recognition_primer/</span></span><br><span class="line"><span class="string">       venv/lib/python3.5/site-packages/speech_recognition/__init__.py"</span>,</span><br><span class="line">       line <span class="number">858</span>, <span class="keyword">in</span> recognize_google</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(actual_result, dict) <span class="keyword">or</span> len(actual_result.get(<span class="string">"alternative"</span>, [])) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> UnknownValueError()</span><br><span class="line">speech_recognition.UnknownValueError</span><br></pre></td></tr></table></figure>
<p>Audio that cannot be matched to text by the <code>API</code> raises an <code>UnknownValueError</code> exception. You should always wrap calls to the <code>API</code> with try and except blocks to handle this exception.<br>&emsp;&emsp;<strong>Note</strong>: You may have to try harder than you expect to get the exception thrown. The <code>API</code> works very hard to transcribe any vocal sounds. Even short grunts were transcribed as words like <code>how</code> for me. Coughing, hand claps, and tongue clicks would consistently raise the exception.</p>
<h4 id="Putting-It-All-Together-A-“Guess-the-Word”-Game"><a href="#Putting-It-All-Together-A-“Guess-the-Word”-Game" class="headerlink" title="Putting It All Together: A “Guess the Word” Game"></a>Putting It All Together: A “Guess the Word” Game</h4><p>&emsp;&emsp;Now that you’ve seen the basics of recognizing speech with the <code>SpeechRecognition</code> package let’s put your newfound knowledge to use and write a small game that picks a random word from a list and gives the user three attempts to guess the word.<br>&emsp;&emsp;Here is the full script:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recognize_speech_from_mic</span><span class="params">(recognizer, microphone)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Transcribe speech from recorded from `microphone`. Returns a dictionary with three keys:</span></span><br><span class="line"><span class="string">    "success": a boolean indicating whether or not the API request was successful</span></span><br><span class="line"><span class="string">    "error": `None` if no error occured, otherwise a string containing an error message</span></span><br><span class="line"><span class="string">             if the API could not be reached or speech was unrecognizable</span></span><br><span class="line"><span class="string">    "transcription": `None` if speech could not be transcribed, otherwise</span></span><br><span class="line"><span class="string">                     a string containing the transcribed text</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># check that recognizer and microphone arguments are appropriate type</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(recognizer, sr.Recognizer):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">"`recognizer` must be `Recognizer` instance"</span>)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(microphone, sr.Microphone):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">"`microphone` must be `Microphone` instance"</span>)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># adjust the recognizer sensitivity to ambient noise and record audio from the microphone</span></span><br><span class="line">    <span class="keyword">with</span> microphone <span class="keyword">as</span> source:</span><br><span class="line">        recognizer.adjust_for_ambient_noise(source)</span><br><span class="line">        audio = recognizer.listen(source)</span><br><span class="line">​</span><br><span class="line">    response = &#123;<span class="string">"success"</span>: <span class="keyword">True</span>, <span class="string">"error"</span>: <span class="keyword">None</span>, <span class="string">"transcription"</span>: <span class="keyword">None</span>&#125;  <span class="comment"># set up the response object</span></span><br><span class="line">​</span><br><span class="line">    <span class="comment"># try recognizing the speech in the recording. if a RequestError or UnknownValueError exception is</span></span><br><span class="line">    <span class="comment"># caught, update the response object accordingly</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response[<span class="string">"transcription"</span>] = recognizer.recognize_google(audio)</span><br><span class="line">    <span class="keyword">except</span> sr.RequestError:  <span class="comment"># API was unreachable or unresponsive</span></span><br><span class="line">        response[<span class="string">"success"</span>] = <span class="keyword">False</span></span><br><span class="line">        response[<span class="string">"error"</span>] = <span class="string">"API unavailable"</span></span><br><span class="line">    <span class="keyword">except</span> sr.UnknownValueError:</span><br><span class="line">        response[<span class="string">"error"</span>] = <span class="string">"Unable to recognize speech"</span>  <span class="comment"># speech was unintelligible</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># set the list of words, maxnumber of guesses, and prompt limit</span></span><br><span class="line">    WORDS = [<span class="string">"apple"</span>, <span class="string">"banana"</span>, <span class="string">"grape"</span>, <span class="string">"orange"</span>, <span class="string">"mango"</span>, <span class="string">"lemon"</span>]</span><br><span class="line">    NUM_GUESSES = <span class="number">3</span></span><br><span class="line">    PROMPT_LIMIT = <span class="number">5</span></span><br><span class="line">​</span><br><span class="line">    <span class="comment"># create recognizer and mic instances</span></span><br><span class="line">    recognizer = sr.Recognizer()</span><br><span class="line">    microphone = sr.Microphone()</span><br><span class="line">​</span><br><span class="line">    word = random.choice(WORDS)  <span class="comment"># get a random word from the list</span></span><br><span class="line">​</span><br><span class="line">    instructions = (  <span class="comment"># format the instructions string</span></span><br><span class="line">        <span class="string">"I'm thinking of one of these words:\n"</span></span><br><span class="line">        <span class="string">"&#123;words&#125;\n"</span></span><br><span class="line">        <span class="string">"You have &#123;n&#125; tries to guess which one.\n"</span></span><br><span class="line">    ).format(words=<span class="string">', '</span>.join(WORDS), n=NUM_GUESSES)</span><br><span class="line">​</span><br><span class="line">    print(instructions)  <span class="comment"># show instructions and wait 3 seconds before starting the game</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_GUESSES):</span><br><span class="line">        <span class="comment"># get the guess from the user</span></span><br><span class="line">        <span class="comment"># if a transcription is returned, break out of the loop and continue</span></span><br><span class="line">        <span class="comment"># if no transcription returned and API request failed, break loop and continue</span></span><br><span class="line">        <span class="comment"># if API request succeeded but no transcription was returned, re-prompt the user</span></span><br><span class="line">        <span class="comment"># to say their guess again. Do this up to PROMPT_LIMIT times</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(PROMPT_LIMIT):</span><br><span class="line">            print(<span class="string">'Guess &#123;&#125;. Speak!'</span>.format(i + <span class="number">1</span>))</span><br><span class="line">            guess = recognize_speech_from_mic(recognizer, microphone)</span><br><span class="line">            <span class="keyword">if</span> guess[<span class="string">"transcription"</span>]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> guess[<span class="string">"success"</span>]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            print(<span class="string">"I didn't catch that. What did you say?\n"</span>)</span><br><span class="line">​</span><br><span class="line">        <span class="keyword">if</span> guess[<span class="string">"error"</span>]:  <span class="comment"># if there was an error, stop the game</span></span><br><span class="line">            print(<span class="string">"ERROR: &#123;&#125;"</span>.format(guess[<span class="string">"error"</span>]))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">​</span><br><span class="line">        print(<span class="string">"You said: &#123;&#125;"</span>.format(guess[<span class="string">"transcription"</span>]))  <span class="comment"># show the user the transcription</span></span><br><span class="line">​</span><br><span class="line">        <span class="comment"># determine if guess is correct and if any attempts remain</span></span><br><span class="line">        guess_is_correct = guess[<span class="string">"transcription"</span>].lower() == word.lower()</span><br><span class="line">        user_has_more_attempts = i &lt; NUM_GUESSES - <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">        <span class="comment"># determine if the user has won the game. if not, repeat the loop if user has</span></span><br><span class="line">        <span class="comment"># more attempts; if no attempts left, the user loses the game</span></span><br><span class="line">        <span class="keyword">if</span> guess_is_correct:</span><br><span class="line">            print(<span class="string">"Correct! You win!"</span>.format(word))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">elif</span> user_has_more_attempts:</span><br><span class="line">            print(<span class="string">"Incorrect. Try again.\n"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Sorry, you lose!\nI was thinking of '&#123;&#125;'."</span>.format(word))</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The <code>recognize_speech_from_mic()</code> function takes a <code>Recognizer</code> and <code>Microphone</code> instance as arguments and returns a dictionary with three keys. The first key, <code>success</code>, is a boolean that indicates whether or not the <code>API</code> request was successful. The second key, <code>error</code>, is either <code>None</code> or an error message indicating that the <code>API</code> is unavailable or the speech was unintelligible. Finally, the <code>transcription</code> key contains the transcription of the audio recorded by the microphone.<br>&emsp;&emsp;The function first checks that the recognizer and microphone arguments are of the correct type, and raises a <code>TypeError</code> if either is invalid:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> isinstance(recognizer, sr.Recognizer):</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">'`recognizer` must be `Recognizer` instance'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> isinstance(microphone, sr.Microphone):</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">'`microphone` must be a `Microphone` instance'</span>)</span><br></pre></td></tr></table></figure>
<p>The <code>listen()</code> method is then used to record microphone input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> microphone <span class="keyword">as</span> source:</span><br><span class="line">    recognizer.adjust_for_ambient_noise(source)</span><br><span class="line">    audio = recognizer.listen(source)</span><br></pre></td></tr></table></figure>
<p>The <code>adjust_for_ambient_noise()</code> method is used to calibrate the recognizer for changing noise conditions each time the <code>recognize_speech_from_mic()</code> function is called.<br>&emsp;&emsp;Next, <code>recognize_google()</code> is called to transcribe any speech in the recording. A <code>try...except</code> block is used to catch the <code>RequestError</code> and <code>UnknownValueError</code> exceptions and handle them accordingly. The success of the <code>API</code> request, any error messages, and the transcribed speech are stored in the success, error and transcription keys of the response dictionary, which is returned by the <code>recognize_speech_from_mic()</code> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">response = &#123;<span class="string">"success"</span>: <span class="keyword">True</span>, <span class="string">"error"</span>: <span class="keyword">None</span>, <span class="string">"transcription"</span>: <span class="keyword">None</span>&#125;</span><br><span class="line">​</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response[<span class="string">"transcription"</span>] = recognizer.recognize_google(audio)</span><br><span class="line"><span class="keyword">except</span> sr.RequestError:  <span class="comment"># API was unreachable or unresponsive</span></span><br><span class="line">    response[<span class="string">"success"</span>] = <span class="keyword">False</span></span><br><span class="line">    response[<span class="string">"error"</span>] = <span class="string">"API unavailable"</span></span><br><span class="line"><span class="keyword">except</span> sr.UnknownValueError:</span><br><span class="line">    response[<span class="string">"error"</span>] = <span class="string">"Unable to recognize speech"</span>  <span class="comment"># speech was unintelligible</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;You can test the <code>recognize_speech_from_mic()</code> function by saving the above script to a file called <code>guessing_game.py</code> and running the following in an interpreter session:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> guessing_game <span class="keyword">import</span> recognize_speech_from_mic</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = sr.Recognizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = sr.Microphone()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recognize_speech_from_mic(r, m)</span><br><span class="line">&#123;<span class="string">'success'</span>: <span class="keyword">True</span>, <span class="string">'error'</span>: <span class="keyword">None</span>, <span class="string">'transcription'</span>: <span class="string">'hello'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Your output will vary depending on what you say</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The game itself is pretty simple. First, a list of words, a maximum number of allowed guesses and a prompt limit are declared:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WORDS = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grape'</span>, <span class="string">'orange'</span>, <span class="string">'mango'</span>, <span class="string">'lemon'</span>]</span><br><span class="line">NUM_GUESSES = <span class="number">3</span></span><br><span class="line">PROMPT_LIMIT = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>Next, a <code>Recognizer</code> and Microphone instance is created and a random word is chosen from <code>WORDS</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recognizer = sr.Recognizer()</span><br><span class="line">microphone = sr.Microphone()</span><br><span class="line">word = random.choice(WORDS)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;After printing some instructions and waiting for three seconds, a for loop is used to manage each user attempt at guessing the chosen word. The first thing inside the for loop is another for loop that prompts the user at most <code>PROMPT_LIMIT</code> times for a guess, attempting to recognize the input each time with the <code>recognize_speech_from_mic()</code> function and storing the dictionary returned to the local variable guess.<br>&emsp;&emsp;If the <code>transcription</code> key of guess is not <code>None</code>, then the user’s speech was transcribed and the inner loop is terminated with break. If the speech was not transcribed and the <code>success</code> key is set to <code>False</code>, then an <code>API</code> error occurred and the loop is again terminated with break. Otherwise, the <code>API</code> request was successful but the speech was unrecognizable. The user is warned and the for loop repeats, giving the user another chance at the current attempt.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(PROMPT_LIMIT):</span><br><span class="line">    print(<span class="string">'Guess &#123;&#125;. Speak!'</span>.format(i + <span class="number">1</span>))</span><br><span class="line">    guess = recognize_speech_from_mic(recognizer, microphone)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> guess[<span class="string">"transcription"</span>]:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> guess[<span class="string">"success"</span>]:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"I didn't catch that. What did you say?\n"</span>)</span><br></pre></td></tr></table></figure>
<p>Once the inner for loop terminates, the guess dictionary is checked for errors. If any occurred, the error message is displayed and the outer for loop is terminated with break, which will end the program execution.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> guess[<span class="string">'error'</span>]:</span><br><span class="line">    print(<span class="string">"ERROR: &#123;&#125;"</span>.format(guess[<span class="string">"error"</span>]))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;If there weren’t any errors, the transcription is compared to the randomly selected word. The <code>lower()</code> method for string objects is used to ensure better matching of the guess to the chosen word. The <code>API</code> may return speech matched to the word <code>apple</code> as <code>Apple</code> or <code>apple</code>, and either response should count as a correct answer.<br>&emsp;&emsp;If the guess was correct, the user wins and the game is terminated. If the user was incorrect and has any remaining attempts, the outer for loop repeats and a new guess is retrieved. Otherwise, the user loses the game.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">guess_is_correct = guess[<span class="string">"transcription"</span>].lower() == word.lower()</span><br><span class="line">user_has_more_attempts = i &lt; NUM_GUESSES - <span class="number">1</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> guess_is_correct:</span><br><span class="line">    print(<span class="string">'Correct! You win!'</span>.format(word))</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="keyword">elif</span> user_has_more_attempts:</span><br><span class="line">    print(<span class="string">'Incorrect. Try again.\n'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"Sorry, you lose!\nI was thinking of '&#123;&#125;'."</span>.format(word))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>When run, the output will look something like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">I<span class="string">'m thinking of one of these words:</span></span><br><span class="line"><span class="string">apple, banana, grape, orange, mango, lemon</span></span><br><span class="line"><span class="string">You have 3 tries to guess which one.</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">Guess 1. Speak!</span></span><br><span class="line"><span class="string">You said: banana</span></span><br><span class="line"><span class="string">Incorrect. Try again.</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">Guess 2. Speak!</span></span><br><span class="line"><span class="string">You said: lemon</span></span><br><span class="line"><span class="string">Incorrect. Try again.</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">Guess 3. Speak!</span></span><br><span class="line"><span class="string">You said: Orange</span></span><br><span class="line"><span class="string">Correct! You win!</span></span><br></pre></td></tr></table></figure>
<h4 id="Recognizing-Speech-in-Languages-Other-Than-English"><a href="#Recognizing-Speech-in-Languages-Other-Than-English" class="headerlink" title="Recognizing Speech in Languages Other Than English"></a>Recognizing Speech in Languages Other Than English</h4><p>&emsp;&emsp;Throughout this tutorial, we’ve been recognizing speech in <code>English</code>, which is the default language for each <code>recognize_*()</code> method of the <code>SpeechRecognition</code> package. However, it is absolutely possible to recognize speech in other languages, and is quite simple to accomplish.<br>&emsp;&emsp;To recognize speech in a different language, set the language keyword argument of the <code>recognize_*()</code> method to a string corresponding to the desired language. Most of the methods accept a <code>BCP-47</code> language tag, such as <code>en-US</code> for <code>American English</code>, or <code>fr-FR</code> for <code>French</code>. For example, the following recognizes <code>French</code> speech in an audio file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line">​</span><br><span class="line">r = sr.Recognizer()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> sr.AudioFile(<span class="string">'path/to/audiofile.wav'</span>) <span class="keyword">as</span> source:</span><br><span class="line">    audio = r.record(source)</span><br><span class="line">​</span><br><span class="line">r.recognize_google(audio, language=<span class="string">'fr-FR'</span>)</span><br></pre></td></tr></table></figure>
<p>Only the following methods accept a language keyword argument: <code>recognize_bing</code>, <code>recognize_google</code>, <code>recognize_google_cloud</code>, <code>recognize_ibm</code> and <code>recognize_sphinx</code>.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之线程和队列/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之线程和队列/" itemprop="url">TensorFlow之线程和队列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T15:33:07+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;在使用<code>TensorFlow</code>进行异步计算时，队列是一种强大的机制。正如<code>TensorFlow</code>中的其他组件一样，队列就是<code>TensorFlow</code>图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(<code>rear</code>)，也可以把队列前端(<code>front</code>)的元素删除。<br>&emsp;&emsp;为了感受一下队列，来看一个简单的例子。我们先创建一个<code>先入先出</code>的队列(<code>FIFOQueue</code>)，并将其内部所有元素初始化为零。然后构建一个<code>TensorFlow</code>图，它从队列前端取走一个元素，加上<code>1</code>之后，放回队列的后端。慢慢地，队列的元素的值就会增加。</p>
<p><img src="/2019/02/13/深度学习/TensorFlow之线程和队列/1.png" height="247" width="606"></p>
<p>&emsp;&emsp;<code>Enqueue</code>、<code>EnqueueMany</code>和<code>Dequeue</code>都是特殊的节点。它们需要获取队列指针，而非普通的值，这样才能修改队列内容。我们建议您将它们看作队列的方法，事实上在<code>Python</code>的<code>API</code>中，它们就是队列对象的方法(例如<code>q.enqueue(...)</code>)。</p>
<h3 id="队列使用概述"><a href="#队列使用概述" class="headerlink" title="队列使用概述"></a>队列使用概述</h3><p>&emsp;&emsp;队列(如<code>FIFOQueue</code>和<code>RandomShuffleQueue</code>)在<code>TensorFlow</code>的张量异步计算时都非常重要。例如，一个典型的输入结构使用一个<code>RandomShuffleQueue</code>来作为模型训练的输入：</p>
<ul>
<li>多个线程准备训练样本，并且把这些样本推入队列。</li>
<li>一个训练线程执行一个训练操作，此操作会从队列中移除最小批次的样本(<code>mini-batches</code>)。</li>
</ul>
<p>&emsp;&emsp;<code>TensorFlow</code>的<code>Session</code>对象是可以支持多线程的，因此多个线程可以很方便地使用同一个会话(<code>Session</code>)，并且并行地执行操作。然而，在<code>Python</code>程序实现这样的并行运算却并不容易。所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候，队列必须能被正确地关闭。<br>&emsp;&emsp;所幸<code>TensorFlow</code>提供了两个类来帮助多线程的实现：<code>tf.Coordinator</code>和<code>tf.QueueRunner</code>，从设计上这两个类必须被一起使用。<code>Coordinator</code>类可以用来同时停止多个工作线程，并且向那个在等待所有工作线程终止的程序报告异常。<code>QueueRunner</code>类用来协调多个工作线程同时将多个张量推入同一个队列中。</p>
<h3 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h3><p>&emsp;&emsp;<code>Coordinator</code>类用来帮助多个线程协同工作，多个线程同步终止。其主要方法有：</p>
<ul>
<li><code>should_stop()</code>：如果线程应该停止，则返回<code>True</code>。</li>
<li><code>request_stop(&lt;exception&gt;)</code>：请求该线程停止。</li>
<li><code>join(&lt;list of threads&gt;)</code>：等待被指定的线程终止。</li>
</ul>
<p>首先创建一个<code>Coordinator</code>对象，然后建立一些使用<code>Coordinator</code>对象的线程。这些线程通常一直循环运行，一直到<code>should_stop</code>返回<code>True</code>时停止。任何线程都可以决定计算什么时候应该停止。它只需要调用<code>request_stop</code>，同时其他线程的<code>should_stop</code>将会返回<code>True</code>，然后都停下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MyLoop</span><span class="params">(coord)</span>:</span>  <span class="comment"># 循环执行，直到Coordinator收到了停止请求</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">        do something</span><br><span class="line">        <span class="keyword">if</span> some condition: <span class="comment"># 如果某些条件为真，请求Coordinator去停止其他线程</span></span><br><span class="line">            coord.request_stop()</span><br><span class="line">​</span><br><span class="line">coord = Coordinator()  <span class="comment"># Main code: create a coordinator</span></span><br><span class="line"><span class="comment"># Create 10 threads that run 'MyLoop()'</span></span><br><span class="line">threads = [threading.Thread(target=MyLoop, args=(coord)) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:  <span class="comment"># Start the threads and wait for all of them to stop</span></span><br><span class="line">    t.start()</span><br><span class="line">​</span><br><span class="line">coord.join(threads)</span><br></pre></td></tr></table></figure>
<p>显然，<code>Coordinator</code>可以管理线程去做不同的事情。上面的代码只是一个简单的例子，在设计实现的时候不必完全照搬。<code>Coordinator</code>还支持捕捉和报告异常，具体可以参考<code>Coordinator class</code>的文档。</p>
<h3 id="QueueRunner"><a href="#QueueRunner" class="headerlink" title="QueueRunner"></a>QueueRunner</h3><p>&emsp;&emsp;<code>QueueRunner</code>类会创建一组线程，这些线程可以重复的执行<code>Enquene</code>操作，它们使用同一个<code>Coordinator</code>来处理线程同步终止。此外，一个<code>QueueRunner</code>会运行一个<code>closer thread</code>，当<code>Coordinator</code>收到异常报告时，这个<code>closer thread</code>会自动关闭队列。您可以使用一个<code>queue runner</code>来实现上述结构。首先建立一个<code>TensorFlow</code>图表，这个图表使用队列来输入样本。增加处理样本并将样本推入队列中的操作，增加<code>training</code>操作来移除队列中的样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">example = ...ops to create one example...</span><br><span class="line"><span class="comment"># Create a queue, and an op that enqueues examples one at a time in the queue</span></span><br><span class="line">queue = tf.RandomShuffleQueue(...)</span><br><span class="line">enqueue_op = queue.enqueue(example)</span><br><span class="line"><span class="comment"># Create a training graph that starts by dequeuing a batch of examples</span></span><br><span class="line">inputs = queue.dequeue_many(batch_size)</span><br><span class="line">train_op = ...use <span class="string">'inputs'</span> to build the training part of the graph...</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在<code>Python</code>的训练程序中，创建一个<code>QueueRunner</code>来运行几个线程，这几个线程处理样本，并且将样本推入队列。创建一个<code>Coordinator</code>，让<code>queue runner</code>使用<code>Coordinator</code>来启动这些线程，创建一个训练的循环，并且使用<code>Coordinator</code>来控制<code>QueueRunner</code>的线程们的终止。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a queue runner that will run 4 threads in parallel to enqueue examples</span></span><br><span class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">4</span>)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()  <span class="comment"># Launch the graph</span></span><br><span class="line">coord = tf.train.Coordinator()  <span class="comment"># Create a coordinator, launch the queue runner threads</span></span><br><span class="line">enqueue_threads = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):  <span class="comment"># Run the training loop, controlling termination with the coordinator</span></span><br><span class="line">    <span class="keyword">if</span> coord.should_stop():</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    sess.run(train_op)</span><br><span class="line">​</span><br><span class="line">coord.request_stop()  <span class="comment"># When done, ask the threads to stop</span></span><br><span class="line">coord.join(threads)  <span class="comment"># And wait for them to actually do it</span></span><br></pre></td></tr></table></figure>
<h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><p>&emsp;&emsp;通过<code>queue runners</code>启动的线程不仅仅只处理推送样本到队列，它们还捕捉和处理由队列产生的异常，包括<code>OutOfRangeError</code>异常，这个异常用于报告队列被关闭。使用<code>Coordinator</code>的训练程序在主循环中必须同时捕捉和报告异常。下面是对上面训练循环的改进版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</span><br><span class="line">        <span class="keyword">if</span> coord.should_stop():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        sess.run(train_op)</span><br><span class="line"><span class="keyword">except</span> Exception, e:</span><br><span class="line">   coord.request_stop(e)  <span class="comment"># Report exceptions to the coordinator</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Terminate as usual. It is innocuous to request stop twice</span></span><br><span class="line">coord.request_stop()</span><br><span class="line">coord.join(threads)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="理解TensorFlow的Queue"><a href="#理解TensorFlow的Queue" class="headerlink" title="理解TensorFlow的Queue"></a>理解TensorFlow的Queue</h3><p>&emsp;&emsp;这篇文章来说明<code>TensorFlow</code>里与<code>Queue</code>有关的概念和用法，其实概念只有三个：</p>
<ul>
<li><code>Queue</code>是<code>TF</code>队列和缓存机制的实现。</li>
<li><code>QueueRunner</code>是<code>TF</code>中对操作<code>Queue</code>的线程的封装。</li>
<li><code>Coordinator</code>是<code>TF</code>中用来协调线程运行的工具。</li>
</ul>
<p>虽然它们经常同时出现，但这三样东西在<code>TensorFlow</code>中是可以单独使用的。</p>
<h4 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h4><p>&emsp;&emsp;根据实现的方式不同，分成具体的几种类型：</p>
<ul>
<li><code>tf.FIFOQueue</code>：按入列顺序出列的队列(如果希望读入的训练样本是有序的)。</li>
<li><code>tf.RandomShuffleQueue</code>：随机顺序出列的队列。</li>
<li><code>tf.PaddingFIFOQueue</code>：以固定长度批量出列的队列。</li>
<li><code>tf.PriorityQueue</code>：带优先级出列的队列。</li>
</ul>
<p>这些类型的<code>Queue</code>除了自身的性质不太一样外，创建、使用的方法基本是相同的。<br>&emsp;&emsp;创建函数的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.FIFOQueue(capacity, dtypes, shapes=<span class="keyword">None</span>, names=<span class="keyword">None</span> ...)</span><br></pre></td></tr></table></figure>
<p><code>Queue</code>主要包含入列(<code>enqueue</code>)和出列(<code>dequeue</code>)两个操作。<code>enqueue</code>操作返回计算图中的一个<code>Operation</code>节点，<code>dequeue</code>操作返回一个<code>Tensor</code>值。<code>Tensor</code>在创建时同样只是一个定义(或称为<code>声明</code>)，需要放在<code>Session</code>中运行才能获得真正的数值。下面是一个单独使用<code>Queue</code>的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 创建一个FIFO队列，初始化队列插入0.1、0.2、0.3这三个数字</span></span><br><span class="line">q = tf.FIFOQueue(<span class="number">3</span>, <span class="string">"float"</span>)</span><br><span class="line">init = q.enqueue_many(([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],))</span><br><span class="line"><span class="comment"># 定义出队、“+2”、入队操作</span></span><br><span class="line">x = q.dequeue()</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">q_inc = q.enqueue([y])</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        sess.run(q_inc)  <span class="comment"># 执行2次操作，队列的值变为0.3,、2.1、2.2</span></span><br><span class="line">​</span><br><span class="line">    quelen = sess.run(q.size())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(quelen):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.3</span></span><br><span class="line"><span class="number">2.1</span></span><br><span class="line"><span class="number">2.2</span></span><br></pre></td></tr></table></figure>
<p>注意，如果一次性入列超过<code>Queue Size</code>的数据，<code>enqueue</code>操作会卡住，直到有数据(被其他线程)从队列取出；对一个已经取空的队列使用<code>dequeue</code>操作也会卡住，直到有新的数据(从其他线程)写入。</p>
<h4 id="RandomShuffleQueue"><a href="#RandomShuffleQueue" class="headerlink" title="RandomShuffleQueue"></a>RandomShuffleQueue</h4><p>&emsp;&emsp;<code>RandomShuffleQueue</code>在<code>TensorFlow</code>使用异步计算时非常重要，因为<code>TensorFlow</code>的会话是支持多线程的，可以在主线程里执行训练操作，使用<code>RandomShuffleQueue</code>作为训练输入，开多个线程来准备训练样本，将样本压入队列后，主线程会从队列中每次取出<code>mini-batch</code>的样本进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 最大长度为10，最小长度为2，类型为float的随机队列</span></span><br><span class="line">q = tf.RandomShuffleQueue(capacity=<span class="number">10</span>, min_after_dequeue=<span class="number">2</span>, dtypes=<span class="string">'float'</span>)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    sess.run(q.enqueue(i))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">8</span>):</span><br><span class="line">    print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<p>执行结果是乱序的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4.0</span></span><br><span class="line"><span class="number">0.0</span></span><br><span class="line"><span class="number">2.0</span></span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"><span class="number">5.0</span></span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"><span class="number">7.0</span></span><br></pre></td></tr></table></figure>
<h4 id="QueueRunner-1"><a href="#QueueRunner-1" class="headerlink" title="QueueRunner"></a>QueueRunner</h4><p>&emsp;&emsp;<code>Tensorflow</code>的计算主要在使用<code>CPU/GPU</code>和内存，而数据读取涉及磁盘操作，速度远低于前者操作。因此通常会使用多个线程读取数据，然后使用一个线程消费数据。<code>QueueRunner</code>就是来管理这些读写队列的线程的。<br>&emsp;&emsp;<code>QueueRunner</code>需要与<code>Queue</code>一起使用，但并不一定必须使用<code>Coordinator</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">q = tf.FIFOQueue(<span class="number">10</span>, <span class="string">"float"</span>)</span><br><span class="line">counter = tf.Variable(<span class="number">0.0</span>)  <span class="comment"># 计数器</span></span><br><span class="line">increment_op = tf.assign_add(counter, <span class="number">1.0</span>)  <span class="comment"># 给计数器加一</span></span><br><span class="line">enqueue_op = q.enqueue(counter)  <span class="comment"># 将计数器加入队列</span></span><br><span class="line"><span class="comment"># 创建QueueRunner，用多个线程向队列添加数据。这里实际创建了4个线程，两个增加计数，两个执行入队</span></span><br><span class="line">qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * <span class="number">2</span>)</span><br><span class="line">sess = tf.InteractiveSession()  <span class="comment"># 主线程</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">qr.create_threads(sess, start=<span class="keyword">True</span>)  <span class="comment"># 启动入队线程</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">6.0</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">13.0</span></span><br><span class="line"><span class="number">18.0</span></span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="number">25.0</span></span><br><span class="line"><span class="number">26.0</span></span><br><span class="line"><span class="number">28.0</span></span><br><span class="line"><span class="number">31.0</span></span><br></pre></td></tr></table></figure>
<p>增加计数的进程会不停的后台运行，执行入队的进程会先执行<code>10</code>次(因为队列长度只有<code>10</code>)，然后主线程开始消费数据。当一部分数据消费被后，入队的进程又会开始执行。最终主线程消费完<code>20</code>个数据后停止，但其他线程继续运行，程序不会结束。</p>
<h4 id="Coordinator-1"><a href="#Coordinator-1" class="headerlink" title="Coordinator"></a>Coordinator</h4><p>&emsp;&emsp;<code>Coordinator</code>是个用来保存线程组运行状态的协调器对象，它和<code>TensorFlow</code>的<code>Queue</code>没有必然关系，可以单独和<code>Python</code>线程使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">q = tf.FIFOQueue(<span class="number">1000</span>, <span class="string">'float'</span>)</span><br><span class="line">counter = tf.Variable(<span class="number">0.0</span>)  <span class="comment"># 计数器</span></span><br><span class="line">increment_op = tf.assign_add(counter, tf.constant(<span class="number">1.0</span>))  <span class="comment"># 计数器加一</span></span><br><span class="line">enqueue_op = q.enqueue(counter)  <span class="comment"># 入队</span></span><br><span class="line"><span class="comment"># 线程面向队列q，启动2个线程，每个线程中是[in,en]两个操作</span></span><br><span class="line">qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * <span class="number">2</span>)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">​</span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line"><span class="comment"># 线程管理器启动线程，接收协调器管理</span></span><br><span class="line">enqueue_thread = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    print(sess.run(q.dequeue()))</span><br><span class="line">​</span><br><span class="line">coord.request_stop()  <span class="comment"># 向各个线程发终止信号</span></span><br><span class="line">coord.join(enqueue_thread)  <span class="comment"># 等待各个线程成功结束</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5.0</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">17.0</span></span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="number">38.0</span></span><br><span class="line"><span class="number">42.0</span></span><br><span class="line"><span class="number">45.0</span></span><br><span class="line"><span class="number">48.0</span></span><br><span class="line"><span class="number">54.0</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/Tf.reduce类函数/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/Tf.reduce类函数/" itemprop="url">Tf.reduce类函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T14:52:25+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="tf-reduce-max-tf-reduce-min相对"><a href="#tf-reduce-max-tf-reduce-min相对" class="headerlink" title="tf.reduce_max(tf.reduce_min相对)"></a>tf.reduce_max(tf.reduce_min相对)</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reduce_max (</span><br><span class="line">    input_tensor, axis=<span class="keyword">None</span>, keep_dims=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span>, reduction_indices=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Computes the maximum of elements across dimensions of a tensor.<br>&emsp;&emsp;Reduces <code>input_tensor</code> along the dimensions given in <code>axis</code>. Unless <code>keep_dims</code> is <code>true</code>, the rank of the tensor is reduced by <code>1</code> for each entry in <code>axis</code>. If <code>keep_dims</code> is <code>true</code>, the reduced dimensions are retained with length <code>1</code>.<br>&emsp;&emsp;If <code>axis</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>
<ul>
<li><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li>
<li><code>axis</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions. Must be in the range <code>[-rank(input_tensor), rank(input_tensor))</code>.</li>
<li><code>keep_dims</code>: If <code>true</code>, retains reduced dimensions with length <code>1</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>reduction_indices</code>: The old (deprecated) name for <code>axis</code>.</li>
</ul>
<p>Return the reduced tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>]], dtype=tf.float32)  <span class="comment"># x.shape=(2, 3)</span></span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 2行3列，“axis = 1”就在列维度操作，n列变成1列，即每一行求max，合到一列里</span></span><br><span class="line"><span class="comment"># 相当于只有第1维有值，而其他几维没东西了，第1维存的是其他几维的max</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(x.shape)</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 2行3列，“axis = 0”就在行维度操作，n行变成1行，即每一列求max，合到一行里</span></span><br><span class="line"><span class="comment"># 相当于只有第0维有值，而其他几维没东西了，第0维存的是其他几维的max</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(x.shape)</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">x = tf.constant([</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">        [[<span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>], [<span class="number">55</span>, <span class="number">66</span>, <span class="number">77</span>]]</span><br><span class="line">    ], dtype=tf.float32)  <span class="comment"># x.shape=(2, 2, 3)</span></span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">4.</span>]</span><br><span class="line">[<span class="number">6.</span>]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">----------</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">[[<span class="number">4.</span> <span class="number">4.</span> <span class="number">6.</span>]]</span><br><span class="line">(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">----------</span><br><span class="line">[[[<span class="number">22.</span> <span class="number">33.</span> <span class="number">44.</span>]</span><br><span class="line">  [<span class="number">55.</span> <span class="number">66.</span> <span class="number">77.</span>]]]</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">----------</span><br><span class="line">[[[ <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">55.</span> <span class="number">66.</span> <span class="number">77.</span>]]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">----------</span><br><span class="line">[[[ <span class="number">3.</span>]</span><br><span class="line">  [ <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">44.</span>]</span><br><span class="line">  [<span class="number">77.</span>]]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tf-reduce-mean"><a href="#tf-reduce-mean" class="headerlink" title="tf.reduce_mean"></a>tf.reduce_mean</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reduce_mean(</span><br><span class="line">    input_tensor, axis=<span class="keyword">None</span>, keepdims=<span class="keyword">None</span>, name=<span class="keyword">None</span>,</span><br><span class="line">    reduction_indices=<span class="keyword">None</span>, keep_dims=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/math_ops.py</code>. Computes the mean of elements across dimensions of a tensor (deprecated arguments).<br>&emsp;&emsp;<code>SOME ARGUMENTS ARE DEPRECATED!</code> They will be removed in a future version. Instructions for updating: <code>keep_dims</code> is deprecated, use <code>keepdims</code> instead.<br>&emsp;&emsp;Reduces <code>input_tensor</code> along the dimensions given in <code>axis</code>. Unless <code>keepdims</code> is <code>true</code>, the rank of the tensor is reduced by <code>1</code> for each entry in <code>axis</code>. If <code>keepdims</code> is <code>true</code>, the reduced dimensions are retained with length <code>1</code>.<br>&emsp;&emsp;If <code>axis</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1.</span>, <span class="number">1.</span>], [<span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line">tf.reduce_mean(x)  <span class="comment"># 1.5</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">0</span>)  <span class="comment"># [1.5, 1.5]</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">1</span>)  <span class="comment"># [1.,  2.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li>
<li><code>axis</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions. Must be in the range <code>[-rank(input_tensor), rank(input_tensor))</code>.</li>
<li><code>keepdims</code>: If <code>true</code>, retains reduced dimensions with length <code>1</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>reduction_indices</code>: The old (deprecated) name for <code>axis</code>.</li>
<li><code>keep_dims</code>: Deprecated alias for <code>keepdims</code>.</li>
</ul>
<p>Return the reduced tensor. Equivalent to <code>np.mean</code>.<br>&emsp;&emsp;Please note that <code>np.mean</code> has a <code>dtype</code> parameter that could be used to specify the output type. By default this is <code>dtype=float64</code>. On the other hand, <code>tf.reduce_mean</code> has an aggressive type inference from <code>input_tensor</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">tf.reduce_mean(x)  <span class="comment"># 0</span></span><br><span class="line">y = tf.constant([<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">tf.reduce_mean(y)  <span class="comment"># 0.5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum"></a>tf.reduce_sum</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reduce_sum(</span><br><span class="line">    input_tensor, axis=<span class="keyword">None</span>, keep_dims=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span>, reduction_indices=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Computes the sum of elements across dimensions of a tensor.<br>&emsp;&emsp;Reduces <code>input_tensor</code> along the dimensions given in <code>axis</code>. Unless <code>keep_dims</code> is <code>true</code>, the rank of the tensor is reduced by <code>1</code> for each entry in <code>axis</code>. If <code>keep_dims</code> is <code>true</code>, the reduced dimensions are retained with length <code>1</code>.<br>&emsp;&emsp;If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">tf.reduce_sum(x)  <span class="comment"># 6</span></span><br><span class="line">tf.reduce_sum(x, <span class="number">0</span>)  <span class="comment"># [2, 2, 2]</span></span><br><span class="line">tf.reduce_sum(x, <span class="number">1</span>)  <span class="comment"># [3, 3]</span></span><br><span class="line">tf.reduce_sum(x, <span class="number">1</span>, keep_dims=<span class="keyword">True</span>)  <span class="comment"># [[3], [3]]</span></span><br><span class="line">tf.reduce_sum(x, [<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li>
<li><code>axis</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions. Must be in the range <code>[-rank(input_tensor), rank(input_tensor))</code>.</li>
<li><code>keep_dims</code>: If true, retains reduced dimensions with length <code>1</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>reduction_indices</code>: The old (deprecated) name for <code>axis</code>.</li>
</ul>
<p>Return the reduced tensor.</p>
<h3 id="理解reduction-indices"><a href="#理解reduction-indices" class="headerlink" title="理解reduction_indices"></a>理解reduction_indices</h3><p>&emsp;&emsp;在<code>Tensorflow</code>的使用中，经常会使用<code>tf.reduce_mean</code>、<code>tf.reduce_sum</code>等函数。在这些函数中，有一个<code>reduction_indices</code>参数，表示函数的处理维度：</p>
<p><img src="/2019/02/13/深度学习/Tf.reduce类函数/1.png" height="185" width="649"></p>
<p>&emsp;&emsp;需要注意的一点，在很多时候看到别人的代码中并没有<code>reduction_indices</code>这个参数，此时该参数取默认值<code>None</code>，将把<code>input_tensor</code>降到<code>0</code>维，也就是一个数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之Slim/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之Slim/" itemprop="url">TensorFlow之Slim</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T12:16:46+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>TF-Slim</code>是<code>Tensorflow</code>中一个轻量级的库，用于定义、训练和评估复杂的模型。<code>TF-Slim</code>中的组件可以与<code>Tensorflow</code>中原生的函数一起使用，与其他的框架(比如<code>tf.contrib.learn</code>)也可以一起使用。使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>TF-Slim</code>可以使建立、训练和评估神经网络更加简单，具有如下特点：</p>
<ul>
<li>允许用户通过减少模板代码使得模型更加简洁。这个可以通过使用<code>argument scoping</code>和大量的高层<code>layers</code>、<code>variables</code>来实现。</li>
<li>通过使用常用的正则化(<code>regularizers</code>)使得建立模型更加简单。</li>
<li>一些广泛使用的计算机视觉相关的模型(比如<code>VGG</code>、<code>AlexNet</code>)已经在<code>slim</code>中定义好了，用户可以很方便地使用。这些既可以当成黑盒使用，也可以被扩展使用，比如添加一些<code>multiple heads</code>到不同的内部的层。</li>
<li><code>Slim</code>使得扩展复杂模型变得容易，可以使用已经存在的模型的<code>checkpoints</code>来开始训练算法。</li>
</ul>
<h3 id="What-are-the-various-components-of-TF-Slim"><a href="#What-are-the-various-components-of-TF-Slim" class="headerlink" title="What are the various components of TF-Slim?"></a>What are the various components of TF-Slim?</h3><p>&emsp;&emsp;<code>TF-Slim</code>由几个独立存在的组件组成：</p>
<ul>
<li><code>arg_scope</code>：提供一个新的作用域(<code>scope</code>)，称为<code>arg_scope</code>。在该作用域(<code>scope</code>)中，用户可以定义一些默认的参数，用于特定的操作。</li>
<li><code>data</code>：包含<code>TF-Slim</code>的<code>dataset</code>定义、<code>data providers</code>、<code>parallel_reader</code>和<code>decoding utilities</code>。</li>
<li><code>evaluation</code>：包含用于模型评估的常规函数。</li>
<li><code>layers</code>：包含用于建立模型的高级<code>layers</code>。</li>
<li><code>learning</code>：包含一些用于训练模型的常规函数。</li>
<li><code>losses</code>：包含一些用于<code>loss function</code>的函数。</li>
<li><code>metrics</code>：包含一些热门的评价标准。</li>
<li><code>nets</code>：包含一些热门的网络定义，如<code>VGG</code>、<code>AlexNet</code>等模型。</li>
<li><code>queues</code>：提供一个内容管理者，使得可以很容易、很安全地启动和关闭<code>QueueRunners</code>。</li>
<li><code>regularizers</code>：包含权重正则化。</li>
<li><code>variables</code>：提供一个方便的封装，用于变量创建和使用。</li>
</ul>
<h4 id="Defining-Models"><a href="#Defining-Models" class="headerlink" title="Defining Models"></a>Defining Models</h4><p>&emsp;&emsp;使用<code>TF-Slim</code>，结合<code>variables</code>、<code>layers</code>和<code>scopes</code>，模型可以很简洁地被定义。</p>
<h4 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h4><p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，创建<code>Variable</code>需要一个预定义的值或者一种初始化机制(比如从一个高斯分布中随机采样)。此外，如果一个变量需要在一个特定的设备上(如<code>GPU</code>)创建，那么必须被明确说明。为了减少变量创建所需的代码，<code>TF-Slim</code>提供了一些封装函数(定义在<code>variables.py</code>中)，可以使得用户定义变量变得简单。<br>&emsp;&emsp;举个例子，定义一个权重(<code>weight</code>)变量，然后使用一个截断的正态分布来初始化，使用<code>l2 loss</code>正则化，并将该变量放置在<code>CPU</code>中，只需要声明如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = slim.variable(</span><br><span class="line">    <span class="string">'weights'</span>, shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">3</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">    regularizer=slim.l2_regularizer(<span class="number">0.05</span>), device=<span class="string">'/CPU:0'</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，有两种类型的<code>variables</code>：<code>regular variables</code>和<code>local (transient) variables</code>。绝大部分变量是<code>regular variables</code>，一旦被创建，可以使用<code>saver</code>来将这些变量保存到磁盘中；<code>Local variables</code>是那些仅仅存在于一个<code>session</code>内的变量，并不会被保存到磁盘中。<br>&emsp;&emsp;<code>TF-Slim</code>通过定义<code>model variables</code>来进一步区别变量，这些是表示一个模型参数的变量。<code>Model variables</code>在学习期间被训练或者<code>fine-tuned</code>，在评估或者推断期间可以从一个<code>checkpoint</code>中加载。模型变量包括使用<code>slim.fully_connected</code>或者<code>slim.conv2d</code>创建的变量等。非模型变量(<code>Non-model variables</code>)指的是那些在学习或者评估阶段使用，但是在实际的<code>inference</code>中不需要用到的变量。例如<code>global_step</code>在学习和评估阶段会用到的变量，但是实际上并不是模型的一部分。类似的，<code>moving average variables</code>也是非模型变量。<br>&emsp;&emsp;<code>model variables</code>和<code>regular variables</code>在<code>TF-Slim</code>中很容易地被创建和恢复：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Variables</span></span><br><span class="line">weights = slim.model_variable(</span><br><span class="line">    <span class="string">'weights'</span>, shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">3</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">    regularizer=slim.l2_regularizer(<span class="number">0.05</span>), device=<span class="string">'/CPU:0'</span>)</span><br><span class="line">model_variables = slim.get_model_variables()</span><br><span class="line"><span class="comment"># Regular variables</span></span><br><span class="line">my_var = slim.variable(<span class="string">'my_var'</span>, shape=[<span class="number">20</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">regular_variables_and_model_variables = slim.get_variables()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这是如何工作的呢？当你通过<code>TF-Slim</code>的<code>layer</code>或者直接通过<code>slim.model_variable</code>函数创建一个模型的变量时，<code>TF-Slim</code>将变量添加到<code>tf.GraphKeys.MODEL_VARIABLES</code>集合中。如果你想拥有自己定制化的<code>layers</code>或者<code>variables</code>创建机制，但是仍然想利用<code>TF-Slim</code>来管理你的变量，此时<code>TF-Slim</code>提供一个方便的函数，用于添加模型的变量到集合中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_model_variable = CreateViaCustomCode()</span><br><span class="line">slim.add_model_variable(my_model_variable)  <span class="comment"># Letting TF-Slim know about the additional variable</span></span><br></pre></td></tr></table></figure>
<h4 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h4><p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，要定义一些层(比如说卷积层、全连接层、<code>BatchNorm</code>层等)是比较麻烦的。举个例子，神经网络中的卷积层由以下几个步骤组成：</p>
<ul>
<li>创建权重和偏置变量。</li>
<li>将输入与权重做卷积运算。</li>
<li>将偏置加到第二步的卷积运算得到的结果中。</li>
<li>使用一个激活函数。</li>
</ul>
<p>上面的步骤使用原始的<code>Tensorflow</code>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1_1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    kernel = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=tf.float32, stddev=<span class="number">1e-1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    conv = tf.nn.conv2d(input, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">128</span>], dtype=tf.float32), trainable=<span class="keyword">True</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">    conv1 = tf.nn.relu(bias, name=scope)</span><br></pre></td></tr></table></figure>
<p>为了减少重复代码，<code>TF-Slim</code>提供了一些方便的、高级别的、更抽象的神经网络层。例如卷积层实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line">net = slim.conv2d(input, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1_1'</span>)</span><br></pre></td></tr></table></figure>
<p><code>TF-Slim</code>提供了大量的标准的实现，用于建立神经网络：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Layer</th>
<th>TF-Slim</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BiasAdd</code></td>
<td><code>slim.bias_add</code></td>
</tr>
<tr>
<td><code>BatchNorm</code></td>
<td><code>slim.batch_norm</code></td>
</tr>
<tr>
<td><code>Conv2d</code></td>
<td><code>slim.conv2d</code></td>
</tr>
<tr>
<td><code>Conv2dInPlane</code></td>
<td><code>slim.conv2d_in_plane</code></td>
</tr>
<tr>
<td><code>Conv2dTranspose(Deconv)</code></td>
<td><code>slim.conv2d_transpose</code></td>
</tr>
<tr>
<td><code>FullyConnected</code></td>
<td><code>slim.fully_connected</code></td>
</tr>
<tr>
<td><code>AvgPool2D</code></td>
<td><code>slim.avg_pool2d</code></td>
</tr>
<tr>
<td><code>Dropout</code></td>
<td><code>slim.dropout</code></td>
</tr>
<tr>
<td><code>Flatten</code></td>
<td><code>slim.flatten</code></td>
</tr>
<tr>
<td><code>MaxPool2D</code></td>
<td><code>slim.max_pool2d</code></td>
</tr>
<tr>
<td><code>OneHotEncoding</code></td>
<td><code>slim.one_hot_encoding</code></td>
</tr>
<tr>
<td><code>SeparableConv2</code></td>
<td><code>slim.separable_conv2d</code></td>
</tr>
<tr>
<td><code>UnitNorm</code></td>
<td><code>slim.unit_norm</code></td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;<code>TF-Slim</code>也包含两个操作符，称为<code>repeat</code>和<code>stack</code>，允许用户重复执行相同的操作。例如下面几个卷积层加一个池化层是<code>VGG</code>网络的一部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = ...</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>减少重复代码的其中一种方法是利用<code>for</code>循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = ...</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_%d'</span> % (i + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>另一种方式是使用<code>TF-Slim</code>中的<code>repeat</code>操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>在上面例子中，<code>slim.repeat</code>会自动给每一个卷积层的<code>scopes</code>命名为<code>conv3/conv3_1</code>、<code>conv3/conv3_2</code>和<code>conv3/conv3_3</code>。<br>&emsp;&emsp;另外，<code>TF-Slim</code>的<code>slim.stack</code>操作允许用户用不同的参数重复调用同一种操作。<code>slim.stack</code>也为每一个被创建的操作创建一个新的<code>tf.variable_scope</code>。例如下面是一种简单的方式来创建多层感知器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.fully_connected(x, <span class="number">32</span>, scope=<span class="string">'fc/fc_1'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">64</span>, scope=<span class="string">'fc/fc_2'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">128</span>, scope=<span class="string">'fc/fc_3'</span>)</span><br><span class="line"><span class="comment"># Equivalent, TF-Slim way using slim.stack:</span></span><br><span class="line">slim.stack(x, slim.fully_connected, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure>
<p>在上面的例子中，<code>slim.stack</code>调用了<code>slim.fully_connected</code>三次。类似的，我们可以使用<code>stack</code>来简化多层的卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_1'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_2'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_3'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_4'</span>)</span><br><span class="line"><span class="comment"># Using stack:</span></span><br><span class="line">slim.stack(x, slim.conv2d, [(<span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>])], scope=<span class="string">'core'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Scopes"><a href="#Scopes" class="headerlink" title="Scopes"></a>Scopes</h4><p>&emsp;&emsp;除了<code>Tensorflow</code>中作用域(<code>scope</code>)之外(<code>name_scope</code>、<code>variable_scope</code>)，<code>TF-Slim</code>增加了新的作用域机制，称为<code>arg_scope</code>。这个新的作用域允许使用者明确一个或者多个操作和一些参数，这些定义好的操作或者参数会传递给<code>arg_scope</code>内部的每一个操作。先看如下代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(</span><br><span class="line">    inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>从上面的代码中可以清楚地看出来有<code>3</code>层卷积层，其中很多超参数都是一样的。两个卷积层有相同的<code>padding</code>，所有三个卷积层有相同的<code>weights_initializer</code>和<code>weight_regularizer</code>。上面的代码包含了大量重复的值，其中一种解决方法是使用变量来说明一些默认的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">padding = <span class="string">'SAME'</span></span><br><span class="line">initializer = tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">regularizer = slim.l2_regularizer(<span class="number">0.0005</span>)</span><br><span class="line">​</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=padding, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=padding, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的解决方案其实并没有减少代码的混乱程度。通过使用<code>arg_scope</code>，我们既可以保证每一层使用相同的值，也可以简化代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d], padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv2'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的例子表明，使用<code>arg_scope</code>可以使得代码变得更整洁、更干净并且更加容易维护。注意到，在<code>arg_scope</code>中规定的参数值，它们可以被局部覆盖。例如上面的<code>padding</code>参数被设置成<code>SAME</code>，但是在第二个卷积层中用<code>VALID</code>覆盖了这个参数。<br>&emsp;&emsp;我们也可以嵌套使用<code>arg_scope</code>，在相同的作用域内使用多个操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d], stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>):</span><br><span class="line">        net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv1'</span>)</span><br><span class="line">        net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">5</span>, <span class="number">5</span>], weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.03</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">1000</span>, activation_fn=<span class="keyword">None</span>, scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure>
<p>在第一个<code>arg_scope</code>中，卷积层和全连接层被应用于相同的权重初始化和权重正则化；在第二个<code>arg_scope</code>中，额外的参数仅仅对卷积层<code>conv2d</code>起作用。</p>
<h3 id="Working-Example-Specifying-the-VGG16-Layers"><a href="#Working-Example-Specifying-the-VGG16-Layers" class="headerlink" title="Working Example: Specifying the VGG16 Layers"></a>Working Example: Specifying the VGG16 Layers</h3><p>&emsp;&emsp;通过结合<code>TF-Slim</code>的<code>Variables</code>、<code>Operations</code>和<code>scopes</code>，我们可以使用比较少的代码来实现一个比较复杂的网络。例如整个<code>VGG</code>网络定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg16</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">     [slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,</span><br><span class="line">     weights_initializer=tf.truncated_normal_initializer(<span class="number">0.0</span>, <span class="number">0.01</span>),</span><br><span class="line">     weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">        net = slim.repeat(inputs, <span class="number">2</span>, slim.conv2d, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool1'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">2</span>, slim.conv2d, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv2'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool3'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv4'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool4'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv5'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool5'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">4096</span>, scope=<span class="string">'fc6'</span>)</span><br><span class="line">        net = slim.dropout(net, <span class="number">0.5</span>, scope=<span class="string">'dropout6'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">4096</span>, scope=<span class="string">'fc7'</span>)</span><br><span class="line">        net = slim.dropout(net, <span class="number">0.5</span>, scope=<span class="string">'dropout7'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">1000</span>, activation_fn=<span class="keyword">None</span>, scope=<span class="string">'fc8'</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<h3 id="Training-Models"><a href="#Training-Models" class="headerlink" title="Training Models"></a>Training Models</h3><p>&emsp;&emsp;训练<code>Tensorflow</code>模型要求一个模型、一个<code>loss function</code>、梯度计算和一个训练的程序，用来迭代地根据<code>loss</code>计算模型权重的梯度和更新权重。<code>TF-Slim</code>提供了<code>loss function</code>和一些帮助函数来运行训练和评估。</p>
<h4 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h4><p>&emsp;&emsp;<code>Loss function</code>定义了一个我们需要最小化的量。对于分类问题，主要是计算真正的分布与预测的概率分布之间的交叉熵；对于回归问题，主要是计算预测值与真实值均方误差。<br>&emsp;&emsp;特定的模型，比如说多任务学习模型，要求同时使用多个<code>loss function</code>。换句话说，最终被最小化的<code>loss function</code>是多个其他的<code>loss function</code>之和。例如一个同时预测图像中场景的类型和深度的模型，该模型的<code>loss function</code>就是分类<code>loss</code>和深度预测<code>loss</code>之和。<br>&emsp;&emsp;<code>TF-Slim</code>通过<code>losses</code>模块为用户提供了一种机制，使得定义<code>loss function</code>变得简单。例如下面的是我们想要训练<code>VGG</code>网络的简单示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">images, labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">predictions, _ = vgg.vgg_16(images)  <span class="comment"># Create the model</span></span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">loss = slim.losses.softmax_cross_entropy(predictions, labels)</span><br></pre></td></tr></table></figure>
<p>在上面这个例子中，我们首先创建一个模型(利用<code>TF-Slim</code>的<code>VGG</code>实现)，然后增加了标准的分类<code>loss</code>。现在来看看当我们有一个多个输出的多任务模型的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">images, scene_labels, depth_labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">scene_predictions, depth_predictions = CreateMultiTaskModel(images)  <span class="comment"># Create the model</span></span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)</span><br><span class="line">sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)</span><br><span class="line"><span class="comment"># The following two lines have the same effect:</span></span><br><span class="line">total_loss = classification_loss + sum_of_squares_loss</span><br><span class="line">total_loss = slim.losses.get_total_loss(add_regularization_losses=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中有<code>2</code>个<code>loss</code>，是通过调用<code>slim.losses.softmax_cross_entropy</code>和<code>slim.losses.sum_of_squares</code>得到。我们可以将这两个<code>loss</code>加在一起，或者调用<code>slim.losses.get_total_loss</code>来得到全部的<code>loss</code>(<code>total_loss</code>)。这是如何工作的？当你通过<code>TF-Slim</code>创建一个<code>loss</code>时，<code>TF-Slim</code>将<code>loss</code>加到一个特殊的<code>TensorFlow collection of loss functions</code>。这使得你既可以手动地管理全部的<code>loss</code>，也可以让<code>TF-Slim</code>来替你管理它们。<br>&emsp;&emsp;如果你想让<code>TF-Slim</code>为你管理<code>losses</code>，但是你有一个自己实现的<code>loss</code>该怎么办？<code>loss_ops.py</code>也有一个函数可以将你自己实现的<code>loss</code>加到<code>TF-Slims collection</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">images, scene_labels, depth_labels, pose_labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">scene_predictions, depth_predictions, pose_predictions = CreateMultiTaskModel(images)  <span class="comment"># Create the model</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)</span><br><span class="line">sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)</span><br><span class="line">pose_loss = MyCustomLossFunction(pose_predictions, pose_labels)</span><br><span class="line">slim.losses.add_loss(pose_loss)  <span class="comment"># Letting TF-Slim know about the additional loss.</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># The following two ways to compute the total loss are equivalent:</span></span><br><span class="line">regularization_loss = tf.add_n(slim.losses.get_regularization_losses())</span><br><span class="line">total_loss1 = classification_loss + sum_of_squares_loss + pose_loss + regularization_loss</span><br><span class="line">​</span><br><span class="line">total_loss2 = slim.losses.get_total_loss()  <span class="comment"># (Regularization Loss is included in the total loss by default)</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们既可以手动地计算的出全部的<code>loss function</code>，也可以让<code>TF-Slim</code>知道这个额外的<code>loss</code>，然后让<code>TF-Slim</code>处理这个<code>loss</code>。</p>
<h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><p>&emsp;&emsp;<code>TF-Slim</code>提供了一个简单但是很强的用于训练模型的工具(在<code>learning.py</code>中)，其中包括一个可以重复测量<code>loss</code>、计算梯度和将模型保存到磁盘的训练函数。举个例子，一旦定义好了模型、<code>loss function</code>和最优化方法，我们可以调用<code>slim.learning.create_train_op</code>和<code>slim.learning.train</code>来实现优化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the model and specify the losses</span></span><br><span class="line">...</span><br><span class="line">total_loss = slim.losses.get_total_loss()</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line"><span class="comment"># create_train_op ensures that each time we ask for the loss,</span></span><br><span class="line"><span class="comment"># the update_ops are run and the gradients being computed are applied too</span></span><br><span class="line">train_op = slim.learning.create_train_op(total_loss, optimizer)</span><br><span class="line">logdir = ...  <span class="comment"># Where checkpoints are stored.</span></span><br><span class="line">slim.learning.train(train_op, logdir, number_of_steps=<span class="number">1000</span>, save_summaries_secs=<span class="number">300</span>, save_interval_secs=<span class="number">600</span>):</span><br></pre></td></tr></table></figure>
<p>在这个例子中，提供给<code>slim.learning.train</code>的参数有<code>train_op</code>(用于计算<code>loss</code>和梯度)；<code>logdir</code>(用于声明<code>checkpoints</code>和<code>event</code>文件保存的路径)；用<code>number_of_steps</code>参数来限制梯度下降的步数；<code>save_summaries_secs=300</code>表明每<code>5</code>分钟计算一次<code>summaries</code>；<code>save_interval_secs=600</code>表明每<code>10</code>分钟保存一次模型的<code>checkpoint</code>。</p>
<h3 id="Working-Example-Training-the-VGG16-Model"><a href="#Working-Example-Training-the-VGG16-Model" class="headerlink" title="Working Example: Training the VGG16 Model"></a>Working Example: Training the VGG16 Model</h3><p>&emsp;&emsp;下面是训练一个<code>VGG</code>网络的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">...</span><br><span class="line">train_log_dir = ...</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(train_log_dir):</span><br><span class="line">    tf.gfile.MakeDirs(train_log_dir)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    images, labels = ...  <span class="comment"># Set up the data loading</span></span><br><span class="line">    predictions = vgg.vgg_16(images, is_training=<span class="keyword">True</span>)  <span class="comment"># Define the model</span></span><br><span class="line">    <span class="comment"># Specify the loss function:</span></span><br><span class="line">    slim.losses.softmax_cross_entropy(predictions, labels)</span><br><span class="line">    total_loss = slim.losses.get_total_loss()</span><br><span class="line">    tf.summary.scalar(<span class="string">'losses/total_loss'</span>, total_loss)</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">.001</span>)  <span class="comment"># Specify the optimization scheme</span></span><br><span class="line">    <span class="comment"># create_train_op that ensures that when we evaluate it to get the loss,</span></span><br><span class="line">    <span class="comment"># the update_ops are done and the gradient updates are computed</span></span><br><span class="line">    train_tensor = slim.learning.create_train_op(total_loss, optimizer)</span><br><span class="line">    slim.learning.train(train_tensor, train_log_dir)  <span class="comment"># Actually runs training</span></span><br></pre></td></tr></table></figure>
<h3 id="Fine-Tuning-Existing-Models"><a href="#Fine-Tuning-Existing-Models" class="headerlink" title="Fine-Tuning Existing Models"></a>Fine-Tuning Existing Models</h3><h4 id="Brief-Recap-on-Restoring-Variables-from-a-Checkpoint"><a href="#Brief-Recap-on-Restoring-Variables-from-a-Checkpoint" class="headerlink" title="Brief Recap on Restoring Variables from a Checkpoint"></a>Brief Recap on Restoring Variables from a Checkpoint</h4><p>&emsp;&emsp;当一个模型被训练完毕之后，它可以从一个给定的<code>checkpoint</code>中使用<code>tf.train.Saver</code>来恢复变量。在很多情况下，<code>tf.train.Saver</code>提供一个简单的机制来恢复所有变量或者一部分变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables</span></span><br><span class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</span><br><span class="line">restorer = tf.train.Saver()  <span class="comment"># Add ops to restore all the variables</span></span><br><span class="line">restorer = tf.train.Saver([v1, v2])  <span class="comment"># Add ops to restore some variables</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and do some work with the model</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br><span class="line">    print(<span class="string">"Model restored."</span>)</span><br><span class="line">    <span class="comment"># Do some work with the model</span></span><br></pre></td></tr></table></figure>
<h4 id="Partially-Restoring-Models"><a href="#Partially-Restoring-Models" class="headerlink" title="Partially Restoring Models"></a>Partially Restoring Models</h4><p>&emsp;&emsp;在一个新的数据集或者一个新的任务上<code>fine-tune</code>一个预训练的模型通常是比较流行的，我们可以使用<code>TF-Slim</code>的<code>helper</code>函数来选择想要恢复的一部分变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables.</span></span><br><span class="line">v1 = slim.variable(name=<span class="string">"v1"</span>, ...)</span><br><span class="line">v2 = slim.variable(name=<span class="string">"nested/v2"</span>, ...)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Get list of variables to restore (which contains only 'v2').</span></span><br><span class="line"><span class="comment"># These are all equivalent methods:</span></span><br><span class="line">variables_to_restore = slim.get_variables_by_name(<span class="string">"v2"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_by_suffix(<span class="string">"2"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables(scope=<span class="string">"nested"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(include=[<span class="string">"nested"</span>])</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(exclude=[<span class="string">"v1"</span>])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the saver which will be used to restore the variables</span></span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br><span class="line">    print(<span class="string">"Model restored."</span>)</span><br><span class="line">    <span class="comment"># Do some work with the model</span></span><br></pre></td></tr></table></figure>
<h4 id="Restoring-models-with-different-variable-names"><a href="#Restoring-models-with-different-variable-names" class="headerlink" title="Restoring models with different variable names"></a>Restoring models with different variable names</h4><p>&emsp;&emsp;当从一个<code>checkpoint</code>中恢复变量时，<code>Saver</code>定位在<code>checkpoint</code>文件中变量的名字，并且将它们映射到当前图(<code>graph</code>)的变量中去。在上面的例子中，我们通过传递给<code>saver</code>一个变量列表来创建一个<code>saver</code>。在这种情况下，在<code>checkpoint</code>文件中定位的变量名隐式地从每个提供的变量的<code>var.op.name</code>中获得。<br>&emsp;&emsp;当<code>checkpoint</code>文件中的变量名与<code>graph</code>匹配时，将会工作良好。然而有时候，我们想要从一个与当前的<code>graph</code>不同变量名的<code>checkpoint</code>中恢复变量，那么在这种情况下，我们必须给<code>Saver</code>提供一个字典，该字典将每个<code>checkpoint</code>中变量名映射到每个<code>graph</code>的变量。下面的例子通过一个简单的函数获得<code>checkpoint</code>中的变量的名字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assuming than 'conv1/weights' should be restored from 'vgg16/conv1/weights'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_in_checkpoint</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'vgg16/'</span> + var.op.name</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Assuming than 'conv1/weights' and 'conv1/bias' should be restored from 'conv1/params1' and 'conv1/params2'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_in_checkpoint</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"weights"</span> <span class="keyword">in</span> var.op.name:</span><br><span class="line">        <span class="keyword">return</span> var.op.name.replace(<span class="string">"weights"</span>, <span class="string">"params1"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">"bias"</span> <span class="keyword">in</span> var.op.name:</span><br><span class="line">        <span class="keyword">return</span> var.op.name.replace(<span class="string">"bias"</span>, <span class="string">"params2"</span>)</span><br><span class="line">​</span><br><span class="line">variables_to_restore = slim.get_model_variables()</span><br><span class="line">variables_to_restore = &#123;name_in_checkpoint(var): var <span class="keyword">for</span> var <span class="keyword">in</span> variables_to_restore&#125;</span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br></pre></td></tr></table></figure>
<h4 id="Fine-Tuning-a-Model-on-a-different-task"><a href="#Fine-Tuning-a-Model-on-a-different-task" class="headerlink" title="Fine-Tuning a Model on a different task"></a>Fine-Tuning a Model on a different task</h4><p>&emsp;&emsp;考虑这么一种情况：我们有一个预训练好的<code>VGG16</code>模型，该模型是在<code>ImageNet</code>数据集上训练好的，有<code>1000</code>类。然而我们想要将其应用到只有<code>20</code>类的<code>Pascal VOC</code>数据集上。为了实现这个功能，我们可以使用不包括最后一层的预训练模型来初始化新模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">image, label = MyPascalVocDataLoader(...)  <span class="comment"># Load the Pascal VOC data</span></span><br><span class="line">images, labels = tf.train.batch([image, label], batch_size=<span class="number">32</span>)</span><br><span class="line">​</span><br><span class="line">predictions = vgg.vgg_16(images)  <span class="comment"># Create the model</span></span><br><span class="line">train_op = slim.learning.create_train_op(...)</span><br><span class="line"><span class="comment"># Specify where the Model, trained on ImageNet, was saved</span></span><br><span class="line">model_path = <span class="string">'/path/to/pre_trained_on_imagenet.checkpoint'</span></span><br><span class="line">log_dir = <span class="string">'/path/to/my_pascal_model_dir/'</span>  <span class="comment"># Specify where the new model will live</span></span><br><span class="line"><span class="comment"># Restore only the convolutional layers</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(exclude=[<span class="string">'fc6'</span>, <span class="string">'fc7'</span>, <span class="string">'fc8'</span>])</span><br><span class="line">init_fn = assign_from_checkpoint_fn(model_path, variables_to_restore)</span><br><span class="line">slim.learning.train(train_op, log_dir, init_fn=init_fn)  <span class="comment"># Start training</span></span><br></pre></td></tr></table></figure>
<h3 id="Evaluating-Models"><a href="#Evaluating-Models" class="headerlink" title="Evaluating Models"></a>Evaluating Models</h3><p>&emsp;&emsp;一旦我们已经训练好了一个模型(或者模型正在训练之中)，想要看看模型的实际表现能力，这个可以通过使用一些评估度量来实现，该度量可以对模型的表现能力评分。而评估代码实际上是加载数据、做出预测、将预测结果与真实值做比较，最后得到得分。这个步骤可以运行一次或者周期重复。</p>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>&emsp;&emsp;我们将度量定义为一个性能度量，它不是一个<code>loss</code>函数(<code>losses</code>是在训练的时候直接最优化)，但我们仍然感兴趣的是评估模型的目的。例如我们想要最优化<code>log loss</code>，但是我们感兴趣的度量可能是<code>F1</code>得分(<code>test accuracy</code>)，或者是<code>Intersection Over Union score</code>(这是不可微的，因此不能作为损失使用)。<br>&emsp;&emsp;<code>TF-Slim</code>提供了一些使得评估模型变得简单的度量操作。计算度量的值可以分为以下三个步骤：</p>
<ul>
<li>初始化(<code>Initialization</code>)：初始化用于计算度量的变量。</li>
<li>聚合(<code>Aggregation</code>)：使用操作(比如求和操作)来计算度量。</li>
<li>终止化(<code>Finalization</code>)：这是可选的步骤，使用最终的操作来计算度量值，比如说计算均值、最小值、最大值等。</li>
</ul>
<p>举个例子，为了计算<code>mean_absolute_error</code>，变量<code>count</code>和<code>total</code>变量被初始化为<code>0</code>。在聚合期间，我们观测到一些预测值和标签值，计算它们的绝对差值然后加到<code>total</code>中。每一次我们观测到新的一个数据，我们增加<code>count</code>。最后在<code>Finalization</code>期间，<code>total</code>除以<code>count</code>来获得均值<code>mean</code>。<br>&emsp;&emsp;下面的示例演示了声明度量标准的<code>API</code>。由于度量经常在测试集上进行评估，因此我们假设使用的是测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">images, labels = LoadTestData(...)</span><br><span class="line">predictions = MyModel(images)</span><br><span class="line">​</span><br><span class="line">mae_value_op, mae_update_op = slim.metrics.streaming_mean_absolute_error(predictions, labels)</span><br><span class="line">mre_value_op, mre_update_op = slim.metrics.streaming_mean_relative_error(predictions, labels)</span><br><span class="line">pl_value_op, pl_update_op = slim.metrics.percentage_less(mean_relative_errors, <span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>一个度量的创建返回两个值：<code>value_op</code>和<code>update_op</code>。<code>value_op</code>是一个幂等操作，它返回度量的当前值；<code>update_op</code>是执行上面提到的聚合步骤的操作，以及返回度量的值。<br>&emsp;&emsp;跟踪每个<code>value_op</code>和<code>update_op</code>是很费力的，为了解决这个问题，<code>TF-Slim</code>提供了两个便利功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Aggregates the value and update ops in two lists:</span></span><br><span class="line">value_ops, update_ops = slim.metrics.aggregate_metrics(</span><br><span class="line">    slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    slim.metrics.streaming_mean_squared_error(predictions, labels))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Aggregates the value and update ops in two dictionaries</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">"eval/mean_absolute_error"</span>: slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    <span class="string">"eval/mean_squared_error"</span>: slim.metrics.streaming_mean_squared_error(predictions, labels),</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="Working-example-Tracking-Multiple-Metrics"><a href="#Working-example-Tracking-Multiple-Metrics" class="headerlink" title="Working example: Tracking Multiple Metrics"></a>Working example: Tracking Multiple Metrics</h3><p>&emsp;&emsp;将代码全部放在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Load the data</span></span><br><span class="line">images, labels = load_data(...)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the network</span></span><br><span class="line">predictions = vgg.vgg_16(images)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Choose the metrics to compute:</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">"eval/mean_absolute_error"</span>: slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    <span class="string">"eval/mean_squared_error"</span>: slim.metrics.streaming_mean_squared_error(predictions, labels),</span><br><span class="line">&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Evaluate the model using 1000 batches of data:</span></span><br><span class="line">num_batches = <span class="number">1000</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    sess.run(tf.local_variables_initializer())</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> batch_id <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        sess.run(names_to_updates.values())</span><br><span class="line">​</span><br><span class="line">    metric_values = sess.run(names_to_values.values())</span><br><span class="line">    <span class="keyword">for</span> metric, value <span class="keyword">in</span> zip(names_to_values.keys(), metric_values):</span><br><span class="line">        print(<span class="string">'Metric %s has value: %f'</span> % (metric, value))</span><br></pre></td></tr></table></figure>
<h3 id="Evaluation-Loop"><a href="#Evaluation-Loop" class="headerlink" title="Evaluation Loop"></a>Evaluation Loop</h3><p>&emsp;&emsp;<code>TF-Slim</code>提供了一个评估模块(<code>evaluation.py</code>)，它包含了使用来自<code>metric_ops.py</code>模块编写模型评估脚本的辅助函数。这些功能包括定期运行评估、对数据批量进行评估、打印和汇总度量结果的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">images, labels = load_data(...)  <span class="comment"># Load the data</span></span><br><span class="line">predictions = MyModel(images)  <span class="comment"># Define the network</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Choose the metrics to compute:</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">'accuracy'</span>: slim.metrics.accuracy(predictions, labels),</span><br><span class="line">    <span class="string">'precision'</span>: slim.metrics.precision(predictions, labels),</span><br><span class="line">    <span class="string">'recall'</span>: slim.metrics.recall(mean_relative_errors, <span class="number">0.3</span>),</span><br><span class="line">&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the summary ops such that they also print out to std output:</span></span><br><span class="line">summary_ops = []</span><br><span class="line"><span class="keyword">for</span> metric_name, metric_value <span class="keyword">in</span> names_to_values.iteritems():</span><br><span class="line">    op = tf.summary.scalar(metric_name, metric_value)</span><br><span class="line">    op = tf.Print(op, [metric_value], metric_name)</span><br><span class="line">    summary_ops.append(op)</span><br><span class="line">​</span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_batches = math.ceil(num_examples / float(batch_size))</span><br><span class="line">​</span><br><span class="line">slim.get_or_create_global_step()  <span class="comment"># Setup the global step</span></span><br><span class="line">​</span><br><span class="line">output_dir = ...  <span class="comment"># Where the summaries are stored.</span></span><br><span class="line">eval_interval_secs = ...  <span class="comment"># How often to run the evaluation.</span></span><br><span class="line">slim.evaluation.evaluation_loop(</span><br><span class="line">    <span class="string">'local'</span>, checkpoint_dir, log_dir, num_evals=num_batches, eval_op=names_to_updates.values(),</span><br><span class="line">    summary_op=tf.summary.merge(summary_ops), eval_interval_secs=eval_interval_secs)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="tf-nn-conv2d和tf-contrib-slim-conv2d的区别"><a href="#tf-nn-conv2d和tf-contrib-slim-conv2d的区别" class="headerlink" title="tf.nn.conv2d和tf.contrib.slim.conv2d的区别"></a>tf.nn.conv2d和tf.contrib.slim.conv2d的区别</h3><p>&emsp;&emsp;在查看代码的时候，发现有代码用到卷积层是<code>tf.nn.conv2d</code>，但是也有使用的卷积层是<code>tf.contrib.slim.conv2d</code>。<code>tf.nn.conv2d</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：需要做卷积的输入图像，它要求是一个<code>Tensor</code>，具有<code>[batch_size, in_height, in_width, in_channels]</code>这样的<code>shape</code>，具体含义是<code>[训练时一个batch的图片数量，图片高度，图片宽度，图像通道数]</code>。注意这是一个<code>4</code>维的<code>Tensor</code>，要求数据类型为<code>float32</code>和<code>float64</code>其中之一。</li>
<li><code>filter</code>：指定<code>CNN</code>中的卷积核，它要求是一个<code>Tensor</code>，具有<code>[filter_height, filter_width, in_channels, out_channels]</code>这样的<code>shape</code>，具体含义是<code>[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</code>，要求类型与参数<code>input</code>相同。有一个地方需要注意，第三维<code>in_channels</code>就是参数<code>input</code>的第四维，这里是维度一致，不是数值一致。</li>
<li><code>strides</code>：卷积时在图像每一维的步长。这是一个一维的向量，长度为<code>4</code>，对应的是在<code>input</code>的<code>4</code>个维度上的步长。</li>
<li><code>padding</code>：<code>string</code>类型的变量，只能是<code>SAME</code>、<code>VALID</code>其中之一。这个值决定了不同的卷积方式，<code>SAME</code>代表卷积核可以停留图像边缘，<code>VALID</code>表示不能。</li>
<li><code>use_cudnn_on_gpu</code>：指定是否使用<code>cudnn</code>加速。</li>
<li><code>data_format</code>：指定<code>input</code>的格式，默认为<code>NHWC</code>格式。</li>
</ul>
<p>该函数结果返回一个<code>Tensor</code>，这个输出就是我们常说的<code>feature map</code>。<br>&emsp;&emsp;<code>tf.contrib.slim.conv2d</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">convolution(</span><br><span class="line">    inputs, num_outputs, kernel_size, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    rate=<span class="number">1</span>, activation_fn=nn.relu, normalizer_fn=<span class="keyword">None</span>, normalizer_params=<span class="keyword">None</span>,</span><br><span class="line">    weights_initializer=initializers.xavier_initializer(), weights_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    biases_initializer=init_ops.zeros_initializer(), biases_regularizer=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>,</span><br><span class="line">    variables_collections=<span class="keyword">None</span>, outputs_collections=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, scope=<span class="keyword">None</span>):</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：需要做卷积的输入图像。</li>
<li><code>num_outputs</code>：卷积核的个数(就是<code>filter</code>的个数)。</li>
<li><code>kernel_size</code>：卷积核的维度(卷积核的宽度和高度)。</li>
<li><code>stride</code>：卷积时在图像每一维的步长。</li>
<li><code>padding</code>：<code>padding</code>的方式选择，<code>VALID</code>或者<code>SAME</code>。</li>
<li><code>data_format</code>：指定<code>input</code>的格式。</li>
<li><code>rate</code>：使用<code>atrous convolution</code>的膨胀率。</li>
<li><code>activation_fn</code>：指定激活函数，默认为<code>ReLU</code>函数。</li>
<li><code>normalizer_fn</code>：正则化函数。</li>
<li><code>normalizer_params</code>：正则化函数的参数。</li>
<li><code>weights_initializer</code>：权重的初始化程序。</li>
<li><code>weights_regularizer</code>：权重可选的正则化程序。</li>
<li><code>biases_initializer</code>：<code>biase</code>的初始化程序。</li>
<li><code>biases_regularizer</code>：<code>biases</code>可选的正则化程序。</li>
<li><code>reuse</code>：是否共享层或者核变量。</li>
<li><code>variable_collections</code>：所有变量的集合列表或者字典。</li>
<li><code>outputs_collections</code>：输出被添加的集合。</li>
<li><code>trainable</code>：卷积层的参数是否可被训练。</li>
<li><code>scope</code>：共享变量所指的<code>variable_scope</code>。</li>
</ul>
<p>&emsp;&emsp;在上述的<code>API</code>中，可以看出去两者并没有什么不同。只是<code>tf.contrib.slim.conv2d</code>提供了更多可以指定的初始化部分，而对于<code>tf.nn.conv2d</code>而言，其指定<code>filter</code>的方式相比较<code>tf.contrib.slim.conv2d</code>来说更加得复杂。去除掉少用的初始化部分，其实两者的<code>API</code>可以简化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.slim.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    num_outputs,  <span class="comment"># [卷积核个数]</span></span><br><span class="line">    kernel_size,  <span class="comment"># [卷积核的高度，卷积核的宽度]</span></span><br><span class="line">    stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>,)</span><br><span class="line">​</span><br><span class="line">tf.nn.conv2d(</span><br><span class="line">    input,  <span class="comment"># 与上述一致</span></span><br><span class="line">    filter,  <span class="comment"># [卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</span></span><br><span class="line">    strides, padding,)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之variable scope/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之variable scope/" itemprop="url">TensorFlow之variable scope</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T11:10:48+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>Tensorflow</code>为了更好的管理变量，提供了<code>variable scope</code>机制，其官方解释如下：<br>&emsp;&emsp;Variable scope object to carry defaults to provide to <code>get_variable</code>.<br>&emsp;&emsp;Many of the arguments we need for <code>get_variable</code> in a variable store are most easily handled with a context. This object is used for the defaults. Attributes:</p>
<ul>
<li><code>name</code>: <code>name</code> of the current scope, used as prefix in <code>get_variable</code>.</li>
<li><code>initializer</code>: 传给<code>get_variable</code>的默认<code>initializer</code>。如果<code>get_variable</code>的时候指定了<code>initializer</code>，那么将覆盖这个默认的<code>initializer</code>。</li>
<li><code>regularizer</code>: 传给<code>get_variable</code>的默认<code>regulizer</code>。</li>
<li><code>reuse</code>: <code>Boolean</code> or <code>None</code>, setting the reuse in <code>get_variable</code>.</li>
<li><code>caching_device</code>: <code>string</code>, <code>callable</code>, or <code>None</code>: the caching device passed to <code>get_variable</code>.</li>
<li><code>partitioner</code>: <code>callable</code> or <code>None</code>: the partitioner passed to <code>get_variable</code>.</li>
<li><code>custom_getter</code>: default custom getter passed to <code>get_variable</code>.</li>
<li><code>name_scope</code>: The name passed to <code>tf.name_scope</code>.</li>
<li><code>dtype</code>: default type passed to <code>get_variable</code> (defaults to <code>DT_FLOAT</code>).</li>
</ul>
<p>&emsp;&emsp;<code>regularizer</code>参数的作用是给在本<code>variable_scope</code>下创建的<code>weights</code>加上正则项，这样我们就可以不同<code>variable_scope</code>下的参数加不同的正则项了。可以看出，用<code>variable scope</code>管理<code>get_varibale</code>是很方便的。</p>
<h3 id="确定get-variable的prefixed-name"><a href="#确定get-variable的prefixed-name" class="headerlink" title="确定get_variable的prefixed name"></a>确定get_variable的prefixed name</h3><p>&emsp;&emsp;代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"tet1"</span>):</span><br><span class="line">    var3 = tf.get_variable(<span class="string">"var3"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">    print(var3.name)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"tet2"</span>):</span><br><span class="line">    var4 = tf.get_variable(<span class="string">"var4"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">    print(var4.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tet1/var3:<span class="number">0</span></span><br><span class="line">tet2/var4:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><code>variable scope</code>是可以嵌套的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"tet1"</span>):</span><br><span class="line">    var3 = tf.get_variable(<span class="string">"var3"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">    print(var3.name)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"tet2"</span>):</span><br><span class="line">        var4 = tf.get_variable(<span class="string">"var4"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">        print(var4.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tet1/var3:<span class="number">0</span></span><br><span class="line">tet1/tet2/var4:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>get_varibale.name</code>以创建变量的<code>scope</code>作为名字的<code>prefix</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">te2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"te2"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"var2"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">        print(var2.name)</span><br><span class="line">​</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">te1</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"te1"</span>):</span><br><span class="line">                var1 = tf.get_variable(<span class="string">"var1"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">            <span class="keyword">return</span> var1</span><br><span class="line">​</span><br><span class="line">        <span class="keyword">return</span> te1()  <span class="comment"># 在scope的te2内调用的</span></span><br><span class="line">​</span><br><span class="line">res = te2()</span><br><span class="line">print(res.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te2/var2:<span class="number">0</span></span><br><span class="line">te2/te1/var1:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>对比下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">te2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"te2"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"var2"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">        print(var2.name)</span><br><span class="line">​</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">te1</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"te1"</span>):</span><br><span class="line">                var1 = tf.get_variable(<span class="string">"var1"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">            <span class="keyword">return</span> var1</span><br><span class="line">    <span class="keyword">return</span> te1()  <span class="comment"># 在scope的te2外面调用的</span></span><br><span class="line">​</span><br><span class="line">res = te2()</span><br><span class="line">print(res.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te2/var2:<span class="number">0</span></span><br><span class="line">te1/var1:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;需要注意一点的是<code>tf.variable_scope(&quot;name&quot;)</code>与<code>tf.variable_scope(scope)</code>的区别，代码<code>1</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope"</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)  <span class="comment"># 执行结果“scope/w”</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"scope"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        print(var2.name)  <span class="comment"># 执行结果“scope/scope/w”</span></span><br></pre></td></tr></table></figure>
<p>代码<code>2</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope"</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)  <span class="comment"># 执行结果“scope/w”</span></span><br><span class="line">    scope = tf.get_variable_scope()</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):  <span class="comment"># 这种方式设置的scope，是用的外部的scope</span></span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])  <span class="comment"># 这个变量的name也是“scope/w”，因此会报出错误</span></span><br><span class="line">        print(var2.name)</span><br></pre></td></tr></table></figure>
<h3 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h3><p>&emsp;&emsp;共享变量的前提是变量的名字是一样的，变量的名字是由变量名和其<code>scope</code>前缀一起构成，<code>tf.get_variable_scope().reuse_variables()</code>允许共享当前<code>scope</code>下的所有变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"level1"</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)</span><br><span class="line">    scope = tf.get_variable_scope()</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"level2"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        print(var2.name)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"level1"</span>, reuse=<span class="keyword">True</span>):  <span class="comment"># 即使嵌套variable_scop，e也会被reuse</span></span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)</span><br><span class="line">    scope = tf.get_variable_scope()</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        print(var2.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">level1/w:<span class="number">0</span></span><br><span class="line">level1/level2/w:<span class="number">0</span></span><br><span class="line">level1/w:<span class="number">0</span></span><br><span class="line">level1/w:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<hr>
<p>&emsp;&emsp;在<code>Tensorflow</code>中，有两个<code>scope</code>，一个是<code>name_scope</code>，另一个是<code>variable_scope</code>，这两个<code>scope</code>到底有什么区别呢？<br>&emsp;&emsp;先看第一个程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"hello"</span>) <span class="keyword">as</span> name_scope:</span><br><span class="line">    arr1 = tf.get_variable(<span class="string">"arr1"</span>, shape=[<span class="number">2</span>, <span class="number">10</span>], dtype=tf.float32)</span><br><span class="line">    print(name_scope)</span><br><span class="line">    print(arr1.name)</span><br><span class="line">    print(<span class="string">"scope_name:%s "</span> % tf.get_variable_scope().original_name_scope)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello/</span><br><span class="line">arr1:<span class="number">0</span></span><br><span class="line">scope_name:</span><br></pre></td></tr></table></figure>
<p>可以看出，<code>tf.name_scope</code>返回的是一个<code>string</code>，在<code>name_scope</code>中定义的<code>variable</code>的<code>name</code>并没有<code>hello/</code>前缀；<code>tf.get_variable_scope</code>的<code>original_name_scope</code>是空。<br>&emsp;&emsp;第二个程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"hello"</span>) <span class="keyword">as</span> variable_scope:</span><br><span class="line">    arr1 = tf.get_variable(<span class="string">"arr1"</span>, shape=[<span class="number">2</span>, <span class="number">10</span>], dtype=tf.float32)</span><br><span class="line">    print(variable_scope)</span><br><span class="line">    print(variable_scope.name)  <span class="comment"># 打印出变量空间名字</span></span><br><span class="line">    print(arr1.name)</span><br><span class="line">    print(tf.get_variable_scope().original_name_scope)  <span class="comment"># tf.get_variable_scope获取的就是variable_scope</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"xixi"</span>) <span class="keyword">as</span> v_scope2:</span><br><span class="line">        print(tf.get_variable_scope().original_name_scope)  <span class="comment"># tf.get_variable_scope获取的就是v_scope2</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;tensorflow.python.ops.variable_scope.VariableScope object at <span class="number">0x00000000086649B0</span>&gt;</span><br><span class="line">hello</span><br><span class="line">hello/arr1:<span class="number">0</span></span><br><span class="line">hello/</span><br><span class="line">hello/xixi/</span><br></pre></td></tr></table></figure>
<p>可以看出，<code>tf.variable_scope</code>返回的是一个<code>op</code>对象，<code>variable_scope</code>中定义的<code>variable</code>的<code>name</code>加上了<code>hello/</code>前缀；<code>tf.get_variable_scope</code>的<code>original_name_scope</code>是嵌套后的<code>scope name</code>。<br>&emsp;&emsp;第三个程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"name1"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"var1"</span>):</span><br><span class="line">        w = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">2</span>])</span><br><span class="line">        res = tf.add(w, [<span class="number">3</span>])</span><br><span class="line">​</span><br><span class="line">print(w.name)</span><br><span class="line">print(res.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var1/w:<span class="number">0</span></span><br><span class="line">name1/var1/Add:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看出，<code>variable scope</code>和<code>name scope</code>都会给<code>op</code>的<code>name</code>加上前缀。<br>&emsp;&emsp;对比三个个程序可以看出：</p>
<ul>
<li><code>name_scope</code>对<code>get_variable</code>创建的变量的名字不会有任何影响，而创建的<code>op</code>会被加上前缀。</li>
<li><code>tf.get_variable_scope</code>返回的只是<code>variable_scope</code>，不管<code>name_scope</code>，所以以后我们在使用<code>tf.get_variable_scope().reuse_variables()</code>时可以无视<code>name_scope</code>。</li>
</ul>
<p>&emsp;&emsp;其它情况如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"scope1"</span>) <span class="keyword">as</span> scope1:</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"scope2"</span>) <span class="keyword">as</span> scope2:</span><br><span class="line">        print(scope2)  <span class="comment"># 结果为“scope1/scope2/”</span></span><br></pre></td></tr></table></figure>
<p>另外一个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>) <span class="keyword">as</span> scope1:</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"scope2"</span>) <span class="keyword">as</span> scope2:</span><br><span class="line">        print(scope2.name)  <span class="comment"># 结果为“scope1/scope2”</span></span><br></pre></td></tr></table></figure>
<h3 id="name-scope可以用来干什么？"><a href="#name-scope可以用来干什么？" class="headerlink" title="name_scope可以用来干什么？"></a>name_scope可以用来干什么？</h3><p>&emsp;&emsp;典型的<code>TensorFlow</code>可以有数以千计的节点，如此多而难以一下全部看到，甚至无法使用标准图表工具来展示。为简单起见，我们为<code>op/tensor</code>名划定范围，并且可视化把该信息用于在图表中的节点上定义一个层级。默认情况下，只有顶层节点会显示。下面这个例子使用<code>tf.name_scope</code>在<code>hidden</code>命名域下定义了三个操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'hidden'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    a = tf.constant(<span class="number">5</span>, name=<span class="string">'alpha'</span>)</span><br><span class="line">    W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">'biases'</span>)</span><br><span class="line">    print(a.name)</span><br><span class="line">    print(W.name)</span><br><span class="line">    print(b.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hidden/alpha:<span class="number">0</span></span><br><span class="line">hidden/weights:<span class="number">0</span></span><br><span class="line">hidden/biases:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><code>name_scope</code>是给<code>op_name</code>加前缀，<code>variable_scope</code>是给<code>get_variable</code>创建的变量的名字加前缀。<br>&emsp;&emsp;<code>tf.variable_scope</code>有时也会处理命名冲突：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(name=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name, default_name=<span class="string">"scope"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        w = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">2</span>, <span class="number">10</span>])</span><br><span class="line">​</span><br><span class="line">test()</span><br><span class="line">test()</span><br><span class="line">ws = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> ws:</span><br><span class="line">    print(w.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scope/w:<span class="number">0</span></span><br><span class="line">scope_1/w:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看出，如果只是使用<code>default_name</code>这个属性来创建<code>variable_scope</code>的时候，会处理命名冲突。<br>&emsp;&emsp;<code>tf.name_scope(None)</code>有清除<code>name scope</code>的作用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"hehe"</span>):</span><br><span class="line">    w1 = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="keyword">None</span>):</span><br><span class="line">        w2 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">​</span><br><span class="line">print(w1.name)</span><br><span class="line">print(w2.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hehe/Variable:<span class="number">0</span></span><br><span class="line">Variable:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;简单来看：</p>
<ul>
<li>使用<code>tf.Variable</code>的时候，<code>tf.name_scope</code>和<code>tf.variable_scope</code>都会给<code>Variable</code>和<code>op</code>的<code>name</code>属性加上前缀。</li>
<li>使用<code>tf.get_variable</code>的时候，<code>tf.name_scope</code>就不会给<code>tf.get_variable</code>创建出来的<code>Variable</code>加前缀。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/Keras之Regressor回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/Keras之Regressor回归/" itemprop="url">Keras之Regressor回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T11:00:46+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;神经网络可以用来模拟回归问题(<code>regression</code>)，例如给一组数据，用一条线来对数据进行拟合，并可以预测新输入x的输出值。</p>
<h3 id="导入模块并创建数据"><a href="#导入模块并创建数据" class="headerlink" title="导入模块并创建数据"></a>导入模块并创建数据</h3><p>&emsp;&emsp;<code>models.Sequential</code>用来一层一层地去建立神经层，<code>layers.Dense</code>意思是这个神经层是全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 可视化模块</span></span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># create some data</span></span><br><span class="line">X = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>)</span><br><span class="line">np.random.shuffle(X)  <span class="comment"># randomize the data</span></span><br><span class="line">Y = <span class="number">0.5</span> * X + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, (<span class="number">200</span>,))</span><br><span class="line"><span class="comment"># plot data</span></span><br><span class="line">plt.scatter(X, Y)</span><br><span class="line">plt.show()</span><br><span class="line">​</span><br><span class="line">X_train, Y_train = X[:<span class="number">160</span>], Y[:<span class="number">160</span>]  <span class="comment"># train前160个“data points”</span></span><br><span class="line">X_test, Y_test = X[<span class="number">160</span>:], Y[<span class="number">160</span>:]  <span class="comment"># test后40个“data points”</span></span><br></pre></td></tr></table></figure>
<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>&emsp;&emsp;使用<code>Sequential</code>建立<code>model</code>，再用<code>model.add</code>添加神经层，第二行添加的是<code>Dense</code>全连接神经层。<code>Dense</code>的参数有两个，分别是输入数据和输出数据的维度，本例中的<code>x</code>和<code>y</code>是一维的。<br>&emsp;&emsp;如果需要添加下一个神经层，则不用再定义输入的维度，因为它默认就把前一层的输出作为当前层的输入。在这个例子里，只需要一层就够了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(output_dim=<span class="number">1</span>, input_dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="激活模型"><a href="#激活模型" class="headerlink" title="激活模型"></a>激活模型</h3><p>&emsp;&emsp;接下来要激活神经网络，上一步只是定义模型。在<code>compile</code>的参数中，误差函数用的是<code>mse</code>均方误差；优化器用的是<code>sgd</code>随机梯度下降法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># choose loss function and optimizing method</span></span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'sgd'</span>)</span><br></pre></td></tr></table></figure>
<p>于是就构建好了一个神经网络，它比<code>Tensorflow</code>要少了很多代码。</p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>&emsp;&emsp;训练的时候用<code>model.train_on_batch</code>一批一批地训练<code>X_train</code>、<code>Y_train</code>。默认的返回值是<code>cost</code>，每<code>100</code>步输出一下结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training</span></span><br><span class="line">print(<span class="string">'Training -----------'</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">301</span>):</span><br><span class="line">    cost = model.train_on_batch(X_train, Y_train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'train cost: '</span>, cost)</span><br></pre></td></tr></table></figure>
<h3 id="检验模型"><a href="#检验模型" class="headerlink" title="检验模型"></a>检验模型</h3><p>&emsp;&emsp;检验用到的函数是<code>model.evaluate</code>，输入测试集的<code>x</code>和<code>y</code>，输出<code>cost</code>、<code>weights</code>和<code>biases</code>，其中<code>weights</code>和<code>biases</code>是取在模型的第一层<code>model.layers[0]</code>学习到的参数。从学习到的结果可以看到，<code>weights</code>比较接近<code>0.5</code>，<code>bias</code>接近<code>2</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line">cost = model.evaluate(X_test, Y_test, batch_size=<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'test cost:'</span>, cost)</span><br><span class="line">W, b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line">print(<span class="string">'Weights='</span>, W, <span class="string">'\nbiases='</span>, b)</span><br></pre></td></tr></table></figure>
<h3 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h3><p>&emsp;&emsp;最后可以画出预测结果，与测试集的值进行对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plotting the prediction</span></span><br><span class="line">Y_pred = model.predict(X_test)</span><br><span class="line">plt.scatter(X_test, Y_test)</span><br><span class="line">plt.plot(X_test, Y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/13/深度学习/Keras之Regressor回归/1.png" height="299" width="399"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/tf.profiler.ProfileOptionBuilder/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/tf.profiler.ProfileOptionBuilder/" itemprop="url">tf.profiler.ProfileOptionBuilder</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T00:23:29+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Class-ProfileOptionBuilder"><a href="#Class-ProfileOptionBuilder" class="headerlink" title="Class ProfileOptionBuilder"></a>Class ProfileOptionBuilder</h3><p>&emsp;&emsp;Defined in <code>tensorflow/python/profiler/option_builder.py</code>. Option builder for profiling <code>API</code>. For tutorial on the options, see <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Users can use pre-built options:</span></span><br><span class="line">opts = (tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())</span><br><span class="line"><span class="comment"># Or, build your own options:</span></span><br><span class="line">opts = (tf.profiler.ProfileOptionBuilder().with_max_depth(<span class="number">10</span>).with_min_micros(<span class="number">1000</span>) \</span><br><span class="line">        .select([<span class="string">'accelerator_micros'</span>]).with_stdout_output().build())</span><br><span class="line"><span class="comment"># Or customize the pre-built options:</span></span><br><span class="line">opts = (tf.profiler.ProfileOptionBuilder(tf.profiler.ProfileOptionBuilder.time_and_memory()) \</span><br><span class="line">        .with_displaying_options(show_name_regexes=[<span class="string">'.*rnn.*'</span>]).build())</span><br><span class="line"><span class="comment"># Finally, profiling with the options:</span></span><br><span class="line">_ = tf.profiler.profile(tf.get_default_graph(), run_meta=run_meta, cmd=<span class="string">'scope'</span>, options=opts)</span><br></pre></td></tr></table></figure>
<h3 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h3><p>&emsp;&emsp;Constructor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__init__(options=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>options</code>: Optional initial option dict to start with.</li>
</ul>
<h3 id="account-displayed-op-only"><a href="#account-displayed-op-only" class="headerlink" title="account_displayed_op_only"></a>account_displayed_op_only</h3><p>&emsp;&emsp;Whether only account the statistics of displayed profiler nodes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">account_displayed_op_only(is_true)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>is_true</code>: If <code>true</code>, only account statistics of nodes eventually displayed by the outputs. Otherwise, a node’s statistics are accounted by its parents as long as it’s types match <code>account_type_regexes</code>, even if it is hidden from the output, say, by <code>hide_name_regexes</code>.</li>
</ul>
<h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><p>&emsp;&emsp;Build a profiling option:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">build()</span><br></pre></td></tr></table></figure>
<p>Return a dict of profiling options.</p>
<h3 id="float-operation"><a href="#float-operation" class="headerlink" title="float_operation"></a>float_operation</h3><p>&emsp;&emsp;Options used to profile float operations:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">float_operation()</span><br></pre></td></tr></table></figure>
<p>Please see <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md</code> on the caveats of calculating <code>float</code> operations. Return a dict of profiling options.</p>
<h3 id="order-by"><a href="#order-by" class="headerlink" title="order_by"></a>order_by</h3><p>&emsp;&emsp;Order the displayed profiler nodes based on a attribute:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">order_by(attribute)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>attribute</code>: An attribute the profiler node has.</li>
</ul>
<p>Supported attribute includes <code>micros</code>, <code>bytes</code>, <code>occurrence</code>, <code>params</code>, etc. See <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md</code> for supported attributes.</p>
<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p>&emsp;&emsp;Select the attributes to display:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select(attributes)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>attributes</code>: A list of attribute the profiler node has.</li>
</ul>
<h3 id="time-and-memory"><a href="#time-and-memory" class="headerlink" title="time_and_memory"></a>time_and_memory</h3><p>&emsp;&emsp;Show operation time and memory consumptions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">time_and_memory(</span><br><span class="line">        min_micros=<span class="number">1</span>, min_bytes=<span class="number">1</span>, min_accelerator_micros=<span class="number">0</span>, min_cpu_micros=<span class="number">0</span>,</span><br><span class="line">        min_peak_bytes=<span class="number">0</span>, min_residual_bytes=<span class="number">0</span>, min_output_bytes=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_micros</code>: Only show profiler nodes with execution time no less than this. It sums accelerator and cpu times.</li>
<li><code>min_bytes</code>: Only show profiler nodes requested to allocate no less bytes than this.</li>
<li><code>min_accelerator_micros</code>: Only show profiler nodes spend no less than this time on accelerator (e.g. <code>GPU</code>).</li>
<li><code>min_cpu_micros</code>: Only show profiler nodes spend no less than this time on cpu.</li>
<li><code>min_peak_bytes</code>: Only show profiler nodes using no less than this bytes at peak (high watermark). For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>peak_bytes</code>.</li>
<li><code>min_residual_bytes</code>: Only show profiler nodes have no less than this bytes not being <code>de-allocated</code> after <code>Compute()</code> ends. For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>residual_bytes</code>.</li>
<li><code>min_output_bytes</code>: Only show profiler nodes have no less than this bytes output. The output are not necessarily allocated by this profiler nodes.</li>
</ul>
<p>Return a dict of profiling options.</p>
<h3 id="trainable-variables-parameter"><a href="#trainable-variables-parameter" class="headerlink" title="trainable_variables_parameter"></a>trainable_variables_parameter</h3><p>&emsp;&emsp;Options used to profile trainable variable parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">trainable_variables_parameter()</span><br></pre></td></tr></table></figure>
<p>Normally used together with <code>scope</code> view. Return a dict of profiling options.</p>
<h3 id="with-accounted-types"><a href="#with-accounted-types" class="headerlink" title="with_accounted_types"></a>with_accounted_types</h3><p>&emsp;&emsp;Selectively counting statistics based on node types:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_accounted_types(account_type_regexes)</span><br></pre></td></tr></table></figure>
<p>Here, <code>types</code> means the profiler nodes’ properties. Profiler by default consider device name (e.g. <code>/job:xx/.../device:GPU:0</code>) and operation type (e.g. <code>MatMul</code>) as profiler nodes’ properties. User can also associate customized <code>types</code> to profiler nodes through <code>OpLogProto</code> proto.<br>&emsp;&emsp;For example, user can select profiler nodes placed on <code>gpu:0</code> with <code>account_type_regexes=[&#39;.*gpu:0.*&#39;]</code><br>&emsp;&emsp;If none of a node’s properties match the specified regexes, the node is not displayed nor accounted.</p>
<ul>
<li><code>account_type_regexes</code>: A list of regexes specifying the types.</li>
</ul>
<h3 id="with-empty-output"><a href="#with-empty-output" class="headerlink" title="with_empty_output"></a>with_empty_output</h3><p>&emsp;&emsp;Do not generate <code>side-effect</code> outputs:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_empty_output()</span><br></pre></td></tr></table></figure>
<h3 id="with-file-output"><a href="#with-file-output" class="headerlink" title="with_file_output"></a>with_file_output</h3><p>&emsp;&emsp;Print the result to a file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_file_output(outfile)</span><br></pre></td></tr></table></figure>
<h3 id="with-max-depth"><a href="#with-max-depth" class="headerlink" title="with_max_depth"></a>with_max_depth</h3><p>&emsp;&emsp;Set the maximum depth of display:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_max_depth(max_depth)</span><br></pre></td></tr></table></figure>
<p>The depth depends on profiling view. For <code>scope</code> view, it’s the depth of name scope hierarchy (<code>tree</code>), for <code>op</code> view, it’s the number of operation types (<code>list</code>), etc.</p>
<ul>
<li><code>max_depth</code>: Maximum depth of the data structure to display.</li>
</ul>
<h3 id="with-min-execution-time"><a href="#with-min-execution-time" class="headerlink" title="with_min_execution_time"></a>with_min_execution_time</h3><p>&emsp;&emsp;Only show profiler nodes consuming no less than <code>min_micros</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_execution_time(min_micros=<span class="number">0</span>, min_accelerator_micros=<span class="number">0</span>, min_cpu_micros=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_micros</code>: Only show profiler nodes with execution time no less than this. It sums accelerator and cpu times.</li>
<li><code>min_accelerator_micros</code>: Only show profiler nodes spend no less than this time on accelerator (e.g. <code>GPU</code>).</li>
<li><code>min_cpu_micros</code>: Only show profiler nodes spend no less than this time on cpu.</li>
</ul>
<h3 id="with-min-float-operations"><a href="#with-min-float-operations" class="headerlink" title="with_min_float_operations"></a>with_min_float_operations</h3><p>&emsp;&emsp;Only show profiler nodes consuming no less than <code>min_float_ops</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_float_operations(min_float_ops)</span><br></pre></td></tr></table></figure>
<p>Please see <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md</code> on the caveats of calculating float operations.</p>
<ul>
<li><code>min_float_ops</code>: Only show profiler nodes with float operations no less than this.</li>
</ul>
<h3 id="with-min-memory"><a href="#with-min-memory" class="headerlink" title="with_min_memory"></a>with_min_memory</h3><p>&emsp;&emsp;Only show profiler nodes consuming no less than <code>min_bytes</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_memory(min_bytes=<span class="number">0</span>, min_peak_bytes=<span class="number">0</span>, min_residual_bytes=<span class="number">0</span>, min_output_bytes=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_bytes</code>: Only show profiler nodes requested to allocate no less bytes than this.</li>
<li><code>min_peak_bytes</code>: Only show profiler nodes using no less than this bytes at peak (high watermark). For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>peak_bytes</code>.</li>
<li><code>min_residual_bytes</code>: Only show profiler nodes have no less than this bytes not being <code>de-allocated</code> after <code>Compute()</code> ends. For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>residual_bytes</code>.</li>
<li><code>min_output_bytes</code>: Only show profiler nodes have no less than this bytes output. The output are not necessarily allocated by this profiler nodes.</li>
</ul>
<h3 id="with-min-occurrence"><a href="#with-min-occurrence" class="headerlink" title="with_min_occurrence"></a>with_min_occurrence</h3><p>&emsp;&emsp;Only show profiler nodes including no less than <code>min_occurrence</code> graph nodes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_occurrence(min_occurrence)</span><br></pre></td></tr></table></figure>
<p>A <code>node</code> means a profiler output node, which can be a python line (code view), an operation type (op view), or a graph node (<code>graph/scope</code> view). A python line includes all graph nodes created by that line, while an operation type includes all graph nodes of that type.</p>
<ul>
<li><code>min_occurrence</code>: Only show nodes including no less than this.</li>
</ul>
<h3 id="with-min-parameters"><a href="#with-min-parameters" class="headerlink" title="with_min_parameters"></a>with_min_parameters</h3><p>&emsp;&emsp;Only show profiler nodes holding no less than <code>min_params</code> parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_parameters(min_params)</span><br></pre></td></tr></table></figure>
<p><code>Parameters</code> normally refers the weights of in <code>TensorFlow</code> variables. It reflects the <code>capacity</code> of models.</p>
<ul>
<li><code>min_params</code>: Only show profiler nodes holding number parameters no less than this.</li>
</ul>
<h3 id="with-node-names"><a href="#with-node-names" class="headerlink" title="with_node_names"></a>with_node_names</h3><p>&emsp;&emsp;Regular expressions used to select profiler nodes to display:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_node_names(start_name_regexes=<span class="keyword">None</span>, show_name_regexes=<span class="keyword">None</span>, hide_name_regexes=<span class="keyword">None</span>, trim_name_regexes=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>After <code>with_accounted_types</code> is evaluated, <code>with_node_names</code> are evaluated as follows:<br>&emsp;&emsp;For a profile data structure, profiler first finds the profiler nodes matching <code>start_name_regexes</code>, and starts displaying profiler nodes from there. Then, if a node matches <code>show_name_regexes</code> and doesn’t match <code>hide_name_regexes</code>, it’s displayed. If a node matches <code>trim_name_regexes</code>, profiler stops further searching that branch.</p>
<ul>
<li><code>start_name_regexes</code>: list of node name regexes to start displaying.</li>
<li><code>show_name_regexes</code>: list of node names regexes to display.</li>
<li><code>hide_name_regexes</code>: list of node_names regexes that should be hidden.</li>
<li><code>trim_name_regexes</code>: list of node name regexes from where to stop.</li>
</ul>
<h3 id="with-pprof-output"><a href="#with-pprof-output" class="headerlink" title="with_pprof_output"></a>with_pprof_output</h3><p>&emsp;&emsp;Generate a pprof profile <code>gzip</code> file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_pprof_output(pprof_file)</span><br></pre></td></tr></table></figure>
<p>To use the pprof file: <code>pprof -png --nodecount=100 --sample_index=1</code>.</p>
<ul>
<li><code>pprof_file</code>: filename for output, usually suffixed with <code>.pb.gz</code>.</li>
</ul>
<h3 id="with-stdout-output"><a href="#with-stdout-output" class="headerlink" title="with_stdout_output"></a>with_stdout_output</h3><p>&emsp;&emsp;Print the result to <code>stdout</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_stdout_output()</span><br></pre></td></tr></table></figure>
<h3 id="with-step"><a href="#with-step" class="headerlink" title="with_step"></a>with_step</h3><p>&emsp;&emsp;Which profile step to use for profiling:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_step(step)</span><br></pre></td></tr></table></figure>
<p>The <code>step</code> here refers to the step defined by <code>Profiler.add_step() API</code>.</p>
<ul>
<li><code>step</code>: When multiple steps of profiles are available, select which step’s profile to use. If <code>-1</code>, use average of all available steps.</li>
</ul>
<h3 id="with-timeline-output"><a href="#with-timeline-output" class="headerlink" title="with_timeline_output"></a>with_timeline_output</h3><p>&emsp;&emsp;Generate a timeline json file：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_timeline_output(timeline_file)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="How-to-calculate-a-net’s-FLOPs-in-CNN"><a href="#How-to-calculate-a-net’s-FLOPs-in-CNN" class="headerlink" title="How to calculate a net’s FLOPs in CNN"></a>How to calculate a net’s FLOPs in CNN</h3><p>&emsp;&emsp;Problem: I want to design a convolutional neural network which occupy <code>GPU</code> resource no more than <code>Alexnet</code>. I want to use <code>FLOPs</code> to measure it but I don’t know how to calculate it. Is there any tools to do it, please?<br>&emsp;&emsp;Answer: For future visitors, if you use <code>Keras</code> and <code>TensorFlow</code> as Backend, then you can try the following example. It calculates the <code>FLOPs</code> for the <code>MobileNet</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.applications.mobilenet <span class="keyword">import</span> MobileNet</span><br><span class="line">​</span><br><span class="line">run_meta = tf.RunMetadata()</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=tf.Graph()) <span class="keyword">as</span> sess:</span><br><span class="line">    K.set_session(sess)</span><br><span class="line">    net = MobileNet(alpha=<span class="number">.75</span>, input_tensor=tf.placeholder(<span class="string">'float32'</span>, shape=(<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">    opts = tf.profiler.ProfileOptionBuilder.float_operation()</span><br><span class="line">    flops = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd=<span class="string">'op'</span>, options=opts)</span><br><span class="line">    opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()</span><br><span class="line">    params = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd=<span class="string">'op'</span>, options=opts)</span><br><span class="line">    print(<span class="string">"&#123;:,&#125; --- &#123;:,&#125;"</span>.format(flops.total_float_ops, params.total_parameters))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/22/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><span class="page-number current">23</span><a class="page-number" href="/page/24/">24</a><span class="space">&hellip;</span><a class="page-number" href="/page/95/">95</a><a class="extend next" rel="next" href="/page/24/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">943</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
