<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="泥腿子出身">
<meta property="og:url" content="http://fukangwei.gitee.io/page/22/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="泥腿子出身">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/22/">





  <title>泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/15/深度学习/TensorFlow实现多层感知机/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/深度学习/TensorFlow实现多层感知机/" itemprop="url">TensorFlow实现多层感知机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-15T16:09:29+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="多层感知机简介"><a href="#多层感知机简介" class="headerlink" title="多层感知机简介"></a>多层感知机简介</h3><p>&emsp;&emsp;多层感知机(<code>Multilayer Perceptron</code>，<code>MLP</code>)也叫做人工神经网络(<code>Artificial Neural Network</code>，<code>ANN</code>)，除了输入和输出层，它中间可以有多个隐层。最简单的<code>MLP</code>只含一个隐层，即三层的结构，如下图所示。可以看到，多层感知机层与层之间是全连接的(<code>全连接</code>是指上一层的任何一个神经元与下一层的所有神经元都有连接)。单层感知器只能学习线性函数，而多层感知器也可以学习非线性函数。</p>
<p><img src="/2019/02/15/深度学习/TensorFlow实现多层感知机/1.png"></p>
<p>&emsp;&emsp;<code>Softmax</code>回归可以算是多分类问题<code>logistic</code>回归，它和神经网络的最大区别是没有隐含层。理论上只要隐含节点足够多，即使只有一个隐含层的神经网络也可以拟合任意函数；同时，隐含层数越多，越容易拟合复杂结构。为了拟合复杂函数，需要的隐含节点的数目基本上随着隐含层的数量增多呈指数下降的趋势，也就是说层数越多，神经网络所需要的隐含节点可以越少。层数越深，概念越抽象，需要背诵的知识点就越少。在实际应用中，深层神经网络会遇到许多困难，如过拟合、参数调试、梯度弥散等。<br>&emsp;&emsp;参数调试问题尤其是<code>SGD</code>(<code>StochasticGradient Descent</code>)的参数，对<code>SGD</code>设置不同的学习率，最后得到的结果可能差异巨大。神经网络的优化通常不是一个简单的凸优化问题，它处处充满了局部最优。有理论表示，神经网络可能有很多个局部最优解都可以达到比较好的分类效果，而全局最优很可能造成过拟合。对于<code>SGD</code>，我们希望一开始学习率大一些，加速收敛，在训练的后期又希望学习率小一些，这样可以低速进入一个局部最优解。不同的机器学习问题的学习率设置也需要针对性的调试，像<code>Adagrad</code>、<code>Adam</code>、<code>Adadelta</code>等自适应的方法可以减轻调试参数的负担。对于这些优化算法，通常我们使用其默认的参数设置就可以得到比较好的效果。<br>&emsp;&emsp;梯度弥散(<code>Gradient Vanishment</code>)是另一个影响深层神经网络训练的问题，在<code>ReLU</code>激活函数出现之前，神经网络训练是使用<code>Sigmoid</code>作为激活函数。非线性的<code>Sigmoid</code>函数在信号的特征空间映射上，对中央区的信号增益较大，对两侧区的信号增益小。当神经网络层数较多时，<code>Sigmoid</code>函数在反向传播中梯度值会逐渐减小，到达前面几层的梯度值就变得非常小了，在神经网络训练的时候，前面几层的神经网络参数几乎得不到训练更新。直到<code>ReLU</code>(<code>y = max(0, x)</code>)的出现才比较完美地解决了梯度弥散的问题。信号在超过某个阈值时，神经元才会进入兴奋和激活的状态，否则会处于抑制状态。<code>ReLU</code>可以很好地反向传递梯度，经过多层的梯度反向传播，梯度依旧不会大幅减小，因此非常适合深层神经网络的训练。<code>ReLU</code>对比于<code>Sigmoid</code>的主要特点有以下几点：</p>
<ul>
<li>单侧抑制。</li>
<li>相对宽阔的兴奋边界。</li>
<li>稀疏激活性。</li>
</ul>
<p>&emsp;&emsp;目前，<code>ReLU</code>及其变种<code>EIU</code>、<code>PReLU</code>、<code>RReLU</code>已经成为最主流的激活函数。实践中大部分情况下(包括<code>MLP</code>、<code>CNN</code>、<code>RNN</code>)将隐含层的激活函数从<code>Sigmoid</code>替换为<code>ReLU</code>都可以带来训练速度和模型准确率的提升。当然神经网络的输出层一般都是<code>Sigmoid</code>函数，因为它最接近概率输出分布。</p>
<p><img src="/2019/02/15/深度学习/TensorFlow实现多层感知机/2.png" height="164" width="428"></p>
<h3 id="TensorFlow实现多层感知机"><a href="#TensorFlow实现多层感知机" class="headerlink" title="TensorFlow实现多层感知机"></a>TensorFlow实现多层感知机</h3><p>&emsp;&emsp;在<code>TensorFlow</code>上实现的<code>Softmax</code>回归模型对<code>MNIST</code>数据集取得了<code>92%</code>的正确率，现在我们给神经网络加上一层隐含层，并使用减轻过拟合的<code>Dropout</code>、自适应学习率的<code>Adagrad</code>以及解决梯度弥散问题激活函数<code>ReLU</code>。<br>&emsp;&emsp;首先载入<code>TensorFlow</code>并加载<code>MNIST</code>数据集，创建一个<code>TensorFlow</code>默认的<code>Interactive Session</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">​</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;接下来我们要给隐藏层的参数设置<code>Variable</code>并进行初始化，指定输入节点数<code>in_units</code>和隐含层节点数<code>h1_units</code>。初始化隐含层的权重<code>W1</code>和偏置<code>b1</code>，我们将偏置全部赋值为<code>0</code>，并将权重初始化为截断的正态分布，其标准差为<code>0.1</code>，这一步可以通过<code>tf.truncated_normal</code>实现。因为模型使用的激活函数是<code>ReLU</code>，需要使用正态分布对<code>W1</code>进行初始化，给权重参数增加一些噪声来打破完全对称，并且避免<code>0</code>梯度。在其它一些模型中，有时还需要给偏置初始化一些非零初始值来避免<code>dead neuron</code>(死亡神经元)。对于输出层<code>Softmax</code>，直接将权重<code>W2</code>和偏置<code>b2</code>全部初始化为<code>0</code>即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">in_units = <span class="number">784</span></span><br><span class="line">h1_units = <span class="number">300</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 对于ReLU激活函数，常用截断正态分布，避免0梯度和完全对称。</span></span><br><span class="line"><span class="comment"># 对于Softmax分类(也就是sigmoid激活)，由于对0附近最敏感，所以采用全0初始权重</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([in_units, h1_units], stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([h1_units], dtype=tf.float32))</span><br><span class="line">W2 = tf.Variable(tf.zeros([h1_units, <span class="number">10</span>], dtype=tf.float32))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>], dtype=tf.float32))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;接下来为输入<code>x</code>设置<code>placeholder</code>，并为不同的<code>Dropout</code>设置一个输入<code>placeholder</code>，通常在训练时小于<code>1</code>，预测时等于<code>1</code>，所以也把<code>Dropout</code>的比率作为计算图的输入，并定义成一个<code>placeholder</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, in_units])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;下面定义模型结构，首先定义一个隐含层<code>hidden1</code>，通过<code>tf.nn.relu(tf.matmul(x, W1) + b1)</code>实现一个激活函数为<code>ReLU</code>的隐含层，这个隐含层的计算公式就是<code>y = relu(W1 * x + b1)</code>。接下来调用<code>tf.nn.dropout</code>实现<code>Dropout</code>功能，随机将一部分神经元节点置为<code>0</code>，这里的<code>keep_prob</code>参数是保留的数据比例而不是置为<code>0</code>的比例，在训练的时候应该是小于<code>1</code>，用以制造随机性，防止过拟合；在预测的时候应该等于<code>1</code>，即全部特征用来预测样本的类别。最后是输出层，也就是<code>softmax</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hidden1 = tf.nn.relu(tf.add(tf.matmul(x, W1), b1))</span><br><span class="line">hidden1_drop = tf.nn.dropout(hidden1, keep_prob)</span><br><span class="line">y = tf.nn.softmax(tf.add(tf.matmul(hidden1_drop, W2), b2))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在优化器选择上，我们选择<code>Adagrad</code>并把学习率设置为<code>0.3</code>，直接使用<code>tf.train.AdagradOptimizer(0.3)</code>就可以了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis=<span class="number">1</span>))</span><br><span class="line">train_step = tf.train.AdagradOptimizer(<span class="number">0.3</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练步骤如下所示，这里加入<code>keep_prob</code>作为计算图的输入，并且在训练时设为<code>0.75</code>，即保留<code>75%</code>的节点，其余的<code>25%</code>置为<code>0</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y, axis=<span class="number">1</span>), tf.argmax(y_, axis=<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">​</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    train_step.run(&#123;x: batch_xs, y_: batch_ys, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'当前迭代次数&#123;0&#125;，当前准确率&#123;1:.3f&#125;'</span>.format(i, accuracy.eval(&#123;x: batch_xs, y_: batch_ys, keep_prob: <span class="number">1.0</span>&#125;)))</span><br><span class="line"></span><br><span class="line">print(accuracy.eval(&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>最终在测试集上可以得到<code>98%</code>的准确率。<br>&emsp;&emsp;没有隐含层的<code>Softmax Regression</code>只能直接从图像的像素点推断是哪个数字，而没有特征抽象的过程。多层神经网络依靠隐含层，则可以组合出高阶特征，比如横线、竖线、圆圈等，之后可以将这些高阶特征或者说组件再组合成数字，就能实现精准的匹配和分类。隐含层输出的高阶特征经常是可以复用的，所以每一类的判别、概率输出都共享这些高阶特征，而不是各自连接独立的高阶特征。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/15/深度学习/学习速率/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/深度学习/学习速率/" itemprop="url">学习速率</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-15T15:27:46+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;学习速率(<code>learning rate</code>)是神经网络中随着时间推移，信息累积的速度。学习速率决定了网络达到最优值速度，或对于特定期望的输出时，网络的参数达到最优状态的速度。在随机梯度下降(<code>SGD</code>)的平面图中，学习速率与误差梯度的形状无关，因为全局学习速率与误差梯度无关。然而，可以对原始<code>SGD</code>更新规则进行许多修改，将学习速率与错误梯度的大小和方向相关联。</p>
<h3 id="为什么要调整学习速率？"><a href="#为什么要调整学习速率？" class="headerlink" title="为什么要调整学习速率？"></a>为什么要调整学习速率？</h3><p>&emsp;&emsp;随着时间的推移，调整学习速率与根据道路条件对汽车的速度进行调整相似。在高速公路等平稳宽阔的道路上，我们可以提高速度(学习速率)；但是在狭窄的丘陵或山谷道路上，我们必须放慢速度。此外，我们不希望在高速公路上行驶得太慢，否则需要太长时间才能到达目的地(由于参数不正确而导致更长的训练时间)。同样，我们也不想在丘陵和狭窄的道路上(如优化损失函数曲面的沟壑)驾驶太快，因为我们很容易失去对汽车的控制(陷入抖动或产生太多的反弹，我们几乎不能做任何改进)或跳过目的地(最佳值)。<br>&emsp;&emsp;请记住<code>较高的学习速率表示系统含有太多的动能，参数向量在处于混沌状态下，不断来回反弹，无法稳定到损失函数的一个较深且较窄的最优值</code>。<br>&emsp;&emsp;理想的策略是从一个很大的学习速率开始，随后逐渐减半，直到损失值不再分歧(发散)。接近训练结束时，学习速率的衰减应该在100倍以上。这种衰减使学习到的网络模型可以抵抗随机波动，这中随机波动可能会扭转学习(陷入发散状态)。我们将从一个小的学习速率开始，测试一小组数据，并选择适当的值。</p>
<h3 id="衰减学习速率"><a href="#衰减学习速率" class="headerlink" title="衰减学习速率"></a>衰减学习速率</h3><p>&emsp;&emsp;非自适应学习速率可能不是最佳的。学习速率衰减可以通过每几个时间周期做一些较小的常数因子的衰减，或通过指数衰减来实现，指数衰减采用几个时间周期的指数的数学形式来实现。<code>衰减</code>通常被认为是一个消极的概念，同样，学习速率的衰减也是消极的：它指的是学习速率下降的程度。然而，这种衰减的结果实际上是我们非常想要的。例如在一辆车上，我们降低速度以适应道路和交通状况，这种减速可以被理解为汽车速度的<code>衰减</code>。同样，我们从衰减学习速率得到好处，以适应梯度。<br>&emsp;&emsp;动量是一种自适应学习速率方法的参数，允许沿浅方向使用较高的速度，同时沿陡峭方向降低速度前进。这种动量被定义为<code>经典动量</code>，其对速度进行校正，然后在速度方向上进行大跳跃。动量有助于加速或减速学习速率以适应梯度的变化，最终导致网络学习速率的变化而不是其在损失函数表面上的位置的变化。动量使学习到的网络更能抵抗输入数中的噪声和随机性。<br>&emsp;&emsp;将学习速率视为超参数的其他更新规则包括：</p>
<ul>
<li><code>AdaGrad</code>更新规则：基于每个维度历史的均方误差的和，为每一个维度的梯度增加了一个初度变换的规则。</li>
<li><code>RMSProp</code>自适应学习速率法：保持每个权重的平方梯度的移动平均值，以规范化(<code>normalize</code>)当前梯度。<code>RMSProp</code>增加了对波动和随机噪声的更强的抵抗能力。</li>
<li><code>Adam</code>、<code>Kingma</code>和<code>Ba</code>：引入了偏差矫正策略以补偿零初始化带来影响。</li>
<li><code>rprop</code>：只使用梯度的符号来对每个权重的步长自适应调整，这不适用于<code>mini-batch</code>训练。</li>
</ul>
<p>除了这些规则以外，还有基于牛顿的更新规则的二阶方法。然而，二阶方法不将学习速率视为超参数，且由于它们的计算需求很高，所以很少用于大规模的深度学习系统。<br>&emsp;&emsp;下图显示了在类似超参数设置条件下，不同优化技术的对比：</p>
<p><img src="/2019/02/15/深度学习/学习速率/1.jpg" height="258" width="260"></p>
<p>&emsp;&emsp;从本质上讲，我们的目的不是衰减，而是要通过衰减跳入一个正确的地方。必须选择性地增加或减少学习速率，为了达到一个全局最优值或者是期望的目标值。不要害怕这个，因为我们经常要做一遍又一遍。<br>&emsp;&emsp;在<code>Tensorflow</code>中，为解决设定学习率(<code>learning rate</code>)问题，提供了<code>tf.train.exponential_decay</code>函数实现指数衰减学习率。首先使用较大学习率，目的是快速得到一个比较优的解；然后通过迭代逐步减小学习率，目的是使模型在训练后期更加稳定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decayed_learning_rate = learining_rate * decay_rate^(global_step / decay_steps)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>decayed_learning_rate</code>：每一轮优化时使用的学习率。</li>
<li><code>learning_rate</code>：事先设定的初始学习率。</li>
<li><code>decay_rate</code>：衰减系数。</li>
<li><code>decay_steps</code>：衰减速度。</li>
</ul>
<p><code>tf.train.exponential_decay</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.exponential_decay(learning_rate, global_, decay_steps, decay_rate, staircase=<span class="keyword">True</span>/<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">decay_rate = <span class="number">0.96</span></span><br><span class="line">global_steps = <span class="number">1000</span></span><br><span class="line">decay_steps = <span class="number">100</span></span><br><span class="line">​</span><br><span class="line">global_ = tf.Variable(tf.constant(<span class="number">0</span>))</span><br><span class="line">c = tf.train.exponential_decay(learning_rate, global_, decay_steps, decay_rate, staircase=<span class="keyword">True</span>)</span><br><span class="line">d = tf.train.exponential_decay(learning_rate, global_, decay_steps, decay_rate, staircase=<span class="keyword">False</span>)</span><br><span class="line">​</span><br><span class="line">T_C = []</span><br><span class="line">F_D = []</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(global_steps):</span><br><span class="line">        T_c = sess.run(c, feed_dict=&#123;global_: i&#125;)</span><br><span class="line">        T_C.append(T_c)</span><br><span class="line">        F_d = sess.run(d, feed_dict=&#123;global_: i&#125;)</span><br><span class="line">        F_D.append(F_d)</span><br><span class="line">​</span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.plot(range(global_steps), F_D, <span class="string">'r-'</span>)</span><br><span class="line">plt.plot(range(global_steps), T_C, <span class="string">'b-'</span>)</span><br><span class="line">​</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/15/深度学习/学习速率/2.png" height="262" width="366"></p>
<p>初始的学习速率是<code>0.1</code>，总的迭代次数是<code>1000</code>次，如果<code>staircase=True</code>，那就表明每<code>decay_steps</code>次计算学习速率变化，更新原始学习速率；如果是<code>False</code>，那就是每一步都更新学习速率。红色表示<code>False</code>，蓝色表示<code>True</code>。<br>&emsp;&emsp;常数分片学习率衰减如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">piecewise_constant(x, boundaries, values, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>例如前<code>10000</code>轮迭代使用<code>1.0</code>作为学习率，<code>10000</code>轮到<code>12000</code>轮使用<code>0.5</code>作为学习率，以后使用<code>0.1</code>作为学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">global_ = tf.Variable(tf.constant(<span class="number">0</span>), trainable=<span class="keyword">False</span>)</span><br><span class="line">boundaries = [<span class="number">10000</span>, <span class="number">12000</span>]</span><br><span class="line">values = [<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">0.1</span>]</span><br><span class="line">learning_rate = tf.train.piecewise_constant(global_, boundaries, values)</span><br><span class="line">global_steps = <span class="number">20000</span></span><br><span class="line">​</span><br><span class="line">T_L = []</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(global_steps):</span><br><span class="line">        T_l = sess.run(learning_rate, feed_dict=&#123;global_: i&#125;)</span><br><span class="line">        T_L.append(T_l)</span><br><span class="line">​</span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.plot(range(global_steps), T_L, <span class="string">'r-'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/15/深度学习/学习速率/3.png" height="264" width="355"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/15/深度学习/TensorFlow模型的保存和加载/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/深度学习/TensorFlow模型的保存和加载/" itemprop="url">TensorFlow模型的保存和加载</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-15T14:31:21+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;将训练好的模型参数保存起来，以便以后进行验证或测试，这是我们经常要做的事情。<code>tf</code>里面提供模型保存的函数是<code>tf.train.Saver</code>模块。模型保存先要创建一个<code>Saver</code>对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure>
<p>在创建<code>Saver</code>对象的时候，有一个参数经常会用到，就是<code>max_to_keep</code>，这个是用来设置保存模型的个数，默认为<code>5</code>，保存最近的<code>5</code>个模型。如果你想每训练一次(<code>epoch</code>)就想保存一次模型，则可以将<code>max_to_keep</code>设置为<code>None</code>或者<code>0</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(max_to_keep=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>但是这样做除了多占用硬盘，并没有实际多大的用处，因此不推荐。当然，如果你只想保存最后一代的模型，则只需要将<code>max_to_keep</code>设置为<code>1</code>即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(max_to_keep=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>创建完saver对象后，就可以保存训练好的模型了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, <span class="string">'ckpt/mnist.ckpt'</span>, global_step=step)</span><br></pre></td></tr></table></figure>
<p>第一个参数<code>sess</code>，这个就不用说了。第二个参数设定保存的路径和名字，第三个参数将训练的次数作为后缀加入到模型名字中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, <span class="string">'my-model'</span>, global_step=<span class="number">0</span>) == &gt; filename: <span class="string">'my-model-0'</span></span><br><span class="line">saver.save(sess, <span class="string">'my-model'</span>, global_step=<span class="number">1000</span>) == &gt; filename: <span class="string">'my-model-1000'</span></span><br></pre></td></tr></table></figure>
<p>该函数会在目录下生成如下几个文件：</p>
<p><img src="/2019/02/15/深度学习/TensorFlow模型的保存和加载/1.png"></p>
<p>&emsp;&emsp;通过<code>tf.train.Saver</code>类实现保存和载入神经网络模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line">​</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    saver.save(sess, <span class="string">"Model/model.ckpt"</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;加载<code>TensorFlow</code>模型的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line">​</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./Model/model.ckpt"</span>)  <span class="comment"># 注意此处路径前添加“./”</span></span><br><span class="line">    print(sess.run(result))  <span class="comment"># 结果为[ 3.]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;若不希望重复定义计算图上的运算，可直接加载已经持久化的图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"Model/model.ckpt.meta"</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./Model/model.ckpt"</span>)  <span class="comment"># 注意路径写法</span></span><br><span class="line">    print(sess.run(tf.get_default_graph().get_tensor_by_name(<span class="string">"add:0"</span>)))  <span class="comment"># 结果为[ 3.]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>tf.train.Saver</code>类也支持在保存和加载时给变量重命名：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 声明的变量名称name与已保存的模型中的变量名称name不一致</span></span><br><span class="line">u1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"other-v1"</span>)</span><br><span class="line">u2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"other-v2"</span>)</span><br><span class="line">result = u1 + u2</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 若直接声明Saver类对象，会报错变量找不到。使用一个字典dict</span></span><br><span class="line"><span class="comment"># 重命名变量即可，语法为&#123;"已保存的变量的名称name": 重命名变量名&#125;，</span></span><br><span class="line"><span class="comment"># 原来名称name为v1的变量现在加载到变量u1(名称name为other-v1)中</span></span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"v1"</span>: u1, <span class="string">"v2"</span>: u2&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./Model/model.ckpt"</span>)</span><br><span class="line">    print(sess.run(result))  <span class="comment"># [ 3.]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;保存滑动平均模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">v = tf.Variable(<span class="number">0</span>, dtype=tf.float32, name=<span class="string">"v"</span>)</span><br><span class="line"><span class="keyword">for</span> variables <span class="keyword">in</span> tf.global_variables():</span><br><span class="line">    print(variables.name)  <span class="comment"># 结果为“v:0”</span></span><br><span class="line">​</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>)</span><br><span class="line">maintain_averages_op = ema.apply(tf.global_variables())</span><br><span class="line"><span class="keyword">for</span> variables <span class="keyword">in</span> tf.global_variables():</span><br><span class="line">    print(variables.name)  <span class="comment"># 结果为“v:0(换行)v/ExponentialMovingAverage:0”</span></span><br><span class="line">​</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    sess.run(tf.assign(v, <span class="number">10</span>))</span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    saver.save(sess, <span class="string">"Model/model_ema.ckpt"</span>)</span><br><span class="line">    print(sess.run([v, ema.average(v)]))  <span class="comment"># 结果为[10.0, 0.099999905]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;通过变量重命名直接读取变量的滑动平均值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">v = tf.Variable(<span class="number">0</span>, dtype=tf.float32, name=<span class="string">"v"</span>)</span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"v/ExponentialMovingAverage"</span>: v&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./Model/model_ema.ckpt"</span>)</span><br><span class="line">    print(sess.run(v))  <span class="comment"># 结果为0.0999999</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;通过<code>tf.train.ExponentialMovingAverage</code>的<code>variables_to_restore</code>函数获取变量重命名字典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 注意此处的变量名称name一定要与已保存的变量名称一致</span></span><br><span class="line">v = tf.Variable(<span class="number">0</span>, dtype=tf.float32, name=<span class="string">"v"</span>)</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>)</span><br><span class="line"><span class="comment"># 如下结果为&#123;'v/ExponentialMovingAverage': &lt;tf.Variable 'v:0'</span></span><br><span class="line"><span class="comment"># shape=() dtype=float32_ref&gt;&#125;，此处的v取自上面变量v的名称“name="v"”</span></span><br><span class="line">print(ema.variables_to_restore())  </span><br><span class="line">saver = tf.train.Saver(ema.variables_to_restore())</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./Model/model_ema.ckpt"</span>)</span><br><span class="line">    print(sess.run(v))  <span class="comment"># 0.0999999</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;如果变量定义在<code>name_scope</code>的里面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'input'</span>):</span><br><span class="line">    v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">    v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">​</span><br><span class="line">result = v1 + v2</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    saver.save(sess, <span class="string">"./model.ckpt"</span>)</span><br></pre></td></tr></table></figure>
<p>提取变量时使用如下语句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"Model/model.ckpt.meta"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./Model/model.ckpt"</span>)  <span class="comment"># 注意路径写法</span></span><br><span class="line">    print(sess.run(tf.get_default_graph().get_tensor_by_name(<span class="string">"input/v2:0"</span>)))  <span class="comment"># 结果为[2.]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们可以使用<code>tf.train.latest_checkpoint</code>来自动获取最后一次保存的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latest_checkpoint(checkpoint_dir, latest_filename=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">​</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">False</span>)</span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.int32, [<span class="keyword">None</span>, ])</span><br><span class="line">​</span><br><span class="line">dense1 = tf.layers.dense(</span><br><span class="line">    inputs=x, units=<span class="number">1024</span>, activation=tf.nn.relu,</span><br><span class="line">    kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    kernel_regularizer=tf.nn.l2_loss)</span><br><span class="line">dense2 = tf.layers.dense(</span><br><span class="line">    inputs=dense1, units=<span class="number">512</span>, activation=tf.nn.relu,</span><br><span class="line">    kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    kernel_regularizer=tf.nn.l2_loss)</span><br><span class="line">logits = tf.layers.dense(</span><br><span class="line">    inputs=dense2, units=<span class="number">10</span>, activation=<span class="keyword">None</span>,</span><br><span class="line">    kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    kernel_regularizer=tf.nn.l2_loss)</span><br><span class="line">​</span><br><span class="line">loss = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=logits)</span><br><span class="line">train_op = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>).minimize(loss)</span><br><span class="line">correct_prediction = tf.equal(tf.cast(tf.argmax(logits, <span class="number">1</span>), tf.int32), y_)</span><br><span class="line">acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">​</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">​</span><br><span class="line">is_train = <span class="keyword">False</span></span><br><span class="line">saver = tf.train.Saver(max_to_keep=<span class="number">3</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> is_train:  <span class="comment"># 训练阶段</span></span><br><span class="line">    max_acc = <span class="number">0</span></span><br><span class="line">    f = open(<span class="string">'ckpt/acc.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line">        val_loss, val_acc = sess.run([loss, acc], feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br><span class="line">        print(<span class="string">'epoch:%d, val_loss:%f, val_acc:%f'</span> % (i, val_loss, val_acc))</span><br><span class="line">        f.write(str(i + <span class="number">1</span>) + <span class="string">', val_acc: '</span> + str(val_acc) + <span class="string">'\n'</span>)</span><br><span class="line">        <span class="keyword">if</span> val_acc &gt; max_acc:</span><br><span class="line">            max_acc = val_acc</span><br><span class="line">            saver.save(sess, <span class="string">'ckpt/mnist.ckpt'</span>, global_step=i + <span class="number">1</span>)</span><br><span class="line">    f.close()</span><br><span class="line"><span class="keyword">else</span>:  <span class="comment"># 验证阶段</span></span><br><span class="line">    model_file = tf.train.latest_checkpoint(<span class="string">'ckpt/'</span>)</span><br><span class="line">    saver.restore(sess, model_file)</span><br><span class="line">    val_loss, val_acc = sess.run([loss, acc], feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br><span class="line">    print(<span class="string">'val_loss:%f, val_acc:%f'</span> % (val_loss, val_acc))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>这里的<code>latest_checkpoint</code>函数的参数表示模型存储的位置，不需要提供模型的名字，它会去查看<code>checkpoint</code>文件，看看最新保存结果的命名。</p>
<hr>
<h3 id="将TensorFlow的网络导出为单个文件"><a href="#将TensorFlow的网络导出为单个文件" class="headerlink" title="将TensorFlow的网络导出为单个文件"></a>将TensorFlow的网络导出为单个文件</h3><p>&emsp;&emsp;有时我们需要将<code>TensorFlow</code>的模型导出为单个文件(同时包含模型架构定义与权重)，方便在其他地方使用(例如在<code>C++</code>中部署网络)。<code>Tensorflow</code>提供如下<code>API</code>来存储和加载模型：</p>
<ul>
<li>生成检查点文件(<code>checkpoint file</code>)，扩展名一般为<code>.ckpt</code>，通过在<code>tf.train.Saver</code>对象上调用<code>Saver.save</code>生成。它包含权重和其他在程序中定义的变量，不包含图结构。如果需要在另一个程序中使用，需要重新创建图形结构，并告诉<code>TensorFlow</code>如何处理这些权重。</li>
<li>生成图协议文件(<code>graph proto file</code>)，这是一个二进制文件，扩展名一般为<code>.pb</code>，用<code>tf.train.write_graph</code>保存。它只包含图形结构，不包含权重，然后使用<code>tf.import_graph_def</code>来加载图形。</li>
</ul>
<p>&emsp;&emsp;通过<code>convert_variables_to_constants</code>函数将计算图中的变量及其取值通过常量的方式保存于一个文件中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework.graph_util <span class="keyword">import</span> convert_variables_to_constants</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 构造网络</span></span><br><span class="line">a = tf.Variable([[<span class="number">3</span>], [<span class="number">4</span>]], dtype=tf.float32, name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.Variable(<span class="number">4</span>, dtype=tf.float32, name=<span class="string">'b'</span>)</span><br><span class="line">output = tf.add(a, b, name=<span class="string">'out'</span>)  <span class="comment"># 一定要给输出tensor取一个名字</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  <span class="comment"># 转换Variable为constant，并将网络写入到文件</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 这里需要填入输出tensor的名字</span></span><br><span class="line">    graph = convert_variables_to_constants(sess, sess.graph_def, [<span class="string">"out"</span>])</span><br><span class="line">    tf.train.write_graph(graph, <span class="string">'.'</span>, <span class="string">'graph.pb'</span>, as_text=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Converted <span class="number">2</span> variables to const ops.</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;载入包含变量及其取值的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./graph.pb'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        output = tf.import_graph_def(graph_def, return_elements=[<span class="string">'out:0'</span>])</span><br><span class="line">        print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[array([[<span class="number">7.</span>],</span><br><span class="line">       [<span class="number">8.</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;如果我们的网络需要有一个输入自定义数据的接口，可以采用如下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework.graph_util <span class="keyword">import</span> convert_variables_to_constants</span><br><span class="line">​</span><br><span class="line">a = tf.Variable([[<span class="number">3</span>], [<span class="number">4</span>]], dtype=tf.float32, name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.Variable(<span class="number">4</span>, dtype=tf.float32, name=<span class="string">'b'</span>)</span><br><span class="line">input_tensor = tf.placeholder(tf.float32, name=<span class="string">'input'</span>)</span><br><span class="line">output = tf.add((a + b), input_tensor, name=<span class="string">'out'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    graph = convert_variables_to_constants(sess, sess.graph_def, [<span class="string">"out"</span>])</span><br><span class="line">    tf.train.write_graph(graph, <span class="string">'.'</span>, <span class="string">'graph.pb'</span>, as_text=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>上述代码重新保存网络至<code>graph.pb</code>，这次我们有了一个输入<code>placeholder</code>，下面来看看怎么恢复网络并输入自定义数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./graph.pb'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        output = tf.import_graph_def(</span><br><span class="line">                    graph_def, input_map=&#123;<span class="string">'input:0'</span>: <span class="number">4.</span>&#125;,</span><br><span class="line">                    return_elements=[<span class="string">'out:0'</span>], name=<span class="string">'a'</span>)</span><br><span class="line">        print(sess.run(output))</span><br><span class="line">`</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[array([[<span class="number">11.</span>],</span><br><span class="line">       [<span class="number">12.</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;当然也可以在<code>input_map</code>那里可以替换为自定义的<code>placeholder</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">new_input = tf.placeholder(tf.float32, shape=())</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./graph.pb'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        output = tf.import_graph_def(</span><br><span class="line">                    graph_def, input_map=&#123;<span class="string">'input:0'</span>: new_input&#125;,</span><br><span class="line">                    return_elements=[<span class="string">'out:0'</span>], name=<span class="string">'a'</span>)</span><br><span class="line">        print(sess.run(output, feed_dict=&#123;new_input: <span class="number">4</span>&#125;))</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Tensorflow模型持久化"><a href="#Tensorflow模型持久化" class="headerlink" title="Tensorflow模型持久化"></a>Tensorflow模型持久化</h3><p>&emsp;&emsp;当我们使用<code>TensorFlow</code>训练神经网络的时候，模型持久化对于我们的训练有很重要的作用。<br>&emsp;&emsp;如果我们的神经网络比较复杂，训练数据比较多，那么我们的模型训练就会耗时很长，如果在训练过程中出现某些不可预计的错误，导致我们的训练意外终止，那么我们将会前功尽弃。为了避免这个问题，我们就可以通过模型持久化(保存为<code>CKPT</code>格式)来暂存我们训练过程中的临时数据。<br>&emsp;&emsp;如果我们训练的模型需要提供给用户做离线的预测，那么我们只需要前向传播的过程，只需得到预测值就可以了，这个时候我们就可以通过模型持久化(保存为<code>PB</code>格式)只保存前向传播中需要的变量并将变量的值固定下来，这个时候只需用户提供一个输入，我们就可以通过模型得到一个输出给用户。</p>
<h4 id="保存为CKPT格式的模型"><a href="#保存为CKPT格式的模型" class="headerlink" title="保存为CKPT格式的模型"></a>保存为CKPT格式的模型</h4><p>&emsp;&emsp;定义运算过程，声明并得到一个<code>Saver</code>，通过<code>Saver.save</code>保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line">​</span><br><span class="line">MODEL_DIR = <span class="string">"model/ckpt"</span></span><br><span class="line">MODEL_NAME = <span class="string">"model.ckpt"</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(MODEL_DIR):  <span class="comment"># 创建目录</span></span><br><span class="line">    tf.gfile.MakeDirs(MODEL_DIR)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 下面的过程你可以替换成CNN、RNN等你想做的训练过程，这里只是简单的一个计算公式</span></span><br><span class="line"><span class="comment"># 输入占位符，并指定名字，后续模型读取可能会用的</span></span><br><span class="line">input_holder = tf.placeholder(tf.float32, shape=[<span class="number">1</span>], name=<span class="string">"input_holder"</span>)</span><br><span class="line">W1 = tf.Variable(tf.constant(<span class="number">5.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"W1"</span>)</span><br><span class="line">B1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"B1"</span>)</span><br><span class="line">_y = (input_holder * W1) + B1</span><br><span class="line"><span class="comment"># 输出节点名字，后续模型读取会用到，比50大返回true，否则返回false</span></span><br><span class="line">predictions = tf.greater(_y, <span class="number">50</span>, name=<span class="string">"predictions"</span>)</span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">saver = tf.train.Saver()  <span class="comment"># 声明saver用于保存模型</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 输入一个数据测试一下</span></span><br><span class="line">    print(<span class="string">"predictions : "</span>, sess.run(predictions, feed_dict=&#123;input_holder: [<span class="number">10.0</span>]&#125;))</span><br><span class="line">    saver.save(sess, os.path.join(MODEL_DIR, MODEL_NAME))  <span class="comment"># 模型保存</span></span><br><span class="line">    <span class="comment"># 得到当前图有几个操作节点</span></span><br><span class="line">    print(<span class="string">"%d ops in the final graph."</span> % len(tf.get_default_graph().as_graph_def().node))</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> op <span class="keyword">in</span> tf.get_default_graph().get_operations():  <span class="comment"># 打印模型节点信息</span></span><br><span class="line">    print(op.name, op.values())</span><br></pre></td></tr></table></figure>
<p>运行后生成的文件如下：</p>
<ul>
<li><code>checkpoint</code>：记录目录下所有模型文件列表。</li>
<li><code>ckpt.data</code>：保存模型中每个变量的取值。</li>
<li><code>ckpt.meta</code>：保存整个计算图的结构。</li>
</ul>
<h4 id="保存为PB格式模型"><a href="#保存为PB格式模型" class="headerlink" title="保存为PB格式模型"></a>保存为PB格式模型</h4><p>&emsp;&emsp;定义运算过程，通过<code>get_default_graph().as_graph_def</code>得到当前图的计算节点信息，通过<code>graph_util.convert_variables_to_constants</code>将相关节点的<code>values</code>固定，通过<code>tf.gfile.GFile</code>进行模型持久化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line">​</span><br><span class="line">output_graph = <span class="string">"model/add_model.pb"</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 下面的过程你可以替换成CNN、RNN等你想做的训练过程，这里只是简单的一个计算公式</span></span><br><span class="line">input_holder = tf.placeholder(tf.float32, shape=[<span class="number">1</span>], name=<span class="string">"input_holder"</span>)</span><br><span class="line">W1 = tf.Variable(tf.constant(<span class="number">5.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"W1"</span>)</span><br><span class="line">B1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"B1"</span>)</span><br><span class="line">_y = (input_holder * W1) + B1</span><br><span class="line">predictions = tf.add(_y, <span class="number">10</span>, name=<span class="string">"predictions"</span>)  <span class="comment"># 做一个加法运算(输出节点名是“predictions”)</span></span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(<span class="string">"predictions : "</span>, sess.run(predictions, feed_dict=&#123;input_holder: [<span class="number">10.0</span>]&#125;))</span><br><span class="line">    <span class="comment"># 得到当前的图的GraphDef部分，通过这个部分就可以完成重输入层到输出层的计算过程</span></span><br><span class="line">    graph_def = tf.get_default_graph().as_graph_def()</span><br><span class="line">    output_graph_def = graph_util.convert_variables_to_constants(sess, graph_def, [<span class="string">"predictions"</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(output_graph, <span class="string">"wb"</span>) <span class="keyword">as</span> f:  <span class="comment"># 保存模型</span></span><br><span class="line">        f.write(output_graph_def.SerializeToString())  <span class="comment"># 序列化输出</span></span><br><span class="line">    print(<span class="string">"%d ops in the final graph."</span> % len(output_graph_def.node))</span><br><span class="line">    print(predictions)</span><br></pre></td></tr></table></figure>
<p><code>GraphDef</code>这个属性记录了<code>TensorFlow</code>计算图上节点的信息。<code>add_model.pb</code>里面保存了从输入层到输出层这个计算过程的计算图和相关变量的值，我们得到这个模型后传入一个输入，即可以得到一个预估的输出值。</p>
<h4 id="CKPT转换成PB格式"><a href="#CKPT转换成PB格式" class="headerlink" title="CKPT转换成PB格式"></a>CKPT转换成PB格式</h4><p>&emsp;&emsp;通过传入<code>CKPT</code>模型的路径得到模型的图和变量数据，通过<code>import_meta_graph</code>导入模型中的图，通过<code>saver.restore</code>从模型中恢复图中各个变量的数据，通过<code>graph_util.convert_variables_to_constants</code>将模型持久化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line">​</span><br><span class="line">MODEL_DIR = <span class="string">"model/"</span></span><br><span class="line">MODEL_NAME = <span class="string">"frozen_model.pb"</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(MODEL_DIR):  <span class="comment"># 创建目录</span></span><br><span class="line">    tf.gfile.MakeDirs(MODEL_DIR)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">freeze_graph</span><span class="params">(model_folder)</span>:</span></span><br><span class="line">    checkpoint = tf.train.get_checkpoint_state(model_folder)  <span class="comment"># 检查目录下ckpt文件状态是否可用</span></span><br><span class="line">    input_checkpoint = checkpoint.model_checkpoint_path  <span class="comment"># 得到ckpt文件路径</span></span><br><span class="line">    output_graph = os.path.join(MODEL_DIR, MODEL_NAME)  <span class="comment"># PB模型保存路径</span></span><br><span class="line">​</span><br><span class="line">    output_node_names = <span class="string">"predictions"</span>  <span class="comment"># 原模型输出操作节点的名字</span></span><br><span class="line">    <span class="comment"># 得到图、clear_devices：Whether or not to clear the device field for an 'Operation' or 'Tensor' during import</span></span><br><span class="line">    saver = tf.train.import_meta_graph(input_checkpoint + <span class="string">'.meta'</span>, clear_devices=<span class="keyword">True</span>)</span><br><span class="line">​</span><br><span class="line">    graph = tf.get_default_graph()  <span class="comment"># 获得默认的图</span></span><br><span class="line">    input_graph_def = graph.as_graph_def()  <span class="comment"># 返回一个序列化的图代表当前的图</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        saver.restore(sess, input_checkpoint)  <span class="comment"># 恢复图并得到数据</span></span><br><span class="line">        <span class="comment"># 测试读出来的模型是否正确，注意这里传入的是输出和输入节点的tensor的名字，不是操作节点的名字</span></span><br><span class="line">        print(<span class="string">"predictions : "</span>, sess.run(<span class="string">"predictions:0"</span>, feed_dict=&#123;<span class="string">"input_holder:0"</span>: [<span class="number">10.0</span>]&#125;))</span><br><span class="line">        <span class="comment"># 注意，如果有多个输出节点，以逗号隔开</span></span><br><span class="line">        output_graph_def = graph_util.convert_variables_to_constants(sess, input_graph_def, output_node_names.split(<span class="string">","</span>))</span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(output_graph, <span class="string">"wb"</span>) <span class="keyword">as</span> f:  <span class="comment"># 保存模型</span></span><br><span class="line">            f.write(output_graph_def.SerializeToString())  <span class="comment"># 序列化输出</span></span><br><span class="line">        print(<span class="string">"%d ops in the final graph."</span> % len(output_graph_def.node))  <span class="comment"># 得到当前图有几个操作节点</span></span><br><span class="line">​</span><br><span class="line">        <span class="keyword">for</span> op <span class="keyword">in</span> graph.get_operations():</span><br><span class="line">            print(op.name, op.values())</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">"model_folder"</span>, type=str, help=<span class="string">"input ckpt model dir"</span>)  <span class="comment"># 命令行解析，help是提示符，type是输入的类型</span></span><br><span class="line">    <span class="comment"># 这里运行程序时需要带上模型ckpt的路径，不然会报“error: too few arguments”</span></span><br><span class="line">    aggs = parser.parse_args()</span><br><span class="line">    freeze_graph(aggs.model_folder)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/15/外语/日语学习笔记/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/外语/日语学习笔记/" itemprop="url">日语学习笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-15T13:06:16+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/外语/" itemprop="url" rel="index">
                    <span itemprop="name">外语</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;1. <code>わたしも新しい洋服が欲しくなりました</code>：<code>ほしい</code>是形容词不是动词，所以用了<code>ほしくなりました</code>。<code>ほしい</code>的副词形式<code>ほしく</code>加上<code>なりました</code>，表达就完成了。<br>&emsp;&emsp;2. <code>何か趣味がありますか</code>：第一个<code>か</code>不是强调兴趣是什么，而是强调有还是没有；第二个<code>が</code>是提示主语。比较一下<code>か</code>和<code>が</code>的语境：<code>何か趣味がありますか</code>(你有什么兴趣吗？)强调有还是没有；<code>何が趣味がありますか</code>(你有什么兴趣吗？)强调是什么兴趣。<br>&emsp;&emsp;3. <code>日本の歌が好きなんですか</code>：<code>~んです</code>除了对所陈述的事确认外，还有强调、加强语气的作用。<code>好き</code>是二类形容词，名词和二类形容词后用<code>なんです</code>，动词简体和一类形容词后用<code>んです</code>。现在将来时就是现在时或将来时，陈述现在发生的事情或将来发生的事情时用的时态，<code>だ</code>要换成<code>な</code>。<br>&emsp;&emsp;4. <code>います</code>的各种形式：基本形<code>いる</code>、过去<code>いた</code>、否定<code>いない</code>、过去否定<code>いなっかた</code>。<br>&emsp;&emsp;5. <code>そうなんです</code>：<code>そうです -&gt; そうだ -&gt; そうな + んです/のです -&gt; そうなんです/そうなのですが</code><br>&emsp;&emsp;6. <code>李さんや马さんもいろいろと探してくれています</code>：此句的<code>いろいろ</code>是作为副词使用，后面加<code>と</code>表明修饰后面的动词<code>探してくれる</code>。<br>&emsp;&emsp;7. <code>そうなんだ</code>：相当于<code>そうなんです</code>，<code>~んだ</code>是一个固定的接尾方式，和<code>~んです</code>一样，表示强调肯定语气。<br>&emsp;&emsp;8. <code>お母さん先に入ってよ</code>：<code>入ってよ</code>是<code>入ってください</code>的缩略形式。<br>&emsp;&emsp;9. <code>寝ないとね</code>：相当于<code>寝ないといけない</code>(必须睡觉)。<br>&emsp;&emsp;10. <code>~など</code>表示相同事物的列举，相当于汉语的<code>等等</code>。<br>&emsp;&emsp;11. <code>何か予定がありますか</code>：在语法、语义上都可以将此处的<code>か</code>换成<code>の</code>，但是两者的语境完全不同。<code>か</code>是泛指，即不定指；<code>の</code>表示特指。<br>&emsp;&emsp;12. <code>出かけようとした时に</code>：<code>と</code>表示引用或者承接，而且还有<code>一~就~</code>的意思，这里表示引用用法。<code>动词意志形 + とする</code>的方式表示<code>想要做~</code>。<code>した</code>则是表示这的想法是发生在过去，也就是过去式的用法，而并不是做过了，而且在这里也只是说作者在叙述自己那个时候的想法。<br>&emsp;&emsp;13. <code>帰りに寄ろうと思うんです</code>：<code>帰り</code>是广义时间性名词，<code>に</code>表示时间，<code>帰りに</code>即在回来时。整句意思是<code>在回来时想顺路去</code>。<br>&emsp;&emsp;14. <code>必ずだれかがやっていますね</code>：<code>やって</code>的<code>ます</code>形是<code>やります</code>(做)。<br>&emsp;&emsp;15. <code>だから会員になろうと思っているんです</code>：<code>なろう</code>的<code>ます</code>形是<code>なります</code>(变成)。<br>&emsp;&emsp;16. <code>かほかにも特典があるんですか</code>：<code>ほかに</code>是副词，<code>另外</code>的意思。<br>&emsp;&emsp;17. <code>わたしにも手伝ってほしいって言っていました</code>：<code>手伝う(てつたう)</code>变<code>て</code>形为<code>手伝って</code>，而<code>欲しい</code>这个文法项在接续动词时，就是<code>NにV-て欲しい</code>的形式。<code>って</code>是用作表示引用的，表示引用前面那句话。<br>&emsp;&emsp;18. <code>ちょっと早く着いてしまったけど</code>：<code>てしまった</code>是<code>てしまいました</code>的简体形。<br>&emsp;&emsp;19. <code>いくら闻いても何も言わないんだよ</code>：怎么问都什么不说。这句话语法中的<code>ても</code>是典型的问句形式；<code>いくら</code>是<code>如何、多少...</code>的意思，还有就是在询问价格时经常使用，相当于<code>多少钱</code>。<code>んだ</code>多用于口语，用来加强语气。<br>&emsp;&emsp;20. <code>一杯だけでいいです</code>：只喝一杯可以。<code>で</code>是助词，限定范围。这里表述的意思<code>在一杯(少量)范围内是可以的</code>。<br>&emsp;&emsp;21. <code>遅くなって</code>：<code>遅い</code>修饰动词<code>なる</code>(变化)。<code>遅くなる</code>根据场合不同，可以翻译为<code>迟到</code>、<code>变慢</code>、<code>变晚</code>的意思。<br>&emsp;&emsp;22. <code>名词 + のほかに</code>：<code>另外还...，除此之外也...</code>。<br>&emsp;&emsp;23. <code>それにしても</code>：表示在承认前边所述事实的基础上，感觉实际情况超过原有的预想、期待，表示一种转折的语气。译为<code>尽管那样，也...</code>。<br>&emsp;&emsp;24. <code>ここは紫禁城と言って、映画の舞台になった所です</code>：<code>て</code>连接前后句，表示句子的中顿，<code>言って</code>是<code>言う</code>的连用形。<code>なる</code>是成为的意思，变成<code>た</code>形，后面可直接接名词。这句话的意思是：这里叫做紫禁城，经常作为电影中的舞台出现。<br>&emsp;&emsp;25. <code>AをBに(して) + 动词句</code>：就是表示<code>以A作为B，~</code>，可以省略<code>して</code>。中级里有很多这种句型，该句里是以建筑物作为背景，做照相这个动作。其它比如<code>~を頼りに(して)</code>是<code>以XX为依托，~</code>，<code>~をきっかけに(して)</code>是<code>以XX为契机，~</code>，<code>~を境に(して)</code>是<code>以XX为分水岭，~</code>。同时也可<code>AをBにした + 名词</code>或<code>AをBにする + 名词</code>，<code>にする</code>、<code>にした</code>不可省。比如<code>天安門をバックにした写真</code>(以天安门为背景的照片)。<br>&emsp;&emsp;26. <code>梦が実现できる</code>：这句话意思是<code>梦想能实现</code>。<code>できる</code>是<code>する</code>的可能态，表示能够做某事。<br>&emsp;&emsp;27. <code>そんなはずがない</code>：按理说不会这样。<br>&emsp;&emsp;28. <code>西长安街の方へ行く用事があったので</code>：<code>西长安街の方へ行く</code>是定语修饰用事，句子意思是有到西长安街的事情，即因为有事到西长安街的意思。<br>&emsp;&emsp;29. <code>同僚が预かったままだったんです</code>：也可以是<code>~ままでした</code>，如果不需要<code>です</code>，<code>~ままだった</code>也没问题，这里是<code>ままだった + んです</code>。<br>&emsp;&emsp;30. <code>连络もせずに</code>：<code>せずに</code>应该理解为<code>しないのに</code>的书面用法。<br>&emsp;&emsp;31. <code>访ねてしまって</code>：这里用的是<code>てしまう</code>这个语法。它有两种意思：一是表示动作的终了、完成，例如<code>いえを出る前に、ご饭をたべてしまう</code>(出门前先把饭吃了)；做了不该做的事情或出现了不该出现的情况(或者是某种未料到的不好的情况)，例如<code>ちょっと小さすぎてしまった</code>(有点太小了)。<code>て</code>这里是终助词，多作为女性用语，表示一种委婉的强调，相当于<code>呢、啊</code>。<code>访ねてしまう</code>接上<code>て</code>就是<code>访ねてしまって</code>。<br>&emsp;&emsp;32. <code>言うまでもなく品质は重要です</code>：<code>~までもない/~までもなく</code>相当于<code>~する必要はない</code>，意思是<code>没有必要</code>。接续形式为<code>动-辞书形 + までもない</code>，注意<code>~までもない</code>用于句尾，或接名词；<code>~までもなく</code>后接动词、形容词或副词。例如<code>そんな简単な用事のために、わざわざ行くまでもない</code>(就那么简单的事情，没必要特意前去)；<code>今さら注意するまでもなく、吃烟は健康に大きな害を及ぼします</code>(吸烟危害健康，已经是无需现在再加以提醒的)。<code>言うまでもない/言うまでもなく</code>意思是<code>不言而喻</code>。<br>&emsp;&emsp;33. <code>デザイナーにいくつか试作させています</code>：<code>いくつか</code>有<code>若干</code>的意思，例如<code>リンゴをいくつか买った</code>(买了几个苹果)。<br>&emsp;&emsp;34. <code>元気でいて</code>：<code>いて</code>是存续体在口语中的省略用法。日语中的存续体为<code>ている</code>，即<code>て</code>型，这句话完整应该是<code>元気でいている</code>，意为希望对方这种元气的状态是一种长时间状态的持续，在口语中省略为<code>元気でいて</code>。<br>&emsp;&emsp;35. <code>笑いが止まらなくなるかもしれませんよ</code>：<code>ない</code>和形容词一样，<code>い</code>要变化为<code>く</code>，例如<code>大きい -&gt; 大きくなる</code>。形容动词则是词根加<code>に</code>，例如<code>赈やかになる</code>。除了<code>なる</code>，<code>する</code>也一样。<code>なる</code>的意思是<code>变得</code>。<br>&emsp;&emsp;36. <code>いいんじゃない</code>和<code>いいじゃない</code>：<code>いいんじゃない</code>的意思是<code>不是很好吗？</code>，语气比<code>いいじゃない</code>要强烈。<code>いいじゃないか</code>通常的含义就是反问表肯定<code>不是挺好吗</code>。<code>いいじゃないか</code>并不存在明显的升调或降调，<code>か</code>这个疑问助词本来就读升调，只有<code>いいじゃない</code>存在升调或降调，<code>いいじゃない(升调) = いいじゃないか</code>，是一种省略说法，表示<code>不是挺好吗</code>；<code>いいじゃない(降调) = ではない</code>正常表示否定的用法，即<code>不好</code>。另外还有一种情况，若要表达<code>是不好吗？</code>的意思，日语为<code>いいじゃないですか</code>或<code>いいじゃないか</code>，但这容易引起误解，所以在书写时要作如下处理：<code>いいじゃない、ですか</code>或<code>いいじゃない、か</code>，在读的时候<code>いいじゃない</code>和<code>か(ですか)</code>之间要有停顿，这样才能加以区分。<br>&emsp;&emsp;37. <code>社長はこちらにもお寄りになるのかしら</code>：这句话是敬语，意思是<code>社长会不会也来这边？</code>。<code>お寄りになる</code>是<code>来る</code>的尊敬语，用于地位在上的人。<code>の</code>表示强调。<code>かしら</code>是句末语气词，女性专用，无特殊意义。<br>&emsp;&emsp;38. <code>そちらへは寄らずに</code>：<code>寄る</code>的否定说法，<code>ず</code>表示否定，<code>に</code>与后面句子连用，其意思是<code>去那里不顺路...</code>。<br>&emsp;&emsp;39. <code>勉強すればするほど</code>：<code>动词假定型 + 动词原型 + ほど</code>表示<code>越...越...</code>。<code>勉強すればするほど、日本語が難しいと思う</code>的意思是<code>越学越觉得日语难</code>。<code>勉強すれば</code>是假定型，里面的<code>する</code>其实就是<code>勉強する</code>，是动词的原型，因为是<code>サ变动词</code>，这里可以省略词干，只用<code>する</code>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/软件与硬件问题/Linux应用问题/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/软件与硬件问题/Linux应用问题/" itemprop="url">Linux应用问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T19:36:40+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="command-not-found"><a href="#command-not-found" class="headerlink" title="command not found"></a>command not found</h3><h4 id="insmod"><a href="#insmod" class="headerlink" title="insmod"></a>insmod</h4><p>&emsp;&emsp;在<code>Linux</code>环境下使用<code>insmod</code>时，一般会提示<code>command not found</code>。这是因为<code>insmod</code>命令是在<code>/sbin</code>目录下，而且也只有<code>root</code>将<code>/sbin</code>加到路径中，可以尝试用<code>/sbin/insmod</code>替代<code>insmod</code>。</p>
<h4 id="yacc和lex"><a href="#yacc和lex" class="headerlink" title="yacc和lex"></a>yacc和lex</h4><p>&emsp;&emsp;对于<code>yacc</code>，使用命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install byacc</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;对于<code>lex</code>，使用命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install flex</span><br></pre></td></tr></table></figure>
<h4 id="ifconfig"><a href="#ifconfig" class="headerlink" title="ifconfig"></a>ifconfig</h4><p>&emsp;&emsp;执行<code>export PATH=&quot;$PATH:/sbin</code>就可以了。</p>
<h4 id="useradd"><a href="#useradd" class="headerlink" title="useradd"></a>useradd</h4><p>&emsp;&emsp;利用<code>su -</code>而不是<code>su</code>进入<code>root</code>，然后再输入<code>useradd username</code>。<code>su</code>只是取得<code>root</code>的权限，<code>su -</code>是取得<code>root</code>的权限后还执行<code>root</code>的<code>profile</code>来取得<code>root</code>环境变量。</p>
<hr>
<h3 id="stdio-h-No-such-file-or-directory"><a href="#stdio-h-No-such-file-or-directory" class="headerlink" title="stdio.h: No such file or directory"></a>stdio.h: No such file or directory</h3><p>&emsp;&emsp;解决方法如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libc6-dev</span><br></pre></td></tr></table></figure>
<h3 id="stdint-h-No-such-file-or-directory"><a href="#stdint-h-No-such-file-or-directory" class="headerlink" title="stdint.h: No such file or directory"></a>stdint.h: No such file or directory</h3><p>&emsp;&emsp;解决方法如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libnewlib-arm-none-eabi</span><br></pre></td></tr></table></figure>
<h3 id="找不到curl-curl-h"><a href="#找不到curl-curl-h" class="headerlink" title="找不到curl/curl.h"></a>找不到curl/curl.h</h3><p>&emsp;&emsp;使用如下命令解决：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libcurl4-openssl-dev</span><br></pre></td></tr></table></figure>
<h3 id="tar-not-found-in-archive"><a href="#tar-not-found-in-archive" class="headerlink" title="tar not found in archive"></a>tar not found in archive</h3><p>&emsp;&emsp;下载了一个<code>eclipse</code>，想把它解压到<code>/usr</code>目录，却出现如下问题：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf eclipse-jee-indigo-SR2-linux-gtk-x86_64.tar.gz /usr</span><br><span class="line">tar: /usr: Not found <span class="keyword">in</span> archive</span><br><span class="line">tar: Exiting with failure status due to previous errors</span><br></pre></td></tr></table></figure>
<p>原因是因为压缩文件时使用的是相对路径，在当前目录下找不到<code>/usr</code>目录，使用<code>-C</code>指定解压目录即可解决：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf eclipse-jee-indigo-SR2-linux-gtk-x86_64.tar.gz -C /usr</span><br></pre></td></tr></table></figure>
<h3 id="Agent-admitted-failure-to-sign-using-the-key"><a href="#Agent-admitted-failure-to-sign-using-the-key" class="headerlink" title="Agent admitted failure to sign using the key"></a>Agent admitted failure to sign using the key</h3><p>&emsp;&emsp;<code>SSH</code>生成<code>id_rsa</code>、<code>id_rsa.pub</code>后，连接服务器却报<code>Agent admitted failure to sign using the key</code>。解决方法：在当前用户下执行命令<code>ssh-add</code>。</p>
<h3 id="执行程序出现Text-file-busy"><a href="#执行程序出现Text-file-busy" class="headerlink" title="执行程序出现Text file busy"></a>执行程序出现Text file busy</h3><p>&emsp;&emsp;使用<code>fuser</code>命令查看程序文件被哪个进程占用，然后用<code>kill</code>命令杀死该进程，即解决问题。<code>fuser</code>命令是用来显示所有正在使用着指定的<code>file</code>、<code>file system</code>或者<code>socket</code>的进程信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ fuser &lt;程序文件名&gt;</span><br><span class="line">&lt;程序文件名&gt;:         50340</span><br><span class="line">$ <span class="built_in">kill</span> -TERM 50340</span><br></pre></td></tr></table></figure>
<p>注意，在普通用户下使用<code>fuser</code>是没有结果的，要切换至<code>root</code>用户。</p>
<h3 id="Windows压缩包在Linux下解压的乱码"><a href="#Windows压缩包在Linux下解压的乱码" class="headerlink" title="Windows压缩包在Linux下解压的乱码"></a>Windows压缩包在Linux下解压的乱码</h3><p>&emsp;&emsp;有一个<code>Windows</code>下的代码压缩包，内文件夹或文件名含中文，放到<code>Linux</code>系统下解压，中文出现乱码。解决方法如下：<br>&emsp;&emsp;1. 安装<code>7zip</code>和<code>convmv</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install p7zip-full convmv</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;2. 假设压缩包名为<code>abc.zip</code>，进入<code>abc.zip</code>所在路径，执行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LANG=C 7z x abc.zip</span><br><span class="line">convmv -f cp936 -t utf8 -r --notest *</span><br></pre></td></tr></table></figure>
<h3 id="无法解析或打开软件包的列表或是状态文件"><a href="#无法解析或打开软件包的列表或是状态文件" class="headerlink" title="无法解析或打开软件包的列表或是状态文件"></a>无法解析或打开软件包的列表或是状态文件</h3><p>&emsp;&emsp;原因是软件包出错，解决方法如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /var/lib/apt/lists/* -vf</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<h3 id="permission-denied"><a href="#permission-denied" class="headerlink" title="permission denied"></a>permission denied</h3><p>&emsp;&emsp;执行安装命令<code>./install</code>时，遇到<code>permission denied，bash: ./install: Permission denied</code>的错误，即便在<code>root</code>模式下也是同样的问题。解决方法如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决方法一</span></span><br><span class="line">chmod +x install</span><br><span class="line">./install​</span><br><span class="line"><span class="comment"># 解决方法二</span></span><br><span class="line">bash ./install</span><br></pre></td></tr></table></figure>
<p><code>./configure</code>时遇到<code>permission denied</code>也是同样的方法。</p>
<h3 id="insmod模块无显示的解决方法"><a href="#insmod模块无显示的解决方法" class="headerlink" title="insmod模块无显示的解决方法"></a>insmod模块无显示的解决方法</h3><p>&emsp;&emsp;原因是它把输出信息写进系统日志文件里去了，可以去查看<code>var/log/message</code>，或者使用<code>dmesg</code>命令查看输出。我的方案是使用<code>dmesg | tail</code>，只看输出的后几行。或者查看<code>/var/log/syslog</code>文件，使用命令<code>tail /var/log/syslog</code>。</p>
<h3 id="bin-bash-M-bad-interpreter-没有那个文件或目录"><a href="#bin-bash-M-bad-interpreter-没有那个文件或目录" class="headerlink" title="/bin/bash^M: bad interpreter: 没有那个文件或目录"></a>/bin/bash^M: bad interpreter: 没有那个文件或目录</h3><p>&emsp;&emsp;运行脚本时出现了这样一个错误，打开之后并没有找到所谓的<code>^M</code>。后来才知道原来是文件格式的问题，也就是<code>Linux</code>和<code>Windows</code>之间的不完全兼容。<br>&emsp;&emsp;错误分析：在<code>Windows</code>下编辑的脚本，可能有一些不可见字符。那些脚本文件是<code>DOS</code>格式的，即每一行的行尾以<code>\r\n</code>来标识，其<code>ASCII</code>码分别是<code>0x0D</code>和<code>0x0A</code>。<br>&emsp;&emsp;解决方法：使用<code>vim</code>打开文件，然后使用<code>set ff?</code>查看文件的格式，如果出现<code>fileformat=dos</code>，那么就基本可以确定是这个问题了。使用<code>set fileformat=unix</code>，然后保存文件即可。</p>
<h3 id="error-while-loading-shared-libraries"><a href="#error-while-loading-shared-libraries" class="headerlink" title="error while loading shared libraries"></a>error while loading shared libraries</h3><p>&emsp;&emsp;在<code>Linux</code>下运行程序时，出现了<code>error while loading shared libraries</code>这种错误。出现这类错误的原因：系统不知道<code>xxx.so</code>放在哪个目录下，这时就要在<code>/etc/ld.so.conf</code>中加入<code>xxx.so</code>所在的目录。<br>&emsp;&emsp;一般情况下，有很多的<code>.so</code>文件会存放在<code>/usr/local/lib</code>目录下，去该目录寻找，果然发现自己所需要的<code>.so</code>文件。所以在<code>/etc/ld.so.conf</code>中加入<code>/usr/local/lib</code>。保存之后，再运行<code>/sbin/ldconfig -v</code>更新一下配置即可。<code>/etc/ld.so.conf</code>的内容为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include /etc/ld.so.conf.d/*.conf /usr/<span class="built_in">local</span>/lib</span><br></pre></td></tr></table></figure>
<h3 id="更换介质：请把标有…DVD的盘片插入驱动器”-media-cdrom-“再按回车键"><a href="#更换介质：请把标有…DVD的盘片插入驱动器”-media-cdrom-“再按回车键" class="headerlink" title="更换介质：请把标有…DVD的盘片插入驱动器”/media/cdrom/“再按回车键"></a>更换介质：请把标有…DVD的盘片插入驱动器”/media/cdrom/“再按回车键</h3><p>&emsp;&emsp;在通过<code>apt-get install</code>安装软件时，可能会出现上述提示。<br>&emsp;&emsp;解决方法：使用<code>root</code>权限修改<code>/etc/apt/sources.list</code>文件，注释掉<code>deb cdrom:</code>开头的行，然后执行<code>apt-get update</code>。</p>
<h3 id="Lua-Error-during-loading-string-“-usr-share-wireshark-init-lua”-45"><a href="#Lua-Error-during-loading-string-“-usr-share-wireshark-init-lua”-45" class="headerlink" title="Lua: Error during loading: [string “/usr/share/wireshark/init.lua”]:45"></a>Lua: Error during loading: [string “/usr/share/wireshark/init.lua”]:45</h3><p>&emsp;&emsp;运行<code>Wireshark</code>时，可能会报出上述错误。<br>&emsp;&emsp;解决方法：修改<code>/etc/wireshark/init.lua</code>文件，将倒数第二行的<code>dofile(DATA_DIR..&quot;console.lua&quot;)</code>改为<code>--dofile(DATA_DIR..&quot;console.lua&quot;)</code>。</p>
<h3 id="Startup-Error-Unable-to-detect-graphics-environment"><a href="#Startup-Error-Unable-to-detect-graphics-environment" class="headerlink" title="Startup Error: Unable to detect graphics environment"></a>Startup Error: Unable to detect graphics environment</h3><p>&emsp;&emsp;使用<code>Xshell</code>控制<code>Linux</code>系统安装软件时会出现这样的问题，在<code>Linux</code>自带的终端操作就可以了。</p>
<h3 id="E-Unable-to-correct-problems-you-have-held-broken-packages"><a href="#E-Unable-to-correct-problems-you-have-held-broken-packages" class="headerlink" title="E: Unable to correct problems, you have held broken packages"></a>E: Unable to correct problems, you have held broken packages</h3><p>&emsp;&emsp;解决方法如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get upgrade</span><br></pre></td></tr></table></figure>
<h3 id="不支持IPv6"><a href="#不支持IPv6" class="headerlink" title="不支持IPv6"></a>不支持IPv6</h3><p>&emsp;&emsp;如果某些<code>Linux</code>系统(例如树莓派)不支持<code>IPv6</code>，可以这样修改：打开<code>/etc/modprobe.d/ipv6.conf</code>，使用<code>#</code>注释掉<code>alias net-pf-10 off</code>，开启<code>alias ipv6 on</code>，然后<code>reboot</code>。</p>
<h3 id="curses-h-No-such-file-or-directory"><a href="#curses-h-No-such-file-or-directory" class="headerlink" title="curses.h: No such file or directory"></a>curses.h: No such file or directory</h3><p>&emsp;&emsp;原因是少安装了<code>curses</code>库，解决方法如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libncurses5-dev</span><br></pre></td></tr></table></figure>
<h3 id="undefined-reference-to-initscr’"><a href="#undefined-reference-to-initscr’" class="headerlink" title="undefined reference to `initscr’"></a>undefined reference to `initscr’</h3><p>&emsp;&emsp;代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;curses.h&gt;</span></span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    initscr();</span><br><span class="line">    clear();</span><br><span class="line">    move ( <span class="number">10</span>, <span class="number">20</span> );</span><br><span class="line">    addstr ( <span class="string">"Hello world"</span> );</span><br><span class="line">    move ( LINES - <span class="number">1</span>, <span class="number">0</span> );</span><br><span class="line">    refresh();</span><br><span class="line">    getch();</span><br><span class="line">    endwin();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在终端输入<code>gcc -o hello hello.c -Incurses</code>，发现如下错误：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/tmp/ccsmlmYV.o: In function `main':</span><br><span class="line">hello.c:(.text+<span class="number">0xa</span>): undefined reference to `initscr'</span><br><span class="line">hello.c:(.text+<span class="number">0xf</span>): undefined reference to `stdscr'</span><br><span class="line">hello.c:(.text+<span class="number">0x17</span>): undefined reference to `wclear'</span><br><span class="line">hello.c:(.text+<span class="number">0x1c</span>): undefined reference to `stdscr'</span><br><span class="line">hello.c:(.text+<span class="number">0x34</span>): undefined reference to `wmove'</span><br><span class="line">hello.c:(.text+<span class="number">0x39</span>): undefined reference to `stdscr'</span><br><span class="line">hello.c:(.text+<span class="number">0x51</span>): undefined reference to `waddnstr'</span><br><span class="line">hello.c:(.text+<span class="number">0x56</span>): undefined reference to `LINES'</span><br><span class="line">hello.c:(.text+<span class="number">0x5e</span>): undefined reference to `stdscr'</span><br><span class="line">hello.c:(.text+<span class="number">0x72</span>): undefined reference to `wmove'</span><br><span class="line">hello.c:(.text+<span class="number">0x77</span>): undefined reference to `stdscr'</span><br><span class="line">hello.c:(.text+<span class="number">0x7f</span>): undefined reference to `wrefresh'</span><br><span class="line">hello.c:(.text+<span class="number">0x84</span>): undefined reference to `stdscr'</span><br><span class="line">hello.c:(.text+<span class="number">0x8c</span>): undefined reference to `wgetch'</span><br><span class="line">hello.c:(.text+<span class="number">0x91</span>): undefined reference to `endwin'</span><br><span class="line">collect2: ld returned <span class="number">1</span> <span class="built_in">exit</span> status</span><br></pre></td></tr></table></figure>
<p>在终端输入<code>gcc hello.c -o hello -I/usr/include -L/usr/lib -lncurses</code>即可解决。</p>
<h3 id="E-无法获得锁-var-lib-apt-lists-lock-open-11-资源暂时不可用"><a href="#E-无法获得锁-var-lib-apt-lists-lock-open-11-资源暂时不可用" class="headerlink" title="E: 无法获得锁 /var/lib/apt/lists/lock - open (11: 资源暂时不可用)"></a>E: 无法获得锁 /var/lib/apt/lists/lock - open (11: 资源暂时不可用)</h3><p>&emsp;&emsp;重启系统就可以了，可能有系统的其他<code>apt-get</code>进程在运行中。</p>
<h3 id="Linux和Windows之间ping失败"><a href="#Linux和Windows之间ping失败" class="headerlink" title="Linux和Windows之间ping失败"></a>Linux和Windows之间ping失败</h3><p>&emsp;&emsp;<code>Linux</code>和<code>Windows</code>之间<code>ping</code>失败，可能是防火墙的原因。如果<code>Windows</code>系统对<code>Linux</code>系统<code>ping</code>操作失败，可以考虑关闭<code>Linux</code>的防火墙：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<p>如果<code>linux</code>系统对<code>Windows</code>系统<code>ping</code>失败，可以考虑关闭<code>Windows</code>的防火墙。</p>
<h3 id="WARNING-REMOTE-HOST-IDENTIFICATION-HAS-CHANGED"><a href="#WARNING-REMOTE-HOST-IDENTIFICATION-HAS-CHANGED" class="headerlink" title="WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED"></a>WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED</h3><p>&emsp;&emsp;在使用<code>ssh</code>命令远程连接服务器，可能会出现上述问题。原因是第一次使用<code>ssh</code>连接时，服务器会生成一个认证，并储存在客户端(使用<code>ssh</code>程序的计算机)中的<code>known_hosts</code>。但是如果服务器验证失效了(例如重装服务器系统)，认证当然也需要更改，当服务器与客户端的验证不同时，就会报出上述错误。因此，只要把客户端中的认证删除，连线服务器时重新生成一个，就可以解决了。在客户端输入如下指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -R 服务器的IP</span><br></pre></td></tr></table></figure>
<p>该解决方案对于<code>Windows</code>的<code>cmd</code>和<code>PowerShell</code>会失效，报出错误<code>Updating known_hosts is not supported in Windows yet</code>，建议使用<code>Linux</code>子系统的<code>bash</code>。</p>
<h3 id="conda-command-not-found"><a href="#conda-command-not-found" class="headerlink" title="conda command not found"></a>conda command not found</h3><p>&emsp;&emsp;修改用户目录下的<code>.bash_profile</code>文件(或者<code>.bashrc</code>文件)，输入如下内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">"~/anaconda/bin:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></table></figure>
<p>然后使用命令<code>source ~/.bash_profile</code>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/软件与硬件问题/Keil问题总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/软件与硬件问题/Keil问题总结/" itemprop="url">Keil问题总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T18:12:20+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="transfer-of-control-bypasses-initialization-of"><a href="#transfer-of-control-bypasses-initialization-of" class="headerlink" title="transfer of control bypasses initialization of"></a>transfer of control bypasses initialization of</h3><p>&emsp;&emsp;Example:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">( <span class="keyword">void</span> )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> choice = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> z = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">switch</span> ( choice ) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">int</span> y = <span class="number">1</span>;</span><br><span class="line">            z = y + z;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Here, <code>y</code> is an initialized variable that is in scope (范围) (but unused) in the other cases. The <code>C++ Standard</code> says in section <code>6.7</code>: It is possible to transfer (转移) into a block, but not in a way that bypasses (绕过) declarations with initialization. A program that jumps from a point where a local variable with automatic storage duration is not in scope to a point where it is in scope is <code>ill-formed</code> (病态的) unless the variable has <code>POD</code> type and is declared without an initializer.<br>&emsp;&emsp;The transfer from the condition of a switch statement to a case label is considered a jump in this respect. The usual way to fix this is to enclose (把…围起来) the case that declares <code>y</code> in braces (括弧):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="number">1</span>: &#123;</span><br><span class="line">    <span class="keyword">int</span> y = <span class="number">1</span>;</span><br><span class="line">    z = y + z;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p><code>y</code> is a <code>POD</code> (Plain Old Data) type, so an alternative (替代的) would be to not use initialization：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">int</span> y;</span><br><span class="line">    y = <span class="number">1</span>;</span><br><span class="line">    z = y + z;</span><br><span class="line"></span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<h3 id="warning-1293-D-assignment-in-condition"><a href="#warning-1293-D-assignment-in-condition" class="headerlink" title="warning: #1293-D: assignment in condition"></a>warning: #1293-D: assignment in condition</h3><p>&emsp;&emsp;出现这样的问题，代码是可以继续正常执行的，但是写法不太严谨。因为在<code>while/if</code>的条件中，系统期望的是一个布尔类型的值(即<code>0</code>或<code>1</code>)，当表达式传递的结果非布尔值类型时，就会出现这样的警告。</p>
<h3 id="WARNING-L6306W-‘-PRES8’-SECTION-SHOULD-NOT-USE-‘REQ8’"><a href="#WARNING-L6306W-‘-PRES8’-SECTION-SHOULD-NOT-USE-‘REQ8’" class="headerlink" title="WARNING: L6306W: ‘~PRES8’ SECTION SHOULD NOT USE ‘REQ8’"></a>WARNING: L6306W: ‘~PRES8’ SECTION SHOULD NOT USE ‘REQ8’</h3><p>&emsp;&emsp;QUESTION: I have written an interrupt handler in assembly language using the <code>RealView</code> assembler. In this interrupt handler, I am calling another routine that is written in <code>C</code>. When I link my program, I receive the following warning: <code>Warning: L6306W: &#39;~PRES8&#39; section arm_isr.o (asm_irq) should not use the address of &#39;REQ8&#39; function (c_func)</code>.<br>&emsp;&emsp;ANSWER：All <code>C</code> code generated by the <code>RealView</code> compiler assumes that stack allocation is aligned on <code>8-byte</code> boundaries. You must ensure that the assembler interrupt handler has an <code>8-byte</code> aligned stack by specifying the <code>PRESERVE8</code> directive at the beginning of your assembly file. For example,</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    PRESERVE8</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">    AREA irq_asm, CODE, READONLY</span><br></pre></td></tr></table></figure>
<h3 id="integer-operation-result-is-out-of-range-61-D"><a href="#integer-operation-result-is-out-of-range-61-D" class="headerlink" title="integer operation result is out of range (61-D)"></a>integer operation result is out of range (61-D)</h3><p>&emsp;&emsp;在编写测试程序的过程中，有这样的宏定义：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> UART1_EN ( 1 &lt;&lt; 31 )</span></span><br></pre></td></tr></table></figure>
<p>编译后编译器报错<code>integer operation result is out of range (61-D)</code>。经查资料，这是由于溢出所致：宏定义默认常量是有符号型，当左移<code>31</code>位时，常数就变成负数，编译器就会给出如上的警告信息。解决办法是强制类型转换成无符号类型：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> UART1_EN ( ( U32 ) 1 &lt;&lt; 31 )</span></span><br></pre></td></tr></table></figure>
<h3 id="the-size-of-an-array-must-be-greater-than-zero"><a href="#the-size-of-an-array-must-be-greater-than-zero" class="headerlink" title="the size of an array must be greater than zero"></a>the size of an array must be greater than zero</h3><p>&emsp;&emsp;<code>Zero-sized</code> arrays are allowed only when in <code>GNU</code> mode (<code>--gnu</code>)。For example：<code>char name[0];</code>. method: <code>Options for target -&gt; C/C++ -&gt; Misc Control -&gt; input &quot;--gnu&quot;</code>.</p>
<h3 id="error-declaration-may-not-appear-after-executable-statement-in-block"><a href="#error-declaration-may-not-appear-after-executable-statement-in-block" class="headerlink" title="error: declaration may not appear after executable statement in block"></a>error: declaration may not appear after executable statement in block</h3><p>&emsp;&emsp;在程序中声明变量时，需要在可执行语句之前声明，否则会出现以上错误。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vu16 KeyPortState = <span class="number">0</span>;</span><br><span class="line">SystemInit();</span><br><span class="line">KeyScanState_Typedef KeyScanState = KeyScanState_0;</span><br><span class="line">GPIO_Configuration();</span><br></pre></td></tr></table></figure>
<p>编译会出现错误，原因是<code>KeyScanState_Typedef KeyScanState = KeyScanState_0;</code>在<code>SystemInit();</code>之后。正确的函数语句为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vu16 KeyPortState = <span class="number">0</span>;</span><br><span class="line">KeyScanState_Typedef KeyScanState = KeyScanState_0;</span><br><span class="line">SystemInit();</span><br><span class="line">GPIO_Configuration();</span><br></pre></td></tr></table></figure>
<h3 id="warning-1134-D-literal-treated-as-“long-long”"><a href="#warning-1134-D-literal-treated-as-“long-long”" class="headerlink" title="warning: #1134-D: literal treated as “long long”"></a>warning: #1134-D: literal treated as “long long”</h3><p>&emsp;&emsp;在<code>Keil</code>中有如下代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> value = <span class="number">2147483648</span>;</span><br></pre></td></tr></table></figure>
<p><code>value</code>在编译时会产生上述警告。<code>2147483648</code>是一个字面常量，类型是<code>int</code>型，而不是<code>unsigned int</code>型。需要先将<code>int</code>类型转换成<code>unsigned int</code>类型，再赋值给<code>value</code>。在转换过程中，因为这个数值超过<code>32</code>位<code>int</code>类型的表示范围，所以会出现上述警告。如果你想要明白告诉编译器<code>2147483648</code>就是<code>unsigned</code>类型，可以这么做：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> value = <span class="number">2147483648u</span>;</span><br></pre></td></tr></table></figure>
<h3 id="error-20-identifier-“TIM6-IRQn”-is-undefined"><a href="#error-20-identifier-“TIM6-IRQn”-is-undefined" class="headerlink" title="error: #20: identifier “TIM6_IRQn” is undefined"></a>error: #20: identifier “TIM6_IRQn” is undefined</h3><p>&emsp;&emsp;打开<code>STM32F103RB</code>的启动文件<code>starup_stm32f10x_md.s</code>，里面有如下内容：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DCD TIM1_BRK_IRQHandler     ; TIM1 Break</span><br><span class="line">DCD TIM1_UP_IRQHandler      ; TIM1 Update</span><br><span class="line">DCD TIM1_TRG_COM_IRQHandler ; TIM1 Trigger <span class="keyword">and</span> Commutation</span><br><span class="line">DCD TIM1_CC_IRQHandler      ; TIM1 Capture Compare</span><br><span class="line">DCD TIM2_IRQHandler         ; TIM2</span><br><span class="line">DCD TIM3_IRQHandler         ; TIM3</span><br><span class="line">DCD TIM4_IRQHandler         ; TIM4</span><br></pre></td></tr></table></figure>
<p>这里面只有<code>TIM1</code>至<code>TIM4</code>的定时器中断事件，而<code>TIM6</code>是没有中断响应这个功能，这是因为<code>STM32F103RB</code>芯片没有<code>TIM6</code>。</p>
<h3 id="cannot-open-source-input-file-“stm32f10x-lib-h”-No-such-file-or-directory"><a href="#cannot-open-source-input-file-“stm32f10x-lib-h”-No-such-file-or-directory" class="headerlink" title="cannot open source input file “stm32f10x_lib.h”: No such file or directory"></a>cannot open source input file “stm32f10x_lib.h”: No such file or directory</h3><p>&emsp;&emsp;将<code>cortexm3_macro.h</code>、<code>stm32f10x_conf.h</code>、<code>stm32f10x_it.h</code>、<code>stm32f10x_map.h</code>、<code>stm32f10x_nvic.h</code>和<code>stm32f10x_type.h</code>拷贝到工程文件夹下。将源文件中的<code>#include &lt;stm32f10x_lib.h&gt;</code>替换成<code>#include &lt;stm32f10x_map.h&gt;</code>和<code>#include &lt;stm32f10x_nvic.h&gt;</code>。</p>
<hr>
<h3 id="REFERENCE-MADE-TO-UNRESOLVED-EXTERNAL"><a href="#REFERENCE-MADE-TO-UNRESOLVED-EXTERNAL" class="headerlink" title="REFERENCE MADE TO UNRESOLVED EXTERNAL"></a>REFERENCE MADE TO UNRESOLVED EXTERNAL</h3><p>&emsp;&emsp;出现上述警告的原因是变量或者函数只是在头文件中进行了声明，没有在源文件中进行定义。只要在源文件中对变量或者函数进行定义即可。</p>
<h3 id="ERROR-L104-MULTIPLE-PUBLIC-DEFINITIONS"><a href="#ERROR-L104-MULTIPLE-PUBLIC-DEFINITIONS" class="headerlink" title="ERROR L104: MULTIPLE PUBLIC DEFINITIONS"></a>ERROR L104: MULTIPLE PUBLIC DEFINITIONS</h3><p>&emsp;&emsp;出现上述错误则是因为变量被重复定义，把头文件中的变量定义前加<code>extern</code>(只是变量声明不用初始化)，再在某一个你要调用该变量的<code>C</code>文件的程序之前再定义。</p>
<h3 id="MULTIPLE-CALL-TO-SEGMENT"><a href="#MULTIPLE-CALL-TO-SEGMENT" class="headerlink" title="MULTIPLE CALL TO SEGMENT"></a>MULTIPLE CALL TO SEGMENT</h3><p>&emsp;&emsp;该警告表示连接器发现有一个函数可能会被主函数和一个中断服务程序(或者调用中断服务程序的函数)同时调用，或者同时被多个中断服务程序调用。出现这种问题的原因之一是这个函数是不可重入性函数，当该函数运行时它可能会被一个中断打断，从而使得结果发生变化并可能会引起一些变量形式的冲突(即引起函数内一些数据的丢失，可重入性函数在任何时候都可以被<code>ISR</code>打断，一段时间后又可以运行，但是相应数据不会丢失)。可以采用以下几种方法：</p>
<ul>
<li>主程序调用该函数时禁止中断，可以在该函数被调用时用<code>#pragma disable</code>语句来实现禁止中断的目的。</li>
<li>复制两份该函数的代码，一份到主程序中，另一份复制到中断服务程序中。</li>
<li>将该函数设为重入型：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">myfunc</span><span class="params">(<span class="keyword">void</span>)</span> reentrant </span>&#123;</span><br><span class="line">    <span class="comment">/* ... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种设置将会产生一个可重入堆栈，该堆栈被用于存储函数值和局部变量。使用这种方法时，重入堆栈必须在<code>STARTUP.A51</code>文件中配置。这种方法消耗更多的RAM并会降低重入函数的执行速度。</p>
<h3 id="unprintable-character-0xA1-skipped"><a href="#unprintable-character-0xA1-skipped" class="headerlink" title="unprintable character 0xA1 skipped"></a>unprintable character 0xA1 skipped</h3><p>&emsp;&emsp;因为程序中含有全角字符，<code>Keil</code>无法识别。全角字符和<code>ASCII</code>码显示在屏幕上几乎是一样的，所以人眼很容易就错过了。可以使用半角全角转换工具，将文件中的全角字符转换为半角字符。</p>
<h3 id="ERROR-L105-PUBLIC-REFERS-TO-IGNORED-SEGMENT"><a href="#ERROR-L105-PUBLIC-REFERS-TO-IGNORED-SEGMENT" class="headerlink" title="ERROR L105: PUBLIC REFERS TO IGNORED SEGMENT"></a>ERROR L105: PUBLIC REFERS TO IGNORED SEGMENT</h3><p>&emsp;&emsp;该错误说明单片机的<code>data</code>空间已经不够用了，需要在工程<code>Options</code>的<code>Target</code>目录下，将<code>Memory Model</code>设置成<code>Large:variables in XDATA</code>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/软件与硬件问题/ubuntu服务器/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/软件与硬件问题/ubuntu服务器/" itemprop="url">ubuntu服务器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T16:37:15+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="FTP服务器"><a href="#FTP服务器" class="headerlink" title="FTP服务器"></a>FTP服务器</h3><p>&emsp;&emsp;在<code>Linux</code>系统中，<code>ftp</code>服务器的全名叫<code>vsftpd</code>。我们需要利用相关命令来安装<code>ftp</code>服务器，然后在<code>vsftpd.conf</code>中进行相关配置。<br>&emsp;&emsp;1. 首先使用如下命令进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vsftpd</span><br></pre></td></tr></table></figure>
<p>安装完成后，输入<code>vsftpd -version</code>查看是否安装成功。<br>&emsp;&emsp;2. 新建一个用于<code>FTP</code>的工作目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/ftp</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. 新建<code>FTP</code>用户，并设置密码以及工作目录，其中<code>ftpname</code>是创建的用户名：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -d /home/ftp -s /bin/bash ftpname</span><br></pre></td></tr></table></figure>
<p>为新建的用户设置密码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd ftpname</span><br></pre></td></tr></table></figure>
<p>可以使用<code>cat /etc/passwd</code>查看当前系统用户。<br>&emsp;&emsp;4. 修改<code>vsftpd</code>配置文件，该文件位于<code>/etc</code>目录下，相关的配置如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">listen=YES                     <span class="comment"># 服务器监听</span></span><br><span class="line">anonymous_enable=NO            <span class="comment"># 匿名访问允许。此选项很危险，默认不要开启</span></span><br><span class="line">local_enable=YES               <span class="comment"># 本地主机访问允许</span></span><br><span class="line">write_enable=YES               <span class="comment"># 写允许</span></span><br><span class="line"><span class="comment"># anon_upload_enable=YES       # 匿名上传允许</span></span><br><span class="line"><span class="comment"># anon_mkdir_write_enable=YES  # 匿名创建文件夹允许</span></span><br><span class="line">dirmessage_enable=YES          <span class="comment"># 进入文件夹允许</span></span><br><span class="line">xferlog_enable=YES             <span class="comment"># ftp日志记录允许</span></span><br><span class="line">connect_from_port_20=YES       <span class="comment"># 允许使用端口号20作为数据传送的端口</span></span><br><span class="line">secure_chroot_dir=/var/run/vsftpd/empty</span><br><span class="line">pam_service_name=vsftpd</span><br><span class="line">rsa_cert_file=/etc/ssl/private/vsftpd.pem</span><br><span class="line">local_root=/home/ftp           <span class="comment"># 设置ftp的根目录为“/home/ftp”</span></span><br></pre></td></tr></table></figure>
<p>设置好之后，保存退出，然后使用如下命令重启<code>ftp</code>服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service vsftpd restart</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;5. 在浏览器中的地址栏输入<code>ftp://ftp服务器地址</code>，然后输入账号和密码即可实现登录。</p>
<h4 id="vsftp配置参数说明"><a href="#vsftp配置参数说明" class="headerlink" title="vsftp配置参数说明"></a>vsftp配置参数说明</h4><p>&emsp;&emsp;<code>vsftp</code>的各配置参数如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>anonymous_enable=YES</code></td>
<td>是否开启匿名用户</td>
</tr>
<tr>
<td><code>no_anon_password=YES</code></td>
<td>匿名用户<code>login</code>时不询问用户名和口令</td>
</tr>
<tr>
<td><code>anon_umask=077</code></td>
<td>匿名用户上传的文件权限</td>
</tr>
<tr>
<td><code>anon_upload_enable=YES</code></td>
<td>是否允许匿名用户上传文件</td>
</tr>
<tr>
<td><code>anon_mkdir_write_enable=YES</code></td>
<td>是否允许匿名用户建立目录</td>
</tr>
<tr>
<td><code>anon_other_write_enable=YES</code></td>
<td>是否允许匿名用户具有建立目录、上传之外的权限，如重命名、删除</td>
</tr>
<tr>
<td><code>anon_world_readable_only=YES</code></td>
<td>匿名登入者是否能下载可阅读的档案</td>
</tr>
<tr>
<td><code>anon_max_rate=80000</code></td>
<td>匿名用户的下载速度为<code>80KB/s</code></td>
</tr>
<tr>
<td><code>anon_root=(none)</code></td>
<td>匿名用户的宿主目录</td>
</tr>
<tr>
<td><code>allow_anon_ssl=NO</code></td>
<td>匿名用户是否允许使用安全的<code>SSL</code>连接服务器</td>
</tr>
<tr>
<td><code>ftp_username=FTP</code></td>
<td>定义匿名使用者登录的使用者名称(默认为<code>FTP</code>)</td>
</tr>
<tr>
<td><code>banned_email_file=/etc/vsftpd.banned_emails</code></td>
<td>禁止使用的匿名用户登陆时作为密码的电子邮件地址使用表的位置</td>
</tr>
<tr>
<td><code>deny_email_enable=NO</code></td>
<td>禁止使用的匿名用户登陆时作为密码的电子邮件地址</td>
</tr>
<tr>
<td><code>secure_email_list_enable</code></td>
<td>如果你想只接受以指定<code>E-MAIL</code>地址登录的匿名用户的话，启用它</td>
</tr>
<tr>
<td><code>local_enable=YES</code></td>
<td>本地用户是否可以登录</td>
</tr>
<tr>
<td><code>local_umask=022</code></td>
<td>设置本地用户的文件生成掩码</td>
</tr>
<tr>
<td><code>file_open_mode=0666</code></td>
<td>上传文件的权限配合<code>umask</code>使用</td>
</tr>
<tr>
<td><code>local_root=(none)</code></td>
<td>指定所有本地用户登陆后的目录，如果不设置此项，用户都会登陆于自己的主目录</td>
</tr>
<tr>
<td><code>local_max_rate=500000</code></td>
<td>本地用户的下载速度为<code>500KB/s</code></td>
</tr>
<tr>
<td><code>chroot_local_user=YES</code></td>
<td>是否允许用户离开其宿主目录</td>
</tr>
<tr>
<td><code>chroot_list_enable=NO</code></td>
<td>登录用户名字若在<code>/etc/vsftpd.chroot_list</code>内，则会启用<code>chroot</code>机制，将这个用户限制在其<code>home</code>目录下</td>
</tr>
<tr>
<td><code>guest_enable=YES</code></td>
<td>是否开启虚拟用户</td>
</tr>
<tr>
<td><code>guest_username=vsftpd</code></td>
<td>指定虚拟用户名</td>
</tr>
<tr>
<td><code>virtual_use_local_privs=YES</code></td>
<td>虚拟用户和本地用户权限是否相同</td>
</tr>
<tr>
<td><code>userlist_enable=YES</code></td>
<td>是否根据<code>user_list</code>实行访问控制(若启用此选项，<code>userlist_deny</code>选项才被启动)</td>
</tr>
<tr>
<td><code>userlist_deny=NO</code></td>
<td>若为<code>YES</code>，则<code>userlist_file</code>中的用户将不能登录；为<code>NO</code>，则只有<code>userlist_file</code>的用户可以登录</td>
</tr>
<tr>
<td><code>write_enable=YES</code></td>
<td>用户是否具有写的权限</td>
</tr>
<tr>
<td><code>download_enable=YES</code></td>
<td>是否允许下载</td>
</tr>
<tr>
<td><code>chmod_enable=YES</code></td>
<td>是否可以修改文件权限</td>
</tr>
<tr>
<td><code>nopriv_user= nobody</code></td>
<td>设定服务执行者为<code>nobody</code>，<code>vsftpd</code>推荐使用一个权限很低的用户，最好是没有家目录(<code>/dev/null</code>)，没有登陆<code>shell</code>(<code>/sbin/nologin</code>)，系统会更安全</td>
</tr>
<tr>
<td><code>dirmessage_enable=YES</code></td>
<td>当切换到<code>FTP</code>服务器的某个目录时，是否显示该目录下的<code>.message</code>信息</td>
</tr>
<tr>
<td><code>dirlist_enable=YES</code></td>
<td>是否启用通俗命令(如果设置为<code>NO</code>，那么只能使用<code>unix/linux</code>的命令)</td>
</tr>
<tr>
<td><code>xferlog_enable=YES</code></td>
<td>是否启用上传和下载日志</td>
</tr>
<tr>
<td><code>xferlog_std_format=YES</code></td>
<td>是否使用标准的<code>ftpd-xferlog</code>日志格式</td>
</tr>
<tr>
<td><code>xferlog_file=/var/log/vsftpd.log</code></td>
<td>将上传下载日志记录到<code>/var/log/vsftpd.log</code>中</td>
</tr>
<tr>
<td><code>log_ftp_protocol=NO</code></td>
<td>当<code>xferlog_std_format</code>关闭且本选项开启时，记录所有<code>ftp</code>请求和回复，当调试时比较有用</td>
</tr>
<tr>
<td><code>dual_log_enable=NO</code></td>
<td>如果启用，两个<code>LOG</code>文件会各自产生，默认的是<code>/var/log/xferlog</code>和<code>/var/log/vsftpd.log</code></td>
</tr>
<tr>
<td><code>vsftpd_log_file=/var/log/vsftpd.log</code></td>
<td>这是被生成的<code>vsftpd</code>格式的<code>log</code>文件的名字(只有<code>xferlog_enable</code>被设置，而<code>xferlog_std_format</code>没有被设置时，此项才生效)</td>
</tr>
<tr>
<td><code>syslog_enable=NO</code></td>
<td>如果启用，系统<code>log</code>将取代<code>vsftpd</code>的<code>log</code>输出到<code>/var/log/vsftpd.log</code>，<code>FTPD</code>的<code>log</code>工具将不工作</td>
</tr>
<tr>
<td><code>connect_from_port_20=YES</code></td>
<td>是否启用<code>FTP</code>数据端口的连接请求</td>
</tr>
<tr>
<td><code>listen=YES</code></td>
<td>是否使用<code>standalone</code>启动<code>vsftpd</code>，而不是<code>super daemon(xinetd)</code>控制它(<code>vsftpd</code>推荐使用<code>standalone</code>方式)</td>
</tr>
<tr>
<td><code>listen_ipv6=NO</code></td>
<td>与<code>listen</code>功能相同，但此项监听<code>IPV6</code>(两个只能设置一个)</td>
</tr>
<tr>
<td><code>pam_service_name=vsftpd</code></td>
<td><code>PAM</code>认证服务配置文件名称，保存在<code>/etc/pam.d</code>目录下</td>
</tr>
<tr>
<td><code>userlist_enable=YES</code></td>
<td>是否检查<code>userlist_file</code>设置文件</td>
</tr>
<tr>
<td><code>tcp_wrappers=YES</code></td>
<td>是否使用<code>tcp_wrappers</code>作为主机访问控制方式(<code>tcp_wrappers</code>的两个配置文件<code>/etc/hosts.allow</code>允许访问的主机和<code>/etc/hosts.deny</code>拒绝访问的主机</td>
</tr>
<tr>
<td><code>ftpd_banner=Welcome to yayi.biz FTP Service</code></td>
<td><code>FTP</code>欢迎信息(如果设置了<code>banner_file</code>，则此设置无效)</td>
</tr>
<tr>
<td><code>banner_file=/etc/vsftpd/banner</code></td>
<td>定义登录信息文件的位置</td>
</tr>
<tr>
<td><code>check_shell=NO</code></td>
<td>是否检测<code>SHELL</code></td>
</tr>
<tr>
<td><code>chown_uploads=YES</code></td>
<td>是否开启匿名上传用户切换(如果开启，上传用户则变为<code>chown_username=daemon</code>指定的用户)</td>
</tr>
<tr>
<td><code>chown_username=daemon</code></td>
<td>匿名上传文件的属主</td>
</tr>
<tr>
<td><code>file_open_mode=0666</code></td>
<td>对于上传的文件设定权限</td>
</tr>
<tr>
<td><code>idle_session_timeout=600</code></td>
<td>客户端超过<code>600s</code>没有动作则视为超时</td>
</tr>
<tr>
<td><code>data_connection_timeout=300</code></td>
<td>数据传输时超过<code>300s</code>没有动作则视为超时</td>
</tr>
<tr>
<td><code>connect_timeout=60</code></td>
<td>连接超时时间</td>
</tr>
<tr>
<td><code>pasv_enable=YES</code></td>
<td>是否允许使用<code>PASV</code>模式</td>
</tr>
<tr>
<td><code>pasv_promiscuous+NO</code></td>
<td>是否关闭<code>PASV</code>安全检查(删除<code>+NO</code>则开启)</td>
</tr>
<tr>
<td><code>pasv_address=(none)</code></td>
<td>使<code>vsftpd</code>在<code>pasv</code>命令回复时跳转到指定的<code>IP</code>地址</td>
</tr>
<tr>
<td><code>port_enable=YES</code></td>
<td>是否允许使用<code>PORT</code>模式</td>
</tr>
<tr>
<td><code>prot_promiscuous</code></td>
<td>是否开启安全<code>PORT</code>检查(<code>+NO</code>则不开启)</td>
</tr>
<tr>
<td><code>pasv_max_port=0</code></td>
<td>指定为被动模式数据连接分配的最大端口(<code>0</code>为任何)</td>
</tr>
<tr>
<td><code>pasv_min_port=0</code></td>
<td>指定为被动模式数据连接分配的最小端口(<code>0</code>为任何)</td>
</tr>
<tr>
<td><code>ACCEPT_TIMEOUT=60</code></td>
<td><code>PAVS</code>请求<code>60s</code>无响应则视为超时</td>
</tr>
<tr>
<td><code>ascii_upload_enable=YES</code></td>
<td>是否可用<code>ASCII</code>模式上传(默认为<code>NO</code>)</td>
</tr>
<tr>
<td><code>ascii_download_enable=YES</code></td>
<td>是否可用<code>ASCII</code>模式下载(默认为<code>NO</code>)</td>
</tr>
<tr>
<td><code>secure_chroot_dir=/usr/share/empty</code></td>
<td>这个选项必须指定一个空的数据夹，且任何登入者都不能有写入的权限，当<code>vsftpd</code>不需要<code>file system</code>的权限时，就会将使用者限制在此数据夹中，默认值为<code>/usr/share/empty</code></td>
</tr>
<tr>
<td><code>one_process_model=YES</code></td>
<td>是否使用单进程模式</td>
</tr>
<tr>
<td><code>text_userdb_names=NO</code></td>
<td>是否可以查看文件拥有者的<code>UID</code></td>
</tr>
<tr>
<td><code>use_localtime=NO</code></td>
<td>显示目录清单时是用本地时间还是<code>GMT</code>时间，可以通过<code>mdtm</code>命令来达到一样的效果</td>
</tr>
<tr>
<td><code>use_sendfile=YES</code></td>
<td>是否测试平台优化</td>
</tr>
<tr>
<td><code>setproctitle_enable=YES</code></td>
<td>是否显示状态会话信息</td>
</tr>
<tr>
<td><code>user_config_dir=/etc/vsftpd/userconf</code></td>
<td>定义用户配置文件的目录</td>
</tr>
<tr>
<td><code>local_root=xxx</code></td>
<td>定义本地用户登陆的根目录，注意定义根目录可以是相对路径也可以是绝对路径。相对路径是针对用户家目录来说的</td>
</tr>
<tr>
<td><code>max_clients=0</code></td>
<td>可接受的最大<code>client</code>数目(<code>0</code>为不限制)</td>
</tr>
<tr>
<td><code>max_per_ip=0</code></td>
<td>每个<code>ip</code>的最大<code>client</code>数目(<code>0</code>为不限制)</td>
</tr>
<tr>
<td><code>connect_from_port_20=YES</code></td>
<td>是否启用<code>FTP</code>数据端口的数据连接</td>
</tr>
<tr>
<td><code>ftp_data_port=20</code></td>
<td>设定<code>PORT</code>模式下的连接端口(只要<code>connect_from_port_20</code>被激活)</td>
</tr>
<tr>
<td><code>listen_address=192.168.0.2</code></td>
<td>绑定<code>FTP</code>的<code>IP</code>地址(在多网卡或者多<code>IP</code>地址的机器上使用)</td>
</tr>
<tr>
<td><code>listen_port=2121</code></td>
<td>绑定<code>FTP</code>使用使用端口</td>
</tr>
<tr>
<td><code>ftp_data_port=2020</code></td>
<td>绑定<code>FTP</code>数据传输端口</td>
</tr>
<tr>
<td><code>background=NO</code></td>
<td>启用时，<code>VSFTPD</code>将把监听进程置于后台；但访问<code>VSFTPD</code>时，控制台将立即被返回到<code>SHELL</code></td>
</tr>
<tr>
<td><code>force_dot_files=NO</code></td>
<td>如果激活，以<code>.</code>开始的文件和目录在目录列取的时候将会被显示，即使客户端没有使用<code>a</code>标识。这不包括<code>.</code>和<code>..</code>目录</td>
</tr>
<tr>
<td><code>ssl_enable=NO</code></td>
<td>是否启用<code>SSL</code></td>
</tr>
<tr>
<td><code>force_local_data_ssl=YES</code></td>
<td>是否要求非匿名用户使用安全的<code>SSL</code>在数据线路上收发数据</td>
</tr>
<tr>
<td><code>force_local_logins_ssl=YES</code></td>
<td>是否要求非匿名用户使用安全的<code>SSL</code>登录以发送密码</td>
</tr>
<tr>
<td><code>ssl_tlsv1=YES</code></td>
<td>是否允许以<code>TLS V1</code>协议的连接，<code>TLS V1</code>连接将是首选</td>
</tr>
<tr>
<td><code>ssl_sslv2=NO</code></td>
<td>是否允许以<code>SSL V2</code>协议的连接，<code>TLS V1</code>连接将是首选</td>
</tr>
<tr>
<td><code>ssl_sslv3=NO</code></td>
<td>是否允许以<code>SSL V3</code>协议的连接，<code>TLS V1</code>连接将是首选</td>
</tr>
<tr>
<td><code>session_support=NO</code></td>
<td>是否让<code>VSFTPD</code>去尝试管理登录会话</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;<strong>补充说明</strong>：<br>&emsp;&emsp;1. <code>anonymous_enable=NO</code>必须要打开，不能注释。<br>&emsp;&emsp;2. <code>ftp</code>服务器的根目录可以是多级目录，如<code>/home/fukangwei/ftp</code>。<br>&emsp;&emsp;3. 要设置好<code>ftp</code>服务器的根目录权限，否则上传文件时会出现<code>553 Could not create file</code>错误。</p>
<hr>
<h3 id="tftp服务器"><a href="#tftp服务器" class="headerlink" title="tftp服务器"></a>tftp服务器</h3><p>&emsp;&emsp;1. 安装<code>xinetd</code>、<code>tftp-hpa</code>和<code>tftpd-hpa</code>(<code>tftp-hpa</code>是客户端，<code>tftpd-hpa</code>是服务器端)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install xinetd</span><br><span class="line">sudo apt-get install tftp-hpa tftpd-hpa</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;2. 配置<code>TFTP</code>服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/default/tftpd-hpa</span><br></pre></td></tr></table></figure>
<p>将原来的内容改为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TFTP_USERNAME=<span class="string">"tftp"</span></span><br><span class="line">TFTP_ADDRESS=<span class="string">"0.0.0.0:69"</span></span><br><span class="line">TFTP_DIRECTORY=<span class="string">"tftp根目录"</span>  <span class="comment"># 服务器目录，需要设置权限为777</span></span><br><span class="line">TFTP_OPTIONS=<span class="string">"-l -c -s"</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. 重新启动<code>TFTP</code>服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service tftpd-hpa restart</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="ssh服务器"><a href="#ssh服务器" class="headerlink" title="ssh服务器"></a>ssh服务器</h3><p>&emsp;&emsp;1. 安装<code>ssh-server</code>和<code>ssh-client</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br><span class="line">sudo apt-get install openssh-client</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;2. 确认<code>sshserver</code>是否安装好：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -e | grep sshd</span><br></pre></td></tr></table></figure>
<p>如果看到<code>sshd</code>，那说明<code>ssh-server</code>已经启动了；如果只有<code>ssh-agent</code>，说明<code>ssh-server</code>还没有启动，需要执行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh start</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. <code>SSH</code>默认服务端口为<code>22</code>，用户可以自定义成其他端口，需要修改的配置文件为<code>/etc/ssh/sshd_config</code>，把里面的<code>Port</code>参数修改成其他数组即可，然后重启<code>SSH</code>服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/ssh restart</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="VNC服务器"><a href="#VNC服务器" class="headerlink" title="VNC服务器"></a>VNC服务器</h3><p>&emsp;&emsp;在<code>ubuntu</code>上安装并运行<code>VNC</code>服务器，推荐使用<code>tightvnc</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install tightvncserver</span><br></pre></td></tr></table></figure>
<p>安装完成之后，使用如下命令运行<code>tightvnc</code>服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vncserver :1</span><br></pre></td></tr></table></figure>
<p>第一次运行时需要你设置一个密码，该密码和系统用户密码无关。<br>&emsp;&emsp;在<code>Windows</code>上安装<code>vnc viewer</code>，下载地址为<code>http://www.realvnc.com/</code>。启动<code>vnc viewer</code>，在<code>VNC Server</code>上填入<code>VNC服务器IP地址:1</code>，点击<code>Connect</code>，输入<code>vnc</code>密码即可。</p>
<hr>
<h3 id="sftp服务器"><a href="#sftp服务器" class="headerlink" title="sftp服务器"></a>sftp服务器</h3><p>&emsp;&emsp;<code>sftp</code>服务器使用如下命令进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>sftp</code>是<code>Secure File Transfer Protocol</code>的缩写，意思是安全文件传送协议，可以为传输文件提供一种安全的加密方法，与<code>ftp</code>有着几乎一样的语法和功能。<code>SFTP</code>本身没有单独的守护进程，它必须使用<code>sshd</code>守护进程(端口号默认是<code>22</code>)来完成相应的连接操作。所以从某种意义上来说，<code>SFTP</code>并不像一个服务器程序，而更像是一个客户端程序。<code>SFTP</code>使用加密传输认证信息和数据，所以是非常安全的。但由于这种传输方式使用了加密和解密技术，所以传输效率比普通的FTP要低得多。常用的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sftp user@host                      <span class="comment"># 登陆远程主机</span></span><br><span class="line">lcd或lpwd                           <span class="comment"># 针对本机的命令都加上“l”</span></span><br><span class="line">put filename.txt directory          <span class="comment"># 将本机文件上传到远程</span></span><br><span class="line">mput *.*                            <span class="comment"># 将当前文件夹下的文件上传到远程</span></span><br><span class="line">get filename.file directory         <span class="comment"># 下载远程文件到本地</span></span><br><span class="line">mget *.* directory                  <span class="comment"># 下载目录下所有远程文件到本地</span></span><br><span class="line">?                                   <span class="comment"># 帮助</span></span><br><span class="line"><span class="built_in">bye</span>、<span class="built_in">exit</span>或quit                     <span class="comment"># 退出</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/深度学习/TensorFlow之nn的API/" itemprop="url">TensorFlow之nn的API</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T11:02:43+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>&emsp;&emsp;在神经网络中，我们有很多的非线性函数来作为激活函数，比如连续的平滑非线性函数(<code>sigmoid</code>、<code>tanh</code>和<code>softplus</code>)，连续但不平滑的非线性函数(<code>relu</code>、<code>relu6</code>和<code>relu_x</code>)和随机正则化函数(<code>dropout</code>)。所有的激活函数都是单独应用在每个元素上面的，并且输出张量的维度和输入张量的维度一样。<br>&emsp;&emsp;<code>relu</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>relu</code>，即<code>max(features, 0)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.relu(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[0. 2.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>和<code>int8</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>relu6</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu6(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>relu6</code>，即<code>min(max(features, 0), 6)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">12.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.relu6(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[0. 6.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是float、double、int32、int64、uint8、int16或者int8。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>softplus</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softplus(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>softplus</code>，即<code>log(exp(features) + 1)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">12.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.softplus(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[ 0.31326166 12.000006 ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>或者<code>int8</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>dropout</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(x, keep_prob, noise_shape = <span class="keyword">None</span>, seed = <span class="keyword">None</span>, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算神经网络层的<code>dropout</code>。<br>&emsp;&emsp;一个神经元将以概率<code>keep_prob</code>决定是否放电，如果不放电，那么该神经元的输出将是<code>0</code>，如果该神经元放电，那么该神经元的输出值将被放大到原来的<code>1 / keep_prob</code>倍。这里的放大操作是为了保持神经元输出总个数不变。比如，神经元的值为<code>[1, 2]</code>，<code>keep_prob</code>的值是<code>0.5</code>，并且是第一个神经元是放电的，第二个神经元不放电，那么神经元输出的结果是<code>[2, 0]</code>，也就是相当于，第一个神经元被当做了<code>1 / keep_prob</code>个输出，即<code>2</code>个。这样保证了总和<code>2</code>个神经元保持不变。<br>&emsp;&emsp;默认情况下，每个神经元是否放电是相互独立的。但是，如果<code>noise_shape</code>被修改了，那么它对于变量<code>x</code>就是一个广播形式，而且当且仅当<code>noise_shape[i] == shape(x)[i]</code>，<code>x</code>中的元素是相互独立的。比如，如果<code>shape(x) = [k, l, m, n], noise_shape = [k, 1, 1, n]</code>，那么每个批和通道都是相互独立的，但是每行和每列的数据都是关联的，即要不都为<code>0</code>，要不都还是原来的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[[-2.  0.  6.  0.]]</span></span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape=[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[[-2.  4.  6.  8.]]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>keep_prob</code>：一个<code>Python</code>的<code>float</code>类型，表示元素是否放电的概率。</li>
<li><code>noise_shape</code>：一个一维的<code>Tensor</code>，数据类型是<code>int32</code>，代表元素是否独立的标志。</li>
<li><code>seed</code>：一个<code>Python</code>的整数类型，设置随机种子。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和<code>x</code>相同。<br>&emsp;&emsp;<code>bias_add</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.bias_add(value, bias, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是将偏差项<code>bias</code>加到<code>value</code>上面。这个操作你可以看做是<code>tf.add</code>的一个特例，其中<code>bias</code>必须是一维的。该<code>API</code>支持广播形式，因此<code>value</code>可以有任何维度。但是，该<code>API</code>又不像<code>tf.add</code>，可以让<code>bias</code>的维度和<code>value</code>的最后一维不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">1.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">1.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.nn.bias_add(a, b)))</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line"><span class="comment"># 因为a最后一维的维度是2，但是c的维度是1，所以以下语句将发生错误</span></span><br><span class="line"><span class="comment"># print(sess.run(tf.nn.bias_add(a, c)))</span></span><br><span class="line">print(sess.run(tf.add(a, c)))  <span class="comment"># 但是tf.add可以正确运行</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int64</code>、<code>int32</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>或者<code>complex64</code>。</li>
<li><code>bias</code>：一个一维的<code>Tensor</code>，数据维度和<code>value</code>的最后一维相同，数据类型必须和<code>value</code>相同。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>sigmoid</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sigmoid(x, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的<code>sigmoid</code>函数，具体计算公式为<code>y = 1 / (1 + exp(-x))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.sigmoid(a)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7310586</span> <span class="number">0.880797</span> ]</span><br><span class="line"> [<span class="number">0.7310586</span> <span class="number">0.880797</span> ]</span><br><span class="line"> [<span class="number">0.7310586</span> <span class="number">0.880797</span> ]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int32</code>、<code>complex64</code>、<code>int64</code>或者<code>qint32</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，如果<code>x.dtype != qint32</code>，那么返回的数据类型和<code>x</code>相同，否则返回的数据类型是<code>quint8</code>。<br>&emsp;&emsp;<code>tanh</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.tanh(x, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的<code>tanh</code>函数。具体计算公式为<code>(exp(x) - exp(-x))/(exp(x) + exp(-x))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.tanh(a)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7615942</span> <span class="number">0.9640276</span>]</span><br><span class="line"> [<span class="number">0.7615942</span> <span class="number">0.9640276</span>]</span><br><span class="line"> [<span class="number">0.7615942</span> <span class="number">0.9640276</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int32</code>、<code>complex64</code>、<code>int64</code>或者<code>qint32</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，如果<code>x.dtype != qint32</code>，那么返回的数据类型和<code>x</code>相同，否则返回的数据类型是<code>quint8</code>。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>&emsp;&emsp;卷积操作是使用一个二维的卷积核在一个批处理的图片上进行不断扫描。具体操作是将一个卷积核在每张图片上按照一个合适的尺寸在每个通道上面进行扫描。为了达到好的卷积效率，需要在不同的通道和不同的卷积核之间进行权衡。</p>
<ul>
<li><code>conv2d</code>：任意的卷积核，能同时在不同的通道上面进行卷积操作。</li>
<li><code>depthwise_conv2d</code>：卷积核能相互独立的在自己的通道上面进行卷积操作。</li>
<li><code>separable_conv2d</code>：在纵深卷积(<code>depthwise filter</code>)之后进行逐点卷积(<code>separable filter</code>)。</li>
</ul>
<p>注意，虽然这些操作被称之为<code>卷积</code>，但是严格的说，他们只是互相关，因为卷积核没有做一个逆向的卷积过程。<br>&emsp;&emsp;卷积核的卷积过程是按照<code>strides</code>参数来确定的，比如<code>strides = [1, 1, 1, 1]</code>表示卷积核对每个像素点进行卷积，即在二维屏幕上面，两个轴方向的步长都是<code>1</code>。<code>strides = [1, 2, 2, 1]</code>表示卷积核对每隔一个像素点进行卷积，即在二维屏幕上面，两个轴方向的步长都是<code>2</code>。<br>&emsp;&emsp;<code>conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是对一个四维的输入数据<code>input</code>和四维的卷积核<code>filter</code>进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果。<br>&emsp;&emsp;给定的输入张量的维度是<code>[batch, in_height, in_width, in_channels]</code>，卷积核张量的维度是<code>[filter_height, filter_width, in_channels, out_channels]</code>，具体卷积操作如下：</p>
<ol>
<li>将卷积核的维度转换成一个二维的矩阵形状<code>[filter_height * filter_width * in_channels, output_channels]</code>。</li>
<li>对于每个批处理的图片，我们将输入张量转换成一个临时的数据维度<code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>。</li>
<li>对于每个批处理的图片，我们右乘以卷积核，得到最后的输出结果。</li>
</ol>
<p>注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>或者<code>float64</code>。</li>
<li><code>filter</code>：一个<code>Tensor</code>，数据类型必须是<code>input</code>相同。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>use_cudnn_on_gpu</code>：一个可选布尔值，默认情况下是<code>True</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型是<code>input</code>相同。<br>&emsp;&emsp;<code>depthwise_conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.depthwise_conv2d(input, filter, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数也是一个卷积操作。给定一个输入张量，数据维度是<code>[batch, in_height, in_width, in_channels]</code>，一个卷积核的维度是<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>，在通道<code>in_channels</code>上面的卷积深度是<code>1</code>(我的理解是在每个通道上单独进行卷积)，<code>depthwise_conv2d</code>函数将不同的卷积核独立的应用在<code>in_channels</code>的每个通道上(从通道<code>1</code>到通道<code>channel_multiplier</code>)，然后把所以的结果进行汇总。最后输出通道的总数是<code>in_channels * channel_multiplier</code>。<br>&emsp;&emsp;注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维<code>[batch, in_height, in_width, in_channels]</code>。</li>
<li><code>filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如，<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>use_cudnn_on_gpu</code>：一个可选布尔值，默认情况下是<code>True</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个四维的<code>Tensor</code>，数据维度为<code>[batch, out_height, out_width, in_channels * channel_multiplier]</code>。<br>&emsp;&emsp;<code>separable_conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用几个分离的卷积核去做卷积，下图是常规卷积和分离卷积的区别：</p>
<p><img src="/2019/02/14/深度学习/TensorFlow之nn的API/1.png" height="210" width="560"></p>
<p>这个卷积是为了避免卷积核在全通道的情况下进行卷积，这样非常浪费时间。使用这个<code>API</code>，你将应用一个二维的卷积核，在每个通道上，以深度<code>channel_multiplier</code>进行卷积。其实如上图<code>Separable Convolution</code>中，就是先利用<code>depthwise_filter</code>，将<code>ID</code>的通道数映射到<code>ID * DM</code>的通道数上面，之后从<code>ID * DM</code>的通道数映射到<code>OD</code>的通道数上面，这也就是上面说的深度<code>channel_multiplier</code>对应于<code>DM</code>。<br>&emsp;&emsp;<code>strides</code>只是仅仅控制<code>depthwise convolution</code>的卷积步长，因为<code>pointwise convolution</code>的卷积步长是确定的<code>[1, 1, 1, 1]</code>。注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">depthwise_filter = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">pointwise_filter = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">20</span>), dtype=np.float32)</span><br><span class="line"><span class="comment"># out_channels &gt;= channel_multiplier * in_channels</span></span><br><span class="line">y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维<code>[batch, in_height, in_width, in_channels]</code>。</li>
<li><code>depthwise_filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>。其中，<code>in_channels</code>的卷积深度是<code>1</code>。</li>
<li><code>pointwise_filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[1, 1, channel_multiplier * in_channels, out_channels]</code>。其中，<code>pointwise_filter</code>是在<code>depthwise_filter</code>卷积之后的混合卷积。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如，<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个四维的<code>Tensor</code>，数据维度为<code>[batch, out_height, out_width, out_channels]</code>。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>&emsp;&emsp;池化操作是利用一个矩阵窗口在输入张量上进行扫描，并且将每个矩阵窗口中的值通过取最大值，平均值或者<code>XXXX</code>来减少元素个数。每个池化操作的矩阵窗口大小是由<code>ksize</code>来指定的，并且根据步长参数<code>strides</code>来决定移动步长。比如，如果<code>strides</code>中的值都是<code>1</code>，那么每个矩阵窗口都将被使用。如果<code>strides</code>中的值都是<code>2</code>，那么每一维度上的矩阵窗口都是每隔一个被使用。以此类推。更具体的输出结果是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output[i] = reduce(value[strides * i: strides * i + ksize])</span><br></pre></td></tr></table></figure>
<p>输出数据维度是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shape(output) = (shape(value) - ksize + <span class="number">1</span>) / strides</span><br></pre></td></tr></table></figure>
<p>其中，取舍方向取决于参数<code>padding</code>：</p>
<ul>
<li><code>padding = &#39;SAME&#39;</code>：向下取舍，仅适用于全尺寸操作，即输入数据维度和输出数据维度相同。</li>
<li><code>padding = &#39;VALID</code>：向上取舍，适用于部分窗口，即输入数据维度和输出数据维度不同。</li>
</ul>
<p>&emsp;&emsp;<code>avg_pool</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.avg_pool(value, ksize, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.avg_pool(value=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>、<code>float64</code>、<code>qint8</code>、<code>quint8</code>和<code>qint32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>max_pool</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool(value, ksize, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.max_pool(value=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>、<code>float64</code>、<code>qint8</code>、<code>quint8</code>或<code>qint32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>max_pool_with_argmax</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax = <span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置。因为在计算位置<code>argmax</code>的时候，我们将<code>input</code>铺平了进行计算，所以如果<code>input = [b, y, x, c]</code>，那么索引位置是<code>((b * height + y) * width + x) * channels + c</code>。<br>&emsp;&emsp;使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output, argmax = tf.nn.max_pool_with_argmax(input=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>Targmax</code>：一个可选的数据类型，即<code>tf.int32</code>或者<code>tf.int64</code>。默认情况下是<code>tf.int64</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个元祖张量<code>(output, argmax)</code>：</p>
<ul>
<li><code>output</code>：一个<code>Tensor</code>，数据类型是<code>float32</code>，表示池化区域的最大值。</li>
<li><code>argmax</code>：一个<code>Tensor</code>，数据类型是<code>Targmax</code>，数据维度是四维的。</li>
</ul>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>&emsp;&emsp;标准化是能防止模型过拟合的好方法，特别是在大数据的情况下。<br>&emsp;&emsp;<code>l2_normalize</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_normalize(x, dim, epsilon=<span class="number">1e-12</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用<code>L2</code>范数对指定维度<code>dim</code>进行标准化。比如，对于一个一维的张量，指定维度<code>dim = 0</code>，那么计算结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = x / sqrt(max(sum(x ** <span class="number">2</span>), epsilon))</span><br></pre></td></tr></table></figure>
<p>假设<code>x</code>是多维度的，那么标准化只会独立的对维度<code>dim</code>进行，不会影响到别的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.l2_normalize(input_data, dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>dim</code>：需要标准化的维度。</li>
<li><code>epsilon</code>：一个很小的值，确定标准化的下边界。如果<code>norm &lt; sqrt(epsilon)</code>，那么我们将使用<code>sqrt(epsilon)</code>进行标准化。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和x相同。<br>&emsp;&emsp;<code>local_response_normalization</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.local_response_normalization(input, depth_radius=<span class="keyword">None</span>, bias=<span class="keyword">None</span>, alpha=<span class="keyword">None</span>, beta=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算局部数据标准化。输入的数据<code>input</code>是一个四维的张量，但该张量被看做是一个一维的向量(<code>input</code>的最后一维作为向量)，向量中的每一个元素都是一个三维的数组(对应<code>input</code>的前三维)。向量的每一个元素都是独立的被标准化的。具体数学形式如下：<br>&emsp;&emsp;使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.local_response_normalization(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维的，数据类型是<code>float32</code>。</li>
<li><code>depth_radius</code>：可选项，一个整型，默认情况下是<code>5</code>。</li>
<li><code>bias</code>：可选项，一个浮点型，默认情况下是<code>1</code>。一个偏移项，为了避免除<code>0</code>，一般情况下取正值。</li>
<li><code>alpha</code>：可选项，一个浮点型，默认情况下是<code>1</code>。一个比例因子，一般情况下取正值。</li>
<li><code>beta</code>：可选项，一个浮点型，默认情况下是0.5。一个指数。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型是<code>float32</code>。<br>&emsp;&emsp;<code>moments</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.moments(x, axes, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的均值和方差。沿着<code>axes</code>维度，计算<code>x</code>的均值和方差。如果<code>x</code>是一维的，并且<code>axes = [0]</code>，那么就是计算整个向量的均值和方差。如果我们取<code>axes = [0, 1, 2]</code>(<code>batch, height, width</code>)，那么我们就是计算卷积的全局标准化。如果只是计算批处理的标准化，那么我们取<code>axes = [0]</code>(<code>batch</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">mean, variance = tf.nn.moments(input_data, [<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(mean))</span><br><span class="line">    print(sess.run(tf.shape(mean)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>axes</code>：一个整型的数组，确定计算均值和方差的维度。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出两个<code>Tensor</code>，分别是均值<code>mean</code>和方差<code>variance</code>。</p>
<h3 id="误差值"><a href="#误差值" class="headerlink" title="误差值"></a>误差值</h3><p>&emsp;&emsp;度量两个张量或者一个张量和零之间的损失误差，这个可用于在一个回归任务或者用于正则的目的(权重衰减)。<br>&emsp;&emsp;<code>l2_loss</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_loss(t, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用<code>L2</code>范数来计算张量的误差值，但是没有开方并且只取<code>L2</code>范数的值的一半：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = sum(t ** <span class="number">2</span>) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.l2_loss(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.03965811</span> <span class="number">0.9202959</span>  <span class="number">0.83564</span>   ]</span><br><span class="line"> [<span class="number">0.23268144</span> <span class="number">0.77983814</span> <span class="number">0.8602118</span> ]]</span><br><span class="line"><span class="number">1.474532</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>t</code>：一个<code>Tensor</code>。数据类型必须是以下之一：<code>float32</code>、<code>float64</code>、<code>int64</code>、<code>int32</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>、<code>complex64</code>、<code>qint8</code>、<code>quint8</code>或<code>qint32</code>。虽然一般情况下，数据维度是二维的。但是，数据维度可以取任意维度。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>t</code>相同，是一个标量。</p>
<h3 id="分类操作"><a href="#分类操作" class="headerlink" title="分类操作"></a>分类操作</h3><p>&emsp;&emsp;<code>sigmoid_cross_entropy_with_logits</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>logits</code>经<code>sigmoid</code>函数激活之后的交叉熵。对于一个不相互独立的离散分类任务，这个函数作用是去度量概率误差。比如在一张图片中，同时包含多个分类目标(大象和狗)，那么就可以使用这个函数。<br>&emsp;&emsp;为了描述简洁，我们规定<code>x = logits</code>，<code>z = targets</code>，那么<code>Logistic</code>损失值为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x - x * z + log(<span class="number">1</span> + exp(-x))</span><br></pre></td></tr></table></figure>
<p>为了确保计算稳定，避免溢出，真实的计算实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max(x, <span class="number">0</span>) - x * z + log(<span class="number">1</span> + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<p><code>logits</code>和<code>targets</code>必须有相同的数据类型和数据维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.random.rand传入一个shape，返回一个在[0,1)区间符合均匀分布的array</span></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.sigmoid_cross_entropy_with_logits(logits=input_data, labels=[[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))  <span class="comment"># 结果为[[0.3589966 1.1557628 0.9552358]]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个<code>Tensor</code>，数据类型是<code>float32</code>或者<code>float64</code>之一。</li>
<li><code>targets</code>：一个<code>Tensor</code>，数据类型和数据维度都和<code>logits</code>相同。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和<code>logits</code>相同。<br>&emsp;&emsp;<code>softmax</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(logits, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>softmax</code>激活函数。对于每个批<code>i</code>和分类<code>j</code>，我们可以得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))</span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable([[<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.9</span>]], dtype=tf.float32)</span><br><span class="line">output = tf.nn.softmax(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.2</span> <span class="number">0.1</span> <span class="number">0.9</span>]]</span><br><span class="line">[[<span class="number">0.25519383</span> <span class="number">0.23090893</span> <span class="number">0.51389724</span>]]</span><br><span class="line">[<span class="number">1</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个<code>Tensor</code>。数据类型是<code>float32</code>或者<code>float64</code>之一。数据维度是二维<code>[batch_size, num_classes]</code>。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和数据类型都和<code>logits</code>相同。<br>&emsp;&emsp;<code>softmax_cross_entropy_with_logits</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>logits</code>经<code>softmax</code>函数激活之后的交叉熵。对于每个独立的分类任务，这个函数是去度量概率误差。比如，在<code>CIFAR-10</code>数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车，但绝对不可能两者都在一张图中(这也是和<code>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)</code>这个<code>API</code>的区别)。<br>&emsp;&emsp;输入<code>API</code>的数据<code>logits</code>不能进行缩放，因为在这个<code>API</code>的执行中会进行<code>softmax</code>计算，如果<code>logits</code>进行了缩放，那么会影响计算正确率。不要调用这个<code>API</code>去计算<code>softmax</code>的值，因为这个<code>API</code>最终输出的结果并不是经过<code>softmax</code>函数的值。<br>&emsp;&emsp;<code>logits</code>和<code>labels</code>必须有相同的数据维度<code>[batch_size, num_classes]</code>，和相同的数据类型(<code>float32</code>或者<code>float64</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable([[<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.9</span>]], dtype=tf.float32)</span><br><span class="line">output = tf.nn.softmax_cross_entropy_with_logits(logits=input_data, labels=[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.2</span> <span class="number">0.1</span> <span class="number">0.9</span>]]</span><br><span class="line">[<span class="number">1.365732</span>]</span><br><span class="line">[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个没有缩放的对数张量。</li>
<li><code>labels</code>：每一行<code>labels[i]</code>必须是一个有效的概率分布值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度是一维的，长度是<code>batch_size</code>，数据类型都和<code>logits</code>相同。</p>
<h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h3><p>&emsp;&emsp;<code>Tensorflow</code>提供了从张量中嵌入查找的库。<br>&emsp;&emsp;`embedding_lookup函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(params, ids, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是查询<code>params</code>中索引是<code>ids</code>的值。这个操作是<code>tf.gather</code>的一个泛化，但它可以被并行计算处理，其中<code>params</code>被认为是一个大型的张量库，<code>ids</code>中的值对应于各个分区。如果<code>len(params) &gt; 1</code>，<code>ids</code>中每个元素<code>id</code>对应于<code>params</code>中每个分区<code>p</code>，即<code>p = id % len(params)</code>。那么，我们得到的每个切片是<code>params[p][id // len(params), ...]</code>。最后得到的切片结果被重新连接成一个稠密张量，最后返回的张量维度是<code>shape(ids) + shape(params)[1:]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">params = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">ids = tf.constant([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">output = tf.nn.embedding_lookup(params, ids)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">'params:'</span>, sess.run(params))</span><br><span class="line">    print(<span class="string">'-------------------------------'</span>)</span><br><span class="line">    print(<span class="string">'输出第0行和第2行:'</span>, sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params: [[<span class="number">0.59815351</span> <span class="number">0.65806083</span> <span class="number">0.1297678</span>  <span class="number">0.8921319</span> ]</span><br><span class="line"> [<span class="number">0.29017073</span> <span class="number">0.32162826</span> <span class="number">0.12856839</span> <span class="number">0.14271805</span>]</span><br><span class="line"> [<span class="number">0.3123268</span>  <span class="number">0.8437347</span>  <span class="number">0.49344986</span> <span class="number">0.29818202</span>]]</span><br><span class="line">-------------------------------</span><br><span class="line">输出第<span class="number">0</span>行和第<span class="number">2</span>行: [[<span class="number">0.59815351</span> <span class="number">0.65806083</span> <span class="number">0.1297678</span>  <span class="number">0.8921319</span> ]</span><br><span class="line"> [<span class="number">0.3123268</span>  <span class="number">0.8437347</span>  <span class="number">0.49344986</span> <span class="number">0.29818202</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>params</code>：一个拥有相同数据维度和数据类型的张量。</li>
<li><code>ids</code>：一个张量，数据类型是<code>int32</code>。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>params</code>相同。</p>
<h3 id="评估操作"><a href="#评估操作" class="headerlink" title="评估操作"></a>评估操作</h3><p>&emsp;&emsp;评估操作对于测量网络的性能是有用的。由于它们是不可微分的，所以它们通常只是被用在评估阶段。<br>&emsp;&emsp;<code>top_k</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.top_k(input, k, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是返回<code>input</code>中每行最大的k个数，并且返回它们所在位置的索引。<code>value(i, j)</code>表示输入数据<code>input(i)</code>的第<code>j</code>大的元素。<code>indices(i, j)</code>给出对应元素的列索引，即<code>input(i, indices(i, j)) = values(i, j)</code>。如果遇到两个相等的元素，那么我们先取索引小的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">k = <span class="number">2</span></span><br><span class="line">output = tf.nn.top_k(input, k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(input))</span><br><span class="line">    print(<span class="string">'--------------------'</span>)</span><br><span class="line">    print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.9522081</span>  <span class="number">0.59109382</span> <span class="number">0.44528462</span> <span class="number">0.34926363</span>]</span><br><span class="line"> [<span class="number">0.46024959</span> <span class="number">0.59693116</span> <span class="number">0.36231259</span> <span class="number">0.55662777</span>]</span><br><span class="line"> [<span class="number">0.37875119</span> <span class="number">0.4205928</span>  <span class="number">0.43581744</span> <span class="number">0.489631</span>  ]]</span><br><span class="line">--------------------</span><br><span class="line">TopKV2(values=array([[<span class="number">0.9522081</span> , <span class="number">0.59109382</span>],</span><br><span class="line">       [<span class="number">0.59693116</span>, <span class="number">0.55662777</span>],</span><br><span class="line">       [<span class="number">0.489631</span>  , <span class="number">0.43581744</span>]]), indices=array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">2</span>]]))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个张量，数据类型必须是以下之一：<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>。数据维度是<code>batch_size</code>乘上<code>x</code>个类别。</li>
<li><code>k</code>：一个整型，必须大于等于<code>1</code>。在每行中，查找最大的<code>k</code>个值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个元组<code>Tensor</code>，数据元素是<code>(values, indices)</code>：</p>
<ul>
<li><code>values</code>：一个张量，数据类型和<code>input</code>相同。数据维度是<code>batch_size</code>乘上<code>k</code>个最大值。</li>
<li><code>indices</code>：一个张量，数据类型是<code>int32</code>。每个最大值在<code>input</code>中的索引位置。</li>
</ul>
<p>&emsp;&emsp;<code>in_top_k</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.in_top_k(predictions, targets, k, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是返回一个布尔向量，说明目标值是否存在于预测值之中。输出数据是一个<code>batch_size</code>长度的布尔向量，如果目标值存在于预测值之中，那么<code>out[i] = true</code>。注意，<code>targets</code>是<code>predictions</code>中的索引位，并不是<code>predictions</code>中具体的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>), tf.float32)</span><br><span class="line">k = <span class="number">2</span></span><br><span class="line">output = tf.nn.in_top_k(input, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(input))</span><br><span class="line">    print(<span class="string">'--------------------'</span>)</span><br><span class="line">    print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.34494555</span> <span class="number">0.9111343</span>  <span class="number">0.93085057</span> <span class="number">0.419403</span>  ]</span><br><span class="line"> [<span class="number">0.14929643</span> <span class="number">0.46961188</span> <span class="number">0.72727567</span> <span class="number">0.04639981</span>]</span><br><span class="line"> [<span class="number">0.13701458</span> <span class="number">0.83330005</span> <span class="number">0.33437857</span> <span class="number">0.1281736</span> ]]</span><br><span class="line">--------------------</span><br><span class="line">[<span class="keyword">False</span> <span class="keyword">False</span> <span class="keyword">False</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>predictions</code>：一个张量，数据类型是<code>float32</code>。数据维度是<code>batch_size</code>乘上<code>x</code>个类别。</li>
<li><code>targets</code>：一个张量，数据类型是<code>int32</code>。一个长度是<code>batch_size</code>的向量，里面的元素是目标<code>class ID</code>。</li>
<li><code>k</code>：一个整型。在每行中，查找最大的<code>k</code>个值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个张量，数据类型是<code>bool</code>，判断是否预测正确。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之函数总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之函数总结/" itemprop="url">TensorFlow之函数总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T19:48:39+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="get-shape函数"><a href="#get-shape函数" class="headerlink" title="get_shape函数"></a>get_shape函数</h3><p>&emsp;&emsp;<code>get_shape</code>函数主要用于获取一个张量的维度，并且输出张量每个维度上面的值。如果是二维矩阵，也就是输出行和列的值，使用非常方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    A = tf.random_normal(shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    print(A.get_shape())</span><br><span class="line">    print(A.get_shape)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">&lt;bound method Tensor.get_shape of &lt;tf.Tensor <span class="string">'random_normal:0'</span> shape=(<span class="number">3</span>, <span class="number">4</span>) dtype=float32&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>第一个输出是一个元祖，就是数值，而第二输出就是一个张量的对象，里面包含更多的东西。如果你需要输出某一个维度上面的值，那就用下面的这种方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.get_shape()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这就表示第一个维度。该函数经常和<code>as_list</code>一起使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">varX = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">varX_shape = tf.shape(varX)</span><br><span class="line">print(sess.run(varX_shape))  <span class="comment"># 输出“[3 3]”</span></span><br><span class="line">varX_shape = varX.get_shape().as_list()</span><br><span class="line">print(varX_shape)  <span class="comment"># 输出“[3, 3]”</span></span><br></pre></td></tr></table></figure>
<h3 id="random-normal函数"><a href="#random-normal函数" class="headerlink" title="random_normal函数"></a>random_normal函数</h3><p>&emsp;&emsp;<code>tf.random_normal</code>函数用于从服从指定正态分布的数值中取出指定个数的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random_normal(shape, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32, seed=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>shape</code>：输出张量的形状。</li>
<li><code>mean</code>：正态分布的均值。</li>
<li><code>stddev</code>是正态分布的标准差。</li>
<li><code>dtype</code>：输出的类型。</li>
<li><code>seed</code>：随机数种子，是一个整数。</li>
<li><code>name</code>：操作的名称。</li>
</ul>
<p>&emsp;&emsp;以下程序定义一个<code>w1</code>变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(w1)</span><br><span class="line">    print(sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable <span class="string">'Variable:0'</span> shape=(<span class="number">2</span>, <span class="number">3</span>) dtype=float32_ref&gt;</span><br><span class="line">[[<span class="number">-0.8113182</span>   <span class="number">1.4845988</span>   <span class="number">0.06532937</span>]</span><br><span class="line"> [<span class="number">-2.4427042</span>   <span class="number">0.0992484</span>   <span class="number">0.5912243</span> ]]</span><br></pre></td></tr></table></figure>
<p>变量<code>w1</code>声明之后并没有被赋值，需要在<code>Session</code>中调用<code>run(tf.global_variables_initializer())</code>方法初始化之后才会被具体赋值。<code>tf</code>中张量与常规向量不同的是，执行<code>print(w1)</code>输出的是<code>w1</code>的形状和数据类型等属性信息，获取<code>w1</code>的值需要调用<code>sess.run(w1)</code>方法。</p>
<h3 id="TensorFlow的算术操作"><a href="#TensorFlow的算术操作" class="headerlink" title="TensorFlow的算术操作"></a>TensorFlow的算术操作</h3><p>&emsp;&emsp;<code>TensorFlow</code>的算术操作如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tf.add(x, y, name=None)</code></td>
<td>求和</td>
</tr>
<tr>
<td><code>tf.multiply(x, y, name=None)</code></td>
<td>减法</td>
</tr>
<tr>
<td><code>tf.multiply(x, y, name=None)</code></td>
<td>乘法</td>
</tr>
<tr>
<td><code>tf.div(x, y, name=None)</code>，推荐使用<code>tf.divide</code></td>
<td>除法</td>
</tr>
<tr>
<td><code>tf.mod(x, y, name=None)</code></td>
<td>取模</td>
</tr>
<tr>
<td><code>tf.abs(x, name=None)</code></td>
<td>求绝对值</td>
</tr>
<tr>
<td><code>tf.negative(x, name=None)</code></td>
<td>取负(<code>y = -x</code>)</td>
</tr>
<tr>
<td><code>tf.sign(x, name=None)</code></td>
<td>返回符号</td>
</tr>
<tr>
<td><code>tf.inv(x, name=None)</code></td>
<td>取反</td>
</tr>
<tr>
<td><code>tf.square(x, name=None)</code></td>
<td>计算平方</td>
</tr>
<tr>
<td><code>tf.round(x, name=None)</code></td>
<td>四舍五入最接近的整数</td>
</tr>
<tr>
<td><code>tf.sqrt(x, name=None)</code></td>
<td>开根号</td>
</tr>
<tr>
<td><code>tf.pow(x, y, name=None)</code></td>
<td>幂次方</td>
</tr>
<tr>
<td><code>tf.exp(x, name=None)</code></td>
<td>计算<code>e</code>的次方</td>
</tr>
<tr>
<td><code>tf.log(x, name=None)</code></td>
<td>计算<code>log</code></td>
</tr>
<tr>
<td><code>tf.maximum(x, y, name=None)</code></td>
<td>返回最大值</td>
</tr>
<tr>
<td><code>tf.minimum(x, y, name=None)</code></td>
<td>返回最小值</td>
</tr>
<tr>
<td><code>tf.cos(x, name=None)</code></td>
<td>三角函数<code>cos</code></td>
</tr>
<tr>
<td><code>tf.sin(x, name=None)</code></td>
<td>三角函数<code>sin</code></td>
</tr>
<tr>
<td><code>tf.tan(x, name=None)</code></td>
<td>三角函数<code>tan</code></td>
</tr>
<tr>
<td><code>tf.atan(x, name=None)</code></td>
<td>三角函数<code>atan</code></td>
</tr>
<tr>
<td><code>tf.sign(x, name=None)</code></td>
<td><code>y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0</code></td>
</tr>
<tr>
<td><code>tf.round(x, name=None)</code></td>
<td><code>a is [0.9, 2.5, 2.3, -4.4], tf.round(a) is [1.0, 3.0, 2.0, -4.0]</code></td>
</tr>
<tr>
<td><code>tf.pow(x, y, name=None)</code></td>
<td><code>x is [[2, 2], [3, 3]], y is [[8, 16], [2, 3]], tf.pow(x, y) is [[256, 65536], [9, 27]]</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="assign函数"><a href="#assign函数" class="headerlink" title="assign函数"></a>assign函数</h3><p>&emsp;&emsp;<code>tf.assign(A, new_number)</code>的功能是把<code>A</code>的值变为<code>new_number</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)  <span class="comment"># 定义常量one</span></span><br><span class="line">new_value = tf.add(state, one)  <span class="comment"># 定义加法步骤(注意，此步并没有直接计算)</span></span><br><span class="line">update = tf.assign(state, new_value)  <span class="comment"># 将State更新成new_value</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>assign</code>函数也可以用于给图变量赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">In [<span class="number">2</span>]: v = tf.Variable(<span class="number">3</span>, name=<span class="string">'v'</span>)</span><br><span class="line">In [<span class="number">3</span>]: v2 = v.assign(<span class="number">5</span>)</span><br><span class="line">In [<span class="number">4</span>]: sess = tf.InteractiveSession()</span><br><span class="line">In [<span class="number">5</span>]: sess.run(v.initializer)</span><br><span class="line">In [<span class="number">6</span>]: sess.run(v)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">3</span></span><br><span class="line">In [<span class="number">7</span>]: sess.run(v2)</span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-contrib-layers-separable-conv2d"><a href="#tf-contrib-layers-separable-conv2d" class="headerlink" title="tf.contrib.layers.separable_conv2d"></a>tf.contrib.layers.separable_conv2d</h3><p>&emsp;&emsp;Aliases:</p>
<ul>
<li>tf.contrib.layers.separable_conv2d</li>
<li>tf.contrib.layers.separable_convolution2d</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.separable_conv2d(</span><br><span class="line">    inputs, num_outputs, kernel_size, depth_multiplier, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=DATA_FORMAT_NHWC, rate=<span class="number">1</span>, activation_fn=tf.nn.relu, normalizer_fn=<span class="keyword">None</span>,</span><br><span class="line">    normalizer_params=<span class="keyword">None</span>, weights_initializer=initializers.xavier_initializer(),</span><br><span class="line">    weights_regularizer=<span class="keyword">None</span>, biases_initializer=tf.zeros_initializer(), biases_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    reuse=<span class="keyword">None</span>, variables_collections=<span class="keyword">None</span>, outputs_collections=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/layers.py</code>. Adds a <code>depth-separable</code> <code>2D</code> convolution with optional <code>batch_norm</code> layer.<br>&emsp;&emsp;This op first performs a depthwise convolution that acts separately on channels, creating a variable called <code>depthwise_weights</code>. If <code>num_outputs</code> is not <code>None</code>, it adds a pointwise convolution that mixes channels, creating a variable called <code>pointwise_weights</code>. Then, if <code>normalizer_fn</code> is <code>None</code>, it adds bias to the result, creating a variable called <code>biases</code>, otherwise, <code>the normalizer_fn</code> is applied. It finally applies an activation function to produce the end result.</p>
<ul>
<li><code>inputs</code>: A tensor of size <code>[batch_size, height, width, channels]</code>.</li>
<li><code>num_outputs</code>: The number of pointwise convolution output filters. If <code>num_outputs</code> is <code>None</code>, then we skip the pointwise convolution stage.</li>
<li><code>kernel_size</code>: A list of length <code>2</code>: <code>[kernel_height, kernel_width]</code> of the filters. Can be an <code>int</code> if both values are the same.</li>
<li><code>depth_multiplier</code>: The number of depthwise convolution output channels for each input channel.</li>
<li><code>stride</code>: A list of length <code>2</code>: <code>[stride_height, stride_width]</code>, specifying the depthwise convolution stride. Can be an <code>int</code> if both strides are the same.</li>
<li><code>padding</code>: One of <code>VALID</code> or <code>SAME</code>.</li>
<li><code>data_format</code>: A string. <code>NHWC</code> (default) and <code>NCHW</code> are supported.</li>
<li><code>rate</code>: A list of length <code>2</code>: <code>[rate_height, rate_width]</code>, specifying the dilation rates for atrous convolution. Can be an <code>int</code> if both rates are the same. If any value is larger than one, then both stride values need to be one.</li>
<li><code>activation_fn</code>: Activation function. The default value is a <code>ReLU</code> function. Explicitly set it to <code>None</code> to skip it and maintain a linear activation.</li>
<li><code>normalizer_fn</code>: Normalization function to use instead of biases. If <code>normalizer_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and biases are not created nor added. default set to <code>None</code> for no normalizer function</li>
<li><code>normalizer_params</code>: Normalization function parameters.</li>
<li><code>weights_initializer</code>: An initializer for the weights.</li>
<li><code>weights_regularizer</code>: Optional regularizer for the weights.</li>
<li><code>biases_initializer</code>: An initializer for the biases. If <code>None</code>, skip biases.</li>
<li><code>biases_regularizer</code>: Optional regularizer for the biases.</li>
<li><code>reuse</code>: Whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li>
<li><code>variables_collections</code>: Optional list of collections for all the variables or a dictionary containing a different list of collection per variable.</li>
<li><code>outputs_collections</code>: Collection to add the outputs.</li>
<li><code>trainable</code>: Whether or not the variables should be trainable or not.</li>
<li><code>scope</code>: Optional scope for <code>variable_scope</code>.</li>
</ul>
<p>Returns a <code>Tensor</code> representing the output of the operation.</p>
<h3 id="tf-contrib-layers-xavier-initializer"><a href="#tf-contrib-layers-xavier-initializer" class="headerlink" title="tf.contrib.layers.xavier_initializer"></a>tf.contrib.layers.xavier_initializer</h3><p>&emsp;&emsp;Aliases:</p>
<ul>
<li>tf.contrib.layers.xavier_initializer</li>
<li>tf.contrib.layers.xavier_initializer_conv2d</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.xavier_initializer(uniform=<span class="keyword">True</span>, seed=<span class="keyword">None</span>, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/initializers.py</code>. Returns an initializer performing <code>Xavier</code> initialization for weights. This function implements the weight initialization from <code>Xavier Glorot</code> and <code>Yoshua Bengio</code> (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.<br>&emsp;&emsp;This initializer is designed to keep the scale of the gradients roughly the same in all layers.</p>
<ul>
<li><code>uniform</code>: Whether to use uniform or normal distributed random initialization.</li>
<li><code>seed</code>: A <code>Python</code> integer. Used to create random seeds. See <code>tf.set_random_seed</code> for behavior.</li>
<li><code>dtype</code>: The data type. Only floating point types are supported.</li>
</ul>
<h3 id="tf-contrib-layers-l2-regularizer"><a href="#tf-contrib-layers-l2-regularizer" class="headerlink" title="tf.contrib.layers.l2_regularizer"></a>tf.contrib.layers.l2_regularizer</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.l2_regularizer(scale, scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/regularizers.py</code>. Returns a function that can be used to apply <code>L2</code> regularization to weights. Small values of <code>L2</code> can help prevent overfitting the training data.</p>
<ul>
<li><code>scale</code>: A scalar multiplier <code>Tensor</code>. <code>0.0</code> disables the regularizer.</li>
<li><code>scope</code>: An optional scope name.</li>
</ul>
<h3 id="tf-placeholder-with-default"><a href="#tf-placeholder-with-default" class="headerlink" title="tf.placeholder_with_default"></a>tf.placeholder_with_default</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder_with_default(input, shape, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/gen_array_ops.py</code>. A placeholder op that passes through input when its output is not fed.</p>
<ul>
<li><code>input</code>: A <code>Tensor</code>. The default value to produce when output is not fed.</li>
<li><code>shape</code>: A <code>tf.TensorShape</code> or list of <code>ints</code>. The (possibly partial) shape of the tensor.</li>
<li><code>name</code>: A name for the operation (optional).</li>
</ul>
<p>Returns a <code>Tensor</code>. Has the same type as input.</p>
<h3 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split"></a>tf.split</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.split(value, num_or_size_splits, axis=<span class="number">0</span>, num=<span class="keyword">None</span>, name=<span class="string">'split'</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Splits a tensor into sub tensors.<br>&emsp;&emsp;If <code>num_or_size_splits</code> is an integer type, <code>num_split</code>, then splits <code>value</code> along dimension <code>axis</code> into <code>num_split</code> smaller tensors. Requires that <code>num_split</code> evenly divides <code>value.shape[axis]</code>.<br>&emsp;&emsp;If <code>num_or_size_splits</code> is not an integer type, it is presumed to be a <code>Tensor</code> <code>size_splits</code>, then splits <code>value</code> into <code>len(size_splits)</code> pieces. The shape of the <code>i-th</code> piece has the same size as the <code>value</code> except along dimension <code>axis</code> where the size is <code>size_splits[i]</code>.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 'value' is a tensor with shape [5, 30]. Split 'value' into 3 tensors</span></span><br><span class="line"><span class="comment"># with sizes [4, 15, 11] along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, [<span class="number">4</span>, <span class="number">15</span>, <span class="number">11</span>], <span class="number">1</span>)</span><br><span class="line">tf.shape(split0)  <span class="comment"># [5, 4]</span></span><br><span class="line">tf.shape(split1)  <span class="comment"># [5, 15]</span></span><br><span class="line">tf.shape(split2)  <span class="comment"># [5, 11]</span></span><br><span class="line"><span class="comment"># Split 'value' into 3 tensors along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, num_or_size_splits=<span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">tf.shape(split0)  <span class="comment"># [5, 10]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>: The <code>Tensor</code> to split.</li>
<li><code>num_or_size_splits</code>: Either a <code>0-D</code> integer <code>Tensor</code> indicating the number of splits along <code>split_dim</code> or a <code>1-D</code> integer <code>Tensor</code> containing the sizes of each output tensor along <code>split_dim</code>. If a scalar then it must evenly divide <code>value.shape[axis]</code>; otherwise the sum of sizes along the split dimension must match that of the value.</li>
<li><code>axis</code>: A <code>0-D</code> <code>int32</code> <code>Tensor</code>. The dimension along which to split. Must be in the range <code>[-rank(value), rank(value))</code>.</li>
<li><code>num</code>: Optional, used to specify the number of outputs when it cannot be inferred from the shape of <code>size_splits</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>&emsp;&emsp;Returns: if <code>num_or_size_splits</code> is a scalar returns <code>num_or_size_splits</code> <code>Tensor</code> objects; if <code>num_or_size_splits</code> is a <code>1-D</code> <code>Tensor</code> returns <code>num_or_size_splits.get_shape[0]</code> <code>Tensor</code> objects resulting from splitting value.</p>
<h3 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape"></a>tf.reshape</h3><p>&emsp;&emsp;<code>tf.reshape(tensor, shape, name=None)</code>的作用是将<code>tensor</code>变换为参数<code>shape</code>的形式。其中<code>shape</code>为一个列表形式，特殊的一点是列表中可以存在<code>-1</code>，<code>-1</code>代表的含义是不用我们自己指定这一维的大小，函数会自动计算，但列表中只能存在一个<code>-1</code>(当然如果存在多个<code>-1</code>，就是一个存在多解的方程了)。<br>&emsp;&emsp;<code>TensorFlow</code>根据<code>shape</code>变换矩阵的方式为<code>reshape(t, shape) =&gt; reshape(t, [-1]) =&gt; reshape(t, shape)</code>，首先将矩阵t变为一维矩阵，然后再对矩阵的形式进行更改。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]. tensor 't' has shape [9]</span></span><br><span class="line">reshape(t, [3, 3]) ==&gt; [[1, 2, 3],</span><br><span class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></span><br><span class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</span><br><span class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></span><br><span class="line"><span class="comment">#                 [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3, 3],</span></span><br><span class="line"><span class="comment">#                 [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#                [[5, 5, 5],</span></span><br><span class="line"><span class="comment">#                 [6, 6, 6]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]. pass '[-1]' to flatten 't'</span></span><br><span class="line">reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</span><br><span class="line"><span class="comment"># -1 can also be used to infer the shape</span></span><br><span class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],  # -1 is inferred to be 9</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line">reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],  # -1 is inferred to be 2</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line">reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],  # -1 is inferred to be 3</span><br><span class="line">                              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                              [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">                             [[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">                              [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                              [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line">reshape(t, []) ==&gt; 7  # tensor 't' is [7]. shape `[]` reshapes to a scalar</span><br></pre></td></tr></table></figure>
<h3 id="tf-shape"><a href="#tf-shape" class="headerlink" title="tf.shape"></a>tf.shape</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.shape(input, name=<span class="keyword">None</span>, out_type=tf.int32)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Returns the shape of a tensor. This operation returns a <code>1-D</code> integer tensor representing the shape of input.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]], [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]])</span><br><span class="line">tf.shape(t)  <span class="comment"># [2, 2, 3]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li>
<li><code>name</code>: A name for the operation (optional).</li>
<li><code>out_type</code>: (Optional) The specified output type of the operation (<code>int32</code> or <code>int64</code>).</li>
</ul>
<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat"></a>tf.concat</h3><p>&emsp;&emsp;该函数用于连接两个矩阵的操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(concat_dim, values, name=<span class="string">'concat'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>concat_dim</code>：<code>tensor</code>连接的方向(维度)，<code>cancat_dim</code>维度可以不一样，其他维度的尺寸必须一样。</li>
<li><code>values</code>：两个或者一组待连接的<code>tensor</code>。</li>
<li><code>name</code>：指定该操作的<code>name</code>。</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">tf.concat(0, [t1, t2]) =&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</span><br><span class="line">tf.concat(1, [t1, t2]) =&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span><br></pre></td></tr></table></figure>
<p>这里要注意的是，如果是两个向量，它们是无法调用<code>tf.concat(1, [t1, t2])</code>来连接的，因为它们对应的<code>shape</code>只有一个维度，当然不能在第二维上连了，虽然实际中两个向量可以在行上连，但是放在程序里是会报错的。如果要连，必须要调用<code>tf.expand_dims</code>来扩维：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t1 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t2 = tf.constant([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># concated = tf.concat(1, [t1, t2])  # 这样会报错</span></span><br><span class="line">t1 = tf.expand_dims(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]),<span class="number">1</span>)</span><br><span class="line">t2 = tf.expand_dims(tf.constant([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]),<span class="number">1</span>)</span><br><span class="line">concated = tf.concat(<span class="number">1</span>, [t1, t2])  <span class="comment"># 这样就是正确的</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-add-n"><a href="#tf-add-n" class="headerlink" title="tf.add_n"></a>tf.add_n</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add_n(inputs, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/math_ops.py</code>. Adds all input tensors <code>element-wise</code>.</p>
<ul>
<li><code>inputs</code>: A list of <code>Tensor</code> objects, each with same shape and type.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>&emsp;&emsp;Returns a <code>Tensor</code> of same shape and type as the elements of <code>inputs</code>.</p>
<h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze"></a>tf.squeeze</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.squeeze(input, squeeze_dims=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Removes dimensions of size <code>1</code> from the shape of a tensor.<br>&emsp;&emsp;Given a tensor <code>input</code>, this operation returns a tensor of the same type with all dimensions of size <code>1</code> removed. If you don’t want to remove all size <code>1</code> dimensions, you can remove specific size <code>1</code> dimensions by specifying axis.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]</span></span><br><span class="line">tf.shape(tf.squeeze(t))  <span class="comment"># [2, 3]</span></span><br><span class="line">Or, to remove specific size <span class="number">1</span> dimensions:</span><br><span class="line"><span class="comment"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]</span></span><br><span class="line">tf.shape(tf.squeeze(t, [<span class="number">2</span>, <span class="number">4</span>]))  <span class="comment"># [1, 2, 3, 1]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>: A <code>Tensor</code>. The input to squeeze.</li>
<li><code>axis</code>: An optional list of <code>ints</code>. If specified, only squeezes the dimensions listed. The dimension index starts at <code>0</code>. It is an error to squeeze a dimension that is not <code>1</code>. Must be in the range <code>[-rank(input), rank(input))</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>squeeze_dims</code>: Deprecated keyword argument that is now <code>axis</code>.</li>
</ul>
<p>&emsp;&emsp;Return a <code>Tensor</code>. Has the same type as <code>input</code>. Contains the same data as <code>input</code>, but has one or more dimensions of size <code>1</code> removed.</p>
<h3 id="tf-expand-dims"><a href="#tf-expand-dims" class="headerlink" title="tf.expand_dims"></a>tf.expand_dims</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(input, axis=<span class="keyword">None</span>, name=<span class="keyword">None</span>, dim=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Inserts a dimension of <code>1</code> into a tensor’s shape.<br>&emsp;&emsp;Given a tensor <code>input</code>, this operation inserts a dimension of <code>1</code> at the dimension index <code>axis</code> of <code>input&#39;s</code> shape. The dimension index <code>axis</code> starts at <code>zero</code>; if you specify a negative number for <code>axis</code> it is counted backward from the end.<br>&emsp;&emsp;This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape <code>[height, width, channels]</code>, you can make it a batch of <code>1</code> image with <code>expand_dims(image, 0)</code>, which will make the shape <code>[1, height, width, channels]</code>.<br>&emsp;&emsp;Other examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"><span class="comment"># 't2' is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure>
<p>This operation requires that:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-1</span> - input.dims() &lt;= dim &lt;= input.dims()</span><br></pre></td></tr></table></figure>
<p>This operation is related to <code>squeeze()</code>, which removes dimensions of size <code>1</code>.</p>
<ul>
<li><code>input</code>: A <code>Tensor</code>.</li>
<li><code>axis</code>: <code>0-D</code> (scalar). Specifies the dimension index at which to expand the shape of <code>input</code>. Must be in the range <code>[-rank(input) - 1, rank(input)]</code>.</li>
<li><code>name</code>: The <code>name</code> of the output <code>Tensor</code>.</li>
<li><code>dim</code>: <code>0-D</code> (scalar). Equivalent to <code>axis</code>, to be deprecated.</li>
</ul>
<p>&emsp;&emsp;Return a <code>Tensor</code> with the same data as <code>input</code>, but its shape has an additional dimension of size <code>1</code> added.</p>
<h3 id="tf-Session-as-default"><a href="#tf-Session-as-default" class="headerlink" title="tf.Session.as_default"></a>tf.Session.as_default</h3><p>&emsp;&emsp;如果使用关键字<code>with</code>来指定会话，可以在会话中执行<code>Operation.run</code>或<code>Tensor.eval</code>，以得到运行的<code>tensor</code>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant(..)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="keyword">assert</span> tf.get_default_session() <span class="keyword">is</span> sess</span><br><span class="line">    print(c.eval())</span><br></pre></td></tr></table></figure>
<p>使用函数<code>tf.get_default_session</code>来得到当前默认的会话。需要注意的是，退出<code>as_default</code>上下文管理器时，并没有关闭该会话(<code>session</code>)，所以你必须明确地关闭会话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant(...)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line">sess.close()  <span class="comment"># 关闭会话</span></span><br></pre></td></tr></table></figure>
<p>而使用<code>with tf.Session()</code>的方式可以创建并自动关闭会话。</p>
<h3 id="错误类-Error-classes"><a href="#错误类-Error-classes" class="headerlink" title="错误类(Error classes)"></a>错误类(Error classes)</h3><p>&emsp;&emsp;错误类如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>class tf.OpError</code></td>
<td>一个基本的错误类型，在当TF执行失败时候报错</td>
</tr>
<tr>
<td><code>tf.OpError.op</code></td>
<td>返回执行失败的操作节点，有的操作(如<code>Send</code>或<code>Recv</code>)可能不会返回，那就要用用到<code>node_def</code>方法</td>
</tr>
<tr>
<td><code>tf.OpError.node_def</code></td>
<td>以<code>NodeDef proto</code>形式表示失败的<code>op</code></td>
</tr>
<tr>
<td><code>tf.OpError.error_code</code></td>
<td>描述该错误的整数错误代码</td>
</tr>
<tr>
<td><code>tf.OpError.message</code></td>
<td>返回错误信息</td>
</tr>
<tr>
<td><code>class tf.errors.CancelledError</code></td>
<td>当操作或者阶段被取消时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.UnknownError</code></td>
<td>未知错误类型</td>
</tr>
<tr>
<td><code>class tf.errors.InvalidArgumentError</code></td>
<td>在接收到非法参数时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.NotFoundError</code></td>
<td>当发现不存在所请求的一个实体时候，比如文件或目录</td>
</tr>
<tr>
<td><code>class tf.errors.AlreadyExistsError</code></td>
<td>当创建的实体已经存在的时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.PermissionDeniedError</code></td>
<td>没有执行权限做某操作的时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.FailedPreconditionError</code></td>
<td>系统没有条件执行某个行为时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.AbortedError</code></td>
<td>操作中止时报错，常常发生在并发情形</td>
</tr>
<tr>
<td><code>class tf.errors.OutOfRangeError</code></td>
<td>超出范围报错</td>
</tr>
<tr>
<td><code>class tf.errors.UnimplementedError</code></td>
<td>某个操作没有执行时报错</td>
</tr>
<tr>
<td><code>class tf.errors.InternalError</code></td>
<td>当系统经历了一个内部错误时报出</td>
</tr>
<tr>
<td><code>class tf.errors.ResourceExhaustedError</code></td>
<td>资源耗尽时报错</td>
</tr>
<tr>
<td><code>class tf.errors.DataLossError</code></td>
<td>当出现不可恢复的错误，例如在运行<code>tf.WholeFileReader.read</code>读取整个文件的同时文件被删减</td>
</tr>
<tr>
<td><code>tf.errors.XXXXX.__init__(node_def, op, message)</code></td>
<td>使用该形式方法创建以上各种错误类</td>
</tr>
</tbody>
</table>
</div>
<h3 id="tf-one-hot"><a href="#tf-one-hot" class="headerlink" title="tf.one_hot"></a>tf.one_hot</h3><p>&emsp;&emsp;该函数用于将输入转换成<code>one-hot</code>形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(indices, depth, on_value, off_value, axis)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>indices</code>：非负整数表示的标签列表，<code>len(indices)</code>就是分类的类别数。<code>tf.one_hot</code>返回的张量的阶数为<code>indeces</code>的阶数加上<code>1</code>。当<code>indices</code>的某个分量取<code>-1</code>时，即对应的向量没有独热值。</li>
<li><code>depth</code>：每个独热向量的维度。</li>
<li><code>on_value</code>：独热值。</li>
<li><code>off_value</code>：非独热值。</li>
<li><code>axis</code>：指定第几阶为<code>depth</code>维独热向量，默认为<code>-1</code>，即指定张量的最后一维为独热向量。例如对于一个<code>2</code>阶张量而言，<code>axis = 0</code>时，每个列向量是一个独热的<code>depth</code>维向量；<code>axis = 1</code>时，每个行向量是一个独热的<code>depth</code>维向量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=[<span class="number">10</span>])</span><br><span class="line">y = tf.one_hot(z, <span class="number">10</span>, on_value=<span class="number">1</span>, off_value=<span class="keyword">None</span>, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session()<span class="keyword">as</span> sess:</span><br><span class="line">    print(z)</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4</span> <span class="number">4</span> <span class="number">5</span> <span class="number">8</span> <span class="number">9</span> <span class="number">0</span> <span class="number">5</span> <span class="number">0</span> <span class="number">3</span> <span class="number">1</span>]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="tf-sparse-to-dense"><a href="#tf-sparse-to-dense" class="headerlink" title="tf.sparse_to_dense"></a>tf.sparse_to_dense</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是将一个稀疏表示转换成一个密集张量。具体将稀疏张量<code>sparse</code>转换成密集张量<code>dense</code>的步骤如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If sparse_indices is scalar</span></span><br><span class="line">dense[i] = (i == sparse_indices ? sparse_values : default_value)</span><br><span class="line"><span class="comment"># If sparse_indices is a vector, then for each i</span></span><br><span class="line">dense[sparse_indices[i]] = sparse_values[i]</span><br><span class="line"><span class="comment"># If sparse_indices is an n by d matrix, then for each i in [0, n)</span></span><br><span class="line">dense[sparse_indices[i][<span class="number">0</span>], ..., sparse_indices[i][d<span class="number">-1</span>]] = sparse_values[i]</span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>dense</code>中的填充值<code>default_value</code>都是<code>0</code>，除非该值被设置成一个标量。</p>
<ul>
<li><code>sparse_indices</code>是稀疏矩阵中那些个别元素对应的索引值，有三种情况：</li>
</ul>
<ol>
<li>如果<code>sparse_indices</code>是个数，那么它只能指定一维矩阵的某一个元素。</li>
<li>如果<code>sparse_indices</code>是个向量，那么它可以指定一维矩阵的多个元素。</li>
<li>如果<code>sparse_indices</code>是个矩阵，那么它可以指定二维矩阵的多个元素。</li>
</ol>
<ul>
<li><code>output_shape</code>是输出的稀疏矩阵的<code>shape</code>。</li>
<li><code>sparse_values</code>是个别元素的值，分为两种情况：</li>
</ul>
<ol>
<li>如果sparse_values是个数，则所有索引指定的位置都用这个数。</li>
<li>如果sparse_values是个向量，则输出矩阵的某一行向量里某一行对应的数(所以这里向量的长度应该和输出矩阵的行数对应，不然报错)。</li>
</ol>
<ul>
<li><code>default_value</code>是未指定元素的默认值，一般如果是稀疏矩阵，就是0了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span></span><br><span class="line"><span class="comment"># 真实标签，shape为[5, 1]</span></span><br><span class="line">label = tf.expand_dims(tf.constant([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]), <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 真实标签的索引，shape为[5, 1]</span></span><br><span class="line">index = tf.expand_dims(tf.range(<span class="number">0</span>, BATCH_SIZE), <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 将标签和索引tensor在第二个维度上连接起来，新的concated的shape为[5, 2]</span></span><br><span class="line">concated = tf.concat([index, label], <span class="number">1</span>)</span><br><span class="line"><span class="comment"># onehot_labels的shape为[5, 10]</span></span><br><span class="line">onehot_labels = tf.sparse_to_dense(concated, [BATCH_SIZE, <span class="number">10</span>], <span class="number">1.0</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(concated))</span><br><span class="line">    print(<span class="string">"----------------"</span>)</span><br><span class="line">    onehot1 = sess.run(onehot_labels)</span><br><span class="line">    print(onehot1)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">9</span>]]</span><br><span class="line">----------------</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;如果<code>output_shape</code>是一个行向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">predicted_class = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line">one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    onehot1 = sess.run(one_hot)</span><br><span class="line">    print(onehot1)  <span class="comment"># 输出“[0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]”</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-nn-in-top-k"><a href="#tf-nn-in-top-k" class="headerlink" title="tf.nn.in_top_k"></a>tf.nn.in_top_k</h3><p>&emsp;&emsp;<code>tf.nn.in_top_k</code>用于计算预测的结果和实际结果的是否相等，并返回一个<code>bool</code>类型的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.in_top_k(prediction, target, K)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>prediction</code>：预测的结果，大小就是预测样本的数量乘以输出的维度。</li>
<li><code>target</code>：实际样本类别的标签，大小就是样本数量的个数。</li>
<li><code>K</code>：每个样本的预测结果的前<code>k</code>个最大的数里面是否包含<code>targets</code>预测中的标签，一般都是取<code>1</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.3</span>], [<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.4</span>]]</span><br><span class="line">B = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">out = tf.nn.in_top_k(A, B, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(out))  <span class="comment"># 输出“[False  True]”</span></span><br></pre></td></tr></table></figure>
<p>因为<code>A</code>张量里面的第一个元素的最大值的标签是<code>0</code>，第二个元素的最大值的标签是<code>1</code>，但实际上是<code>1</code>和<code>1</code>，所以输出就是<code>False</code>和<code>True</code>。如果把<code>K</code>改成<code>2</code>，那么第一个元素的前面<code>2</code>个最大元素的位置是<code>0</code>和<code>1</code>，第二个元素的就是<code>1</code>和<code>2</code>。而<code>B</code>是<code>[1, 1]</code>，包含在里面，所以输出结果就是<code>True</code>和<code>True</code>。</p>
<h3 id="initialized-value"><a href="#initialized-value" class="headerlink" title="initialized_value"></a>initialized_value</h3><p>&emsp;&emsp;你有时候会需要用另一个变量的初始化值给当前变量初始化。由于<code>tf.initialize_all_variables</code>是并行地初始化所有变量，所以在有这种需求的情况下需要小心。<br>&emsp;&emsp;用其它变量的值初始化一个新的变量时，使用其它变量的<code>initialized_value</code>属性。你可以直接把已初始化的值作为新变量的初始值，或者把它当做<code>tensor</code>计算得到一个值赋予新变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a variable with a random value</span></span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=<span class="string">"weights"</span>)</span><br><span class="line"><span class="comment"># Create another variable with the same value as 'weights'</span></span><br><span class="line">w2 = tf.Variable(weights.initialized_value(), name=<span class="string">"w2"</span>)</span><br><span class="line"><span class="comment"># Create another variable with twice the value of 'weights'</span></span><br><span class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">0.2</span>, name=<span class="string">"w_twice"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tf-py-func"><a href="#tf-py-func" class="headerlink" title="tf.py_func"></a>tf.py_func</h3><p>&emsp;&emsp;这是一个可以把<code>TensorFlow</code>和<code>Python</code>原生代码无缝衔接起来的函数，有了它，你就可以在<code>TensorFlow</code>里面自由的实现你想要的功能，而不用考虑<code>TensorFlow</code>有没有实现它的<code>API</code>，并且可以帮助我们实现自由地检查该功能模块的输入输出是否正确，而不受到<code>TensorFlow</code>的先构造计算图再运行导致的不能单独检测单一模块的功能的限制。<br>&emsp;&emsp;它的具体功能描述是包装一个普通的<code>Python</code>函数，这个函数接受<code>numpy</code>的数组作为输入和输出，让这个函数可以作为<code>TensorFlow</code>计算图上的计算节点<code>OP</code>来使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">py_func(func, inp, Tout, stateful=<span class="keyword">True</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>func</code>：一个<code>Python</code>函数，它接受<code>NumPy</code>数组作为输入和输出，并且数组的类型和大小必须和输入和输出用来衔接的<code>Tensor</code>大小和数据类型相匹配。</li>
<li><code>inp</code>：输入的<code>Tensor</code>列表。</li>
<li><code>Tout</code>：输出<code>Tensor</code>数据类型的列表或元祖。</li>
<li><code>stateful</code>：状态，布尔值。</li>
<li><code>name</code>：节点<code>OP</code>的名称。</li>
</ul>
<p>&emsp;&emsp;<code>operation</code>分为有状态的与无状态的<code>operation</code>：无状态的<code>operation</code>主要进行数学计算，比如矩阵乘法、加法等，如果给该<code>OP</code>同一个输入，那么将会得到同一个输出；有状态的<code>operation</code>(<code>stateful operation</code>)分为<code>variable</code>以及<code>queue</code>，<code>variable</code>负责保存机器学习模型的模型参数，<code>queue</code>提供更加复杂的模型架构，给定同一个输入，可能会得到不同的输出。<code>common subexpression elimination</code>(<code>CSE</code>)公共子表达式消除只会在无状态的节点<code>OP</code>上执行。<br>&emsp;&emsp;简单代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sinh(x)</span><br><span class="line"></span><br><span class="line">inp = tf.placeholder(tf.float32)</span><br><span class="line">y = tf.py_func(my_func, [inp], tf.float32)</span><br></pre></td></tr></table></figure>
<p>缺点如下：</p>
<ul>
<li>这个被包装过的的计算函数的内部部分不会被序列化到<code>GraphDef</code>里面去，所以，如果你要序列化存储和恢复模型，就不能使用该函数。</li>
<li>这个被包装的计算节点<code>OP</code>与调用它的<code>Python</code>程序必须运行在同一个物理设备上，也就是说，如果使用分布式<code>TensorFlow</code>，必须使用<code>tf.train.Server</code>和<code>with tf.device</code>来保证二者在同一个服务器内。</li>
</ul>
<h3 id="tf-trainable-variables"><a href="#tf-trainable-variables" class="headerlink" title="tf.trainable_variables"></a>tf.trainable_variables</h3><p>&emsp;&emsp;在创造变量(<code>tf.Variable</code>、<code>tf.get_variable</code>等)时，都会有一个<code>trainable</code>的选项，表示该变量是否可训练，这个函数会返回图中所有<code>trainable=True</code>的变量。<code>tf.get_variable</code>和<code>tf.Variable</code>的默认选项是<code>True</code>，而<code>tf.constant</code>只能是<code>False</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.get_variable(<span class="string">'a'</span>, shape=[<span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>, shape=[<span class="number">2</span>, <span class="number">5</span>], trainable=<span class="keyword">False</span>)</span><br><span class="line">c = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.int32, shape=[<span class="number">8</span>], name=<span class="string">'c'</span>)</span><br><span class="line">d = tf.Variable(tf.random_uniform(shape=[<span class="number">3</span>, <span class="number">3</span>]), name=<span class="string">'d'</span>)</span><br><span class="line">tvar = tf.trainable_variables()</span><br><span class="line">tvar_name = [x.name <span class="keyword">for</span> x <span class="keyword">in</span> tvar]</span><br><span class="line">print(tvar)</span><br><span class="line">print(tvar_name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable <span class="string">'a:0'</span> shape=(<span class="number">5</span>, <span class="number">2</span>) dtype=float32_ref&gt;, &lt;tf.Variable <span class="string">'d:0'</span> shape=(<span class="number">3</span>, <span class="number">3</span>) dtype=float32_ref&gt;]</span><br><span class="line">[<span class="string">'a:0'</span>, <span class="string">'d:0'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a>tf.train.ExponentialMovingAverage</h3><p>&emsp;&emsp;该函数用于更新参数，就是采用滑动平均的方法更新参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.ExponentialMovingAverage(decay, steps)</span><br></pre></td></tr></table></figure>
<p>这个函数初始化需要提供一个衰减速率(<code>decay</code>)，用于控制模型的更新速度。这个函数还会维护一个影子变量，也就是更新参数后的参数值，这个影子变量的初始值就是这个变量的初始值，影子变量值的更新方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shadow_variable = decay * shadow_variable + (<span class="number">1</span> - decay) * variable</span><br></pre></td></tr></table></figure>
<p><code>shadow_variable</code>是影子变量；<code>variable</code>表示待更新的变量，也就是变量被赋予的值；<code>decay</code>为衰减速率，一般设为接近于<code>1</code>的数(<code>0.99</code>或<code>0.999</code>)，其越大模型越稳定，因为<code>decay</code>越大，参数更新的速度就越慢，趋于稳定。<br>&emsp;&emsp;<code>tf.train.ExponentialMovingAverage</code>这个函数还提供了自己动更新<code>decay</code>的计算方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decay = min(decay, (<span class="number">1</span> + steps) / (<span class="number">10</span> + steps))</span><br></pre></td></tr></table></figure>
<p><code>steps</code>是迭代的次数，可以自己设定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line">step = tf.Variable(tf.constant(<span class="number">0</span>))</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>, step)</span><br><span class="line">maintain_average = ema.apply([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))  <span class="comment"># 初始的值都为0</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">5</span>))  <span class="comment"># 把v1变为5</span></span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, 1/10) = 0.1”，“v1 = 0.1 * 0 + 0.9 * 5 = 4.5”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    sess.run(tf.assign(step, <span class="number">10000</span>))  <span class="comment"># steps = 10000</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">10</span>))  <span class="comment"># v1 = 10</span></span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, (1 + 10000)/(10 + 10000)) = 0.99”，“v1 = 0.99 * 4.5 + 0.01 * 10 = 4.555”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, (1 + 10000)/(10 + 10000)) = 0.99”，“v1 = 0.99 * 4.555 + 0.01 * 10 = 4.6”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure>
<h3 id="tf-moving-average-variables-scope-None"><a href="#tf-moving-average-variables-scope-None" class="headerlink" title="tf.moving_average_variables(scope=None)"></a>tf.moving_average_variables(scope=None)</h3><p>&emsp;&emsp;Returns all variables that maintain their moving averages. If an <code>ExponentialMovingAverage</code> object is created and <code>the apply()</code> method is called on a list of variables, these variables will be added to the <code>GraphKeys.MOVING_AVERAGE_VARIABLES</code> collection. This convenience function returns the contents of that collection.<br>&emsp;&emsp;<code>scope</code> (optional) is a string. If supplied, the resulting list is filtered to include only items whose name attribute matches <code>scope</code> using <code>re.match</code>. Items without a name attribute are never returned if a <code>scope</code> is supplied. The choice of <code>re.match</code> means that a s<code>cope</code> without special tokens filters by prefix.<br>&emsp;&emsp;使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">variable_averages = tf.train.ExponentialMovingAverage(decay, global_step)</span><br><span class="line">variables_to_average = (tf.trainable_variables() + tf.moving_average_variables())</span><br><span class="line">variables_averages_op = variable_averages.apply(variables_to_average)</span><br><span class="line">train_op = tf.group(opt, variables_averages_op)</span><br></pre></td></tr></table></figure>
<h3 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a>tf.control_dependencies</h3><p>&emsp;&emsp;在有些机器学习程序中，我们想要指定某些操作执行的依赖关系，这时可以使用<code>tf.control_dependencies(control_inputs)</code>来实现。该函数返回一个控制依赖的上下文管理器，使用<code>with</code>关键字可以让在这个上下文环境中的操作都在<code>control_inputs</code>执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</span><br><span class="line">    <span class="comment"># 'd' and 'e' will only run after 'a', 'b', and 'c' have executed</span></span><br><span class="line">    d = ...</span><br><span class="line">    e = ...</span><br></pre></td></tr></table></figure>
<p>可以嵌套<code>control_dependencies</code>使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after 'a' and 'b'</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">        <span class="comment"># Ops constructed here run after 'a', 'b', 'c', and 'd'</span></span><br></pre></td></tr></table></figure>
<p>可以传入<code>None</code>来消除依赖：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after 'a' and 'b'</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</span><br><span class="line">        <span class="comment"># Ops constructed here run normally, not waiting for either 'a' or 'b'</span></span><br><span class="line">        <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">            <span class="comment"># Ops constructed here run after 'c' and 'd',</span></span><br><span class="line">            <span class="comment"># also not waiting for either 'a' or 'b'</span></span><br></pre></td></tr></table></figure>
<p>注意，控制依赖只对那些在上下文环境中建立的操作有效，仅仅在<code>context</code>中使用一个操作或张量是没用的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span>  <span class="comment"># WRONG</span></span><br><span class="line">    t = tf.matmul(tensor, tensor)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">        <span class="comment"># The matmul op is created outside the context,</span></span><br><span class="line">        <span class="comment"># so no control dependency will be added</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span>  <span class="comment"># RIGHT</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">        <span class="comment"># The matmul op is created in the context,</span></span><br><span class="line">        <span class="comment"># so a control dependency will be added</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure>
<h3 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.group(*inputs, **kwargs)</span><br></pre></td></tr></table></figure>
<p>Create an op that groups multiple operations. When this op finishes, all ops in <code>inputs</code> have finished. This op has no output.</p>
<ul>
<li><code>*inputs</code>: Zero or more tensors to group.</li>
<li><code>name</code>: A name for this operation (optional).</li>
</ul>
<p>Returns an Operation that executes all its <code>inputs</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">b = tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>], minval=<span class="number">10</span>, maxval=<span class="number">11</span>)</span><br><span class="line">w = tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>], minval=<span class="number">10</span>, maxval=<span class="number">11</span>)</span><br><span class="line">mul = tf.multiply(w, <span class="number">2</span>)</span><br><span class="line">add = tf.add(w, <span class="number">2</span>)</span><br><span class="line">group = tf.group(mul, add)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(group))</span><br></pre></td></tr></table></figure>
<h3 id="tf-add-to-collection和tf-get-collection"><a href="#tf-add-to-collection和tf-get-collection" class="headerlink" title="tf.add_to_collection和tf.get_collection"></a>tf.add_to_collection和tf.get_collection</h3><p>&emsp;&emsp;<code>tf.add_to_collection</code>是把变量放入一个集合，把很多变量变成一个列表；<code>tf.get_collection</code>是从一个集合中取出全部变量，返回值是一个列表；<code>tf.add_n</code>是把一个列表的东西都依次加起来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v1 = tf.get_variable(name=<span class="string">'v1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'loss'</span>, v1)</span><br><span class="line">v2 = tf.get_variable(name=<span class="string">'v2'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'loss'</span>, v2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(tf.get_collection(<span class="string">'loss'</span>))</span><br><span class="line">    print(sess.run(tf.add_n(tf.get_collection(<span class="string">'loss'</span>))))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable <span class="string">'v1:0'</span> shape=(<span class="number">1</span>,) dtype=float32_ref&gt;, &lt;tf.Variable <span class="string">'v2:0'</span> shape=(<span class="number">1</span>,) dtype=float32_ref&gt;]</span><br><span class="line">[<span class="number">2.</span>]</span><br></pre></td></tr></table></figure>
<h3 id="tf-stack"><a href="#tf-stack" class="headerlink" title="tf.stack"></a>tf.stack</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack(values, axis=<span class="number">0</span>, name=<span class="string">'stack'</span>)</span><br></pre></td></tr></table></figure>
<p>Stacks a list of <code>rank-R</code> tensors into one <code>rank-(R+1)</code> tensor.</p>
<ul>
<li><code>values</code>: A list of <code>Tensor</code> objects with the same shape and type.</li>
<li><code>axis</code>: An <code>int</code>. The <code>axis</code> to stack along. Defaults to the first dimension. Negative <code>values</code> wrap around, so the valid range is <code>[-(R+1), R+1)</code>.</li>
<li><code>name</code>: A <code>name</code> for this operation (optional).</li>
</ul>
<p>Return a stacked <code>Tensor</code> with the same type as <code>values</code>.<br>&emsp;&emsp;Packs the list of tensors in <code>values</code> into a tensor with rank one higher than each tensor in <code>values</code>, by packing them along the <code>axis</code> dimension.<br>&emsp;&emsp;Given a list of length <code>N</code> of tensors of shape <code>(A, B, C)</code>: if <code>axis == 0</code> then the output tensor will have the shape <code>(N, A, B, C)</code>; if <code>axis == 1</code> then the output tensor will have the shape <code>(A, N, B, C)</code>. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">y = tf.constant([<span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line">z = tf.constant([<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.stack([x, y, z])  <span class="comment"># [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)</span></span><br><span class="line">tf.stack([x, y, z], axis=<span class="number">1</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6]]</span></span><br></pre></td></tr></table></figure>
<p>This is the opposite of <code>unstack</code>. The <code>numpy</code> equivalent is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([x, y, z]) = np.stack([x, y, z])</span><br></pre></td></tr></table></figure>
<h3 id="tf-unstack"><a href="#tf-unstack" class="headerlink" title="tf.unstack"></a>tf.unstack</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.unstack(value, num=<span class="keyword">None</span>, axis=<span class="number">0</span>, name=<span class="string">'unstack'</span>)</span><br></pre></td></tr></table></figure>
<p>Unpacks the given dimension of a <code>rank-R</code> tensor into <code>rank-(R-1)</code> tensors.<br>&emsp;&emsp;Unpacks <code>num</code> tensors from <code>value</code> by chipping it along the <code>axis</code> dimension. If <code>num</code> is not specified (the default), it is inferred from <code>value&#39;s</code> shape. If <code>value.shape[axis]</code> is not known, <code>ValueError</code> is raised.<br>&emsp;&emsp;For example, given a tensor of shape <code>(A, B, C, D)</code>:</p>
<ul>
<li>If <code>axis == 0</code> then the <code>i&#39;th</code> tensor in output is the slice <code>value[i, :, :, :]</code> and each tensor in output will have shape <code>(B, C, D)</code>. Note that the dimension unpacked along is gone, unlike split.</li>
<li>If <code>axis == 1</code> then the <code>i&#39;th</code> tensor in output is the slice <code>value[:, i, :, :]</code> and each tensor in output will have shape <code>(A, C, D)</code>.<br>&emsp;&emsp;Code example:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">d = tf.unstack(c, axis=<span class="number">0</span>)</span><br><span class="line">e = tf.unstack(c, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(d))  <span class="comment"># 输出“[array([1, 2, 3]), array([4, 5, 6])]”</span></span><br><span class="line">    print(sess.run(e))  <span class="comment"># 输出“[array([1, 4]), array([2, 5]), array([3, 6])]”</span></span><br></pre></td></tr></table></figure>
<p>This is the opposite of <code>stack</code>.</p>
<ul>
<li><code>value</code>: A rank <code>R &gt; 0</code> <code>Tensor</code> to be unstacked.</li>
<li><code>num</code>: An <code>int</code>. The length of the dimension <code>axis</code>. Automatically inferred if <code>None</code> (the default).</li>
<li><code>axis</code>: An <code>int</code>. The <code>axis</code> to <code>unstack</code> along. Defaults to the first dimension. Negative values wrap around, so the valid range is <code>[-R, R)</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>Returns the list of <code>Tensor</code> objects unstacked from <code>value</code>.</p>
<h3 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose"></a>tf.transpose</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.transpose(a, perm=<span class="keyword">None</span>, name=<span class="string">'transpose'</span>, conjugate=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>Transposes <code>a</code>. Permutes the dimensions according to <code>perm</code>. The returned tensor’s dimension <code>i</code> will correspond to the input dimension <code>perm[i]</code>. If <code>perm</code> is not given, it is set to <code>(n-1...0)</code>, where <code>n</code> is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on <code>2-D</code> input <code>Tensors</code>. If <code>conjugate</code> is <code>True</code> and <code>a.dtype</code> is either <code>complex64</code> or <code>complex128</code> then the values of <code>a</code> are conjugated and transposed.</p>
<ul>
<li><code>a</code>: A <code>Tensor</code>.</li>
<li><code>perm</code>: A permutation of the dimensions of <code>a</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>conjugate</code>: Optional <code>bool</code>. Setting it to <code>True</code> is mathematically equivalent to <code>tf.conj(tf.transpose(input))</code>.</li>
</ul>
<p>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tf.transpose(x)</span><br><span class="line"><span class="comment"># [[1, 4]</span></span><br><span class="line"><span class="comment">#  [2, 5]</span></span><br><span class="line"><span class="comment">#  [3, 6]]</span></span><br><span class="line">tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># Equivalently</span></span><br><span class="line"><span class="comment"># [[1, 4]</span></span><br><span class="line"><span class="comment">#  [2, 5]</span></span><br><span class="line"><span class="comment">#  [3, 6]]</span></span><br><span class="line"><span class="comment"># If x is complex, setting conjugate=True gives the conjugate transpose</span></span><br><span class="line">x = tf.constant([[<span class="number">1</span> + <span class="number">1j</span>, <span class="number">2</span> + <span class="number">2j</span>, <span class="number">3</span> + <span class="number">3j</span>], [<span class="number">4</span> + <span class="number">4j</span>, <span class="number">5</span> + <span class="number">5j</span>, <span class="number">6</span> + <span class="number">6j</span>]])</span><br><span class="line">tf.transpose(x, conjugate=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># [[1 - 1j, 4 - 4j],</span></span><br><span class="line"><span class="comment">#  [2 - 2j, 5 - 5j],</span></span><br><span class="line"><span class="comment">#  [3 - 3j, 6 - 6j]]</span></span><br><span class="line"><span class="comment"># "perm" is more useful for n-dimensional tensors, for n &gt; 2</span></span><br><span class="line">x = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">                 [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]])</span><br><span class="line"><span class="comment"># Take the transpose of the matrices in dimension-0(this common operation has a shorthand "matrix_transpose")</span></span><br><span class="line">tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># [[[1,  4],</span></span><br><span class="line"><span class="comment">#   [2,  5],</span></span><br><span class="line"><span class="comment">#   [3,  6]],</span></span><br><span class="line"><span class="comment">#  [[7, 10],</span></span><br><span class="line"><span class="comment">#   [8, 11],</span></span><br><span class="line"><span class="comment">#   [9, 12]]]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;In <code>numpy</code>, transposes are <code>memory-efficient</code> constant time operations as they simply return a new view of the same data with adjusted strides. <code>TensorFlow</code> does not support strides, so transpose returns a new tensor with the items permuted.</p>
<h3 id="tf-set-random-seed"><a href="#tf-set-random-seed" class="headerlink" title="tf.set_random_seed"></a>tf.set_random_seed</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.set_random_seed(seed)</span><br></pre></td></tr></table></figure>
<p>Sets the <code>graph-level</code> random <code>seed</code>. Operations that rely on a random <code>seed</code> actually derive it from two seeds: the <code>graph-level</code> and <code>operation-level</code> seeds. This sets the <code>graph-level</code> seed.<br>&emsp;&emsp;Its interactions with <code>operation-level</code> seeds is as follows:</p>
<ul>
<li>If neither the <code>graph-level</code> nor the operation <code>seed</code> is set: A random <code>seed</code> is used for this op.</li>
<li>If the <code>graph-level</code> <code>seed</code> is set, but the operation <code>seed</code> is not: The system deterministically picks an operation <code>seed</code> in conjunction with the <code>graph-level</code> <code>seed</code> so that it gets a unique random sequence.</li>
<li>If the <code>graph-level</code> <code>seed</code> is not set, but the operation <code>seed</code> is set: A default <code>graph-level</code> <code>seed</code> and the specified operation <code>seed</code> are used to determine the random sequence.</li>
<li>If both the <code>graph-level</code> and the operation <code>seed</code> are set: Both seeds are used in conjunction to determine the random sequence.</li>
</ul>
<p>&emsp;&emsp;To generate different sequences across sessions, set neither <code>graph-level</code> nor <code>op-level</code> seeds:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>])</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A3'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A4'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B3'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B4'</span></span><br></pre></td></tr></table></figure>
<p>To generate the same repeatable sequence for an op across sessions, set the <code>seed</code> for the op:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>], seed=<span class="number">1</span>)</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Repeatedly running this block with the same graph will generate the same</span></span><br><span class="line"><span class="comment"># sequence of values for 'a', but different sequences of values for 'b'.</span></span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B3'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B4'</span></span><br></pre></td></tr></table></figure>
<p>To make the random sequences generated by all ops be repeatable across sessions, set a graph-level seed:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.set_random_seed(<span class="number">1234</span>)</span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>])</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Repeatedly running this block with the same graph</span></span><br><span class="line"><span class="comment"># will generate the same sequences of 'a' and 'b'.</span></span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B2'</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-identity"><a href="#tf-identity" class="headerlink" title="tf.identity"></a>tf.identity</h3><p>&emsp;&emsp;下面的程序想要执行<code>5</code>次循环，每次循环给<code>x</code>加<code>1</code>并赋值给<code>y</code>，然后打印出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)  <span class="comment"># 返回一个op，表示给变量x加1的操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>打印出的结果都是<code>0.0</code>，也就是说没有达到预期的效果。这是因为<code>y</code>只是复制了<code>x</code>变量内容，并未和<code>tensorflow</code>图上的节点相联系，不能执行节点上的操作。进行如下修改就能实现需要的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = tf.identity(x)  <span class="comment"># 修改部分</span></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>也就是说，<code>tf.identity</code>返回了一个和<code>x</code>相同的的新<code>tensor</code>。</p>
<h3 id="TensorFlow之命令行参数"><a href="#TensorFlow之命令行参数" class="headerlink" title="TensorFlow之命令行参数"></a>TensorFlow之命令行参数</h3><p>&emsp;&emsp;<code>TensorFlow</code>定义了<code>tf.app.flags</code>(也可以用它的别名<code>tf.flags</code>)，用于支持接受命令行传递参数，相当于接收<code>argv</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是参数名称，第二个参数是默认值，第三个是参数描述</span></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'str_name'</span>, <span class="string">'def_v_1'</span>, <span class="string">"descript1"</span>)</span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">'int_name'</span>, <span class="number">10</span>, <span class="string">"descript2"</span>)</span><br><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">'bool_name'</span>, <span class="keyword">False</span>, <span class="string">"descript3"</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    print(FLAGS.str_name)</span><br><span class="line">    print(FLAGS.int_name)</span><br><span class="line">    print(FLAGS.bool_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()  <span class="comment"># run main function</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python test.py --str_name <span class="string">"hello"</span> --int_name <span class="number">12</span> --bool_name <span class="keyword">True</span></span><br><span class="line">hello</span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<h3 id="Session-run和Tensor-eval"><a href="#Session-run和Tensor-eval" class="headerlink" title="Session.run和Tensor.eval"></a>Session.run和Tensor.eval</h3><p>&emsp;&emsp;<code>TensorFlow</code>运行代码时，在会话中需要运行节点，可能会碰到两种方式，即<code>Session.run</code>和<code>Tensor.eval</code>，两者之间的差异如下。<br>&emsp;&emsp;如果你有一个<code>Tensor t</code>，在使用<code>t.eval</code>时，等价于<code>tf.get_default_session().run(t)</code>，实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant(<span class="number">42.0</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():  <span class="comment"># or `with sess:` to close on exit</span></span><br><span class="line">    <span class="keyword">assert</span> sess <span class="keyword">is</span> tf.get_default_session()</span><br><span class="line">    <span class="keyword">assert</span> t.eval() == sess.run(t)</span><br></pre></td></tr></table></figure>
<p>这其中最主要的区别就在于，你可以使用<code>sess.run</code>在同一步获取多个<code>tensor</code>中的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant(<span class="number">42.0</span>)</span><br><span class="line">u = tf.constant(<span class="number">37.0</span>)</span><br><span class="line">tu = tf.mul(t, u)</span><br><span class="line">ut = tf.mul(u, t)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    tu.eval()  <span class="comment"># runs one step</span></span><br><span class="line">    ut.eval()  <span class="comment"># runs one step</span></span><br><span class="line">    sess.run([tu, ut])  <span class="comment"># evaluates both tensors in a single step</span></span><br></pre></td></tr></table></figure>
<p>注意到，每次使用<code>eval</code>和<code>run</code>时，都会执行整个计算图，为了获取计算的结果，将它分配给<code>tf.Variable</code>，然后获取。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/The Ultimate Guide To Speech Recognition With Python/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/The Ultimate Guide To Speech Recognition With Python/" itemprop="url">The Ultimate Guide To Speech Recognition With Python</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T16:11:08+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;Have you ever wondered how to add speech recognition to your <code>Python</code> project? If so, then keep reading! It’s easier than you might think.<br>&emsp;&emsp;Far from a being a fad, the overwhelming success of <code>speech-enabled</code> products like <code>Amazon Alexa</code> has proven that some degree of speech support will be an essential aspect of household tech for the foreseeable future. If you think about it, the reasons why are pretty obvious. Incorporating speech recognition into your <code>Python</code> application offers a level of interactivity and accessibility that few technologies can match.<br>&emsp;&emsp;The accessibility improvements alone are worth considering. Speech recognition allows the elderly and the physically and visually impaired to interact with <code>state-of-the-art</code> products and services quickly and naturally.<br>&emsp;&emsp;Best of all, including speech recognition in a <code>Python</code> project is really simple. In this guide, you’ll find out how. You’ll learn:</p>
<ul>
<li>How speech recognition works?</li>
<li>What packages are available on <code>PyPI</code>?</li>
<li>How to install and use the <code>SpeechRecognition</code> package - a <code>full-featured</code> and <code>easy-to-use</code> <code>Python</code> speech recognition library.</li>
</ul>
<p>&emsp;&emsp;In the end, you’ll apply what you’ve learned to a simple <code>Guess the Word</code> game and see how it all comes together.</p>
<h3 id="How-Speech-Recognition-Works-An-Overview"><a href="#How-Speech-Recognition-Works-An-Overview" class="headerlink" title="How Speech Recognition Works - An Overview"></a>How Speech Recognition Works - An Overview</h3><p>&emsp;&emsp;Before we get to the <code>nitty-gritty</code> of doing speech recognition in <code>Python</code>, let’s take a moment to talk about how speech recognition works. A full discussion would fill a book, so I won’t bore you with all of the technical details here. In fact, this section is not <code>pre-requisite</code> to the rest of the tutorial. If you’d like to get straight to the point, then feel free to skip ahead.<br>&emsp;&emsp;Speech recognition has its roots in research done at <code>Bell Labs</code> in the early <code>1950s</code>. Early systems were limited to a single speaker and had limited vocabularies of about a dozen words. Modern speech recognition systems have come a long way since their ancient counterparts. They can recognize speech from multiple speakers and have enormous vocabularies in numerous languages.<br>&emsp;&emsp;The first component of speech recognition is, of course, speech. Speech must be converted from physical sound to an electrical signal with a microphone, and then to digital data with an <code>analog-to-digital</code> converter. Once digitized, several models can be used to transcribe the audio to text.<br>&emsp;&emsp;Most modern speech recognition systems rely on what is known as a <code>Hidden Markov Model</code> (<code>HMM</code>). This approach works on the assumption that a speech signal, when viewed on a short enough timescale (say, ten milliseconds), can be reasonably approximated as a stationary process - that is, a process in which statistical properties do not change over time.<br>&emsp;&emsp;In a typical <code>HMM</code>, the speech signal is divided into <code>10-millisecond</code> fragments. The power spectrum of each fragment, which is essentially a plot of the signal’s power as a function of frequency, is mapped to a vector of real numbers known as cepstral coefficients. The dimension of this vector is usually small - sometimes as low as <code>10</code>, although more accurate systems may have dimension <code>32</code> or more. The final output of the <code>HMM</code> is a sequence of these vectors.<br>&emsp;&emsp;To decode the speech into text, groups of vectors are matched to one or more phonemes - a fundamental unit of speech. This calculation requires training, since the sound of a phoneme varies from speaker to speaker, and even varies from one utterance to another by the same speaker. A special algorithm is then applied to determine the most likely word (or words) that produce the given sequence of phonemes.<br>&emsp;&emsp;One can imagine that this whole process may be computationally expensive. In many modern speech recognition systems, neural networks are used to simplify the speech signal using techniques for feature transformation and dimensionality reduction before <code>HMM</code> recognition. <code>Voice activity detectors</code> (<code>VADs</code>) are also used to reduce an audio signal to only the portions that are likely to contain speech. This prevents the recognizer from wasting time analyzing unnecessary parts of the signal.<br>&emsp;&emsp;Fortunately, as a <code>Python</code> programmer, you don’t have to worry about any of this. A number of speech recognition services are available for use online through an <code>API</code>, and many of these services offer <code>Python SDKs</code>.</p>
<h3 id="Picking-a-Python-Speech-Recognition-Package"><a href="#Picking-a-Python-Speech-Recognition-Package" class="headerlink" title="Picking a Python Speech Recognition Package"></a>Picking a Python Speech Recognition Package</h3><p>&emsp;&emsp;A handful of packages for speech recognition exist on <code>PyPI</code>. A few of them include: <code>apiai</code>, <code>assemblyai</code>, <code>google-cloud-speech</code>, <code>pocketsphinx</code>, <code>SpeechRecognition</code>, <code>watson-developer-cloud</code>, <code>wit</code>.<br>&emsp;&emsp;Some of these packages - such as <code>wit</code> and <code>apiai</code> - offer <code>built-in</code> features, like natural language processing for identifying a speaker’s intent, which go beyond basic speech recognition. Others, like <code>google-cloud-speech</code>, focus solely on <code>speech-to-text</code> conversion.<br>&emsp;&emsp;There is one package that stands out in terms of <code>ease-of-use</code>: <code>SpeechRecognition</code>. Recognizing speech requires audio input, and <code>SpeechRecognition</code> makes retrieving this input really easy. Instead of having to build scripts for accessing microphones and processing audio files from scratch, <code>SpeechRecognition</code> will have you up and running in just a few minutes.<br>&emsp;&emsp;The <code>SpeechRecognition</code> library acts as a wrapper for several popular speech <code>APIs</code> and is thus extremely flexible. One of these - the <code>Google Web Speech API</code> - supports a default <code>API</code> key that is <code>hard-coded</code> into the <code>SpeechRecognition</code> library. That means you can get off your feet without having to sign up for a service.<br>&emsp;&emsp;The flexibility and <code>ease-of-use</code> of the <code>SpeechRecognition</code> package make it an excellent choice for any <code>Python</code> project. However, support for every feature of each <code>API</code> it wraps is not guaranteed. You will need to spend some time researching the available options to find out if <code>SpeechRecognition</code> will work in your particular case.<br>&emsp;&emsp;So, now that you’re convinced you should try out <code>SpeechRecognition</code>, the next step is getting it installed in your environment.</p>
<h3 id="Installing-SpeechRecognition"><a href="#Installing-SpeechRecognition" class="headerlink" title="Installing SpeechRecognition"></a>Installing SpeechRecognition</h3><p>&emsp;&emsp;<code>SpeechRecognition</code> is compatible with <code>Python 2.6</code>, <code>2.7</code> and <code>3.3+</code>, but requires some additional installation steps for <code>Python 2</code>. For this tutorial, I’ll assume you are using <code>Python 3.3+</code>.<br>&emsp;&emsp;You can install <code>SpeechRecognition</code> from a terminal with <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install SpeechRecognition</span><br></pre></td></tr></table></figure>
<p>Once installed, you should verify the installation by opening an interpreter session and typing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sr.__version__</span><br><span class="line"><span class="string">'3.8.1'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Go ahead and keep this session open. You’ll start to work with it in just a bit.<br>&emsp;&emsp;<code>SpeechRecognition</code> will work out of the box if all you need to do is work with existing audio files. Specific use cases, however, require a few dependencies. Notably, the <code>PyAudio</code> package is needed for capturing microphone input.<br>&emsp;&emsp;You’ll see which dependencies you need as you read further. For now, let’s dive in and explore the basics of the package.</p>
<h3 id="The-Recognizer-Class"><a href="#The-Recognizer-Class" class="headerlink" title="The Recognizer Class"></a>The Recognizer Class</h3><p>&emsp;&emsp;All of the magic in <code>SpeechRecognition</code> happens with the <code>Recognizer</code> class. The primary purpose of a <code>Recognizer</code> instance is, of course, to recognize speech. Each instance comes with a variety of settings and functionality for recognizing speech from an audio source.<br>&emsp;&emsp;Creating a <code>Recognizer</code> instance is easy. In your current interpreter session, just type:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = sr.Recognizer()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Each Recognizer instance has seven methods for recognizing speech from an audio source using various <code>APIs</code>. These are:</p>
<ul>
<li><code>recognize_bing()</code>: <code>Microsoft Bing Speech</code></li>
<li><code>recognize_google()</code>: <code>Google Web Speech API</code></li>
<li><code>recognize_google_cloud()</code>: <code>Google Cloud Speech</code> - requires installation of the <code>google-cloud-speech</code> package</li>
<li><code>recognize_houndify()</code>: <code>Houndify by SoundHound</code></li>
<li><code>recognize_ibm()</code>: <code>IBM Speech to Text</code></li>
<li><code>recognize_sphinx()</code>: <code>CMU Sphinx</code> - requires installing <code>PocketSphinx</code></li>
<li><code>recognize_wit()</code>: <code>Wit.ai</code></li>
</ul>
<p>Of the seven, only <code>recognize_sphinx()</code> works offline with the <code>CMU Sphinx</code> engine. The other six all require an internet connection.<br>&emsp;&emsp;A full discussion of the features and benefits of each <code>API</code> is beyond the scope of this tutorial. Since <code>SpeechRecognition</code> ships with a default <code>API</code> key for the <code>Google Web Speech API</code>, you can get started with it right away. For this reason, we’ll use the <code>Web Speech API</code> in this guide. The other six <code>APIs</code> all require authentication with either an <code>API</code> key or a <code>username/password</code> combination. For more information, consult the <code>SpeechRecognition</code> docs.<br>&emsp;&emsp;Caution: The default key provided by <code>SpeechRecognition</code> is for testing purposes only, and <code>Google</code> may revoke it at any time. It is not a good idea to use the <code>Google Web Speech API</code> in production. Even with a valid <code>API</code> key, you’ll be limited to only <code>50</code> requests per day, and there is no way to raise this quota. Fortunately, <code>SpeechRecognition&#39;s</code> interface is nearly identical for each <code>API</code>, so what you learn today will be easy to translate to a <code>real-world</code> project.<br>&emsp;&emsp;Each <code>recognize_*()</code> method will throw a <code>RequestError</code> exception if the <code>API</code> is unreachable. For <code>recognize_sphinx()</code>, this could happen as the result of a missing, corrupt or incompatible <code>Sphinx</code> installation. For the other six methods, <code>RequestError</code> may be thrown if quota limits are met, the server is unavailable, or there is no internet connection.<br>&emsp;&emsp;Let’s get our hands dirty. Go ahead and try to call <code>recognize_google()</code> in your interpreter session.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google()</span><br></pre></td></tr></table></figure>
<p>What happened? You probably got something that looks like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: recognize_google() missing <span class="number">1</span> required positional argument: <span class="string">'audio_data'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;You might have guessed this would happen. How could something be recognized from nothing? All seven <code>recognize_*()</code> methods of the <code>Recognizer</code> class require an <code>audio_data</code> argument. In each case, <code>audio_data</code> must be an instance of <code>SpeechRecognition&#39;s</code> <code>AudioData</code> class.<br>&emsp;&emsp;There are two ways to create an <code>AudioData</code> instance: from an audio file or audio recorded by a microphone. Audio files are a little easier to get started with, so let’s take a look at that first.</p>
<h3 id="Working-With-Audio-Files"><a href="#Working-With-Audio-Files" class="headerlink" title="Working With Audio Files"></a>Working With Audio Files</h3><p>&emsp;&emsp;Before you continue, you’ll need to download an audio file. The one I used to get started, <code>harvard.wav</code> can be found here (<code>https://github.com/realpython/python-speech-recognition</code>). Make sure you save it to the same directory in which your <code>Python</code> interpreter session is running.<br>&emsp;&emsp;<code>SpeechRecognition</code> makes working with audio files easy thanks to its handy <code>AudioFile</code> class. This class can be initialized with the path to an audio file and provides a context manager interface for reading and working with the file’s contents.</p>
<h4 id="Supported-File-Types"><a href="#Supported-File-Types" class="headerlink" title="Supported File Types"></a>Supported File Types</h4><p>&emsp;&emsp;Currently, <code>SpeechRecognition</code> supports the following file formats: <code>WAV</code>: must be in <code>PCM/LPCM</code> format, <code>AIFF</code>, <code>AIFF-C</code>, <code>FLAC</code>: must be native <code>FLAC</code> format; <code>OGG-FLAC</code> is not supported.<br>&emsp;&emsp;If you are working on <code>X86</code> based <code>Linux</code>, <code>macOS</code> or <code>Windows</code>, you should be able to work with <code>FLAC</code> files without a problem. On other platforms, you will need to install a <code>FLAC</code> encoder and ensure you have access to the flac command line tool. You can find more information here if this applies to you.</p>
<h4 id="Using-record-to-Capture-Data-From-a-File"><a href="#Using-record-to-Capture-Data-From-a-File" class="headerlink" title="Using record() to Capture Data From a File"></a>Using record() to Capture Data From a File</h4><p>&emsp;&emsp;Type the following into your interpreter session to process the contents of the <code>harvard.wav</code> file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>harvard = sr.AudioFile(<span class="string">'harvard.wav'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>   audio = r.record(source)</span><br></pre></td></tr></table></figure>
<p>The context manager opens the file and reads its contents, storing the data in an <code>AudioFile</code> instance called source. Then the <code>record()</code> method records the data from the entire file into an <code>AudioData</code> instance. You can confirm this by checking the type of audio:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(audio)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">speech_recognition</span>.<span class="title">AudioData</span>'&gt;</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;You can now invoke <code>recognize_google()</code> to attempt to recognize any speech in the audio. Depending on your internet connection speed, you may have to wait several seconds before seeing the result.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the stale smell of old beer lingers it takes heat to bring out</span></span><br><span class="line"><span class="string"> the odor a cold dip restores health and zest a salt pickle taste</span></span><br><span class="line"><span class="string"> fine with ham tacos al Pastore are my favorite a zestful food is the hot cross bun'</span></span><br></pre></td></tr></table></figure>
<p>Congratulations! You’ve just transcribed your first audio file!<br>&emsp;&emsp;If you’re wondering where the phrases in the <code>harvard.wav</code> file come from, they are examples of <code>Harvard Sentences</code>. These phrases were published by the <code>IEEE</code> in <code>1965</code> for use in speech intelligibility testing of telephone lines. They are still used in <code>VoIP</code> and cellular testing today.<br>&emsp;&emsp;The <code>Harvard Sentences</code> are comprised of <code>72</code> lists of ten phrases. You can find freely available recordings of these phrases on the <code>Open Speech Repository</code> website. Recordings are available in <code>English</code>, <code>Mandarin Chinese</code>, <code>French</code>, and <code>Hindi</code>. They provide an excellent source of free material for testing your code.</p>
<h4 id="Capturing-Segments-With-offset-and-duration"><a href="#Capturing-Segments-With-offset-and-duration" class="headerlink" title="Capturing Segments With offset and duration"></a>Capturing Segments With offset and duration</h4><p>&emsp;&emsp;What if you only want to capture a portion of the speech in a file? The <code>record()</code> method accepts a duration keyword argument that stops the recording after a specified number of seconds.<br>&emsp;&emsp;For example, the following captures any speech in the first four seconds of the file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source, duration=<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the stale smell of old beer lingers'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The <code>record()</code> method, when used inside a with block, always moves ahead in the file stream. This means that if you record once for four seconds and then record again for four seconds, the second time returns the four seconds of audio after the first four seconds.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio1 = r.record(source, duration=<span class="number">4</span>)</span><br><span class="line"><span class="meta">... </span>    audio2 = r.record(source, duration=<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio1)</span><br><span class="line"><span class="string">'the stale smell of old beer lingers'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio2)</span><br><span class="line"><span class="string">'it takes heat to bring out the odor a cold dip'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Notice that <code>audio2</code> contains a portion of the third phrase in the file. When specifying a duration, the recording might stop <code>mid-phrase</code> or even <code>mid-word-which</code> can hurt the accuracy of the transcription. More on this in a bit.<br>&emsp;&emsp;In addition to specifying a recording duration, the <code>record()</code> method can be given a specific starting point using the offset keyword argument. This value represents the number of seconds from the beginning of the file to ignore before starting to record.<br>&emsp;&emsp;To capture only the second phrase in the file, you could start with an offset of four seconds and record for, say, three seconds.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source, offset=<span class="number">4</span>, duration=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recognizer.recognize_google(audio)</span><br><span class="line"><span class="string">'it takes heat to bring out the odor'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The offset and duration keyword arguments are useful for segmenting an audio file if you have prior knowledge of the structure of the speech in the file. However, using them hastily can result in poor transcriptions. To see this effect, try the following in your interpreter:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> harvard <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source, offset=<span class="number">4.7</span>, duration=<span class="number">2.8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recognizer.recognize_google(audio)</span><br><span class="line"><span class="string">'Mesquite to bring out the odor Aiko'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;By starting the recording at <code>4.7</code> seconds, you miss the <code>it t</code> portion a the beginning of the phrase <code>it takes heat to bring out the odor</code>, so the <code>API</code> only got <code>akes heat</code> which it matched to <code>Mesquite</code>.<br>&emsp;&emsp;Similarly, at the end of the recording, you captured <code>a co</code>, which is the beginning of the third phrase <code>a cold dip restores health and zest</code>. This was matched to <code>Aiko</code> by the <code>API</code>.<br>&emsp;&emsp;There is another reason you may get inaccurate transcriptions. Noise! The above examples worked well because the audio file is reasonably clean. In the real world, unless you have the opportunity to process audio files beforehand, you can not expect the audio to be <code>noise-free</code>.</p>
<h4 id="The-Effect-of-Noise-on-Speech-Recognition"><a href="#The-Effect-of-Noise-on-Speech-Recognition" class="headerlink" title="The Effect of Noise on Speech Recognition"></a>The Effect of Noise on Speech Recognition</h4><p>&emsp;&emsp;Noise is a fact of life. All audio recordings have some degree of noise in them, and <code>un-handled</code> noise can wreck the accuracy of speech recognition apps.<br>&emsp;&emsp;To get a feel for how noise can affect speech recognition, download the <code>jackhammer.wav</code> file here (<code>https://github.com/realpython/python-speech-recognition</code>). As always, make sure you save this to your interpreter session’s working directory.<br>&emsp;&emsp;This file has the phrase <code>the stale smell of old beer lingers</code> spoken with a loud jackhammer in the background. What happens when you try to transcribe this file?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>jackhammer = sr.AudioFile(<span class="string">'jackhammer.wav'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> jackhammer <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the snail smell of old gear vendors'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;So how do you deal with this? One thing you can try is using the <code>adjust_for_ambient_noise()</code> method of the Recognizer class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> jackhammer <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    r.adjust_for_ambient_noise(source)</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'still smell of old beer vendors'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;That got you a little closer to the actual phrase, but it still isn’t perfect. Also, <code>the</code> is missing from the beginning of the phrase. Why is that?<br>&emsp;&emsp;The <code>adjust_for_ambient_noise()</code> method reads the first second of the file stream and calibrates the recognizer to the noise level of the audio. Hence, that portion of the stream is consumed before you call <code>record()</code> to capture the data.<br>&emsp;&emsp;You can adjust the <code>time-frame</code> that <code>adjust_for_ambient_noise()</code> uses for analysis with the duration keyword argument. This argument takes a numerical value in seconds and is set to <code>1</code> by default. Try lowering this value to <code>0.5</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> jackhammer <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    r.adjust_for_ambient_noise(source, duration=<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">... </span>    audio = r.record(source)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'the snail smell like old Beer Mongers'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Well, that got you <code>the</code> at the beginning of the phrase, but now you have some new issues! Sometimes it isn’t possible to remove the effect of the noise - the signal is just too noisy to be dealt with successfully. That’s the case with this file.<br>&emsp;&emsp;If you find yourself running up against these issues frequently, you may have to resort to some <code>pre-processing</code> of the audio. This can be done with audio editing software or a <code>Python</code> package (such as <code>SciPy</code>) that can apply filters to the files. A detailed discussion of this is beyond the scope of this tutorial - check out <code>Allen Downey&#39;s Think DSP</code> book if you are interested. For now, just be aware that ambient noise in an audio file can cause problems and must be addressed in order to maximize the accuracy of speech recognition.<br>&emsp;&emsp;When working with noisy files, it can be helpful to see the actual <code>API</code> response. Most <code>APIs</code> return a <code>JSON</code> string containing many possible transcriptions. The <code>recognize_google()</code> method will always return the most likely transcription unless you force it to give you the full response.<br>&emsp;&emsp;You can do this by setting the <code>show_all</code> keyword argument of the <code>recognize_google()</code> method to <code>True</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio, show_all=<span class="keyword">True</span>)</span><br><span class="line">&#123;<span class="string">'alternative'</span>: [</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the snail smell like old Beer Mongers'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the snail smell like old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the stale smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the snail smell like old beermongers'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'destihl smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell like old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'bastille smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell like old beermongers'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell of old beer venders'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smelling old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'musty smell of old beer vendors'</span>&#125;,</span><br><span class="line">  &#123;<span class="string">'transcript'</span>: <span class="string">'the still smell of old beer vendor'</span>&#125;</span><br><span class="line">], <span class="string">'final'</span>: <span class="keyword">True</span>&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;As you can see, <code>recognize_google()</code> returns a dictionary with the key <code>alternative</code> that points to a list of possible transcripts. The structure of this response may vary from <code>API</code> to <code>API</code> and is mainly useful for debugging.<br>&emsp;&emsp;By now, you have a pretty good idea of the basics of the <code>SpeechRecognition</code> package. You’ve seen how to create an <code>AudioFile</code> instance from an audio file and use the <code>record()</code> method to capture data from the file. You learned how record segments of a file using the offset and duration keyword arguments of <code>record()</code>, and you experienced the detrimental effect noise can have on transcription accuracy.<br>&emsp;&emsp;Now for the fun part. Let’s transition from transcribing static audio files to making your project interactive by accepting input from a microphone.</p>
<h3 id="Working-With-Microphones"><a href="#Working-With-Microphones" class="headerlink" title="Working With Microphones"></a>Working With Microphones</h3><p>&emsp;&emsp;To access your microphone with <code>SpeechRecognizer</code>, you’ll have to install the <code>PyAudio</code> package. Go ahead and close your current interpreter session, and let’s do that.</p>
<h4 id="Installing-PyAudio"><a href="#Installing-PyAudio" class="headerlink" title="Installing PyAudio"></a>Installing PyAudio</h4><p>&emsp;&emsp;The process for installing <code>PyAudio</code> will vary depending on your operating system.<br>&emsp;&emsp;<code>Debian Linux</code>: If you’re on <code>Debian-based Linux</code> (like <code>Ubuntu</code>), you can install <code>PyAudio</code> with apt:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python-pyaudio python3-pyaudio</span><br></pre></td></tr></table></figure>
<p>Once installed, you may still need to run pip install <code>pyaudio</code>, especially if you are working in a virtual environment.<br>&emsp;&emsp;<code>macOS</code>: For <code>macOS</code>, first you will need to install <code>PortAudio</code> with <code>Homebrew</code>, and then install <code>PyAudio</code> with <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew install portaudio</span><br><span class="line">pip install pyaudio</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>Windows</code>: On <code>Windows</code>, you can install <code>PyAudio</code> with <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyaudio</span><br></pre></td></tr></table></figure>
<h4 id="Testing-the-Installation"><a href="#Testing-the-Installation" class="headerlink" title="Testing the Installation"></a>Testing the Installation</h4><p>&emsp;&emsp;Once you’ve got <code>PyAudio</code> installed, you can test the installation from the console.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m speech_recognition</span><br></pre></td></tr></table></figure>
<p>Make sure your default microphone is on and unmuted. If the installation worked, you should see something like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A moment of silence, please...</span><br><span class="line">Set minimum energy threshold to <span class="number">600.4452854381937</span></span><br><span class="line">Say something!</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Go ahead and play around with it a little bit by speaking into your microphone and seeing how well <code>SpeechRecognition</code> transcribes your speech.<br>&emsp;&emsp;<strong>Note</strong>: If you are on <code>Ubuntu</code> and get some funky output like <code>ALSA lib ... Unknown PCM</code>, refer to this page for tips on suppressing these messages. This output comes from the <code>ALSA</code> package installed with <code>Ubuntu</code> - not <code>SpeechRecognition</code> or <code>PyAudio</code>. In all reality, these messages may indicate a problem with your <code>ALSA</code> configuration, but in my experience, they do not impact the functionality of your code. They are mostly a nuisance.</p>
<h4 id="The-Microphone-Class"><a href="#The-Microphone-Class" class="headerlink" title="The Microphone Class"></a>The Microphone Class</h4><p>&emsp;&emsp;Open up another interpreter session and create an instance of the recognizer class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = sr.Recognizer()</span><br></pre></td></tr></table></figure>
<p>Now, instead of using an audio file as the source, you will use the default system microphone. You can access this by creating an instance of the <code>Microphone</code> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mic = sr.Microphone()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;If your system has no default microphone (such as on a <code>RaspberryPi</code>), or you want to use a microphone other than the default, you will need to specify which one to use by supplying a device index. You can get a list of microphone names by calling the <code>list_microphone_names()</code> static method of the <code>Microphone</code> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sr.Microphone.list_microphone_names()</span><br><span class="line">[<span class="string">'HDA Intel PCH: ALC272 Analog (hw:0,0)'</span>,</span><br><span class="line"> <span class="string">'HDA Intel PCH: HDMI 0 (hw:0,3)'</span>,</span><br><span class="line"> <span class="string">'sysdefault'</span>,</span><br><span class="line"> <span class="string">'front'</span>,</span><br><span class="line"> <span class="string">'surround40'</span>,</span><br><span class="line"> <span class="string">'surround51'</span>,</span><br><span class="line"> <span class="string">'surround71'</span>,</span><br><span class="line"> <span class="string">'hdmi'</span>,</span><br><span class="line"> <span class="string">'pulse'</span>,</span><br><span class="line"> <span class="string">'dmix'</span>,</span><br><span class="line"> <span class="string">'default'</span>]</span><br></pre></td></tr></table></figure>
<p>Note that your output may differ from the above example. The device index of the microphone is the index of its name in the list returned by <code>list_microphone_names()</code>. For example, given the above output, if you want to use the microphone called <code>front</code>, which has index <code>3</code> in the list, you would create a microphone instance like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is just an example, do not run</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mic = sr.Microphone(device_index=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>For most projects, though, you’ll probably want to use the default system microphone.</p>
<h4 id="Using-listen-to-Capture-Microphone-Input"><a href="#Using-listen-to-Capture-Microphone-Input" class="headerlink" title="Using listen() to Capture Microphone Input"></a>Using listen() to Capture Microphone Input</h4><p>&emsp;&emsp;Now that you’ve got a <code>Microphone</code> instance ready to go, it’s time to capture some input.<br>&emsp;&emsp;Just like the <code>AudioFile</code> class, <code>Microphone</code> is a context manager. You can capture input from the microphone using the <code>listen()</code> method of the <code>Recognizer</code> class inside of the with block. This method takes an audio source as its first argument and records input from the source until silence is detected.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> mic <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    audio = r.listen(source)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Once you execute the with block, try speaking <code>hello</code> into your microphone. Wait a moment for the interpreter prompt to display again. Once the <code>&gt;&gt;&gt;</code> prompt returns, you’re ready to recognize the speech.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.recognize_google(audio)</span><br><span class="line"><span class="string">'hello'</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;If the prompt never returns, your microphone is most likely picking up too much ambient noise. You can interrupt the process with <code>ctrl + c</code> to get your prompt back.<br>&emsp;&emsp;To handle ambient noise, you’ll need to use the <code>adjust_for_ambient_noise()</code> method of the <code>Recognizer</code> class, just like you did when trying to make sense of the noisy audio file. Since input from a microphone is far less predictable than input from an audio file, it is a good idea to do this anytime you listen for microphone input.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> mic <span class="keyword">as</span> source:</span><br><span class="line"><span class="meta">... </span>    r.adjust_for_ambient_noise(source)</span><br><span class="line"><span class="meta">... </span>    audio = r.listen(source)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;After running the above code, wait a second for <code>adjust_for_ambient_noise()</code> to do its thing, then try speaking <code>hello</code> into the microphone. Again, you will have to wait a moment for the interpreter prompt to return before trying to recognize the speech.<br>&emsp;&emsp;<code>Recall that adjust_for_ambient_noise()</code> analyzes the audio source for one second. If this seems too long to you, feel free to adjust this with the duration keyword argument.<br>&emsp;&emsp;The <code>SpeechRecognition</code> documentation recommends using a duration no less than <code>0.5</code> seconds. In some cases, you may find that durations longer than the default of one second generate better results. The minimum value you need depends on the microphone’s ambient environment. Unfortunately, this information is typically unknown during development. In my experience, the default duration of one second is adequate for most applications.</p>
<h4 id="Handling-Unrecognizable-Speech"><a href="#Handling-Unrecognizable-Speech" class="headerlink" title="Handling Unrecognizable Speech"></a>Handling Unrecognizable Speech</h4><p>&emsp;&emsp;Try typing the previous code example in to the interpeter and making some unintelligible noises into the microphone. You should get something like this in response:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">"/home/david/real_python/speech_recognition_primer/</span></span><br><span class="line"><span class="string">       venv/lib/python3.5/site-packages/speech_recognition/__init__.py"</span>,</span><br><span class="line">       line <span class="number">858</span>, <span class="keyword">in</span> recognize_google</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(actual_result, dict) <span class="keyword">or</span> len(actual_result.get(<span class="string">"alternative"</span>, [])) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> UnknownValueError()</span><br><span class="line">speech_recognition.UnknownValueError</span><br></pre></td></tr></table></figure>
<p>Audio that cannot be matched to text by the <code>API</code> raises an <code>UnknownValueError</code> exception. You should always wrap calls to the <code>API</code> with try and except blocks to handle this exception.<br>&emsp;&emsp;<strong>Note</strong>: You may have to try harder than you expect to get the exception thrown. The <code>API</code> works very hard to transcribe any vocal sounds. Even short grunts were transcribed as words like <code>how</code> for me. Coughing, hand claps, and tongue clicks would consistently raise the exception.</p>
<h4 id="Putting-It-All-Together-A-“Guess-the-Word”-Game"><a href="#Putting-It-All-Together-A-“Guess-the-Word”-Game" class="headerlink" title="Putting It All Together: A “Guess the Word” Game"></a>Putting It All Together: A “Guess the Word” Game</h4><p>&emsp;&emsp;Now that you’ve seen the basics of recognizing speech with the <code>SpeechRecognition</code> package let’s put your newfound knowledge to use and write a small game that picks a random word from a list and gives the user three attempts to guess the word.<br>&emsp;&emsp;Here is the full script:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recognize_speech_from_mic</span><span class="params">(recognizer, microphone)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Transcribe speech from recorded from `microphone`. Returns a dictionary with three keys:</span></span><br><span class="line"><span class="string">    "success": a boolean indicating whether or not the API request was successful</span></span><br><span class="line"><span class="string">    "error": `None` if no error occured, otherwise a string containing an error message</span></span><br><span class="line"><span class="string">             if the API could not be reached or speech was unrecognizable</span></span><br><span class="line"><span class="string">    "transcription": `None` if speech could not be transcribed, otherwise</span></span><br><span class="line"><span class="string">                     a string containing the transcribed text</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># check that recognizer and microphone arguments are appropriate type</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(recognizer, sr.Recognizer):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">"`recognizer` must be `Recognizer` instance"</span>)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(microphone, sr.Microphone):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">"`microphone` must be `Microphone` instance"</span>)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># adjust the recognizer sensitivity to ambient noise and record audio from the microphone</span></span><br><span class="line">    <span class="keyword">with</span> microphone <span class="keyword">as</span> source:</span><br><span class="line">        recognizer.adjust_for_ambient_noise(source)</span><br><span class="line">        audio = recognizer.listen(source)</span><br><span class="line">​</span><br><span class="line">    response = &#123;<span class="string">"success"</span>: <span class="keyword">True</span>, <span class="string">"error"</span>: <span class="keyword">None</span>, <span class="string">"transcription"</span>: <span class="keyword">None</span>&#125;  <span class="comment"># set up the response object</span></span><br><span class="line">​</span><br><span class="line">    <span class="comment"># try recognizing the speech in the recording. if a RequestError or UnknownValueError exception is</span></span><br><span class="line">    <span class="comment"># caught, update the response object accordingly</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response[<span class="string">"transcription"</span>] = recognizer.recognize_google(audio)</span><br><span class="line">    <span class="keyword">except</span> sr.RequestError:  <span class="comment"># API was unreachable or unresponsive</span></span><br><span class="line">        response[<span class="string">"success"</span>] = <span class="keyword">False</span></span><br><span class="line">        response[<span class="string">"error"</span>] = <span class="string">"API unavailable"</span></span><br><span class="line">    <span class="keyword">except</span> sr.UnknownValueError:</span><br><span class="line">        response[<span class="string">"error"</span>] = <span class="string">"Unable to recognize speech"</span>  <span class="comment"># speech was unintelligible</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># set the list of words, maxnumber of guesses, and prompt limit</span></span><br><span class="line">    WORDS = [<span class="string">"apple"</span>, <span class="string">"banana"</span>, <span class="string">"grape"</span>, <span class="string">"orange"</span>, <span class="string">"mango"</span>, <span class="string">"lemon"</span>]</span><br><span class="line">    NUM_GUESSES = <span class="number">3</span></span><br><span class="line">    PROMPT_LIMIT = <span class="number">5</span></span><br><span class="line">​</span><br><span class="line">    <span class="comment"># create recognizer and mic instances</span></span><br><span class="line">    recognizer = sr.Recognizer()</span><br><span class="line">    microphone = sr.Microphone()</span><br><span class="line">​</span><br><span class="line">    word = random.choice(WORDS)  <span class="comment"># get a random word from the list</span></span><br><span class="line">​</span><br><span class="line">    instructions = (  <span class="comment"># format the instructions string</span></span><br><span class="line">        <span class="string">"I'm thinking of one of these words:\n"</span></span><br><span class="line">        <span class="string">"&#123;words&#125;\n"</span></span><br><span class="line">        <span class="string">"You have &#123;n&#125; tries to guess which one.\n"</span></span><br><span class="line">    ).format(words=<span class="string">', '</span>.join(WORDS), n=NUM_GUESSES)</span><br><span class="line">​</span><br><span class="line">    print(instructions)  <span class="comment"># show instructions and wait 3 seconds before starting the game</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_GUESSES):</span><br><span class="line">        <span class="comment"># get the guess from the user</span></span><br><span class="line">        <span class="comment"># if a transcription is returned, break out of the loop and continue</span></span><br><span class="line">        <span class="comment"># if no transcription returned and API request failed, break loop and continue</span></span><br><span class="line">        <span class="comment"># if API request succeeded but no transcription was returned, re-prompt the user</span></span><br><span class="line">        <span class="comment"># to say their guess again. Do this up to PROMPT_LIMIT times</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(PROMPT_LIMIT):</span><br><span class="line">            print(<span class="string">'Guess &#123;&#125;. Speak!'</span>.format(i + <span class="number">1</span>))</span><br><span class="line">            guess = recognize_speech_from_mic(recognizer, microphone)</span><br><span class="line">            <span class="keyword">if</span> guess[<span class="string">"transcription"</span>]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> guess[<span class="string">"success"</span>]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            print(<span class="string">"I didn't catch that. What did you say?\n"</span>)</span><br><span class="line">​</span><br><span class="line">        <span class="keyword">if</span> guess[<span class="string">"error"</span>]:  <span class="comment"># if there was an error, stop the game</span></span><br><span class="line">            print(<span class="string">"ERROR: &#123;&#125;"</span>.format(guess[<span class="string">"error"</span>]))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">​</span><br><span class="line">        print(<span class="string">"You said: &#123;&#125;"</span>.format(guess[<span class="string">"transcription"</span>]))  <span class="comment"># show the user the transcription</span></span><br><span class="line">​</span><br><span class="line">        <span class="comment"># determine if guess is correct and if any attempts remain</span></span><br><span class="line">        guess_is_correct = guess[<span class="string">"transcription"</span>].lower() == word.lower()</span><br><span class="line">        user_has_more_attempts = i &lt; NUM_GUESSES - <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">        <span class="comment"># determine if the user has won the game. if not, repeat the loop if user has</span></span><br><span class="line">        <span class="comment"># more attempts; if no attempts left, the user loses the game</span></span><br><span class="line">        <span class="keyword">if</span> guess_is_correct:</span><br><span class="line">            print(<span class="string">"Correct! You win!"</span>.format(word))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">elif</span> user_has_more_attempts:</span><br><span class="line">            print(<span class="string">"Incorrect. Try again.\n"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Sorry, you lose!\nI was thinking of '&#123;&#125;'."</span>.format(word))</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The <code>recognize_speech_from_mic()</code> function takes a <code>Recognizer</code> and <code>Microphone</code> instance as arguments and returns a dictionary with three keys. The first key, <code>success</code>, is a boolean that indicates whether or not the <code>API</code> request was successful. The second key, <code>error</code>, is either <code>None</code> or an error message indicating that the <code>API</code> is unavailable or the speech was unintelligible. Finally, the <code>transcription</code> key contains the transcription of the audio recorded by the microphone.<br>&emsp;&emsp;The function first checks that the recognizer and microphone arguments are of the correct type, and raises a <code>TypeError</code> if either is invalid:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> isinstance(recognizer, sr.Recognizer):</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">'`recognizer` must be `Recognizer` instance'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> isinstance(microphone, sr.Microphone):</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">'`microphone` must be a `Microphone` instance'</span>)</span><br></pre></td></tr></table></figure>
<p>The <code>listen()</code> method is then used to record microphone input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> microphone <span class="keyword">as</span> source:</span><br><span class="line">    recognizer.adjust_for_ambient_noise(source)</span><br><span class="line">    audio = recognizer.listen(source)</span><br></pre></td></tr></table></figure>
<p>The <code>adjust_for_ambient_noise()</code> method is used to calibrate the recognizer for changing noise conditions each time the <code>recognize_speech_from_mic()</code> function is called.<br>&emsp;&emsp;Next, <code>recognize_google()</code> is called to transcribe any speech in the recording. A <code>try...except</code> block is used to catch the <code>RequestError</code> and <code>UnknownValueError</code> exceptions and handle them accordingly. The success of the <code>API</code> request, any error messages, and the transcribed speech are stored in the success, error and transcription keys of the response dictionary, which is returned by the <code>recognize_speech_from_mic()</code> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">response = &#123;<span class="string">"success"</span>: <span class="keyword">True</span>, <span class="string">"error"</span>: <span class="keyword">None</span>, <span class="string">"transcription"</span>: <span class="keyword">None</span>&#125;</span><br><span class="line">​</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response[<span class="string">"transcription"</span>] = recognizer.recognize_google(audio)</span><br><span class="line"><span class="keyword">except</span> sr.RequestError:  <span class="comment"># API was unreachable or unresponsive</span></span><br><span class="line">    response[<span class="string">"success"</span>] = <span class="keyword">False</span></span><br><span class="line">    response[<span class="string">"error"</span>] = <span class="string">"API unavailable"</span></span><br><span class="line"><span class="keyword">except</span> sr.UnknownValueError:</span><br><span class="line">    response[<span class="string">"error"</span>] = <span class="string">"Unable to recognize speech"</span>  <span class="comment"># speech was unintelligible</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;You can test the <code>recognize_speech_from_mic()</code> function by saving the above script to a file called <code>guessing_game.py</code> and running the following in an interpreter session:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> guessing_game <span class="keyword">import</span> recognize_speech_from_mic</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = sr.Recognizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = sr.Microphone()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recognize_speech_from_mic(r, m)</span><br><span class="line">&#123;<span class="string">'success'</span>: <span class="keyword">True</span>, <span class="string">'error'</span>: <span class="keyword">None</span>, <span class="string">'transcription'</span>: <span class="string">'hello'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Your output will vary depending on what you say</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;The game itself is pretty simple. First, a list of words, a maximum number of allowed guesses and a prompt limit are declared:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WORDS = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grape'</span>, <span class="string">'orange'</span>, <span class="string">'mango'</span>, <span class="string">'lemon'</span>]</span><br><span class="line">NUM_GUESSES = <span class="number">3</span></span><br><span class="line">PROMPT_LIMIT = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>Next, a <code>Recognizer</code> and Microphone instance is created and a random word is chosen from <code>WORDS</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recognizer = sr.Recognizer()</span><br><span class="line">microphone = sr.Microphone()</span><br><span class="line">word = random.choice(WORDS)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;After printing some instructions and waiting for three seconds, a for loop is used to manage each user attempt at guessing the chosen word. The first thing inside the for loop is another for loop that prompts the user at most <code>PROMPT_LIMIT</code> times for a guess, attempting to recognize the input each time with the <code>recognize_speech_from_mic()</code> function and storing the dictionary returned to the local variable guess.<br>&emsp;&emsp;If the <code>transcription</code> key of guess is not <code>None</code>, then the user’s speech was transcribed and the inner loop is terminated with break. If the speech was not transcribed and the <code>success</code> key is set to <code>False</code>, then an <code>API</code> error occurred and the loop is again terminated with break. Otherwise, the <code>API</code> request was successful but the speech was unrecognizable. The user is warned and the for loop repeats, giving the user another chance at the current attempt.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(PROMPT_LIMIT):</span><br><span class="line">    print(<span class="string">'Guess &#123;&#125;. Speak!'</span>.format(i + <span class="number">1</span>))</span><br><span class="line">    guess = recognize_speech_from_mic(recognizer, microphone)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> guess[<span class="string">"transcription"</span>]:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> guess[<span class="string">"success"</span>]:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"I didn't catch that. What did you say?\n"</span>)</span><br></pre></td></tr></table></figure>
<p>Once the inner for loop terminates, the guess dictionary is checked for errors. If any occurred, the error message is displayed and the outer for loop is terminated with break, which will end the program execution.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> guess[<span class="string">'error'</span>]:</span><br><span class="line">    print(<span class="string">"ERROR: &#123;&#125;"</span>.format(guess[<span class="string">"error"</span>]))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;If there weren’t any errors, the transcription is compared to the randomly selected word. The <code>lower()</code> method for string objects is used to ensure better matching of the guess to the chosen word. The <code>API</code> may return speech matched to the word <code>apple</code> as <code>Apple</code> or <code>apple</code>, and either response should count as a correct answer.<br>&emsp;&emsp;If the guess was correct, the user wins and the game is terminated. If the user was incorrect and has any remaining attempts, the outer for loop repeats and a new guess is retrieved. Otherwise, the user loses the game.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">guess_is_correct = guess[<span class="string">"transcription"</span>].lower() == word.lower()</span><br><span class="line">user_has_more_attempts = i &lt; NUM_GUESSES - <span class="number">1</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> guess_is_correct:</span><br><span class="line">    print(<span class="string">'Correct! You win!'</span>.format(word))</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="keyword">elif</span> user_has_more_attempts:</span><br><span class="line">    print(<span class="string">'Incorrect. Try again.\n'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"Sorry, you lose!\nI was thinking of '&#123;&#125;'."</span>.format(word))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>When run, the output will look something like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">I<span class="string">'m thinking of one of these words:</span></span><br><span class="line"><span class="string">apple, banana, grape, orange, mango, lemon</span></span><br><span class="line"><span class="string">You have 3 tries to guess which one.</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">Guess 1. Speak!</span></span><br><span class="line"><span class="string">You said: banana</span></span><br><span class="line"><span class="string">Incorrect. Try again.</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">Guess 2. Speak!</span></span><br><span class="line"><span class="string">You said: lemon</span></span><br><span class="line"><span class="string">Incorrect. Try again.</span></span><br><span class="line"><span class="string">​</span></span><br><span class="line"><span class="string">Guess 3. Speak!</span></span><br><span class="line"><span class="string">You said: Orange</span></span><br><span class="line"><span class="string">Correct! You win!</span></span><br></pre></td></tr></table></figure>
<h4 id="Recognizing-Speech-in-Languages-Other-Than-English"><a href="#Recognizing-Speech-in-Languages-Other-Than-English" class="headerlink" title="Recognizing Speech in Languages Other Than English"></a>Recognizing Speech in Languages Other Than English</h4><p>&emsp;&emsp;Throughout this tutorial, we’ve been recognizing speech in <code>English</code>, which is the default language for each <code>recognize_*()</code> method of the <code>SpeechRecognition</code> package. However, it is absolutely possible to recognize speech in other languages, and is quite simple to accomplish.<br>&emsp;&emsp;To recognize speech in a different language, set the language keyword argument of the <code>recognize_*()</code> method to a string corresponding to the desired language. Most of the methods accept a <code>BCP-47</code> language tag, such as <code>en-US</code> for <code>American English</code>, or <code>fr-FR</code> for <code>French</code>. For example, the following recognizes <code>French</code> speech in an audio file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line">​</span><br><span class="line">r = sr.Recognizer()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> sr.AudioFile(<span class="string">'path/to/audiofile.wav'</span>) <span class="keyword">as</span> source:</span><br><span class="line">    audio = r.record(source)</span><br><span class="line">​</span><br><span class="line">r.recognize_google(audio, language=<span class="string">'fr-FR'</span>)</span><br></pre></td></tr></table></figure>
<p>Only the following methods accept a language keyword argument: <code>recognize_bing</code>, <code>recognize_google</code>, <code>recognize_google_cloud</code>, <code>recognize_ibm</code> and <code>recognize_sphinx</code>.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/21/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/21/">21</a><span class="page-number current">22</span><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/94/">94</a><a class="extend next" rel="next" href="/page/23/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">939</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
