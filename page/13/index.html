<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="暴徒">
<meta property="og:url" content="http://fukangwei.gitee.io/page/13/index.html">
<meta property="og:site_name" content="暴徒">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="暴徒">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/13/">





  <title>暴徒</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">暴徒</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/03/01/软件与硬件问题/Qt问题总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/01/软件与硬件问题/Qt问题总结/" itemprop="url">Qt问题总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-01T17:24:52+08:00">
                2019-03-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="undefined-reference-to-vtable"><a href="#undefined-reference-to-vtable" class="headerlink" title="undefined reference to vtable"></a>undefined reference to vtable</h3><p>&emsp;&emsp;问题描述：在某一个类中，如果加入<code>Q_OBJECT</code>，则<code>link</code>时会提示<code>undefined reference to vtable for &quot;xxx::xxx&quot;</code>。可是删掉<code>Q_OBJECT</code>，再编译程序时，<code>Qt</code>的应用程序输出中又会显示<code>Object::connect: No such slot XXX::XXX()</code>。<br>&emsp;&emsp;原因：删除<code>Q_OBJECT</code>之后，该类的信号槽机制就失效了，因此在其他类中<code>connect</code>该类的槽时，就会显示<code>No such slot...</code>。在该类中加上<code>Q_OBJECT</code>之后，<code>link</code>提示<code>undefined reference to vtable for &quot;xxx::xxx&quot;</code>。如果不是语法错误，那么可能的原因是该类原来没有添加<code>Q_OBJECT</code>，则在程序输出目录中，由<code>qmake</code>生成的<code>makefile</code>文件里不存在编译<code>Q_OBJECT</code>的信息，因此信号槽机制失效。而在该类加上<code>Q_OBJECT</code>后，<code>Qt Creator</code>没有自动执行<code>qmake</code>来更新<code>makefile</code>文件，这就会在<code>link</code>时有上述错误。<br>&emsp;&emsp;解决办法：删除程序的输出目录<code>build-XXX-XXX-release</code>，重新构建项目就行了。</p>
<h3 id="gcc-warning-will-be-initialized-after-Wreorder"><a href="#gcc-warning-will-be-initialized-after-Wreorder" class="headerlink" title="gcc warning will be initialized after [-Wreorder]"></a>gcc warning will be initialized after [-Wreorder]</h3><p>&emsp;&emsp;构造函数时，初始化成员变量的顺序要与类声明中的变量顺序相对应。若不对应，则出现题目中的错误。解决方法就是按照顺序进行初始化。对于这个问题，<code>StackOverflow</code>上也发生了讨论，以下摘录原文：<br>&emsp;&emsp;Question: I am getting a lot of these warnings from <code>3rd</code> party code that I cannot modify. Is there a way to disable this warning or at least disable it for certain areas (like <code>#pragma push/pop</code> in <code>VC++</code>)?</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>.h:<span class="number">1122</span>: warning: `<span class="built_in">list</span>&lt;LogOutput*, allocator&lt;LogOutput*&gt; &gt;::node_alloc_' will be initialized after</span><br><span class="line"><span class="built_in">list</span>.h:<span class="number">1117</span>: warning: `allocator&lt;LogOutput*&gt; <span class="built_in">list</span>&lt;LogOutput*, allocator&lt;LogOutput*&gt; &gt;::alloc_'</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Best Answer：Make sure the members appear in the initializer list in the same order as they appear in the class:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Class C &#123;</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    C() : b ( <span class="number">1</span> ), a ( <span class="number">2</span> ) &#123;&#125; <span class="comment">/* warning, should be “C(): a(2), b(1)” */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Qt-Creator开启C-11选项"><a href="#Qt-Creator开启C-11选项" class="headerlink" title="Qt Creator开启C++11选项"></a>Qt Creator开启C++11选项</h3><p>&emsp;&emsp;想要支持<code>C++11</code>的话，<code>GCC</code>的版本不能太低。具体方法：在<code>.pro</code>文件中加入下面这一句话：</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">QMAKE_CXXFLAGS += -std=c++0x  <span class="comment"># 对于Qt4.7版本</span></span><br><span class="line">CONFIG += c++11  <span class="comment"># 对于Qt5版本</span></span><br></pre></td></tr></table></figure>
<h3 id="Qt-Creator快捷操作"><a href="#Qt-Creator快捷操作" class="headerlink" title="Qt Creator快捷操作"></a>Qt Creator快捷操作</h3><ul>
<li>按<code>F2</code>快速切换到光标选中对象的源码。</li>
<li><code>Ctrl + /</code>注释或取消注释选定内容。</li>
</ul>
<h3 id="Qt-Creator控制台方式输出"><a href="#Qt-Creator控制台方式输出" class="headerlink" title="Qt Creator控制台方式输出"></a>Qt Creator控制台方式输出</h3><p>&emsp;&emsp;新建<code>Qt Console Application</code>项目：</p>
<p><img src="/2019/03/01/软件与硬件问题/Qt问题总结/1.png" height="235" width="278"></p>
<p>输入以下代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;QtCore/QCoreApplication&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">( <span class="keyword">int</span> argc, <span class="keyword">char</span> * argv[] )</span> </span>&#123;</span><br><span class="line">    <span class="function">QCoreApplication <span class="title">a</span> <span class="params">( argc, argv )</span></span>;</span><br><span class="line">    <span class="built_in">printf</span> ( <span class="string">"Hello World"</span> );</span><br><span class="line">    <span class="keyword">return</span> a.exec();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Object-connect-No-such-slot"><a href="#Object-connect-No-such-slot" class="headerlink" title="Object::connect: No such slot"></a>Object::connect: No such slot</h3><p>&emsp;&emsp;编译代码时，发现<code>make</code>的时候提示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Object::connect: No such slot FindDialog::enbleFindButton(const QString &amp;) no such slot</span><br></pre></td></tr></table></figure>
<p>根据网上资料，首先查看类声明中有没有<code>Q_OBJECT</code>，其次需要为新增的函数添加声明：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> slots:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">xxxx</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<p>同时不要忘记检查<code>slot</code>函数是否出现了拼写错误，如果有，可能也会出现上述问题。</p>
<h3 id="QFtp-No-such-file-or-directory"><a href="#QFtp-No-such-file-or-directory" class="headerlink" title="QFtp: No such file or directory"></a>QFtp: No such file or directory</h3><p>&emsp;&emsp;在项目的<code>.pro</code>文件中加入如下代码：</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QT += network</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/03/01/软件与硬件问题/IAR问题总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/01/软件与硬件问题/IAR问题总结/" itemprop="url">IAR问题总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-01T09:51:01+08:00">
                2019-03-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="IAR的有用的快捷键"><a href="#IAR的有用的快捷键" class="headerlink" title="IAR的有用的快捷键"></a>IAR的有用的快捷键</h3><ul>
<li>显示行号：在代码段点击右键，找到<code>Options -&gt; Editor</code>，将右面的<code>Show Line Numbers</code>勾选上。</li>
<li>注释：<code>Ctrl + K</code>；取消注释：<code>Ctrl + Shfit + K</code>。</li>
<li>自动缩进：选中某些行，然后使用<code>Ctrl + Shift + I</code>，可以实现自动缩进。</li>
<li>自动往后：选中某些行，然后使用<code>TAB</code>键，可以实现自动后退。</li>
</ul>
<h3 id="function-has-no-prototype"><a href="#function-has-no-prototype" class="headerlink" title="function has no prototype"></a>function has no prototype</h3><p>&emsp;&emsp;有时在<code>zigbee</code>协议栈代码中增加自己的代码时，<code>IAR</code>编译器会报告出<code>function has no prototype</code>的错误。解决方法：进入<code>options -&gt; C/C++ Compiler -&gt; Language</code>选项中，去掉<code>Require prototype</code>选项：</p>
<p><img src="/2019/03/01/软件与硬件问题/IAR问题总结/1.png" height="190" width="259"></p>
<h3 id="Error-Pe007-unrecognize-token"><a href="#Error-Pe007-unrecognize-token" class="headerlink" title="Error Pe007: unrecognize token"></a>Error Pe007: unrecognize token</h3><p>&emsp;&emsp;一般情况下是英文符号写成了中文符号，例如<code>;</code>写成了<code>；</code>。</p>
<h3 id="Segment-BANKED-CODE-must-be-defined-in-a-segment-definition-option-Z-b-or-P"><a href="#Segment-BANKED-CODE-must-be-defined-in-a-segment-definition-option-Z-b-or-P" class="headerlink" title="Segment BANKED_CODE must be defined in a segment definition option (-Z, -b or -P)"></a>Segment BANKED_CODE must be defined in a segment definition option (-Z, -b or -P)</h3><p>&emsp;&emsp;原因是<code>IAR</code>新版本使用旧版本的工程(芯片为<code>CC2530F256</code>)。解决方法：打开<code>project -&gt; options -&gt; linker -&gt; config -&gt; override default</code>，选择文件<code>C:\Program Files (x86)\IAR Systems\Embedded Workbench 6.0 Evaluation\8051\config\devices\Texas Instruments\lnk51ew_cc2530F256_banked.xcl</code>，选择<code>banked</code>的<code>xcl</code>文件即可。</p>
<h3 id="Unable-to-open-file-lnk51ew-cc2530b-xcl"><a href="#Unable-to-open-file-lnk51ew-cc2530b-xcl" class="headerlink" title="Unable to open file lnk51ew_cc2530b.xcl"></a>Unable to open file lnk51ew_cc2530b.xcl</h3><p>&emsp;&emsp;在做<code>light_switch</code>实验时，遇到问题这样的问题：<code>Error[e12]: Unable to open file &#39;lnk51ew_cc2530b.xcl&#39;</code>。下面是<code>TI</code>官网给的解决方案：Linker missing the right path to linker configuration file. In <code>IAR</code> open project options (<code>Alt + F7</code>), then go to <code>Linker -&gt; Config</code>. Under <code>Linker configuration file</code> check the override default check box, then click on <code>...</code> button and locate the linker configuration file, should be something like that: <code>...\IAR Systems\Embedded Workbench 6.0\8051\config\devices\Texas Instruments\lnk51ew_cc2530F256.xcl</code>.</p>
<h3 id="IAR设置自定义头文件路径"><a href="#IAR设置自定义头文件路径" class="headerlink" title="IAR设置自定义头文件路径"></a>IAR设置自定义头文件路径</h3><p>&emsp;&emsp;直接右击<code>IAR</code>左侧的<code>Project</code>名称，选中<code>Options</code>，就出现下面的选项。使用<code>$PROJ_DIR$</code>引用环境变量，这个环境变量在<code>IAR</code>中被解释为当前工程的绝对路径。然后依次在每一行添加一个目录即可。</p>
<p><img src="/2019/03/01/软件与硬件问题/IAR问题总结/2.png" height="242" width="356"></p>
<p>&emsp;&emsp;在<code>options</code>窗口的<code>C/C++ compiler</code>选项和<code>Assembler</code>选项里，有个<code>preprocessor</code>栏，通常情况下有如下两行：</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$TOOLKIT_DIR$\INC\</span><br><span class="line">$TOOLKIT_DIR$\INC\CLIB\</span><br></pre></td></tr></table></figure>
<p>以上两行的<code>$TOOLKIT_DIR$\</code>意思是安装目录，表示包含头文件的路径在<code>IAR</code>安装路径的<code>8051</code>文件夹下，也就是说如果<code>IAR</code>安装在<code>C</code>盘中，那么它就表示<code>C:\Program Files\IAR Systems\Embedded Workbench 4.05 Evaluation version\8051</code>这个路径。<br>&emsp;&emsp;在<code>additional include directories:</code>下面的空白框里填上你的头文件包含目录。为了备份和拷贝方便，最好把工程文件放到项目目录中，设置方法如下：<code>$PROJ_DIR$\</code>即你当前工作的<code>workspace</code>的目录，<code>..\</code>表示对应目录的上一层目录，比如<code>$PROJ_DIR$\..\inc</code>表示你的<code>WORKSPACE</code>目录上一层的<code>INC</code>目录。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/软件与硬件问题/Qt的安装/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/软件与硬件问题/Qt的安装/" itemprop="url">Qt的安装</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T20:48:51+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Windows安装Qt"><a href="#Windows安装Qt" class="headerlink" title="Windows安装Qt"></a>Windows安装Qt</h3><p>&emsp;&emsp;<code>Qt</code>一直以来分为商业、开源两个版本。商业版本为用户提供了二级制的动态库，直接安装既可以使用，但是需要花钱购买<code>license</code>；开源版本则遵守<code>GPL</code>协议，提供了源码，用户需要自行编译才能生产动态库文件。<br>&emsp;&emsp;<code>Qt</code>开发首先要安装<code>Qt library</code>和<code>Qt Creator</code>。<code>Qt library</code>为<code>Qt</code>程序提供头文件、静态链接库和动态链接库；<code>Qt Creator</code>则是用于编程的<code>IDE</code>，提供了<code>GUI</code>界面绘制、代码编写、程序调试等多个功能。<code>Qt</code>的官方下载网址为<code>http://qt-project.org/downloads</code>。<br>&emsp;&emsp;<code>Qt Library</code>有很多版本，在<code>Windows</code>下可以选择的就有<code>MinGW</code>、<code>VS2008</code>、<code>VS2010</code>、<code>VS2012</code>。这些版本的主要是编译器的不同，因为<code>Qt Library</code>还需要我们自己编译才可以使用，所以你要决定好用什么编译器，然后在选择那个版本。<br>&emsp;&emsp;选择好了<code>Qt Library</code>之后，在安装之前确认做好了足够的准备。如果你下载的是<code>MinGW</code>版本的，那么请安装<code>MinGW</code>之后再安装<code>Qt Library</code>。安装过程中会提示你选择<code>MinGW</code>的安装目录。<br>&emsp;&emsp;以下介绍<code>MinGW</code>版本的安装：<br>&emsp;&emsp;1. 安装<code>MinGW</code>：下载地址是<code>http://mingwbuilds.sourceforge.net/</code>，将解压之后的文件放在<code>C</code>盘根目录下(其他盘也可以，但是在安装<code>Qt Library</code>时记得选择路径)。<br>&emsp;&emsp;2. 安装<code>Qt Library</code>：开始安装<code>Qt Library</code>，按照提示安装下去。中间选择<code>MinGW</code>安装目录，选择刚才你安装的地方，一般为<code>C:\mingw</code>，最后提示安装完成。这里下载的是开源版本，实际上就是<code>Qt library</code>的源代码和一些静态库。还需要对其进行编译，方法是<code>开始菜单 -&gt; 程序 -&gt; Qt by Digia v4.8.5 (MinGW OpenSource) -&gt; Qt 4.8.5 (Build Debug Libraries)</code>，会出现命令行窗口，接下来需要做的就是编译<code>Library</code>。命令行只有两次交互，第一次是让你选择开源版本<code>o</code>，还是商业版本<code>c</code>，键盘点击<code>o</code>；第二次是提示你是否同意条款，键盘点击<code>y</code>。接下来就是漫长的编译过程。<br>&emsp;&emsp;全部安装完成之后，如果想在<code>Qt creator</code>中使用<code>Qt</code>的<code>sdk</code>，还需要进行一些设置。打开<code>creator</code>，菜单栏中的<code>工具 -&gt; 选项</code>，在左侧中选择<code>构建和调试</code>，在<code>compiler</code>、<code>Qt版本</code>选项卡中，选择<code>MinGW</code>的<code>g++.exe</code>的路径(通常是在<code>mingw</code>目录下的<code>bin</code>中)，选择<code>Qt</code>的<code>qmake</code>路径以及版本号(<code>qmake</code>通常是<code>Qt\4.8.4\bin\qmake.exe</code>)。最后选择<code>Kit</code>选项卡，单击<code>add</code>按钮创建一个新的编译设置项，按照提示选择编译器、<code>Debuger</code>(通常是<code>mingw</code>目录的<code>bin\gdb.exe</code>)、<code>Qt</code>版本号。</p>
<p><img src="/2019/02/28/软件与硬件问题/Qt的安装/1.png" height="201" width="636"></p>
<hr>
<h3 id="树莓派安装Qt"><a href="#树莓派安装Qt" class="headerlink" title="树莓派安装Qt"></a>树莓派安装Qt</h3><h4 id="apt-get"><a href="#apt-get" class="headerlink" title="apt-get"></a>apt-get</h4><p>&emsp;&emsp;Firstly I got the development tools needed by <code>Qt Creator</code> in the hope it would be less heavy for the <code>Pi</code> to download separately.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install qt4-dev-tools</span><br></pre></td></tr></table></figure>
<p>Then I went for <code>Qt Creator</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install qtcreator</span><br></pre></td></tr></table></figure>
<p>I also installed:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gcc</span><br><span class="line">sudo apt-get install xterm</span><br><span class="line">sudo apt-get install git-core</span><br><span class="line">sudo apt-get install subversion</span><br></pre></td></tr></table></figure>
<h4 id="Problem-no-toolchain"><a href="#Problem-no-toolchain" class="headerlink" title="Problem: no toolchain"></a>Problem: no toolchain</h4><p>&emsp;&emsp;We can only compile for remote embedded devices and this is not the case here, because we are on the <code>Pi</code> and not remotely accessing it.<br>&emsp;&emsp;I added a gcc toolchain: <code>Tools/Options</code> &gt; <code>build &amp; run</code> &gt; <code>tab tool chains</code> &gt; <code>button add</code>, choose <code>GCC</code>, then set:</p>
<ul>
<li><code>compiler path</code>: <code>/usr/bin/arm-linux-gnueabihf-gcc-4.6</code></li>
<li><code>Debugger</code>: <code>/usr/bin/gdb</code></li>
<li><code>Mkspec</code>: <code>default</code></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/软件与硬件问题/OpenCV的安装/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/软件与硬件问题/OpenCV的安装/" itemprop="url">OpenCV的安装</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T19:16:28+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Linux下的安装-2-4-9版"><a href="#Linux下的安装-2-4-9版" class="headerlink" title="Linux下的安装(2.4.9版)"></a>Linux下的安装(2.4.9版)</h3><p>&emsp;&emsp;1. 安装<code>cmake</code>以及一些依赖库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install cmake</span><br><span class="line">sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev</span><br><span class="line">sudo apt-get install libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;2. 从官网下载<code>opencv</code>源代码，随后进行如下操作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">unzip OpenCV-2.4.9.zip</span><br><span class="line"><span class="built_in">cd</span> opencv-2.4.9</span><br><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">sudo make -j4</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. 将<code>opencv</code>库路径加入到系统中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/ld.so.conf.d/opencv.conf</span><br></pre></td></tr></table></figure>
<p>在其末尾加入<code>/usr/local/lib</code>，保存并退出，随后使配置生效：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ldconfig  <span class="comment"># 使配置生效</span></span><br></pre></td></tr></table></figure>
<p>打开文件<code>/etc/bash.bashrc</code>，并在文件末尾加入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PKG_CONFIG_PATH=<span class="variable">$PKG_CONFIG_PATH</span>:/usr/<span class="built_in">local</span>/lib/pkgconfig</span><br><span class="line"><span class="built_in">export</span> PKG_CONFIG_PATH</span><br></pre></td></tr></table></figure>
<p>最后使配置生效：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/bash.bashrc  <span class="comment"># 使配置生效</span></span><br><span class="line">sudo updatedb  <span class="comment"># 更新database</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;4. 测试：进入目录<code>opencv-2.4.9/samples/c</code>，运行<code>build_all.sh</code>脚本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build_all.sh</span><br></pre></td></tr></table></figure>
<p>使用下面的一个<code>sample</code>进行测试：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./pyramid_segmentation</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/软件与硬件问题/OpenCV的安装/1.png" height="212" width="270"></p>
<p>&emsp;&emsp;<strong>补充说明</strong>：<br>&emsp;&emsp;1. 一定不要忘记安装<code>g++</code>编译器。<br>&emsp;&emsp;2. 最好不要在<code>samba</code>服务器的目录下编译<code>opencv</code>。</p>
<hr>
<h3 id="Windows下的安装-2-4-13版本"><a href="#Windows下的安装-2-4-13版本" class="headerlink" title="Windows下的安装(2.4.13版本)"></a>Windows下的安装(2.4.13版本)</h3><p>&emsp;&emsp;在官网(<code>http://opencv.org/</code>)上找到<code>OpenCV</code>的<code>Windows</code>版，并下载下来。双击该文件后会提示解压到某个地方：</p>
<p><img src="/2019/02/28/软件与硬件问题/OpenCV的安装/2.png" height="212" width="270"></p>
<p>&emsp;&emsp;在解压目录下会有<code>build</code>和<code>source</code>两个文件夹，<code>build</code>下是使用<code>OpenCV</code>相关的文件，<code>sources</code>下是<code>OpenCV</code>的源代码。<br>&emsp;&emsp;接下来添加环境变量。如果是<code>64</code>位系统，将文件夹下的<code>opencv\build\x64\vc14\bin</code>目录添加到系统变量<code>Path</code>中；如果是<code>32</code>位系统，则将<code>opencv\build\x86\vc14\bin</code>目录添加到环境变量。</p>
<h4 id="工程包含-include-目录的配置"><a href="#工程包含-include-目录的配置" class="headerlink" title="工程包含(include)目录的配置"></a>工程包含(include)目录的配置</h4><p>&emsp;&emsp;使用<code>visual studio</code>新建一个<code>hello world</code>工程，然后点击<code>View -&gt; Other Windows -&gt; Property Manager</code>进行配置：</p>
<p><img src="/2019/02/28/软件与硬件问题/OpenCV的安装/3.png" height="169" width="161"></p>
<p>&emsp;&emsp;如果想采用<code>Debug</code>模式编译代码，则双击<code>Debug|64</code>，出现如下界面：</p>
<p><img src="/2019/02/28/软件与硬件问题/OpenCV的安装/4.png" height="122" width="644"></p>
<p>&emsp;&emsp;在<code>通用属性 -&gt; VC++目录 -&gt; 包含目录</code>中添加如下目录：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">D:\Program Files\opencv\build\include</span><br><span class="line">D:\Program Files\opencv\build\include\opencv</span><br><span class="line">D:\Program Files\opencv\build\include\opencv2</span><br></pre></td></tr></table></figure>
<h4 id="工程库-lib-目录的配置"><a href="#工程库-lib-目录的配置" class="headerlink" title="工程库(lib)目录的配置"></a>工程库(lib)目录的配置</h4><p>&emsp;&emsp;在<code>通用属性 -&gt; VC++目录 -&gt; 库目录</code>中添加如下目录：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\Program Files\opencv\build\x64\vc14\lib</span><br></pre></td></tr></table></figure>
<h4 id="链接库的配置"><a href="#链接库的配置" class="headerlink" title="链接库的配置"></a>链接库的配置</h4><p>&emsp;&emsp;在<code>通用属性 -&gt; 链接器 -&gt; 输入 -&gt; 附加的依赖项</code>中输入如下内容：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">opencv_calib3d2413d.lib</span><br><span class="line">opencv_contrib2413d.lib</span><br><span class="line">opencv_core2413d.lib</span><br><span class="line">opencv_features2d2413d.lib</span><br><span class="line">opencv_flann2413d.lib</span><br><span class="line">opencv_gpu2413d.lib</span><br><span class="line">opencv_highgui2413d.lib</span><br><span class="line">opencv_imgproc2413d.lib</span><br><span class="line">opencv_legacy2413d.lib</span><br><span class="line">opencv_ml2413d.lib</span><br><span class="line">opencv_nonfree2413d.lib</span><br><span class="line">opencv_objdetect2413d.lib</span><br><span class="line">opencv_ocl2413d.lib</span><br><span class="line">opencv_photo2413d.lib</span><br><span class="line">opencv_stitching2413d.lib</span><br><span class="line">opencv_superres2413d.lib</span><br><span class="line">opencv_ts2413d.lib</span><br><span class="line">opencv_video2413d.lib</span><br><span class="line">opencv_videostab2413d.lib</span><br></pre></td></tr></table></figure>
<p>需要注意的是，所粘贴内容即为之前解压的<code>OpencV</code>目录<code>D:\opencv\build\x64\vc14\lib</code>下所有<code>lib</code>库文件的名字，其中的<code>2413</code>代表<code>OpenCV</code>版本为<code>2.4.13</code>。<code>Debug</code>文件库名有<code>d</code>结尾，<code>Release</code>则没有，例如<code>opencv_ts2413d.lib</code>是<code>debug</code>版本，<code>opencv_ts248.lib</code>是<code>release</code>版本。<br>&emsp;&emsp;最后使用如下代码进行测试：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Mat img = imread(<span class="string">"empire.jpg"</span>);</span><br><span class="line">    imshow(<span class="string">"Picture"</span>, img);</span><br><span class="line">    waitKey(<span class="number">6000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/软件与硬件问题/Libcoap安装和使用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/软件与硬件问题/Libcoap安装和使用/" itemprop="url">Libcoap安装和使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T18:03:08+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>libcoap</code>是<code>CoAP</code>协议的<code>C</code>语言实现，它提供<code>server</code>和<code>client</code>功能，是调试<code>CoAP</code>的有力工具。通过<code>git clone</code>获取最新版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/authmillenon/libcoap.git</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;由于<code>libcoap</code>源码中有<code>configure.in</code>文件，需要使用<code>autoconf</code>生成<code>configure</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">autoconf  <span class="comment"># 生成configure文件</span></span><br><span class="line">./configure  <span class="comment"># 生成makefile文件</span></span><br><span class="line">make  <span class="comment"># 编译</span></span><br><span class="line">sudo make install  <span class="comment"># 安装</span></span><br></pre></td></tr></table></figure>
<p>安装完成之后，系统便会新建<code>/usr/local/include/libcoap</code>目录，并把<code>coap.h</code>、<code>config.h</code>、<code>debug.h</code>、<code>pdu.h</code>等头文件复制到该目录中。另外，编译生成的<code>libcoap.a</code>静态链接库被复制到了<code>/usr/local/lib</code>中。<br>&emsp;&emsp;进入<code>example</code>文件夹，在该文件夹中还有两个可执行文件，即<code>coap-client</code>和<code>coap-server</code>。<code>coap-client</code>提供非常多的客户端测试指令，而<code>coap-server</code>提供一个较为简单的服务器端功能。</p>
<h3 id="服务器端测试"><a href="#服务器端测试" class="headerlink" title="服务器端测试"></a>服务器端测试</h3><p>&emsp;&emsp;在开始服务器端测试之前，必须知道服务器端的<code>IP</code>地址，假设为<code>10.13.11.85</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./coap-server  <span class="comment"># 运行coap-server</span></span><br></pre></td></tr></table></figure>
<p>在<code>firefox</code>中运行使用<code>GET</code>方法访问该服务器，该步骤需要保证<code>firefox</code>已经安装<code>copper</code>插件。在浏览器地址栏中输入<code>coap://10.13.11.85</code>，点击工具栏中的<code>GET</code>按钮，那么<code>coap-server</code>便会返回以下内容：</p>
<p><img src="/2019/02/28/软件与硬件问题/Libcoap安装和使用/1.png" height="212" width="325"></p>
<h3 id="客户端测试"><a href="#客户端测试" class="headerlink" title="客户端测试"></a>客户端测试</h3><p>&emsp;&emsp;运行<code>coap-server</code>和<code>coap-client</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./coap-server  <span class="comment"># 运行coap-server</span></span><br><span class="line">./coap-client -m get -o result.txt coap://localhost  <span class="comment"># 运行coap-client</span></span><br></pre></td></tr></table></figure>
<p><code>-m</code>表示<code>coap</code>访问方法，此处设置为<code>get</code>方法。除了<code>get</code>方法，还包括<code>put</code>、<code>post</code>和<code>delete</code>。<code>-o</code>表示访问获得的内容保存到<code>result.txt</code>文件中。<br>&emsp;&emsp;如果访问成功，那么控制台输出<code>v:1 t:0 tkl:0 c:1 id:45104</code>。<code>v</code>表示<code>coap</code>的版本编号，此时的<code>coap</code>版本编号为<code>1</code>；<code>t</code>表示报文类型为<code>CON</code>；<code>tkl</code>表示<code>token</code>区域的长度，此时的<code>token</code>区域的长度为<code>0</code>；<code>c</code>表示访问方法；<code>id</code>表示<code>message id</code>。<code>result.txt</code>文件内容为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">This is a <span class="built_in">test</span> server made with libcoap (see http://libcoap.sf.net)</span><br><span class="line">Copyright (C) 2010--2013 Olaf Bergmann &lt;bergmann@tzi.org&gt;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<strong>补充说明</strong>：<br>&emsp;&emsp;1. <code>Ubuntu</code>安装<code>autoconf</code>需要使用命令<code>sudo apt-get install autoconf</code>。<br>&emsp;&emsp;2. <code>libcoap</code>的<code>post</code>使用方法如下(<code>-e</code>指定<code>post</code>方法的负载)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./coap-client coap://ip地址/<span class="built_in">test</span>/led -m post -e <span class="string">"&amp;color=r&amp;mode=on"</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;3. 对于<code>IPv6</code>地址，要使用方括号括起来，例如<code>coap://[aaaa::212:4b00:102a:a556]/test/hello</code>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/RNN和LSTM/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/RNN和LSTM/" itemprop="url">RNN和LSTM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T16:41:09+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="RNN的用途"><a href="#RNN的用途" class="headerlink" title="RNN的用途"></a>RNN的用途</h3><p>&emsp;&emsp;现在请你看着这个名字，不出意外的话，你应该可以脱口而出，因为你很可能就用了他们家的一款产品。那么现在请抛开这个产品，只想着<code>史蒂芬乔布斯</code>这个名字，请你再把它逆序念出来。有点难吧，这就说明对于预测，顺序排列是多么得重要。我们可以预测下一个按照一定顺序排列的字，但是如果打乱顺序，我们就没办法分析自己到底在说什么了。</p>
<p><img src="/2019/02/28/深度学习/RNN和LSTM/1.png" height="128" width="376"></p>
<h3 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h3><p>&emsp;&emsp;想象现在有一组序列数据<code>data0</code>、<code>data1</code>、<code>data2</code>和<code>data3</code>，当预测<code>result0</code>时，我们是基于<code>data0</code>；同样在预测其他数据的时候，我们也都只单单基于单个的数据，每次使用的神经网络都是同一个<code>NN</code>。不过某些数据是有关联顺序的，就像在厨房做菜，酱料<code>A</code>要比酱料<code>B</code>早放，不然就串味了。所以普通的神经网络结构并不能让<code>NN</code>了解这些数据之间的关联。</p>
<p><img src="/2019/02/28/深度学习/RNN和LSTM/2.png" height="196" width="379"></p>
<h3 id="处理序列数据的神经网络"><a href="#处理序列数据的神经网络" class="headerlink" title="处理序列数据的神经网络"></a>处理序列数据的神经网络</h3><p>&emsp;&emsp;那我们如何让数据间的关联也被<code>NN</code>加以分析呢？想想我们人类是怎么分析各种事物的关联吧，最基本的方式就是记住之前发生的事情。那我们让神经网络也具备这种记住之前发生的事的能力，在分析<code>Data0</code>的时候，把分析结果存入记忆，然后当分析<code>data1</code>的时候，<code>NN</code>会产生新的记忆。但是新记忆和老记忆是没有联系的，我们就简单地把老记忆调用过来，一起分析。如果继续分析更多的有序数据，<code>RNN</code>就会把之前的记忆都累积起来，一起分析。</p>
<p><img src="/2019/02/28/深度学习/RNN和LSTM/3.png" height="214" width="777"></p>
<p>&emsp;&emsp;我们再重复一遍刚才的流程，不过这次是以加入一些数学方面的东西。每次<code>RNN</code>运算完之后都会产生一个对于当前状态的描述<code>state</code>，我们用简写<code>s(t)</code>代替，然后这个<code>RNN</code>开始分析<code>x(t + 1)</code>，它会根据<code>x(t + 1)</code>产生<code>s(t + 1)</code>，不过此时<code>y(t + 1)</code>是由<code>s(t)</code>和<code>s(t + 1)</code>共同创造的，所以我们通常看到的<code>RNN</code>也可以表达成最右边这种样子。</p>
<h3 id="RNN的弊端"><a href="#RNN的弊端" class="headerlink" title="RNN的弊端"></a>RNN的弊端</h3><p>&emsp;&emsp;之前我们说过，<code>RNN</code>是在有顺序的数据上进行学习的。为了记住这些数据，<code>RNN</code>会像人一样产生对先前发生事件的记忆。不过一般形式的<code>RNN</code>就像一个老爷爷，有时候比较健忘，为什么会这样呢？</p>
<p><img src="/2019/02/28/深度学习/RNN和LSTM/4.png" height="197" width="688"></p>
<p>&emsp;&emsp;想像现在有这样一个<code>RNN</code>，它的输入值是一句话<code>我今天要做红烧排骨，首先要准备排骨，然后...，最后美味的一道菜就出锅了</code>，现在请<code>RNN</code>来分析，我今天做的到底是什么菜呢。<code>RNN</code>可能会给出<code>辣子鸡</code>这个答案，由于判断失误，<code>RNN</code>就要开始学习这个长序列<code>X</code>和<code>红烧排骨</code>的关系，而<code>RNN</code>需要的关键信息<code>红烧排骨</code>却出现在句子开头。</p>
<p><img src="/2019/02/28/深度学习/RNN和LSTM/5.png" height="188" width="772"></p>
<p>&emsp;&emsp;再来看看<code>RNN</code>是怎样学习的。<code>红烧排骨</code>这个信息的记忆要进过长途跋涉才能抵达最后一个时间点，然后我们得到误差，而且在反向传递得到的误差的时候，它在每一步都会乘以一个自己的参数<code>W</code>。如果这个<code>W</code>是一个小于<code>1</code>的数(比如<code>0.9</code>)，这个<code>0.9</code>不断乘以误差，误差传到初始时间点也会是一个接近于零的数。所以对于初始时刻，误差相当于就消失了。我们把这个问题叫做梯度消失(<code>Gradient vanishing</code>)；反之如果<code>W</code>是一个大于<code>1</code>的数(比如<code>1.1</code>)，不断累乘，则到最后变成了无穷大的数，<code>RNN</code>被这无穷大的数撑死了，这种情况叫做梯度爆炸(<code>Gradient exploding</code>)。这就是普通<code>RNN</code>没有办法回忆起久远记忆的原因。</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>&emsp;&emsp;<code>LSTM</code>就是为了解决这个问题而诞生的。和普通<code>RNN</code>相比，<code>LSTM</code>多出了三个控制器，即输入控制、输出控制和忘记控制。</p>
<p><img src="/2019/02/28/深度学习/RNN和LSTM/6.png" height="209" width="252"></p>
<p>&emsp;&emsp;<code>LSTM</code>多了一个控制全局的记忆，这里用粗线代替。为了方便理解，我们把粗线想象成电影或游戏当中的主线剧情，而原本的<code>RNN</code>体系就是分线剧情，三个控制器都是在原始的<code>RNN</code>体系上。我们先看输入方面，如果此时的分线剧情对于剧终结果十分重要，输入控制就会将这个分线剧情按重要程度写入主线剧情进行分析；再看忘记方面，如果此时的分线剧情更改了我们对之前剧情的想法，那么忘记控制就会将之前的某些主线剧情忘记，按比例替换成现在的新剧情，所以主线剧情的更新就取决于输入和忘记控制。对于最后的输出方面，输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么。基于这些控制机制，<code>LSTM</code>就像延缓记忆衰退的良药，可以带来更好的结果。</p>
<hr>
<h3 id="RNN之Classifier"><a href="#RNN之Classifier" class="headerlink" title="RNN之Classifier"></a>RNN之Classifier</h3><p>&emsp;&emsp;这次用循环神经网络进行分类，采用<code>MNIST</code>数据集，主要用到<code>SimpleRNN</code>层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> SimpleRNN, Activation, Dense</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br></pre></td></tr></table></figure>
<p><code>MNIST</code>的图像分辨率是<code>28 * 28</code>，为了使用<code>RNN</code>，我们将图像理解为序列化数据。每一行作为一个输入单元，所以输入数据大小<code>INPUT_SIZE</code>为<code>28</code>。先是第<code>1</code>行输入，再是第<code>2</code>行，直到第<code>28</code>行输入，一张图片也就是一个序列，所以步长<code>TIME_STEPS</code>为<code>28</code>。训练数据要进行归一化处理，因为原始数据是<code>8bit</code>灰度图像所以需要除以<code>255</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">TIME_STEPS = <span class="number">28</span>  <span class="comment"># same as the height of the image</span></span><br><span class="line">INPUT_SIZE = <span class="number">28</span>  <span class="comment"># same as the width of the image</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">BATCH_INDEX = <span class="number">0</span></span><br><span class="line">OUTPUT_SIZE = <span class="number">10</span></span><br><span class="line">CELL_SIZE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># download the mnist to the path '~/.keras/datasets/' if it is the first time to be called</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()  <span class="comment"># X shape (60,000 28x28), y shape (10,000, )</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># data pre-processing</span></span><br><span class="line">X_train = X_train.reshape(<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>) / <span class="number">255.</span>  <span class="comment"># normalize</span></span><br><span class="line">X_test = X_test.reshape(<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>) / <span class="number">255.</span>  <span class="comment"># normalize</span></span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">10</span>)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;首先添加<code>RNN</code>层，输入为训练数据，输出数据大小由<code>CELL_SIZE</code>定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()  <span class="comment"># build RNN model</span></span><br><span class="line">​</span><br><span class="line">model.add(SimpleRNN(  <span class="comment"># RNN cell</span></span><br><span class="line">    <span class="comment"># for batch_input_shape, if using tensorflow as the backend, we have to</span></span><br><span class="line">    <span class="comment"># put None for the batch_size. Otherwise, model.evaluate() will get error.</span></span><br><span class="line">    batch_input_shape=(<span class="keyword">None</span>, TIME_STEPS, INPUT_SIZE),  <span class="comment"># Or: input_dim = INPUT_SIZE, input_length = TIME_STEPS</span></span><br><span class="line">    output_dim=CELL_SIZE,</span><br><span class="line">    unroll=<span class="keyword">True</span>,</span><br><span class="line">))</span><br></pre></td></tr></table></figure>
<p>然后添加输出层，激励函数选择<code>softmax</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.add(Dense(OUTPUT_SIZE))  <span class="comment"># output layer</span></span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">adam = Adam(LR)  <span class="comment"># optimizer</span></span><br><span class="line">model.compile(optimizer=adam, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>设置优化方法、<code>loss</code>函数和<code>metrics</code>方法之后就可以开始训练了。每次训练的时候并不是取所有的数据，只是取<code>BATCH_SIZE</code>个序列，或者称为<code>BATCH_SIZE</code>张图片，这样可以大大降低运算时间，提高训练效率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">4001</span>):  <span class="comment"># training</span></span><br><span class="line">    <span class="comment"># data shape = (batch_num, steps, inputs/outputs)</span></span><br><span class="line">    X_batch = X_train[BATCH_INDEX: BATCH_INDEX + BATCH_SIZE, :, :]</span><br><span class="line">    Y_batch = y_train[BATCH_INDEX: BATCH_INDEX + BATCH_SIZE, :]</span><br><span class="line">    cost = model.train_on_batch(X_batch, Y_batch)</span><br><span class="line">    BATCH_INDEX += BATCH_SIZE</span><br><span class="line">    BATCH_INDEX = <span class="number">0</span> <span class="keyword">if</span> BATCH_INDEX &gt;= X_train.shape[<span class="number">0</span>] <span class="keyword">else</span> BATCH_INDEX</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        cost, accuracy = model.evaluate(X_test, y_test, batch_size=y_test.shape[<span class="number">0</span>], verbose=<span class="keyword">False</span>)</span><br><span class="line">        print(<span class="string">'test cost: '</span>, cost, <span class="string">'test accuracy: '</span>, accuracy)</span><br></pre></td></tr></table></figure>
<h3 id="RNN之Regressor"><a href="#RNN之Regressor" class="headerlink" title="RNN之Regressor"></a>RNN之Regressor</h3><p>&emsp;&emsp;这次使用<code>RNN</code>来求解回归问题，首先生成序列<code>sin(x)</code>，对应输出数据为<code>cos(x)</code>，设置序列步长为<code>20</code>，每次训练的<code>BATCH_SIZE</code>为<code>50</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM, TimeDistributed, Dense</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line">​</span><br><span class="line">BATCH_START = <span class="number">0</span></span><br><span class="line">TIME_STEPS = <span class="number">20</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span></span><br><span class="line">OUTPUT_SIZE = <span class="number">1</span></span><br><span class="line">CELL_SIZE = <span class="number">20</span></span><br><span class="line">LR = <span class="number">0.006</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> BATCH_START, TIME_STEPS</span><br><span class="line">    <span class="comment"># xs shape (50 batch, 20 steps)</span></span><br><span class="line">    xs = np.arange(BATCH_START, BATCH_START + TIME_STEPS * BATCH_SIZE) \</span><br><span class="line">            .reshape((BATCH_SIZE, TIME_STEPS)) / (<span class="number">10</span> * np.pi)</span><br><span class="line">    seq = np.sin(xs)</span><br><span class="line">    res = np.cos(xs)</span><br><span class="line">    BATCH_START += TIME_STEPS</span><br><span class="line">    <span class="keyword">return</span> [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;然后添加<code>LSTM</code>层，输入为训练数据，输出数据大小由<code>CELL_SIZE</code>定义。因为每一个输入都对应一个输出，所以<code>return_sequences=True</code>。每一个点的当前输出都受前面所有输出的影响，<code>BATCH</code>之间的参数也需要记忆，故<code>stateful=True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">​</span><br><span class="line">model.add(LSTM(  <span class="comment"># build a LSTM RNN</span></span><br><span class="line">    <span class="comment"># Or: input_dim=INPUT_SIZE, input_length=TIME_STEPS</span></span><br><span class="line">    batch_input_shape=(BATCH_SIZE, TIME_STEPS, INPUT_SIZE),</span><br><span class="line">    output_dim=CELL_SIZE,</span><br><span class="line">    return_sequences=<span class="keyword">True</span>,  <span class="comment"># True: output at all steps. False: output as last step.</span></span><br><span class="line">    stateful=<span class="keyword">True</span>,  <span class="comment"># True: the final state of batch1 is feed into the initial state of batch2</span></span><br><span class="line">))</span><br></pre></td></tr></table></figure>
<p>最后添加输出层，<code>LSTM</code>层的每一步都有输出，使用<code>TimeDistributed</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(TimeDistributed(Dense(OUTPUT_SIZE)))</span><br><span class="line">adam = Adam(LR)</span><br><span class="line">model.compile(optimizer=adam, loss=<span class="string">'mse'</span>, )</span><br></pre></td></tr></table></figure>
<p>设置优化方法、<code>loss</code>函数和<code>metrics</code>方法之后就可以开始训练了。训练<code>501</code>次，调用<code>matplotlib</code>函数采用动画的方式输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Training ------------'</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">501</span>):</span><br><span class="line">    <span class="comment"># data shape = (batch_num, steps, inputs/outputs)</span></span><br><span class="line">    X_batch, Y_batch, xs = get_batch()</span><br><span class="line">    cost = model.train_on_batch(X_batch, Y_batch)</span><br><span class="line">    pred = model.predict(X_batch, BATCH_SIZE)</span><br><span class="line">    plt.plot(xs[<span class="number">0</span>, :], Y_batch[<span class="number">0</span>].flatten(), <span class="string">'r'</span>, xs[<span class="number">0</span>, :], pred.flatten()[:TIME_STEPS], <span class="string">'b--'</span>)</span><br><span class="line">    plt.ylim((<span class="number">-1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">    plt.draw()</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'train cost: '</span>, cost)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/自编码和解码/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/自编码和解码/" itemprop="url">自编码和解码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T14:00:44+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h3><p>&emsp;&emsp;有一个神经网络，它所做的事情是接收一张图片，然后给它打码，最后再从打码后的图片中还原，具体过程如下：</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/1.png" height="229" width="832"></p>
<p>&emsp;&emsp;可以看出图片其实是经过了压缩再解压的这一道工序，当压缩的时候，原有的图片质量被缩减，解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片。为什么要这样做呢？</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/2.png" height="200" width="394"></p>
<p>&emsp;&emsp;原来有时神经网络要接受大量的输入信息，比如输入信息是高清图片时，输入信息量可能达到上千万，让神经网络直接从上千万个信息源中学习是一件很吃力的工作。所以需要压缩一下，提取出原图片中的最具代表性的信息，缩减输入信息量，再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了，自编码就能在这时发挥作用。通过将原数据白色的<code>X</code>压缩并解压成黑色的<code>X</code>，然后通过对比黑白<code>X</code>，求出预测误差，进行反向传递，逐步提升自编码的准确性。训练好的自编码中间这一部分就是能总结原数据的精髓。可以看出，从头到尾我们只用到了输入数据<code>X</code>，并没有用到<code>X</code>对应的数据标签，所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。</p>
<h3 id="编码器Encoder"><a href="#编码器Encoder" class="headerlink" title="编码器Encoder"></a>编码器Encoder</h3><p>&emsp;&emsp;这部分也叫作<code>encoder</code>编码器，编码器能得到原数据的精髓，然后我们只需要再创建一个小的神经网络学习这个精髓的数据，不仅减少了神经网络的负担，而且同样能达到很好的效果。</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/3.png" height="262" width="682"></p>
<p>&emsp;&emsp;右图是一个通过自编码整理出来的数据，它能从原数据中总结出每种类型数据的特征，如果把这些特征类型都放在一张二维的图片上，每种类型都已经被很好的用原数据的精髓区分开来。如果你了解<code>PCA</code>主成分分析，在提取主要特征时，自编码和它一样，甚至超越了<code>PCA</code>。换句话说，自编码可以像<code>PCA</code>一样给特征属性降维。</p>
<hr>
<p>&emsp;&emsp;今天的代码会运用两个类型：</p>
<ul>
<li>通过<code>Feature</code>的压缩并解压，并将结果与原始数据进行对比，观察处理过后的数据是不是如预期跟原始数据很相像。</li>
<li>只看<code>encoder</code>压缩的过程，使用它将一个数据集压缩到只有两个<code>Feature</code>时，将数据放入一个二维坐标系内。</li>
</ul>
<h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><p>&emsp;&emsp;基本设置代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">training_epochs = <span class="number">20</span>  <span class="comment"># 训练的轮数</span></span><br><span class="line">batch_size = <span class="number">256</span>  <span class="comment"># 每次训练的数据多少</span></span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># 每隔多少轮显示一次训练结果</span></span><br><span class="line">examples_to_show = <span class="number">10</span>  <span class="comment"># 从测试集中中选取10张图片去验证自动编码器的结果</span></span><br><span class="line">n_input = <span class="number">784</span>  <span class="comment"># 输入数据的特征值个数，MNIST data input (img shape: 28 * 28)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>压缩环节：我们要把这个<code>Features</code>不断压缩，经过第一个隐藏层压缩至<code>256</code>个<code>Features</code>，再经过第二个隐藏层压缩至<code>128</code>个。</li>
<li>解压环节：我们将<code>128</code>个<code>Features</code>还原至<code>256</code>个，再经过一步还原至<code>784</code>个。</li>
<li>对比环节：比较原始数据与还原后的拥有<code>784</code>个<code>Features</code>的数据进行<code>cost</code>的对比，根据<code>cost</code>来提升<code>Autoencoder</code>的准确率。</li>
</ul>
<p>下面是两个隐藏层的<code>weights</code>和<code>biases</code>定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">n_hidden_1 = <span class="number">256</span>  <span class="comment"># (第一个隐藏层神经元的个数，也是特征值的个数)1st layer num features</span></span><br><span class="line">n_hidden_2 = <span class="number">128</span>  <span class="comment"># (第二个隐藏层神经元的个数，也是特征值的个数)2nd layer num features</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 然后定义输入数据，这里是无监督学习，所以只要输入图片数据，不需要标记数据</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 初始化每一层的权重和偏置</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'encoder_h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_h1'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_input])),</span><br><span class="line">&#125;</span><br><span class="line">​</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'encoder_b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_b2'</span>: tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;下面来定义<code>Encoder</code>和<code>Decoder</code>，使用的<code>Activation function</code>是<code>sigmoid</code>，压缩之后的值应该在<code>[0, 1]</code>这个范围内。在<code>decoder</code>过程中，通常使用对应于<code>encoder</code>的<code>Activation function</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span>  <span class="comment"># (定义压缩函数)Building the encoder</span></span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'encoder_h1'</span>]), biases[<span class="string">'encoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'encoder_h2'</span>]), biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span>  <span class="comment"># (定义解压缩函数)Building the decoder</span></span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'decoder_h1'</span>]), biases[<span class="string">'decoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'decoder_h2'</span>]), biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br></pre></td></tr></table></figure>
<p>来实现<code>Encoder</code>和<code>Decoder</code>输出的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (构建模型)Construct model</span></span><br><span class="line">encoder_op = encoder(X)  <span class="comment"># 128 Features</span></span><br><span class="line">decoder_op = decoder(encoder_op)  <span class="comment"># 784 Features</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># (得出预测值)Prediction</span></span><br><span class="line">y_pred = decoder_op  <span class="comment"># After</span></span><br><span class="line"><span class="comment"># (得出真实值，即输入值)Targets (Labels) are the input data</span></span><br><span class="line">y_true = X  <span class="comment"># Before</span></span><br></pre></td></tr></table></figure>
<p>再通过非监督学习进行对照，即对<code>原始的有784个Features的数据集</code>和<code>通过Prediction得出的有784个Features的数据集</code>进行最小二乘法的计算，并且使<code>cost</code>最小化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (定义损失)Define loss and optimizer, minimize the squared error</span></span><br><span class="line">cost = tf.reduce_mean(tf.pow(y_true - y_pred, <span class="number">2</span>))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure>
<p>最后通过<code>Matplotlib</code>的<code>pyplot</code>模块将结果显示出来，注意在输出时<code>MNIST</code>数据集经过压缩之后<code>x</code>的最大值是<code>1</code>，而非<code>255</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  <span class="comment"># Launch the graph</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):  <span class="comment"># Training cycle</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):  <span class="comment"># Loop over all batches</span></span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  <span class="comment"># max(x) = 1, min(x) = 0</span></span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_xs&#125;)</span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:  <span class="comment"># Display logs per epoch step</span></span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost ="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(c))</span><br><span class="line">​</span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    <span class="comment"># Applying encode and decode over test set</span></span><br><span class="line">    encode_decode = sess.run(y_pred, feed_dict=&#123;X: mnist.test.images[:examples_to_show]&#125;)</span><br><span class="line">    <span class="comment"># Compare original images with their reconstructions</span></span><br><span class="line">    f, a = plt.subplots(<span class="number">2</span>, <span class="number">10</span>, figsize=(<span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(examples_to_show):</span><br><span class="line">        a[<span class="number">0</span>][i].imshow(np.reshape(mnist.test.images[i], (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">        a[<span class="number">1</span>][i].imshow(np.reshape(encode_decode[i], (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>通过<code>20</code>个<code>Epoch</code>的训练，结果如下所示，上面一行是真实数据，下面一行是经过<code>encoder</code>和<code>decoder</code>之后的数据。如果继续进行训练，效果会更好。</p>
<p><img src="/2019/02/28/深度学习/自编码和解码/4.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/全卷积网络FCN/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/全卷积网络FCN/" itemprop="url">全卷积网络FCN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T12:21:42+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;神经网络大神<code>Jonathan Long</code>发表了论文<code>Fully Convolutional Networks for Semantic Segmentation</code>，使得图像语义分割成为了现实。<br>&emsp;&emsp;通常<code>CNN</code>网络在卷积层之后会接上若干个全连接层，将卷积层产生的特征图(<code>feature map</code>)映射成一个固定长度的特征向量。以<code>AlexNet</code>为代表的经典<code>CNN</code>结构适合于图像级的分类和回归任务，因为它们最后都期望得到整个输入图像的一个数值描述(概率)，比如<code>AlexNet</code>的<code>ImageNet</code>模型输出一个<code>1000</code>维的向量表示输入图像属于每一类的概率(<code>softmax</code>归一化)。<br>&emsp;&emsp;对于下图中的猫，使用<code>AlexNet</code>网络将得到一个长为<code>1000</code>的输出向量，表示输入图像属于每一类的概率，其中在<code>tabby cat</code>这一类统计概率最高：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/1.jpg" height="165" width="563"></p>
<p>&emsp;&emsp;<code>FCN</code>对图像进行像素级的分类，从而解决了语义级别的图像分割(<code>semantic segmentation</code>)问题。与经典<code>CNN</code>在卷积层之后使用全连接层得到固定长度的特征向量进行分类(<code>全连接层 + softmax输出</code>)不同，<code>FCN</code>可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的<code>feature map</code>进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测，同时保留了原始输入图像中的空间信息，最后在上采样的特征图上进行逐像素分类。最终逐个像素计算<code>softmax</code>分类的损失，相当于每一个像素对应一个训练样本。下图是用于语义分割所采用的全卷积网络(<code>FCN</code>)的结构示意图：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/2.jpg" height="214" width="403"></p>
<p>&emsp;&emsp;简单的来说，<code>FCN</code>与<code>CNN</code>的区别在于把<code>CNN</code>最后的全连接层换成卷积层，输出的是一张已经<code>Label</code>好的图片：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/3.png" height="267" width="465"></p>
<p>&emsp;&emsp;实际上，<code>CNN</code>的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征：较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。这些抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高。下图<code>CNN</code>分类网络的示意图：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/4.png" height="151" width="578"></p>
<p>&emsp;&emsp;这些抽象的特征对分类很有帮助，可以很好地判断出一幅图像中包含什么类别的物体。但是因为丢失了一些物体的细节，不能很好地给出物体的具体轮廓、指出每个像素具体属于哪个物体，因此做到精确的分割就很有难度。<br>&emsp;&emsp;传统的基于<code>CNN</code>的分割方法：为了对一个像素分类，使用该像素周围的一个图像块作为<code>CNN</code>的输入用于训练和预测。这种方法有几个缺点：一是存储开销很大，例如对每个像素使用的图像块的大小为<code>15 * 15</code>，然后不断滑动窗口，每次滑动的窗口给<code>CNN</code>进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升；二是计算效率低下，相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复；三是像素块大小的限制了感知区域的大小，通常像素块的大小比整幅图像的大小要小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。<br>&emsp;&emsp;而全卷积网络则是从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。全连接层和卷积层之间唯一的不同，就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在这两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。<br>&emsp;&emsp;相反，任何全连接层都可以被转化为卷积层。比如，一个<code>K = 4096</code>的全连接层，输入数据体的尺寸是<code>7 * 7 * 512</code>，这个全连接层可以被等效地看做一个<code>F = 7</code>、<code>P = 0</code>、<code>S = 1</code>以及<code>K = 4096</code>的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成<code>1 * 1 * 4096</code>，这个结果就和使用初始的那个全连接层一样了。<br>&emsp;&emsp;在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是<code>224 * 224 * 3</code>的图像，一系列的卷积层和下采样层将图像数据变为尺寸为<code>7 * 7 * 512</code>的激活数据体。<code>AlexNet</code>使用了两个尺寸为<code>4096</code>的全连接层，最后一个有<code>1000</code>个神经元的全连接层用于计算分类评分。我们可以将这<code>3</code>个全连接层中的任意一个转化为卷积层：</p>
<ul>
<li>针对第一个连接区域是<code>7 * 7 * 512</code>的全连接层，令其滤波器尺寸为<code>F = 7</code>，这样输出数据体就为<code>1 * 1 * 4096</code>了。</li>
<li>针对第二个全连接层，令其滤波器尺寸为<code>F = 1</code>，这样输出数据体为<code>1 * 1 * 4096</code>。</li>
<li>对最后一个全连接层也做类似的处理，令其<code>F = 1</code>，最终输出为<code>1 * 1 * 1000</code>。</li>
</ul>
<p>&emsp;&emsp;实际操作中，每次这样的变换都需要把全连接层的权重<code>W</code>重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动，得到多个输出(可以理解为一个<code>label map</code>)，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。<br>&emsp;&emsp;举个例子，如果我们想让<code>224 * 224</code>尺寸的浮窗，以步长为<code>32</code>在<code>384 * 384</code>的图片上滑动，把每个经停的位置都带入卷积网络，最后得到<code>6 * 6</code>个位置的类别得分。将上述的过程把全连接层转换成卷积层则会更简便，如果<code>224 * 224</code>的输入图片经过卷积层和下采样层之后得到了<code>7 * 7 * 512</code>的数组，那么<code>384 * 384</code>的大图片直接经过同样的卷积层和下采样层之后会得到<code>12 * 12 * 512</code>的数组。然后再经过上面由<code>3</code>个全连接层转化得到的<code>3</code>个卷积层，最终得到<code>6 * 6 * 1000</code>的输出(<code>(12 - 7) / 1 + 1 = 6</code>)，这个结果正是浮窗在原图经停的<code>6 * 6</code>个位置的得分！面对<code>384 * 384</code>的图像，让(含全连接层)的初始卷积神经网络以<code>32</code>像素的步长独立对图像中的<code>224 * 224</code>块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。<br>&emsp;&emsp;如下图所示，<code>FCN</code>将传统<code>CNN</code>中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层。<code>FCN</code>将这<code>3</code>层表示为卷积层，卷积核的大小<code>(通道数, 宽, 高)</code>分别为<code>(4096, 1, 1)</code>、<code>(4096, 1, 1)</code>和<code>(1000, 1, 1)</code>。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前<code>CNN</code>已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。因此<code>FCN</code>网络中所有的层都是卷积层，故称为全卷积网络。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/5.png" height="201" width="447"></p>
<p>&emsp;&emsp;下图是一个全卷积层，与上图不一样的是图像对应的大小下标，<code>CNN</code>中输入的图像大小是同意固定<code>resize</code>成<code>227 * 227</code>大小的图像，第一层<code>pooling</code>后为<code>55 * 55</code>，第二层<code>pooling</code>后图像大小为<code>27 * 27</code>，第五层<code>pooling</code>后的图像大小为<code>13 * 13</code>。而<code>FCN</code>输入的图像是<code>H * W</code>大小，第一层<code>pooling</code>后变为原图大小的<code>1/4</code>，第二层变为原图大小的<code>1/8</code>，第五层变为原图大小的<code>1/16</code>，第八层变为原图大小的<code>1/32</code>(实际上代码当中第一层是<code>1/2</code>，以此类推)。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/6.png" height="165" width="590"></p>
<p>&emsp;&emsp;经过多次卷积和<code>pooling</code>以后，得到的图像越来越小，分辨率越来越低。其中图像到<code>H/32 * W/32</code>的时候图片是最小的一层时，所产生图叫做<code>heatmap</code>热图，热图就是我们最重要的高维特征图。得到高维特征的<code>heatmap</code>之后就是最重要的一步，也是最后的一步，对原图像进行<code>upsampling</code>，把图像逐渐放大，直到原图像的大小。最后的输出是<code>1000</code>张<code>heatmap</code>经过<code>upsampling</code>变为原图大小的图片：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/7.png"></p>
<p>&emsp;&emsp;为了对每个像素进行分类预测，并<code>label</code>成最后已经进行语义分割的图像，这里有一个小<code>trick</code>，就是最后通过逐个像素地求其在<code>1000</code>张图像该像素位置的最大数值描述(概率)作为该像素的分类。因此产生了一张已经分类好的图片，如下图右侧有狗狗和猫猫的图：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/8.png" height="206" width="456"></p>
<p>&emsp;&emsp;相较于使用被转化前的原始卷积神经网络对所有<code>36</code>个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为<code>36</code>次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。例如通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。<br>&emsp;&emsp;如果我们想用步长小于<code>32</code>的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为<code>16</code>的浮窗，那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移<code>16</code>个像素，然后把这些平移之后的图分别带入卷积网络。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/9.jpg" height="220" width="197"></p>
<p>&emsp;&emsp;如下图所示，当图片在网络中经过处理后变成越小的图片，其特征也越明显，就像图像中颜色所示。当然最后一层的图片不再是一个<code>1</code>个像素的图片，而是原图像<code>H/32 * W/32</code>大小的图，这里为了简化而画成一个像素而已：</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/10.png" height="224" width="450"></p>
<p>&emsp;&emsp;如下图所示，对原图像进行卷积<code>conv1</code>、<code>pool1</code>后原图像缩小为<code>1/2</code>；之后对图像进行第二次<code>conv2</code>、<code>pool2</code>后图像缩小为<code>1/4</code>；接着继续对图像进行第三次卷积操作<code>conv3</code>、<code>pool3</code>缩小为原图像的<code>1/8</code>，此时保留<code>pool3</code>的<code>featureMap</code>；接着继续对图像进行第四次卷积操作<code>conv4</code>、<code>pool4</code>，缩小为原图像的<code>1/16</code>，保留<code>pool4</code>的<code>featureMap</code>；最后对图像进行第五次卷积操作<code>conv5</code>、<code>pool5</code>，缩小为原图像的<code>1/32</code>，然后把原来<code>CNN</code>操作中的全连接变成卷积操作<code>conv6</code>、<code>conv7</code>，图像的<code>featureMap</code>数量改变但是图像大小依然为原图的<code>1/32</code>，此时图像不再叫<code>featureMap</code>而是叫<code>heatMap</code>。<br>&emsp;&emsp;现在我们有<code>1/32</code>尺寸的<code>heatMap</code>，<code>1/16</code>尺寸的<code>featureMap</code>和<code>1/8</code>尺寸的<code>featureMap</code>，<code>1/32</code>尺寸的<code>heatMap</code>进行<code>upsampling</code>操作之后，因为这样的操作还原的图片仅仅是<code>conv5</code>中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把<code>conv4</code>中的卷积核对上一次<code>upsampling</code>之后的图进行反卷积补充细节(相当于一个差值过程)，最后把<code>conv3</code>中的卷积核对刚才<code>upsampling</code>之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。</p>
<p><img src="/2019/02/28/深度学习/全卷积网络FCN/11.png" height="246" width="454"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/28/深度学习/TensorFlow处理图片/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/深度学习/TensorFlow处理图片/" itemprop="url">TensorFlow处理图片</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T08:45:16+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="TensorFlow与OpenCV"><a href="#TensorFlow与OpenCV" class="headerlink" title="TensorFlow与OpenCV"></a>TensorFlow与OpenCV</h3><p>&emsp;&emsp;<code>OpenCV</code>读入图片，使用<code>tf.Variable</code>初始化为<code>tensor</code>，加载到<code>tensorflow</code>对图片进行转置操作，然后<code>opencv</code>显示转置后的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">filename = <span class="string">"index.jpg"</span></span><br><span class="line">image = cv2.imread(filename, <span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'image'</span>, image)</span><br><span class="line">​</span><br><span class="line">x = tf.Variable(image, name=<span class="string">'x'</span>)</span><br><span class="line">​</span><br><span class="line">model = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    x = tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">    session.run(model)</span><br><span class="line">    result = session.run(x)</span><br><span class="line">​</span><br><span class="line">cv2.imshow(<span class="string">'result'</span>, result)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/1.png" height="220" width="419"></p>
<p>&emsp;&emsp;<code>OpenCV</code>读入图片，使用<code>tf.placeholder</code>符号变量加载到<code>tensorflow</code>里，然后<code>tensorflow</code>对图片进行剪切操作，最后<code>opencv</code>显示处理后的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">filename = <span class="string">"index.jpg"</span></span><br><span class="line">raw_image_data = cv2.imread(filename)</span><br><span class="line">cv2.imshow(<span class="string">'image'</span>, raw_image_data)</span><br><span class="line">image = tf.placeholder(<span class="string">"uint8"</span>, [<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">3</span>])</span><br><span class="line">slice = tf.slice(image, [<span class="number">100</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">200</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(slice, feed_dict=&#123;image: raw_image_data&#125;)</span><br><span class="line">    print(result.shape)</span><br><span class="line">​</span><br><span class="line">cv2.imshow(<span class="string">'result'</span>, result)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/2.png" height="218" width="418"></p>
<p>&emsp;&emsp;<code>TensorFlow</code>解码<code>png</code>格式文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">file_name = <span class="string">"call.1.png"</span></span><br><span class="line">file_contents = tf.read_file(file_name)</span><br><span class="line">​</span><br><span class="line">image = tf.image.decode_png(file_contents)  <span class="comment"># 解码png格式</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    img = sess.run(image)</span><br><span class="line">    print(img.shape)</span><br><span class="line">    cv2.imshow(<span class="string">"show"</span>, img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/3.png" height="239" width="359"></p>
<hr>
<h3 id="image类图像操作Ops之编码"><a href="#image类图像操作Ops之编码" class="headerlink" title="image类图像操作Ops之编码"></a>image类图像操作Ops之编码</h3><p>&emsp;&emsp;在源码路径<code>tensorflow/tensorflow/python/ops</code>下存在很多使用<code>python</code>实现的<code>ops</code>，其中<code>tensorflow</code>的<code>image</code>类操作就是在该目录下定义实现的，对应的文件是<code>image_ops_impl.py</code>。</p>
<h4 id="编码-decode"><a href="#编码-decode" class="headerlink" title="编码(decode_*)"></a>编码(decode_*)</h4><p>&emsp;&emsp;我们读进一张图片后，往往得到的是字符串张量，如下图所示，并非是常见的张量形式。我们是没办法直接使用图片来训练的，于是<code>decode_*</code>的作用就是要把字符串张量转为特定格式的张量。</p>
<p><img src="/2019/02/28/深度学习/TensorFlow处理图片/4.png" height="156" width="400"></p>
<h4 id="解码-encode"><a href="#解码-encode" class="headerlink" title="解码(encode_*)"></a>解码(encode_*)</h4><p>&emsp;&emsp;解码操作可以把一个张量编码成为某种图片格式，得到的是一个字符串张量，它是某种图片格式数据。</p>
<h4 id="tf-image-decode-gif"><a href="#tf-image-decode-gif" class="headerlink" title="tf.image.decode_gif"></a>tf.image.decode_gif</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_gif(contents, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：<code>gif</code>图片字符串<code>0-D</code>张量。</li>
<li><code>name</code>：指定的<code>OP</code>操作名字(可选项)。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>uint8</code>的<code>4-D</code>张量，其<code>shape</code>为<code>[num_frames, height, width, 3]</code>。</p>
<h4 id="tf-image-decode-jpeg"><a href="#tf-image-decode-jpeg" class="headerlink" title="tf.image.decode_jpeg"></a>tf.image.decode_jpeg</h4><p>&emsp;&emsp;<code>tf.image.decode_jpeg</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_jpeg(</span><br><span class="line">    contents, channels=<span class="number">0</span>, ratio=<span class="number">1</span>, fancy_upscaling=<span class="keyword">True</span>,</span><br><span class="line">    try_recover_truncated=<span class="keyword">False</span>, acceptable_fraction=<span class="number">1</span>,</span><br><span class="line">    dct_method=<span class="string">""</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：<code>jpeg</code>图片字符串<code>0-D</code>张量。</li>
<li><code>channels</code>：图片通道数，指定<code>decode</code>生成张量的<code>RGB</code>通道数。</li>
<li><code>ratio</code>：缩放系数，默认设定为<code>1</code>，即不做缩放，只能为<code>int</code>。</li>
<li><code>name</code>：指定的<code>OP</code>操作名字(可选项)。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>uint8</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</p>
<h4 id="tf-image-encode-jpeg"><a href="#tf-image-encode-jpeg" class="headerlink" title="tf.image.encode_jpeg"></a>tf.image.encode_jpeg</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.image.encode_jpeg(</span><br><span class="line">    image, format=<span class="string">""</span>, quality=<span class="number">95</span>, progressive=<span class="keyword">False</span>, optimize_size=<span class="keyword">False</span>, chroma_downsampling=<span class="keyword">True</span>,</span><br><span class="line">    density_unit=<span class="string">"in"</span>, x_density=<span class="number">300</span>, y_density=<span class="number">300</span>, xmp_metadata=<span class="string">""</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>image</code>：<code>dtype</code>为<code>uint8</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</li>
<li><code>format</code>：编码格式，可选择<code>grayscale</code>和<code>rgb</code>，默认为空(根据传入<code>image</code>的格式来决定输出格式)。</li>
<li><code>quality</code>：压缩品质，范围为<code>0</code>至<code>100</code>，值越高其压缩得到的图片品质越好。</li>
<li><code>chroma_downsampling</code>：色度抽样，这里默认压缩色度。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>string</code>的<code>0-D</code>张量，且其格式为<code>jpeg</code>。<br>&emsp;&emsp;下面是一个<code>encode_jpeg</code>设定<code>format</code>为<code>grayscale</code>，且缩放系数为<code>2</code>，然后再用<code>opencv</code>来<code>show</code>出的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">img_name = <span class="string">"girl.jpg"</span></span><br><span class="line">image_jpg = tf.read_file(img_name)</span><br><span class="line">imgage_decode_jpeg = tf.image.decode_jpeg(image_jpg, channels=<span class="number">1</span>, ratio=<span class="number">2</span>, name=<span class="string">"decode_jpeg_1"</span>)</span><br><span class="line">print(imgage_decode_jpeg.shape)</span><br><span class="line">print(imgage_decode_jpeg.dtype)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">imgage_encode_jpeg = tf.image.encode_jpeg(sess.run(imgage_decode_jpeg), format=<span class="string">'grayscale'</span>, name=<span class="string">"encode_jpeg"</span>)</span><br><span class="line">print(imgage_encode_jpeg.shape)</span><br><span class="line">print(imgage_encode_jpeg.dtype)</span><br><span class="line">img = tf.image.decode_jpeg(sess.run(imgage_encode_jpeg), ratio=<span class="number">2</span>, name=<span class="string">"decode_jpeg_2"</span>)</span><br><span class="line">​</span><br><span class="line">img = cv2.imshow(<span class="string">"img"</span>, sess.run(img))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf-image-decode-png"><a href="#tf-image-decode-png" class="headerlink" title="tf.image.decode_png"></a>tf.image.decode_png</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_png(contents, channels=<span class="number">0</span>, dtype=_dtypes.uint8, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：<code>dtype</code>为<code>string</code>的<code>0-D</code>张量，图像为<code>png</code>格式。</li>
<li><code>channels</code>：<code>decode</code>的输出通道数，可选<code>0</code>至<code>4</code>，默认根据所传图像来决定。</li>
</ul>
<p>该函数返回所传参数<code>dtype</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</p>
<h4 id="tf-image-encode-png"><a href="#tf-image-encode-png" class="headerlink" title="tf.image.encode_png"></a>tf.image.encode_png</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.encode_png(image, compression=<span class="number">-1</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>image</code>：<code>dtype</code>为<code>uint8</code>或<code>uint16</code>的<code>3-D</code>张量，其<code>shape</code>为<code>[height, width, channels]</code>。</li>
<li><code>compression</code>：压缩级数，数值越高得到的图像质量越差。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>string</code>的<code>0-D</code>张量，且其格式为<code>png</code>。<br>&emsp;&emsp;下面的代码将图片从<code>jpg</code>格式转为<code>png</code>格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">img_name = <span class="string">"girl.jpg"</span></span><br><span class="line">image_jpg = tf.read_file(img_name)</span><br><span class="line">imgage_decode_jpeg = tf.image.decode_jpeg(image_jpg, channels=<span class="number">3</span>, ratio=<span class="number">2</span>, name=<span class="string">"decode_jpeg_1"</span>)</span><br><span class="line">print(imgage_decode_jpeg.shape)</span><br><span class="line">print(imgage_decode_jpeg.dtype)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">imgage_encode_png = tf.image.encode_png(sess.run(imgage_decode_jpeg), name=<span class="string">"encode_png"</span>)</span><br><span class="line">print(imgage_encode_png.shape)</span><br><span class="line">print(imgage_encode_png.dtype)</span><br><span class="line">​</span><br><span class="line">img = tf.image.decode_png(sess.run(imgage_encode_png), name=<span class="string">"decode_png"</span>)</span><br><span class="line">​</span><br><span class="line">img = cv2.imshow(<span class="string">"img"</span>, sess.run(img))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tf-image-decode-image"><a href="#tf-image-decode-image" class="headerlink" title="tf.image.decode_image"></a>tf.image.decode_image</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.decode_image(contents, channels=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>contents</code>：可传入任何格式的图像，包括<code>jpeg</code>、<code>png</code>、<code>gif</code>及<code>bmp</code>，<code>dtype</code>为<code>string</code>的<code>0-D</code>张量。</li>
<li><code>channels</code>：<code>decode</code>的输出通道数，可选<code>0</code>至<code>4</code>，默认根据所传图像来决定。</li>
</ul>
<p>如果图片的格式是<code>BMP</code>、<code>JPEG</code>或<code>PNG</code>时，返回<code>[height, width, num_channels]</code>，而<code>gif</code>返回<code>[num_frames, height, width, 3]</code>，且它们的<code>dtype</code>都为<code>uint8</code>，这会导致<code>dtype</code>为<code>uint16</code>的<code>png</code>格式图像失真。这是一个兼容<code>gif</code>、<code>jpeg</code>、<code>png</code>及<code>bmp</code>格式的<code>decode</code>接口，它将会根据所传的图像自动返回特定格式的张量。<br>&emsp;&emsp;以下是传入<code>jpg</code>格式图像到<code>decode_image</code>，然后再使用<code>opencv</code>来显示<code>jpeg</code>编解码器生成的图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">​</span><br><span class="line">img_name = <span class="string">"apple.jpg"</span></span><br><span class="line">image_jpg = tf.read_file(img_name)</span><br><span class="line">imgage_decode = tf.image.decode_image(image_jpg, name=<span class="string">"decode_image"</span>)</span><br><span class="line">print(imgage_decode.shape)</span><br><span class="line">print(imgage_decode.dtype)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">imgage_encode_jpeg = tf.image.encode_jpeg(</span><br><span class="line">                        sess.run(imgage_decode), quality=<span class="number">100</span>, progressive=<span class="keyword">True</span>,</span><br><span class="line">                        chroma_downsampling=<span class="keyword">False</span>, optimize_size=<span class="keyword">True</span>, name=<span class="string">"encode_jpeg"</span>)</span><br><span class="line">print(imgage_encode_jpeg.shape)</span><br><span class="line">print(imgage_encode_jpeg.dtype)</span><br><span class="line">​</span><br><span class="line">img = tf.image.decode_png(sess.run(imgage_encode_jpeg), name=<span class="string">"decode_jpeg"</span>)</span><br><span class="line">​</span><br><span class="line">img = cv2.imshow(<span class="string">"img"</span>, sess.run(img))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="image类图像操作Ops之大小重调和图像标注框"><a href="#image类图像操作Ops之大小重调和图像标注框" class="headerlink" title="image类图像操作Ops之大小重调和图像标注框"></a>image类图像操作Ops之大小重调和图像标注框</h3><h4 id="resize-images"><a href="#resize-images" class="headerlink" title="resize_images"></a>resize_images</h4><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resize_images(images, size, method=ResizeMethod.BILINEAR, align_corners=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>images</code>：<code>shape</code>为<code>[batch, height, width, channels]</code>的<code>4-D</code>图像张量或者<code>shape</code>为<code>[height, width, channels]</code>的<code>3-D</code>图像张量，如果传入图像张量不兼容，所制定规则会报错。</li>
<li><code>size</code>：一个<code>dtype</code>为<code>int32</code>拥有两个元素的<code>1-D</code>张量，格式为<code>[new_height, new_width]</code>。</li>
<li><code>method</code>：<code>resize</code>使用的方法，有四种方式：</li>
</ul>
<ol>
<li><code>ResizeMethod.BILINEAR</code>：双线性内插，其核心思想是在两个方向分别进行一次线性插值。</li>
<li><code>ResizeMethod.NEAREST_NEIGHBOR</code>：最近邻插值法，将变换后的图像中的原像素点最邻近像素的灰度值赋给原像素点的方法，返回图像张量<code>dtype</code>与所传入的相同。</li>
<li><code>ResizeMethod.BICUBIC</code>：双三次插值，双三次插值是一种更加复杂的插值方式，它能创造出比双线性插值更平滑的图像边缘。</li>
<li><code>ResizeMethod.AREA</code>：基于区域的图像插值算法，首先将原始低分辨率图像分割成不同区域，然后将插值点映射到低分辨率图像，判断其所属区域，最后根据插值点的邻域像素设计不同的插值公式，计算插值点的值。</li>
</ol>
<ul>
<li><code>align_corners</code>：精确对准输入输出图像的四个角，默认为<code>false</code>，不精确对准。</li>
</ul>
<p>该函数返回<code>dtype</code>为<code>float</code>的<code>3-D</code>或<code>4-D</code>图像张量，其<code>shape</code>分别为<code>[batch, new_height, new_width, channels]</code>和<code>[new_height, new_width, channels]</code>。<br>&emsp;&emsp;而其余四个接口则是具体的不同实现图像缩放处理的方法，它们的参数都形如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(images, size, align_corners=<span class="keyword">False</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>第一个参数要求其<code>shape</code>一定是形如<code>[batch, height, width, channels]</code>的<code>4-D</code>格式，中间两个参数如<code>resize_image</code>所解释，后一个<code>name</code>是操作的名称(可选项)。<br>&emsp;&emsp;下面是用<code>matplotlib</code>来显示四种不同缩放处理方法后的图像对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">img_name = [<span class="string">"apple.jpg"</span>]</span><br><span class="line">filename_queue = tf.train.string_input_producer(img_name)</span><br><span class="line">img_reader = tf.WholeFileReader()</span><br><span class="line">_, image_jpg = img_reader.read(filename_queue)</span><br><span class="line">​</span><br><span class="line">image_decode_jpeg = tf.image.decode_png(image_jpg)</span><br><span class="line">image_decode_jpeg = tf.image.convert_image_dtype(image_decode_jpeg, dtype=tf.float32)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">​</span><br><span class="line">image_bilinear = tf.image.resize_images(image_decode_jpeg, size=[<span class="number">1200</span>, <span class="number">1920</span>], method=tf.image.ResizeMethod.BILINEAR)</span><br><span class="line">image_nearest_neighbor = tf.image.resize_images(</span><br><span class="line">                            image_decode_jpeg, size=[<span class="number">728</span>, <span class="number">1280</span>],</span><br><span class="line">                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</span><br><span class="line">image_bicubic = tf.image.resize_images(image_decode_jpeg, size=[<span class="number">720</span>, <span class="number">1440</span>], method=tf.image.ResizeMethod.BICUBIC)</span><br><span class="line">image_area = tf.image.resize_images(image_decode_jpeg, size=[<span class="number">1080</span>, <span class="number">1920</span>], method=tf.image.ResizeMethod.AREA)</span><br><span class="line">​</span><br><span class="line">print(image_bicubic.shape)</span><br><span class="line">print(image_bicubic.dtype)</span><br><span class="line">​</span><br><span class="line">plt.figure()</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.imshow(sess.run(image_bilinear))</span><br><span class="line">plt.title(<span class="string">"bilinear interpolation"</span>)</span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.imshow(sess.run(image_nearest_neighbor))</span><br><span class="line">plt.title(<span class="string">"nearest neighbor interpolation"</span>)</span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.imshow(sess.run(tf.image.convert_image_dtype(image_bicubic, dtype=tf.uint8)))</span><br><span class="line">plt.title(<span class="string">"bicubic interpolation"</span>)</span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.imshow(sess.run(tf.image.convert_image_dtype(image_area, dtype=tf.float32)))</span><br><span class="line">plt.title(<span class="string">"area interpolation"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="tf-image-resize-bilinear"><a href="#tf-image-resize-bilinear" class="headerlink" title="tf.image.resize_bilinear"></a>tf.image.resize_bilinear</h4><p>&emsp;&emsp;该函数使用双线性插值调整<code>images</code>为<code>size</code>，输入图像可以是不同的类型，但输出图像总是浮点型的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.image.resize_bilinear(images, size, align_corners=<span class="keyword">False</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>images</code>：一个<code>Tensor</code>，必须是下列类型之一：<code>int8</code>、<code>uint8</code>、<code>int16</code>、<code>uint16</code>、<code>int32</code>、<code>int64</code>、<code>float16</code>、<code>float32</code>和<code>float64</code>，<code>4</code>维的并且具有形状<code>[batch, height, width, channels]</code>。</li>
<li><code>size</code>：<code>2</code>个元素<code>(new_height, new_width)</code>的<code>1</code>维<code>int32</code>张量，用来表示图像的新大小。</li>
<li><code>align_corners</code>：可选的<code>bool</code>。如果为<code>True</code>，则输入和输出张量的<code>4</code>个角像素的中心对齐，并且保留角落像素处的值。</li>
<li><code>name</code>：操作的名称(可选项)。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/27/机器学习/词袋模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暴徒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/机器学习/词袋模型/" itemprop="url">词袋模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T18:12:34+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>BoW</code>(<code>Bag of Words</code>)词袋模型最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子(因为里面装的都是词汇，所以称为词袋)，然后看这个袋子里装的都是些什么词汇，将其分类。如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。举个例子，有如下两个文档：</p>
<ul>
<li>文档一：<code>Bob likes to play basketball, Jim likes too.</code></li>
<li>文档二：<code>Bob also likes to play football games.</code></li>
</ul>
<p>基于这两个文本文档，构造一个词典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dictionary = &#123;</span><br><span class="line">    <span class="number">1</span>: <span class="string">"Bob"</span>, <span class="number">2</span>: <span class="string">"like"</span>, <span class="number">3</span>: <span class="string">"to"</span>, <span class="number">4</span>: <span class="string">"play"</span>, <span class="number">5</span>: <span class="string">"basketball"</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">"also"</span>, <span class="number">7</span>: <span class="string">"football"</span>, <span class="number">8</span>: <span class="string">"games"</span>, <span class="number">9</span>: <span class="string">"Jim"</span>, <span class="number">10</span>: <span class="string">"too"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个词典一共包含<code>10</code>个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个<code>10</code>维向量表示(用整数数字<code>0</code>至<code>n</code>(<code>n</code>为正整数)表示某个单词在文档中出现的次数)：</p>
<ul>
<li>文档一：<code>[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]</code></li>
<li>文档二：<code>[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]</code></li>
</ul>
<p>向量中每个元素表示词典中相关元素在文档中出现的次数。不过，在构造文档向量的过程中可以看到，我们并没有表达单词在原来句子中出现的次序(这是词袋模型的缺点之一，不过瑕不掩瑜，甚至在此处无关紧要)。<br>&emsp;&emsp;<code>SIFT</code>特征虽然也能描述一幅图像，但是每个<code>SIFT</code>矢量都是<code>128</code>维的，而且一幅图像通常都包含成百上千个<code>SIFT</code>矢量。在进行相似度计算时，这个计算量是非常大的，通行的做法是用聚类算法对这些矢量数据进行聚类，然后用聚类中的一个簇代表<code>BoW</code>中的一个视觉词，将同一幅图像的<code>SIFT</code>矢量映射到视觉词序列生成码本，这样每一幅图像只用一个码本矢量来描述，这样计算相似度时效率就大大提高了。<br>&emsp;&emsp;现在想象在一个巨大的文档集合<code>D</code>，里面一共有<code>M</code>个文档，而文档里面的所有单词提取出来后，一起构成一个包含<code>N</code>个单词的词典。利用<code>BoW</code>模型，每个文档都可以被表示成为一个<code>N</code>维向量，计算机非常擅长于处理数值向量，这样就可以利用计算机来完成海量文档的分类过程。为了表示一幅图像，我们可以将图像看作文档，即若干个<code>视觉词汇</code>的集合，同样的，视觉词汇相互之间没有顺序。</p>
<p><img src="/2019/02/27/机器学习/词袋模型/1.png" height="265" width="355"></p>
<p>&emsp;&emsp;由于图像中的词汇不像文本文档中的那样是现成的，我们需要先从图像中提取出相互独立的视觉词汇。这通常需要经过三个步骤：特征检测，特征表示和单词本的生成。从图像中提取出相互独立的视觉词汇：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/2.png" height="139" width="542"></p>
<p>&emsp;&emsp;通过观察会发现，同一类目标的不同实例之间虽然存在差异，但我们仍然可以找到它们之间的一些共同的地方。比如说人脸，虽然说不同人的脸差别比较大，但眼睛、嘴、鼻子等一些比较细小的部位却观察不到太大差别，我们可以把这些不同实例之间共同的部位提取出来，作为识别这一类目标的视觉词汇。而<code>SIFT</code>算法是提取图像中局部不变特征的应用最广泛的算法，因此可以用<code>SIFT</code>算法从图像中提取不变特征点，作为视觉词汇，并构造单词表，用单词表中的单词表示一幅图像。<br>&emsp;&emsp;接下来通过上述图像展示如何通过<code>BoW</code>模型，将图像表示成数值向量。现在有三个目标类，分别是人脸、自行车和吉他。<code>BoW</code>模型的第一步是利用<code>SIFT</code>算法，从每类图像中提取视觉词汇，将所有的视觉词汇集合在一起：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/3.png"></p>
<p>&emsp;&emsp;第二步是利用<code>K-Means</code>算法构造单词表。<code>K-Means</code>算法是一种基于样本间相似性度量的间接聚类方法，此算法以<code>K</code>为参数，把<code>N</code>个对象分为<code>K</code>个簇，以使簇内具有较高的相似度，而簇间相似度较低。<code>SIFT</code>提取的视觉词汇向量之间根据距离的远近，可以利用<code>K-Means</code>算法将词义相近的词汇合并，作为单词表中的基础词汇。假定我们将<code>K</code>设为<code>4</code>，那么单词表的构造过程如下：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/4.png" height="252" width="401"></p>
<p>&emsp;&emsp;第三步是利用单词表的中词汇表示图像。利用<code>SIFT</code>算法，可以从每幅图像中提取很多个特征点，这些特征点都可以用单词表中的单词近似代替。通过统计单词表中每个单词在图像中出现的次数，可以将图像表示成为一个<code>K = 4</code>维数值向量：</p>
<p><img src="/2019/02/27/机器学习/词袋模型/5.png" height="146" width="548"></p>
<p>&emsp;&emsp;我们从人脸、自行车和吉他三个目标类图像中提取出的不同视觉词汇，而构造的词汇表中，会把词义相近的视觉词汇合并为同一类。经过合并，词汇表中只包含了四个视觉单词，分别按索引值标记为<code>1</code>、<code>2</code>、<code>3</code>和<code>4</code>。通过观察可以看到，它们分别属于自行车、人脸、吉他、人脸类。统计这些词汇在不同目标类中出现的次数可以得到每幅图像的直方图表示(我们假定存在误差，实际情况亦不外如此)：</p>
<ul>
<li>人脸：<code>[3, 30, 3, 20]</code></li>
<li>自行车：<code>[20, 3, 3, 2]</code></li>
<li>吉他：<code>[8, 12, 32, 7]</code></li>
</ul>
<p>其实这个过程非常简单，就是针对人脸、自行车和吉他这三个文档，抽取出相似的部分(或者词义相近的视觉词汇合并为同一类)，构造一个词典，词典中包含<code>4</code>个视觉单词，即<code>Dictionary = {1:&quot;自行车&quot;, 2:&quot;人脸&quot;, 3:&quot;吉他&quot;, 4:&quot;人脸类&quot;}</code>，最终人脸、自行车和吉他这三个文档皆可以用一个<code>4</code>维向量表示，最后根据三个文档相应部分出现的次数画成了上面对应的直方图。需要说明的是，以上过程只是针对三个目标类非常简单的一个示例，实际应用中，为了达到较好的效果，单词表中的词汇数量<code>K</code>往往非常庞大，并且目标类数目越多，对应的<code>K</code>值也越大。一般情况下，<code>K</code>的取值在几百到上千，在这里取<code>K = 4</code>仅仅是为了方便说明。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/94/">94</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">934</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
