<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="泥腿子出身">
<meta property="og:url" content="http://fukangwei.gitee.io/page/24/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="泥腿子出身">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/24/">





  <title>泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之线程和队列/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之线程和队列/" itemprop="url">TensorFlow之线程和队列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T15:33:07+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;在使用<code>TensorFlow</code>进行异步计算时，队列是一种强大的机制。正如<code>TensorFlow</code>中的其他组件一样，队列就是<code>TensorFlow</code>图中的节点。这是一种有状态的节点，就像变量一样：其他节点可以修改它的内容。具体来说，其他节点可以把新元素插入到队列后端(<code>rear</code>)，也可以把队列前端(<code>front</code>)的元素删除。<br>&emsp;&emsp;为了感受一下队列，来看一个简单的例子。我们先创建一个<code>先入先出</code>的队列(<code>FIFOQueue</code>)，并将其内部所有元素初始化为零。然后构建一个<code>TensorFlow</code>图，它从队列前端取走一个元素，加上<code>1</code>之后，放回队列的后端。慢慢地，队列的元素的值就会增加。</p>
<p><img src="/2019/02/13/深度学习/TensorFlow之线程和队列/1.png" height="247" width="606"></p>
<p>&emsp;&emsp;<code>Enqueue</code>、<code>EnqueueMany</code>和<code>Dequeue</code>都是特殊的节点。它们需要获取队列指针，而非普通的值，这样才能修改队列内容。我们建议您将它们看作队列的方法，事实上在<code>Python</code>的<code>API</code>中，它们就是队列对象的方法(例如<code>q.enqueue(...)</code>)。</p>
<h3 id="队列使用概述"><a href="#队列使用概述" class="headerlink" title="队列使用概述"></a>队列使用概述</h3><p>&emsp;&emsp;队列(如<code>FIFOQueue</code>和<code>RandomShuffleQueue</code>)在<code>TensorFlow</code>的张量异步计算时都非常重要。例如，一个典型的输入结构使用一个<code>RandomShuffleQueue</code>来作为模型训练的输入：</p>
<ul>
<li>多个线程准备训练样本，并且把这些样本推入队列。</li>
<li>一个训练线程执行一个训练操作，此操作会从队列中移除最小批次的样本(<code>mini-batches</code>)。</li>
</ul>
<p>&emsp;&emsp;<code>TensorFlow</code>的<code>Session</code>对象是可以支持多线程的，因此多个线程可以很方便地使用同一个会话(<code>Session</code>)，并且并行地执行操作。然而，在<code>Python</code>程序实现这样的并行运算却并不容易。所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候，队列必须能被正确地关闭。<br>&emsp;&emsp;所幸<code>TensorFlow</code>提供了两个类来帮助多线程的实现：<code>tf.Coordinator</code>和<code>tf.QueueRunner</code>，从设计上这两个类必须被一起使用。<code>Coordinator</code>类可以用来同时停止多个工作线程，并且向那个在等待所有工作线程终止的程序报告异常。<code>QueueRunner</code>类用来协调多个工作线程同时将多个张量推入同一个队列中。</p>
<h3 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h3><p>&emsp;&emsp;<code>Coordinator</code>类用来帮助多个线程协同工作，多个线程同步终止。其主要方法有：</p>
<ul>
<li><code>should_stop()</code>：如果线程应该停止，则返回<code>True</code>。</li>
<li><code>request_stop(&lt;exception&gt;)</code>：请求该线程停止。</li>
<li><code>join(&lt;list of threads&gt;)</code>：等待被指定的线程终止。</li>
</ul>
<p>首先创建一个<code>Coordinator</code>对象，然后建立一些使用<code>Coordinator</code>对象的线程。这些线程通常一直循环运行，一直到<code>should_stop</code>返回<code>True</code>时停止。任何线程都可以决定计算什么时候应该停止。它只需要调用<code>request_stop</code>，同时其他线程的<code>should_stop</code>将会返回<code>True</code>，然后都停下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MyLoop</span><span class="params">(coord)</span>:</span>  <span class="comment"># 循环执行，直到Coordinator收到了停止请求</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">        do something</span><br><span class="line">        <span class="keyword">if</span> some condition: <span class="comment"># 如果某些条件为真，请求Coordinator去停止其他线程</span></span><br><span class="line">            coord.request_stop()</span><br><span class="line">​</span><br><span class="line">coord = Coordinator()  <span class="comment"># Main code: create a coordinator</span></span><br><span class="line"><span class="comment"># Create 10 threads that run 'MyLoop()'</span></span><br><span class="line">threads = [threading.Thread(target=MyLoop, args=(coord)) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:  <span class="comment"># Start the threads and wait for all of them to stop</span></span><br><span class="line">    t.start()</span><br><span class="line">​</span><br><span class="line">coord.join(threads)</span><br></pre></td></tr></table></figure>
<p>显然，<code>Coordinator</code>可以管理线程去做不同的事情。上面的代码只是一个简单的例子，在设计实现的时候不必完全照搬。<code>Coordinator</code>还支持捕捉和报告异常，具体可以参考<code>Coordinator class</code>的文档。</p>
<h3 id="QueueRunner"><a href="#QueueRunner" class="headerlink" title="QueueRunner"></a>QueueRunner</h3><p>&emsp;&emsp;<code>QueueRunner</code>类会创建一组线程，这些线程可以重复的执行<code>Enquene</code>操作，它们使用同一个<code>Coordinator</code>来处理线程同步终止。此外，一个<code>QueueRunner</code>会运行一个<code>closer thread</code>，当<code>Coordinator</code>收到异常报告时，这个<code>closer thread</code>会自动关闭队列。您可以使用一个<code>queue runner</code>来实现上述结构。首先建立一个<code>TensorFlow</code>图表，这个图表使用队列来输入样本。增加处理样本并将样本推入队列中的操作，增加<code>training</code>操作来移除队列中的样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">example = ...ops to create one example...</span><br><span class="line"><span class="comment"># Create a queue, and an op that enqueues examples one at a time in the queue</span></span><br><span class="line">queue = tf.RandomShuffleQueue(...)</span><br><span class="line">enqueue_op = queue.enqueue(example)</span><br><span class="line"><span class="comment"># Create a training graph that starts by dequeuing a batch of examples</span></span><br><span class="line">inputs = queue.dequeue_many(batch_size)</span><br><span class="line">train_op = ...use <span class="string">'inputs'</span> to build the training part of the graph...</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在<code>Python</code>的训练程序中，创建一个<code>QueueRunner</code>来运行几个线程，这几个线程处理样本，并且将样本推入队列。创建一个<code>Coordinator</code>，让<code>queue runner</code>使用<code>Coordinator</code>来启动这些线程，创建一个训练的循环，并且使用<code>Coordinator</code>来控制<code>QueueRunner</code>的线程们的终止。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a queue runner that will run 4 threads in parallel to enqueue examples</span></span><br><span class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">4</span>)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()  <span class="comment"># Launch the graph</span></span><br><span class="line">coord = tf.train.Coordinator()  <span class="comment"># Create a coordinator, launch the queue runner threads</span></span><br><span class="line">enqueue_threads = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):  <span class="comment"># Run the training loop, controlling termination with the coordinator</span></span><br><span class="line">    <span class="keyword">if</span> coord.should_stop():</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    sess.run(train_op)</span><br><span class="line">​</span><br><span class="line">coord.request_stop()  <span class="comment"># When done, ask the threads to stop</span></span><br><span class="line">coord.join(threads)  <span class="comment"># And wait for them to actually do it</span></span><br></pre></td></tr></table></figure>
<h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><p>&emsp;&emsp;通过<code>queue runners</code>启动的线程不仅仅只处理推送样本到队列，它们还捕捉和处理由队列产生的异常，包括<code>OutOfRangeError</code>异常，这个异常用于报告队列被关闭。使用<code>Coordinator</code>的训练程序在主循环中必须同时捕捉和报告异常。下面是对上面训练循环的改进版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="number">1000000</span>):</span><br><span class="line">        <span class="keyword">if</span> coord.should_stop():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        sess.run(train_op)</span><br><span class="line"><span class="keyword">except</span> Exception, e:</span><br><span class="line">   coord.request_stop(e)  <span class="comment"># Report exceptions to the coordinator</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Terminate as usual. It is innocuous to request stop twice</span></span><br><span class="line">coord.request_stop()</span><br><span class="line">coord.join(threads)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="理解TensorFlow的Queue"><a href="#理解TensorFlow的Queue" class="headerlink" title="理解TensorFlow的Queue"></a>理解TensorFlow的Queue</h3><p>&emsp;&emsp;这篇文章来说明<code>TensorFlow</code>里与<code>Queue</code>有关的概念和用法，其实概念只有三个：</p>
<ul>
<li><code>Queue</code>是<code>TF</code>队列和缓存机制的实现。</li>
<li><code>QueueRunner</code>是<code>TF</code>中对操作<code>Queue</code>的线程的封装。</li>
<li><code>Coordinator</code>是<code>TF</code>中用来协调线程运行的工具。</li>
</ul>
<p>虽然它们经常同时出现，但这三样东西在<code>TensorFlow</code>中是可以单独使用的。</p>
<h4 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h4><p>&emsp;&emsp;根据实现的方式不同，分成具体的几种类型：</p>
<ul>
<li><code>tf.FIFOQueue</code>：按入列顺序出列的队列(如果希望读入的训练样本是有序的)。</li>
<li><code>tf.RandomShuffleQueue</code>：随机顺序出列的队列。</li>
<li><code>tf.PaddingFIFOQueue</code>：以固定长度批量出列的队列。</li>
<li><code>tf.PriorityQueue</code>：带优先级出列的队列。</li>
</ul>
<p>这些类型的<code>Queue</code>除了自身的性质不太一样外，创建、使用的方法基本是相同的。<br>&emsp;&emsp;创建函数的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.FIFOQueue(capacity, dtypes, shapes=<span class="keyword">None</span>, names=<span class="keyword">None</span> ...)</span><br></pre></td></tr></table></figure>
<p><code>Queue</code>主要包含入列(<code>enqueue</code>)和出列(<code>dequeue</code>)两个操作。<code>enqueue</code>操作返回计算图中的一个<code>Operation</code>节点，<code>dequeue</code>操作返回一个<code>Tensor</code>值。<code>Tensor</code>在创建时同样只是一个定义(或称为<code>声明</code>)，需要放在<code>Session</code>中运行才能获得真正的数值。下面是一个单独使用<code>Queue</code>的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 创建一个FIFO队列，初始化队列插入0.1、0.2、0.3这三个数字</span></span><br><span class="line">q = tf.FIFOQueue(<span class="number">3</span>, <span class="string">"float"</span>)</span><br><span class="line">init = q.enqueue_many(([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],))</span><br><span class="line"><span class="comment"># 定义出队、“+2”、入队操作</span></span><br><span class="line">x = q.dequeue()</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">q_inc = q.enqueue([y])</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        sess.run(q_inc)  <span class="comment"># 执行2次操作，队列的值变为0.3,、2.1、2.2</span></span><br><span class="line">​</span><br><span class="line">    quelen = sess.run(q.size())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(quelen):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.3</span></span><br><span class="line"><span class="number">2.1</span></span><br><span class="line"><span class="number">2.2</span></span><br></pre></td></tr></table></figure>
<p>注意，如果一次性入列超过<code>Queue Size</code>的数据，<code>enqueue</code>操作会卡住，直到有数据(被其他线程)从队列取出；对一个已经取空的队列使用<code>dequeue</code>操作也会卡住，直到有新的数据(从其他线程)写入。</p>
<h4 id="RandomShuffleQueue"><a href="#RandomShuffleQueue" class="headerlink" title="RandomShuffleQueue"></a>RandomShuffleQueue</h4><p>&emsp;&emsp;<code>RandomShuffleQueue</code>在<code>TensorFlow</code>使用异步计算时非常重要，因为<code>TensorFlow</code>的会话是支持多线程的，可以在主线程里执行训练操作，使用<code>RandomShuffleQueue</code>作为训练输入，开多个线程来准备训练样本，将样本压入队列后，主线程会从队列中每次取出<code>mini-batch</code>的样本进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 最大长度为10，最小长度为2，类型为float的随机队列</span></span><br><span class="line">q = tf.RandomShuffleQueue(capacity=<span class="number">10</span>, min_after_dequeue=<span class="number">2</span>, dtypes=<span class="string">'float'</span>)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    sess.run(q.enqueue(i))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">8</span>):</span><br><span class="line">    print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<p>执行结果是乱序的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4.0</span></span><br><span class="line"><span class="number">0.0</span></span><br><span class="line"><span class="number">2.0</span></span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"><span class="number">5.0</span></span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"><span class="number">7.0</span></span><br></pre></td></tr></table></figure>
<h4 id="QueueRunner-1"><a href="#QueueRunner-1" class="headerlink" title="QueueRunner"></a>QueueRunner</h4><p>&emsp;&emsp;<code>Tensorflow</code>的计算主要在使用<code>CPU/GPU</code>和内存，而数据读取涉及磁盘操作，速度远低于前者操作。因此通常会使用多个线程读取数据，然后使用一个线程消费数据。<code>QueueRunner</code>就是来管理这些读写队列的线程的。<br>&emsp;&emsp;<code>QueueRunner</code>需要与<code>Queue</code>一起使用，但并不一定必须使用<code>Coordinator</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">q = tf.FIFOQueue(<span class="number">10</span>, <span class="string">"float"</span>)</span><br><span class="line">counter = tf.Variable(<span class="number">0.0</span>)  <span class="comment"># 计数器</span></span><br><span class="line">increment_op = tf.assign_add(counter, <span class="number">1.0</span>)  <span class="comment"># 给计数器加一</span></span><br><span class="line">enqueue_op = q.enqueue(counter)  <span class="comment"># 将计数器加入队列</span></span><br><span class="line"><span class="comment"># 创建QueueRunner，用多个线程向队列添加数据。这里实际创建了4个线程，两个增加计数，两个执行入队</span></span><br><span class="line">qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * <span class="number">2</span>)</span><br><span class="line">sess = tf.InteractiveSession()  <span class="comment"># 主线程</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">qr.create_threads(sess, start=<span class="keyword">True</span>)  <span class="comment"># 启动入队线程</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">6.0</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">13.0</span></span><br><span class="line"><span class="number">18.0</span></span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="number">25.0</span></span><br><span class="line"><span class="number">26.0</span></span><br><span class="line"><span class="number">28.0</span></span><br><span class="line"><span class="number">31.0</span></span><br></pre></td></tr></table></figure>
<p>增加计数的进程会不停的后台运行，执行入队的进程会先执行<code>10</code>次(因为队列长度只有<code>10</code>)，然后主线程开始消费数据。当一部分数据消费被后，入队的进程又会开始执行。最终主线程消费完<code>20</code>个数据后停止，但其他线程继续运行，程序不会结束。</p>
<h4 id="Coordinator-1"><a href="#Coordinator-1" class="headerlink" title="Coordinator"></a>Coordinator</h4><p>&emsp;&emsp;<code>Coordinator</code>是个用来保存线程组运行状态的协调器对象，它和<code>TensorFlow</code>的<code>Queue</code>没有必然关系，可以单独和<code>Python</code>线程使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">q = tf.FIFOQueue(<span class="number">1000</span>, <span class="string">'float'</span>)</span><br><span class="line">counter = tf.Variable(<span class="number">0.0</span>)  <span class="comment"># 计数器</span></span><br><span class="line">increment_op = tf.assign_add(counter, tf.constant(<span class="number">1.0</span>))  <span class="comment"># 计数器加一</span></span><br><span class="line">enqueue_op = q.enqueue(counter)  <span class="comment"># 入队</span></span><br><span class="line"><span class="comment"># 线程面向队列q，启动2个线程，每个线程中是[in,en]两个操作</span></span><br><span class="line">qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * <span class="number">2</span>)</span><br><span class="line">​</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">​</span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line"><span class="comment"># 线程管理器启动线程，接收协调器管理</span></span><br><span class="line">enqueue_thread = qr.create_threads(sess, coord=coord, start=<span class="keyword">True</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    print(sess.run(q.dequeue()))</span><br><span class="line">​</span><br><span class="line">coord.request_stop()  <span class="comment"># 向各个线程发终止信号</span></span><br><span class="line">coord.join(enqueue_thread)  <span class="comment"># 等待各个线程成功结束</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5.0</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">17.0</span></span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="number">38.0</span></span><br><span class="line"><span class="number">42.0</span></span><br><span class="line"><span class="number">45.0</span></span><br><span class="line"><span class="number">48.0</span></span><br><span class="line"><span class="number">54.0</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/Tf.reduce类函数/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/Tf.reduce类函数/" itemprop="url">Tf.reduce类函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T14:52:25+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="tf-reduce-max-tf-reduce-min相对"><a href="#tf-reduce-max-tf-reduce-min相对" class="headerlink" title="tf.reduce_max(tf.reduce_min相对)"></a>tf.reduce_max(tf.reduce_min相对)</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reduce_max (</span><br><span class="line">    input_tensor, axis=<span class="keyword">None</span>, keep_dims=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span>, reduction_indices=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Computes the maximum of elements across dimensions of a tensor.<br>&emsp;&emsp;Reduces <code>input_tensor</code> along the dimensions given in <code>axis</code>. Unless <code>keep_dims</code> is <code>true</code>, the rank of the tensor is reduced by <code>1</code> for each entry in <code>axis</code>. If <code>keep_dims</code> is <code>true</code>, the reduced dimensions are retained with length <code>1</code>.<br>&emsp;&emsp;If <code>axis</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>
<ul>
<li><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li>
<li><code>axis</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions. Must be in the range <code>[-rank(input_tensor), rank(input_tensor))</code>.</li>
<li><code>keep_dims</code>: If <code>true</code>, retains reduced dimensions with length <code>1</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>reduction_indices</code>: The old (deprecated) name for <code>axis</code>.</li>
</ul>
<p>Return the reduced tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>]], dtype=tf.float32)  <span class="comment"># x.shape=(2, 3)</span></span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 2行3列，“axis = 1”就在列维度操作，n列变成1列，即每一行求max，合到一列里</span></span><br><span class="line"><span class="comment"># 相当于只有第1维有值，而其他几维没东西了，第1维存的是其他几维的max</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(x.shape)</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 2行3列，“axis = 0”就在行维度操作，n行变成1行，即每一列求max，合到一行里</span></span><br><span class="line"><span class="comment"># 相当于只有第0维有值，而其他几维没东西了，第0维存的是其他几维的max</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(x.shape)</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">x = tf.constant([</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">        [[<span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>], [<span class="number">55</span>, <span class="number">66</span>, <span class="number">77</span>]]</span><br><span class="line">    ], dtype=tf.float32)  <span class="comment"># x.shape=(2, 2, 3)</span></span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line">y = tf.reduce_max(x, axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(y))</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">4.</span>]</span><br><span class="line">[<span class="number">6.</span>]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">----------</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">[[<span class="number">4.</span> <span class="number">4.</span> <span class="number">6.</span>]]</span><br><span class="line">(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">----------</span><br><span class="line">[[[<span class="number">22.</span> <span class="number">33.</span> <span class="number">44.</span>]</span><br><span class="line">  [<span class="number">55.</span> <span class="number">66.</span> <span class="number">77.</span>]]]</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">----------</span><br><span class="line">[[[ <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">55.</span> <span class="number">66.</span> <span class="number">77.</span>]]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">----------</span><br><span class="line">[[[ <span class="number">3.</span>]</span><br><span class="line">  [ <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">44.</span>]</span><br><span class="line">  [<span class="number">77.</span>]]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tf-reduce-mean"><a href="#tf-reduce-mean" class="headerlink" title="tf.reduce_mean"></a>tf.reduce_mean</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reduce_mean(</span><br><span class="line">    input_tensor, axis=<span class="keyword">None</span>, keepdims=<span class="keyword">None</span>, name=<span class="keyword">None</span>,</span><br><span class="line">    reduction_indices=<span class="keyword">None</span>, keep_dims=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/math_ops.py</code>. Computes the mean of elements across dimensions of a tensor (deprecated arguments).<br>&emsp;&emsp;<code>SOME ARGUMENTS ARE DEPRECATED!</code> They will be removed in a future version. Instructions for updating: <code>keep_dims</code> is deprecated, use <code>keepdims</code> instead.<br>&emsp;&emsp;Reduces <code>input_tensor</code> along the dimensions given in <code>axis</code>. Unless <code>keepdims</code> is <code>true</code>, the rank of the tensor is reduced by <code>1</code> for each entry in <code>axis</code>. If <code>keepdims</code> is <code>true</code>, the reduced dimensions are retained with length <code>1</code>.<br>&emsp;&emsp;If <code>axis</code> has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1.</span>, <span class="number">1.</span>], [<span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line">tf.reduce_mean(x)  <span class="comment"># 1.5</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">0</span>)  <span class="comment"># [1.5, 1.5]</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">1</span>)  <span class="comment"># [1.,  2.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li>
<li><code>axis</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions. Must be in the range <code>[-rank(input_tensor), rank(input_tensor))</code>.</li>
<li><code>keepdims</code>: If <code>true</code>, retains reduced dimensions with length <code>1</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>reduction_indices</code>: The old (deprecated) name for <code>axis</code>.</li>
<li><code>keep_dims</code>: Deprecated alias for <code>keepdims</code>.</li>
</ul>
<p>Return the reduced tensor. Equivalent to <code>np.mean</code>.<br>&emsp;&emsp;Please note that <code>np.mean</code> has a <code>dtype</code> parameter that could be used to specify the output type. By default this is <code>dtype=float64</code>. On the other hand, <code>tf.reduce_mean</code> has an aggressive type inference from <code>input_tensor</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">tf.reduce_mean(x)  <span class="comment"># 0</span></span><br><span class="line">y = tf.constant([<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">tf.reduce_mean(y)  <span class="comment"># 0.5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum"></a>tf.reduce_sum</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reduce_sum(</span><br><span class="line">    input_tensor, axis=<span class="keyword">None</span>, keep_dims=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span>, reduction_indices=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Computes the sum of elements across dimensions of a tensor.<br>&emsp;&emsp;Reduces <code>input_tensor</code> along the dimensions given in <code>axis</code>. Unless <code>keep_dims</code> is <code>true</code>, the rank of the tensor is reduced by <code>1</code> for each entry in <code>axis</code>. If <code>keep_dims</code> is <code>true</code>, the reduced dimensions are retained with length <code>1</code>.<br>&emsp;&emsp;If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">tf.reduce_sum(x)  <span class="comment"># 6</span></span><br><span class="line">tf.reduce_sum(x, <span class="number">0</span>)  <span class="comment"># [2, 2, 2]</span></span><br><span class="line">tf.reduce_sum(x, <span class="number">1</span>)  <span class="comment"># [3, 3]</span></span><br><span class="line">tf.reduce_sum(x, <span class="number">1</span>, keep_dims=<span class="keyword">True</span>)  <span class="comment"># [[3], [3]]</span></span><br><span class="line">tf.reduce_sum(x, [<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</li>
<li><code>axis</code>: The dimensions to reduce. If <code>None</code> (the default), reduces all dimensions. Must be in the range <code>[-rank(input_tensor), rank(input_tensor))</code>.</li>
<li><code>keep_dims</code>: If true, retains reduced dimensions with length <code>1</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>reduction_indices</code>: The old (deprecated) name for <code>axis</code>.</li>
</ul>
<p>Return the reduced tensor.</p>
<h3 id="理解reduction-indices"><a href="#理解reduction-indices" class="headerlink" title="理解reduction_indices"></a>理解reduction_indices</h3><p>&emsp;&emsp;在<code>Tensorflow</code>的使用中，经常会使用<code>tf.reduce_mean</code>、<code>tf.reduce_sum</code>等函数。在这些函数中，有一个<code>reduction_indices</code>参数，表示函数的处理维度：</p>
<p><img src="/2019/02/13/深度学习/Tf.reduce类函数/1.png" height="185" width="649"></p>
<p>&emsp;&emsp;需要注意的一点，在很多时候看到别人的代码中并没有<code>reduction_indices</code>这个参数，此时该参数取默认值<code>None</code>，将把<code>input_tensor</code>降到<code>0</code>维，也就是一个数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之Slim/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之Slim/" itemprop="url">TensorFlow之Slim</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T12:16:46+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>TF-Slim</code>是<code>Tensorflow</code>中一个轻量级的库，用于定义、训练和评估复杂的模型。<code>TF-Slim</code>中的组件可以与<code>Tensorflow</code>中原生的函数一起使用，与其他的框架(比如<code>tf.contrib.learn</code>)也可以一起使用。使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>TF-Slim</code>可以使建立、训练和评估神经网络更加简单，具有如下特点：</p>
<ul>
<li>允许用户通过减少模板代码使得模型更加简洁。这个可以通过使用<code>argument scoping</code>和大量的高层<code>layers</code>、<code>variables</code>来实现。</li>
<li>通过使用常用的正则化(<code>regularizers</code>)使得建立模型更加简单。</li>
<li>一些广泛使用的计算机视觉相关的模型(比如<code>VGG</code>、<code>AlexNet</code>)已经在<code>slim</code>中定义好了，用户可以很方便地使用。这些既可以当成黑盒使用，也可以被扩展使用，比如添加一些<code>multiple heads</code>到不同的内部的层。</li>
<li><code>Slim</code>使得扩展复杂模型变得容易，可以使用已经存在的模型的<code>checkpoints</code>来开始训练算法。</li>
</ul>
<h3 id="What-are-the-various-components-of-TF-Slim"><a href="#What-are-the-various-components-of-TF-Slim" class="headerlink" title="What are the various components of TF-Slim?"></a>What are the various components of TF-Slim?</h3><p>&emsp;&emsp;<code>TF-Slim</code>由几个独立存在的组件组成：</p>
<ul>
<li><code>arg_scope</code>：提供一个新的作用域(<code>scope</code>)，称为<code>arg_scope</code>。在该作用域(<code>scope</code>)中，用户可以定义一些默认的参数，用于特定的操作。</li>
<li><code>data</code>：包含<code>TF-Slim</code>的<code>dataset</code>定义、<code>data providers</code>、<code>parallel_reader</code>和<code>decoding utilities</code>。</li>
<li><code>evaluation</code>：包含用于模型评估的常规函数。</li>
<li><code>layers</code>：包含用于建立模型的高级<code>layers</code>。</li>
<li><code>learning</code>：包含一些用于训练模型的常规函数。</li>
<li><code>losses</code>：包含一些用于<code>loss function</code>的函数。</li>
<li><code>metrics</code>：包含一些热门的评价标准。</li>
<li><code>nets</code>：包含一些热门的网络定义，如<code>VGG</code>、<code>AlexNet</code>等模型。</li>
<li><code>queues</code>：提供一个内容管理者，使得可以很容易、很安全地启动和关闭<code>QueueRunners</code>。</li>
<li><code>regularizers</code>：包含权重正则化。</li>
<li><code>variables</code>：提供一个方便的封装，用于变量创建和使用。</li>
</ul>
<h4 id="Defining-Models"><a href="#Defining-Models" class="headerlink" title="Defining Models"></a>Defining Models</h4><p>&emsp;&emsp;使用<code>TF-Slim</code>，结合<code>variables</code>、<code>layers</code>和<code>scopes</code>，模型可以很简洁地被定义。</p>
<h4 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h4><p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，创建<code>Variable</code>需要一个预定义的值或者一种初始化机制(比如从一个高斯分布中随机采样)。此外，如果一个变量需要在一个特定的设备上(如<code>GPU</code>)创建，那么必须被明确说明。为了减少变量创建所需的代码，<code>TF-Slim</code>提供了一些封装函数(定义在<code>variables.py</code>中)，可以使得用户定义变量变得简单。<br>&emsp;&emsp;举个例子，定义一个权重(<code>weight</code>)变量，然后使用一个截断的正态分布来初始化，使用<code>l2 loss</code>正则化，并将该变量放置在<code>CPU</code>中，只需要声明如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = slim.variable(</span><br><span class="line">    <span class="string">'weights'</span>, shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">3</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">    regularizer=slim.l2_regularizer(<span class="number">0.05</span>), device=<span class="string">'/CPU:0'</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，有两种类型的<code>variables</code>：<code>regular variables</code>和<code>local (transient) variables</code>。绝大部分变量是<code>regular variables</code>，一旦被创建，可以使用<code>saver</code>来将这些变量保存到磁盘中；<code>Local variables</code>是那些仅仅存在于一个<code>session</code>内的变量，并不会被保存到磁盘中。<br>&emsp;&emsp;<code>TF-Slim</code>通过定义<code>model variables</code>来进一步区别变量，这些是表示一个模型参数的变量。<code>Model variables</code>在学习期间被训练或者<code>fine-tuned</code>，在评估或者推断期间可以从一个<code>checkpoint</code>中加载。模型变量包括使用<code>slim.fully_connected</code>或者<code>slim.conv2d</code>创建的变量等。非模型变量(<code>Non-model variables</code>)指的是那些在学习或者评估阶段使用，但是在实际的<code>inference</code>中不需要用到的变量。例如<code>global_step</code>在学习和评估阶段会用到的变量，但是实际上并不是模型的一部分。类似的，<code>moving average variables</code>也是非模型变量。<br>&emsp;&emsp;<code>model variables</code>和<code>regular variables</code>在<code>TF-Slim</code>中很容易地被创建和恢复：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Variables</span></span><br><span class="line">weights = slim.model_variable(</span><br><span class="line">    <span class="string">'weights'</span>, shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">3</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">    regularizer=slim.l2_regularizer(<span class="number">0.05</span>), device=<span class="string">'/CPU:0'</span>)</span><br><span class="line">model_variables = slim.get_model_variables()</span><br><span class="line"><span class="comment"># Regular variables</span></span><br><span class="line">my_var = slim.variable(<span class="string">'my_var'</span>, shape=[<span class="number">20</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">regular_variables_and_model_variables = slim.get_variables()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这是如何工作的呢？当你通过<code>TF-Slim</code>的<code>layer</code>或者直接通过<code>slim.model_variable</code>函数创建一个模型的变量时，<code>TF-Slim</code>将变量添加到<code>tf.GraphKeys.MODEL_VARIABLES</code>集合中。如果你想拥有自己定制化的<code>layers</code>或者<code>variables</code>创建机制，但是仍然想利用<code>TF-Slim</code>来管理你的变量，此时<code>TF-Slim</code>提供一个方便的函数，用于添加模型的变量到集合中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_model_variable = CreateViaCustomCode()</span><br><span class="line">slim.add_model_variable(my_model_variable)  <span class="comment"># Letting TF-Slim know about the additional variable</span></span><br></pre></td></tr></table></figure>
<h4 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h4><p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，要定义一些层(比如说卷积层、全连接层、<code>BatchNorm</code>层等)是比较麻烦的。举个例子，神经网络中的卷积层由以下几个步骤组成：</p>
<ul>
<li>创建权重和偏置变量。</li>
<li>将输入与权重做卷积运算。</li>
<li>将偏置加到第二步的卷积运算得到的结果中。</li>
<li>使用一个激活函数。</li>
</ul>
<p>上面的步骤使用原始的<code>Tensorflow</code>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1_1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    kernel = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=tf.float32, stddev=<span class="number">1e-1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    conv = tf.nn.conv2d(input, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">128</span>], dtype=tf.float32), trainable=<span class="keyword">True</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">    conv1 = tf.nn.relu(bias, name=scope)</span><br></pre></td></tr></table></figure>
<p>为了减少重复代码，<code>TF-Slim</code>提供了一些方便的、高级别的、更抽象的神经网络层。例如卷积层实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line">net = slim.conv2d(input, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1_1'</span>)</span><br></pre></td></tr></table></figure>
<p><code>TF-Slim</code>提供了大量的标准的实现，用于建立神经网络：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Layer</th>
<th>TF-Slim</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BiasAdd</code></td>
<td><code>slim.bias_add</code></td>
</tr>
<tr>
<td><code>BatchNorm</code></td>
<td><code>slim.batch_norm</code></td>
</tr>
<tr>
<td><code>Conv2d</code></td>
<td><code>slim.conv2d</code></td>
</tr>
<tr>
<td><code>Conv2dInPlane</code></td>
<td><code>slim.conv2d_in_plane</code></td>
</tr>
<tr>
<td><code>Conv2dTranspose(Deconv)</code></td>
<td><code>slim.conv2d_transpose</code></td>
</tr>
<tr>
<td><code>FullyConnected</code></td>
<td><code>slim.fully_connected</code></td>
</tr>
<tr>
<td><code>AvgPool2D</code></td>
<td><code>slim.avg_pool2d</code></td>
</tr>
<tr>
<td><code>Dropout</code></td>
<td><code>slim.dropout</code></td>
</tr>
<tr>
<td><code>Flatten</code></td>
<td><code>slim.flatten</code></td>
</tr>
<tr>
<td><code>MaxPool2D</code></td>
<td><code>slim.max_pool2d</code></td>
</tr>
<tr>
<td><code>OneHotEncoding</code></td>
<td><code>slim.one_hot_encoding</code></td>
</tr>
<tr>
<td><code>SeparableConv2</code></td>
<td><code>slim.separable_conv2d</code></td>
</tr>
<tr>
<td><code>UnitNorm</code></td>
<td><code>slim.unit_norm</code></td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;<code>TF-Slim</code>也包含两个操作符，称为<code>repeat</code>和<code>stack</code>，允许用户重复执行相同的操作。例如下面几个卷积层加一个池化层是<code>VGG</code>网络的一部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = ...</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>减少重复代码的其中一种方法是利用<code>for</code>循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = ...</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_%d'</span> % (i + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>另一种方式是使用<code>TF-Slim</code>中的<code>repeat</code>操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>在上面例子中，<code>slim.repeat</code>会自动给每一个卷积层的<code>scopes</code>命名为<code>conv3/conv3_1</code>、<code>conv3/conv3_2</code>和<code>conv3/conv3_3</code>。<br>&emsp;&emsp;另外，<code>TF-Slim</code>的<code>slim.stack</code>操作允许用户用不同的参数重复调用同一种操作。<code>slim.stack</code>也为每一个被创建的操作创建一个新的<code>tf.variable_scope</code>。例如下面是一种简单的方式来创建多层感知器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.fully_connected(x, <span class="number">32</span>, scope=<span class="string">'fc/fc_1'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">64</span>, scope=<span class="string">'fc/fc_2'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">128</span>, scope=<span class="string">'fc/fc_3'</span>)</span><br><span class="line"><span class="comment"># Equivalent, TF-Slim way using slim.stack:</span></span><br><span class="line">slim.stack(x, slim.fully_connected, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure>
<p>在上面的例子中，<code>slim.stack</code>调用了<code>slim.fully_connected</code>三次。类似的，我们可以使用<code>stack</code>来简化多层的卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_1'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_2'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_3'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_4'</span>)</span><br><span class="line"><span class="comment"># Using stack:</span></span><br><span class="line">slim.stack(x, slim.conv2d, [(<span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>])], scope=<span class="string">'core'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Scopes"><a href="#Scopes" class="headerlink" title="Scopes"></a>Scopes</h4><p>&emsp;&emsp;除了<code>Tensorflow</code>中作用域(<code>scope</code>)之外(<code>name_scope</code>、<code>variable_scope</code>)，<code>TF-Slim</code>增加了新的作用域机制，称为<code>arg_scope</code>。这个新的作用域允许使用者明确一个或者多个操作和一些参数，这些定义好的操作或者参数会传递给<code>arg_scope</code>内部的每一个操作。先看如下代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(</span><br><span class="line">    inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>从上面的代码中可以清楚地看出来有<code>3</code>层卷积层，其中很多超参数都是一样的。两个卷积层有相同的<code>padding</code>，所有三个卷积层有相同的<code>weights_initializer</code>和<code>weight_regularizer</code>。上面的代码包含了大量重复的值，其中一种解决方法是使用变量来说明一些默认的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">padding = <span class="string">'SAME'</span></span><br><span class="line">initializer = tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">regularizer = slim.l2_regularizer(<span class="number">0.0005</span>)</span><br><span class="line">​</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=padding, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=padding, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的解决方案其实并没有减少代码的混乱程度。通过使用<code>arg_scope</code>，我们既可以保证每一层使用相同的值，也可以简化代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d], padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv2'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的例子表明，使用<code>arg_scope</code>可以使得代码变得更整洁、更干净并且更加容易维护。注意到，在<code>arg_scope</code>中规定的参数值，它们可以被局部覆盖。例如上面的<code>padding</code>参数被设置成<code>SAME</code>，但是在第二个卷积层中用<code>VALID</code>覆盖了这个参数。<br>&emsp;&emsp;我们也可以嵌套使用<code>arg_scope</code>，在相同的作用域内使用多个操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d], stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>):</span><br><span class="line">        net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv1'</span>)</span><br><span class="line">        net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">5</span>, <span class="number">5</span>], weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.03</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">1000</span>, activation_fn=<span class="keyword">None</span>, scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure>
<p>在第一个<code>arg_scope</code>中，卷积层和全连接层被应用于相同的权重初始化和权重正则化；在第二个<code>arg_scope</code>中，额外的参数仅仅对卷积层<code>conv2d</code>起作用。</p>
<h3 id="Working-Example-Specifying-the-VGG16-Layers"><a href="#Working-Example-Specifying-the-VGG16-Layers" class="headerlink" title="Working Example: Specifying the VGG16 Layers"></a>Working Example: Specifying the VGG16 Layers</h3><p>&emsp;&emsp;通过结合<code>TF-Slim</code>的<code>Variables</code>、<code>Operations</code>和<code>scopes</code>，我们可以使用比较少的代码来实现一个比较复杂的网络。例如整个<code>VGG</code>网络定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg16</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">     [slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,</span><br><span class="line">     weights_initializer=tf.truncated_normal_initializer(<span class="number">0.0</span>, <span class="number">0.01</span>),</span><br><span class="line">     weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">        net = slim.repeat(inputs, <span class="number">2</span>, slim.conv2d, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool1'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">2</span>, slim.conv2d, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv2'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool3'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv4'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool4'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv5'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool5'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">4096</span>, scope=<span class="string">'fc6'</span>)</span><br><span class="line">        net = slim.dropout(net, <span class="number">0.5</span>, scope=<span class="string">'dropout6'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">4096</span>, scope=<span class="string">'fc7'</span>)</span><br><span class="line">        net = slim.dropout(net, <span class="number">0.5</span>, scope=<span class="string">'dropout7'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">1000</span>, activation_fn=<span class="keyword">None</span>, scope=<span class="string">'fc8'</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<h3 id="Training-Models"><a href="#Training-Models" class="headerlink" title="Training Models"></a>Training Models</h3><p>&emsp;&emsp;训练<code>Tensorflow</code>模型要求一个模型、一个<code>loss function</code>、梯度计算和一个训练的程序，用来迭代地根据<code>loss</code>计算模型权重的梯度和更新权重。<code>TF-Slim</code>提供了<code>loss function</code>和一些帮助函数来运行训练和评估。</p>
<h4 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h4><p>&emsp;&emsp;<code>Loss function</code>定义了一个我们需要最小化的量。对于分类问题，主要是计算真正的分布与预测的概率分布之间的交叉熵；对于回归问题，主要是计算预测值与真实值均方误差。<br>&emsp;&emsp;特定的模型，比如说多任务学习模型，要求同时使用多个<code>loss function</code>。换句话说，最终被最小化的<code>loss function</code>是多个其他的<code>loss function</code>之和。例如一个同时预测图像中场景的类型和深度的模型，该模型的<code>loss function</code>就是分类<code>loss</code>和深度预测<code>loss</code>之和。<br>&emsp;&emsp;<code>TF-Slim</code>通过<code>losses</code>模块为用户提供了一种机制，使得定义<code>loss function</code>变得简单。例如下面的是我们想要训练<code>VGG</code>网络的简单示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">images, labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">predictions, _ = vgg.vgg_16(images)  <span class="comment"># Create the model</span></span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">loss = slim.losses.softmax_cross_entropy(predictions, labels)</span><br></pre></td></tr></table></figure>
<p>在上面这个例子中，我们首先创建一个模型(利用<code>TF-Slim</code>的<code>VGG</code>实现)，然后增加了标准的分类<code>loss</code>。现在来看看当我们有一个多个输出的多任务模型的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">images, scene_labels, depth_labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">scene_predictions, depth_predictions = CreateMultiTaskModel(images)  <span class="comment"># Create the model</span></span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)</span><br><span class="line">sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)</span><br><span class="line"><span class="comment"># The following two lines have the same effect:</span></span><br><span class="line">total_loss = classification_loss + sum_of_squares_loss</span><br><span class="line">total_loss = slim.losses.get_total_loss(add_regularization_losses=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中有<code>2</code>个<code>loss</code>，是通过调用<code>slim.losses.softmax_cross_entropy</code>和<code>slim.losses.sum_of_squares</code>得到。我们可以将这两个<code>loss</code>加在一起，或者调用<code>slim.losses.get_total_loss</code>来得到全部的<code>loss</code>(<code>total_loss</code>)。这是如何工作的？当你通过<code>TF-Slim</code>创建一个<code>loss</code>时，<code>TF-Slim</code>将<code>loss</code>加到一个特殊的<code>TensorFlow collection of loss functions</code>。这使得你既可以手动地管理全部的<code>loss</code>，也可以让<code>TF-Slim</code>来替你管理它们。<br>&emsp;&emsp;如果你想让<code>TF-Slim</code>为你管理<code>losses</code>，但是你有一个自己实现的<code>loss</code>该怎么办？<code>loss_ops.py</code>也有一个函数可以将你自己实现的<code>loss</code>加到<code>TF-Slims collection</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">images, scene_labels, depth_labels, pose_labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">scene_predictions, depth_predictions, pose_predictions = CreateMultiTaskModel(images)  <span class="comment"># Create the model</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)</span><br><span class="line">sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)</span><br><span class="line">pose_loss = MyCustomLossFunction(pose_predictions, pose_labels)</span><br><span class="line">slim.losses.add_loss(pose_loss)  <span class="comment"># Letting TF-Slim know about the additional loss.</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># The following two ways to compute the total loss are equivalent:</span></span><br><span class="line">regularization_loss = tf.add_n(slim.losses.get_regularization_losses())</span><br><span class="line">total_loss1 = classification_loss + sum_of_squares_loss + pose_loss + regularization_loss</span><br><span class="line">​</span><br><span class="line">total_loss2 = slim.losses.get_total_loss()  <span class="comment"># (Regularization Loss is included in the total loss by default)</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们既可以手动地计算的出全部的<code>loss function</code>，也可以让<code>TF-Slim</code>知道这个额外的<code>loss</code>，然后让<code>TF-Slim</code>处理这个<code>loss</code>。</p>
<h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><p>&emsp;&emsp;<code>TF-Slim</code>提供了一个简单但是很强的用于训练模型的工具(在<code>learning.py</code>中)，其中包括一个可以重复测量<code>loss</code>、计算梯度和将模型保存到磁盘的训练函数。举个例子，一旦定义好了模型、<code>loss function</code>和最优化方法，我们可以调用<code>slim.learning.create_train_op</code>和<code>slim.learning.train</code>来实现优化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the model and specify the losses</span></span><br><span class="line">...</span><br><span class="line">total_loss = slim.losses.get_total_loss()</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line"><span class="comment"># create_train_op ensures that each time we ask for the loss,</span></span><br><span class="line"><span class="comment"># the update_ops are run and the gradients being computed are applied too</span></span><br><span class="line">train_op = slim.learning.create_train_op(total_loss, optimizer)</span><br><span class="line">logdir = ...  <span class="comment"># Where checkpoints are stored.</span></span><br><span class="line">slim.learning.train(train_op, logdir, number_of_steps=<span class="number">1000</span>, save_summaries_secs=<span class="number">300</span>, save_interval_secs=<span class="number">600</span>):</span><br></pre></td></tr></table></figure>
<p>在这个例子中，提供给<code>slim.learning.train</code>的参数有<code>train_op</code>(用于计算<code>loss</code>和梯度)；<code>logdir</code>(用于声明<code>checkpoints</code>和<code>event</code>文件保存的路径)；用<code>number_of_steps</code>参数来限制梯度下降的步数；<code>save_summaries_secs=300</code>表明每<code>5</code>分钟计算一次<code>summaries</code>；<code>save_interval_secs=600</code>表明每<code>10</code>分钟保存一次模型的<code>checkpoint</code>。</p>
<h3 id="Working-Example-Training-the-VGG16-Model"><a href="#Working-Example-Training-the-VGG16-Model" class="headerlink" title="Working Example: Training the VGG16 Model"></a>Working Example: Training the VGG16 Model</h3><p>&emsp;&emsp;下面是训练一个<code>VGG</code>网络的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">...</span><br><span class="line">train_log_dir = ...</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(train_log_dir):</span><br><span class="line">    tf.gfile.MakeDirs(train_log_dir)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    images, labels = ...  <span class="comment"># Set up the data loading</span></span><br><span class="line">    predictions = vgg.vgg_16(images, is_training=<span class="keyword">True</span>)  <span class="comment"># Define the model</span></span><br><span class="line">    <span class="comment"># Specify the loss function:</span></span><br><span class="line">    slim.losses.softmax_cross_entropy(predictions, labels)</span><br><span class="line">    total_loss = slim.losses.get_total_loss()</span><br><span class="line">    tf.summary.scalar(<span class="string">'losses/total_loss'</span>, total_loss)</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">.001</span>)  <span class="comment"># Specify the optimization scheme</span></span><br><span class="line">    <span class="comment"># create_train_op that ensures that when we evaluate it to get the loss,</span></span><br><span class="line">    <span class="comment"># the update_ops are done and the gradient updates are computed</span></span><br><span class="line">    train_tensor = slim.learning.create_train_op(total_loss, optimizer)</span><br><span class="line">    slim.learning.train(train_tensor, train_log_dir)  <span class="comment"># Actually runs training</span></span><br></pre></td></tr></table></figure>
<h3 id="Fine-Tuning-Existing-Models"><a href="#Fine-Tuning-Existing-Models" class="headerlink" title="Fine-Tuning Existing Models"></a>Fine-Tuning Existing Models</h3><h4 id="Brief-Recap-on-Restoring-Variables-from-a-Checkpoint"><a href="#Brief-Recap-on-Restoring-Variables-from-a-Checkpoint" class="headerlink" title="Brief Recap on Restoring Variables from a Checkpoint"></a>Brief Recap on Restoring Variables from a Checkpoint</h4><p>&emsp;&emsp;当一个模型被训练完毕之后，它可以从一个给定的<code>checkpoint</code>中使用<code>tf.train.Saver</code>来恢复变量。在很多情况下，<code>tf.train.Saver</code>提供一个简单的机制来恢复所有变量或者一部分变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables</span></span><br><span class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</span><br><span class="line">restorer = tf.train.Saver()  <span class="comment"># Add ops to restore all the variables</span></span><br><span class="line">restorer = tf.train.Saver([v1, v2])  <span class="comment"># Add ops to restore some variables</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and do some work with the model</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br><span class="line">    print(<span class="string">"Model restored."</span>)</span><br><span class="line">    <span class="comment"># Do some work with the model</span></span><br></pre></td></tr></table></figure>
<h4 id="Partially-Restoring-Models"><a href="#Partially-Restoring-Models" class="headerlink" title="Partially Restoring Models"></a>Partially Restoring Models</h4><p>&emsp;&emsp;在一个新的数据集或者一个新的任务上<code>fine-tune</code>一个预训练的模型通常是比较流行的，我们可以使用<code>TF-Slim</code>的<code>helper</code>函数来选择想要恢复的一部分变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables.</span></span><br><span class="line">v1 = slim.variable(name=<span class="string">"v1"</span>, ...)</span><br><span class="line">v2 = slim.variable(name=<span class="string">"nested/v2"</span>, ...)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Get list of variables to restore (which contains only 'v2').</span></span><br><span class="line"><span class="comment"># These are all equivalent methods:</span></span><br><span class="line">variables_to_restore = slim.get_variables_by_name(<span class="string">"v2"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_by_suffix(<span class="string">"2"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables(scope=<span class="string">"nested"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(include=[<span class="string">"nested"</span>])</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(exclude=[<span class="string">"v1"</span>])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the saver which will be used to restore the variables</span></span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br><span class="line">    print(<span class="string">"Model restored."</span>)</span><br><span class="line">    <span class="comment"># Do some work with the model</span></span><br></pre></td></tr></table></figure>
<h4 id="Restoring-models-with-different-variable-names"><a href="#Restoring-models-with-different-variable-names" class="headerlink" title="Restoring models with different variable names"></a>Restoring models with different variable names</h4><p>&emsp;&emsp;当从一个<code>checkpoint</code>中恢复变量时，<code>Saver</code>定位在<code>checkpoint</code>文件中变量的名字，并且将它们映射到当前图(<code>graph</code>)的变量中去。在上面的例子中，我们通过传递给<code>saver</code>一个变量列表来创建一个<code>saver</code>。在这种情况下，在<code>checkpoint</code>文件中定位的变量名隐式地从每个提供的变量的<code>var.op.name</code>中获得。<br>&emsp;&emsp;当<code>checkpoint</code>文件中的变量名与<code>graph</code>匹配时，将会工作良好。然而有时候，我们想要从一个与当前的<code>graph</code>不同变量名的<code>checkpoint</code>中恢复变量，那么在这种情况下，我们必须给<code>Saver</code>提供一个字典，该字典将每个<code>checkpoint</code>中变量名映射到每个<code>graph</code>的变量。下面的例子通过一个简单的函数获得<code>checkpoint</code>中的变量的名字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assuming than 'conv1/weights' should be restored from 'vgg16/conv1/weights'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_in_checkpoint</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'vgg16/'</span> + var.op.name</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Assuming than 'conv1/weights' and 'conv1/bias' should be restored from 'conv1/params1' and 'conv1/params2'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_in_checkpoint</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"weights"</span> <span class="keyword">in</span> var.op.name:</span><br><span class="line">        <span class="keyword">return</span> var.op.name.replace(<span class="string">"weights"</span>, <span class="string">"params1"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">"bias"</span> <span class="keyword">in</span> var.op.name:</span><br><span class="line">        <span class="keyword">return</span> var.op.name.replace(<span class="string">"bias"</span>, <span class="string">"params2"</span>)</span><br><span class="line">​</span><br><span class="line">variables_to_restore = slim.get_model_variables()</span><br><span class="line">variables_to_restore = &#123;name_in_checkpoint(var): var <span class="keyword">for</span> var <span class="keyword">in</span> variables_to_restore&#125;</span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br></pre></td></tr></table></figure>
<h4 id="Fine-Tuning-a-Model-on-a-different-task"><a href="#Fine-Tuning-a-Model-on-a-different-task" class="headerlink" title="Fine-Tuning a Model on a different task"></a>Fine-Tuning a Model on a different task</h4><p>&emsp;&emsp;考虑这么一种情况：我们有一个预训练好的<code>VGG16</code>模型，该模型是在<code>ImageNet</code>数据集上训练好的，有<code>1000</code>类。然而我们想要将其应用到只有<code>20</code>类的<code>Pascal VOC</code>数据集上。为了实现这个功能，我们可以使用不包括最后一层的预训练模型来初始化新模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">image, label = MyPascalVocDataLoader(...)  <span class="comment"># Load the Pascal VOC data</span></span><br><span class="line">images, labels = tf.train.batch([image, label], batch_size=<span class="number">32</span>)</span><br><span class="line">​</span><br><span class="line">predictions = vgg.vgg_16(images)  <span class="comment"># Create the model</span></span><br><span class="line">train_op = slim.learning.create_train_op(...)</span><br><span class="line"><span class="comment"># Specify where the Model, trained on ImageNet, was saved</span></span><br><span class="line">model_path = <span class="string">'/path/to/pre_trained_on_imagenet.checkpoint'</span></span><br><span class="line">log_dir = <span class="string">'/path/to/my_pascal_model_dir/'</span>  <span class="comment"># Specify where the new model will live</span></span><br><span class="line"><span class="comment"># Restore only the convolutional layers</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(exclude=[<span class="string">'fc6'</span>, <span class="string">'fc7'</span>, <span class="string">'fc8'</span>])</span><br><span class="line">init_fn = assign_from_checkpoint_fn(model_path, variables_to_restore)</span><br><span class="line">slim.learning.train(train_op, log_dir, init_fn=init_fn)  <span class="comment"># Start training</span></span><br></pre></td></tr></table></figure>
<h3 id="Evaluating-Models"><a href="#Evaluating-Models" class="headerlink" title="Evaluating Models"></a>Evaluating Models</h3><p>&emsp;&emsp;一旦我们已经训练好了一个模型(或者模型正在训练之中)，想要看看模型的实际表现能力，这个可以通过使用一些评估度量来实现，该度量可以对模型的表现能力评分。而评估代码实际上是加载数据、做出预测、将预测结果与真实值做比较，最后得到得分。这个步骤可以运行一次或者周期重复。</p>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>&emsp;&emsp;我们将度量定义为一个性能度量，它不是一个<code>loss</code>函数(<code>losses</code>是在训练的时候直接最优化)，但我们仍然感兴趣的是评估模型的目的。例如我们想要最优化<code>log loss</code>，但是我们感兴趣的度量可能是<code>F1</code>得分(<code>test accuracy</code>)，或者是<code>Intersection Over Union score</code>(这是不可微的，因此不能作为损失使用)。<br>&emsp;&emsp;<code>TF-Slim</code>提供了一些使得评估模型变得简单的度量操作。计算度量的值可以分为以下三个步骤：</p>
<ul>
<li>初始化(<code>Initialization</code>)：初始化用于计算度量的变量。</li>
<li>聚合(<code>Aggregation</code>)：使用操作(比如求和操作)来计算度量。</li>
<li>终止化(<code>Finalization</code>)：这是可选的步骤，使用最终的操作来计算度量值，比如说计算均值、最小值、最大值等。</li>
</ul>
<p>举个例子，为了计算<code>mean_absolute_error</code>，变量<code>count</code>和<code>total</code>变量被初始化为<code>0</code>。在聚合期间，我们观测到一些预测值和标签值，计算它们的绝对差值然后加到<code>total</code>中。每一次我们观测到新的一个数据，我们增加<code>count</code>。最后在<code>Finalization</code>期间，<code>total</code>除以<code>count</code>来获得均值<code>mean</code>。<br>&emsp;&emsp;下面的示例演示了声明度量标准的<code>API</code>。由于度量经常在测试集上进行评估，因此我们假设使用的是测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">images, labels = LoadTestData(...)</span><br><span class="line">predictions = MyModel(images)</span><br><span class="line">​</span><br><span class="line">mae_value_op, mae_update_op = slim.metrics.streaming_mean_absolute_error(predictions, labels)</span><br><span class="line">mre_value_op, mre_update_op = slim.metrics.streaming_mean_relative_error(predictions, labels)</span><br><span class="line">pl_value_op, pl_update_op = slim.metrics.percentage_less(mean_relative_errors, <span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>一个度量的创建返回两个值：<code>value_op</code>和<code>update_op</code>。<code>value_op</code>是一个幂等操作，它返回度量的当前值；<code>update_op</code>是执行上面提到的聚合步骤的操作，以及返回度量的值。<br>&emsp;&emsp;跟踪每个<code>value_op</code>和<code>update_op</code>是很费力的，为了解决这个问题，<code>TF-Slim</code>提供了两个便利功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Aggregates the value and update ops in two lists:</span></span><br><span class="line">value_ops, update_ops = slim.metrics.aggregate_metrics(</span><br><span class="line">    slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    slim.metrics.streaming_mean_squared_error(predictions, labels))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Aggregates the value and update ops in two dictionaries</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">"eval/mean_absolute_error"</span>: slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    <span class="string">"eval/mean_squared_error"</span>: slim.metrics.streaming_mean_squared_error(predictions, labels),</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="Working-example-Tracking-Multiple-Metrics"><a href="#Working-example-Tracking-Multiple-Metrics" class="headerlink" title="Working example: Tracking Multiple Metrics"></a>Working example: Tracking Multiple Metrics</h3><p>&emsp;&emsp;将代码全部放在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Load the data</span></span><br><span class="line">images, labels = load_data(...)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the network</span></span><br><span class="line">predictions = vgg.vgg_16(images)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Choose the metrics to compute:</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">"eval/mean_absolute_error"</span>: slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    <span class="string">"eval/mean_squared_error"</span>: slim.metrics.streaming_mean_squared_error(predictions, labels),</span><br><span class="line">&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Evaluate the model using 1000 batches of data:</span></span><br><span class="line">num_batches = <span class="number">1000</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    sess.run(tf.local_variables_initializer())</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> batch_id <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        sess.run(names_to_updates.values())</span><br><span class="line">​</span><br><span class="line">    metric_values = sess.run(names_to_values.values())</span><br><span class="line">    <span class="keyword">for</span> metric, value <span class="keyword">in</span> zip(names_to_values.keys(), metric_values):</span><br><span class="line">        print(<span class="string">'Metric %s has value: %f'</span> % (metric, value))</span><br></pre></td></tr></table></figure>
<h3 id="Evaluation-Loop"><a href="#Evaluation-Loop" class="headerlink" title="Evaluation Loop"></a>Evaluation Loop</h3><p>&emsp;&emsp;<code>TF-Slim</code>提供了一个评估模块(<code>evaluation.py</code>)，它包含了使用来自<code>metric_ops.py</code>模块编写模型评估脚本的辅助函数。这些功能包括定期运行评估、对数据批量进行评估、打印和汇总度量结果的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">images, labels = load_data(...)  <span class="comment"># Load the data</span></span><br><span class="line">predictions = MyModel(images)  <span class="comment"># Define the network</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Choose the metrics to compute:</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">'accuracy'</span>: slim.metrics.accuracy(predictions, labels),</span><br><span class="line">    <span class="string">'precision'</span>: slim.metrics.precision(predictions, labels),</span><br><span class="line">    <span class="string">'recall'</span>: slim.metrics.recall(mean_relative_errors, <span class="number">0.3</span>),</span><br><span class="line">&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the summary ops such that they also print out to std output:</span></span><br><span class="line">summary_ops = []</span><br><span class="line"><span class="keyword">for</span> metric_name, metric_value <span class="keyword">in</span> names_to_values.iteritems():</span><br><span class="line">    op = tf.summary.scalar(metric_name, metric_value)</span><br><span class="line">    op = tf.Print(op, [metric_value], metric_name)</span><br><span class="line">    summary_ops.append(op)</span><br><span class="line">​</span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_batches = math.ceil(num_examples / float(batch_size))</span><br><span class="line">​</span><br><span class="line">slim.get_or_create_global_step()  <span class="comment"># Setup the global step</span></span><br><span class="line">​</span><br><span class="line">output_dir = ...  <span class="comment"># Where the summaries are stored.</span></span><br><span class="line">eval_interval_secs = ...  <span class="comment"># How often to run the evaluation.</span></span><br><span class="line">slim.evaluation.evaluation_loop(</span><br><span class="line">    <span class="string">'local'</span>, checkpoint_dir, log_dir, num_evals=num_batches, eval_op=names_to_updates.values(),</span><br><span class="line">    summary_op=tf.summary.merge(summary_ops), eval_interval_secs=eval_interval_secs)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="tf-nn-conv2d和tf-contrib-slim-conv2d的区别"><a href="#tf-nn-conv2d和tf-contrib-slim-conv2d的区别" class="headerlink" title="tf.nn.conv2d和tf.contrib.slim.conv2d的区别"></a>tf.nn.conv2d和tf.contrib.slim.conv2d的区别</h3><p>&emsp;&emsp;在查看代码的时候，发现有代码用到卷积层是<code>tf.nn.conv2d</code>，但是也有使用的卷积层是<code>tf.contrib.slim.conv2d</code>。<code>tf.nn.conv2d</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：需要做卷积的输入图像，它要求是一个<code>Tensor</code>，具有<code>[batch_size, in_height, in_width, in_channels]</code>这样的<code>shape</code>，具体含义是<code>[训练时一个batch的图片数量，图片高度，图片宽度，图像通道数]</code>。注意这是一个<code>4</code>维的<code>Tensor</code>，要求数据类型为<code>float32</code>和<code>float64</code>其中之一。</li>
<li><code>filter</code>：指定<code>CNN</code>中的卷积核，它要求是一个<code>Tensor</code>，具有<code>[filter_height, filter_width, in_channels, out_channels]</code>这样的<code>shape</code>，具体含义是<code>[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</code>，要求类型与参数<code>input</code>相同。有一个地方需要注意，第三维<code>in_channels</code>就是参数<code>input</code>的第四维，这里是维度一致，不是数值一致。</li>
<li><code>strides</code>：卷积时在图像每一维的步长。这是一个一维的向量，长度为<code>4</code>，对应的是在<code>input</code>的<code>4</code>个维度上的步长。</li>
<li><code>padding</code>：<code>string</code>类型的变量，只能是<code>SAME</code>、<code>VALID</code>其中之一。这个值决定了不同的卷积方式，<code>SAME</code>代表卷积核可以停留图像边缘，<code>VALID</code>表示不能。</li>
<li><code>use_cudnn_on_gpu</code>：指定是否使用<code>cudnn</code>加速。</li>
<li><code>data_format</code>：指定<code>input</code>的格式，默认为<code>NHWC</code>格式。</li>
</ul>
<p>该函数结果返回一个<code>Tensor</code>，这个输出就是我们常说的<code>feature map</code>。<br>&emsp;&emsp;<code>tf.contrib.slim.conv2d</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">convolution(</span><br><span class="line">    inputs, num_outputs, kernel_size, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    rate=<span class="number">1</span>, activation_fn=nn.relu, normalizer_fn=<span class="keyword">None</span>, normalizer_params=<span class="keyword">None</span>,</span><br><span class="line">    weights_initializer=initializers.xavier_initializer(), weights_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    biases_initializer=init_ops.zeros_initializer(), biases_regularizer=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>,</span><br><span class="line">    variables_collections=<span class="keyword">None</span>, outputs_collections=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, scope=<span class="keyword">None</span>):</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：需要做卷积的输入图像。</li>
<li><code>num_outputs</code>：卷积核的个数(就是<code>filter</code>的个数)。</li>
<li><code>kernel_size</code>：卷积核的维度(卷积核的宽度和高度)。</li>
<li><code>stride</code>：卷积时在图像每一维的步长。</li>
<li><code>padding</code>：<code>padding</code>的方式选择，<code>VALID</code>或者<code>SAME</code>。</li>
<li><code>data_format</code>：指定<code>input</code>的格式。</li>
<li><code>rate</code>：使用<code>atrous convolution</code>的膨胀率。</li>
<li><code>activation_fn</code>：指定激活函数，默认为<code>ReLU</code>函数。</li>
<li><code>normalizer_fn</code>：正则化函数。</li>
<li><code>normalizer_params</code>：正则化函数的参数。</li>
<li><code>weights_initializer</code>：权重的初始化程序。</li>
<li><code>weights_regularizer</code>：权重可选的正则化程序。</li>
<li><code>biases_initializer</code>：<code>biase</code>的初始化程序。</li>
<li><code>biases_regularizer</code>：<code>biases</code>可选的正则化程序。</li>
<li><code>reuse</code>：是否共享层或者核变量。</li>
<li><code>variable_collections</code>：所有变量的集合列表或者字典。</li>
<li><code>outputs_collections</code>：输出被添加的集合。</li>
<li><code>trainable</code>：卷积层的参数是否可被训练。</li>
<li><code>scope</code>：共享变量所指的<code>variable_scope</code>。</li>
</ul>
<p>&emsp;&emsp;在上述的<code>API</code>中，可以看出去两者并没有什么不同。只是<code>tf.contrib.slim.conv2d</code>提供了更多可以指定的初始化部分，而对于<code>tf.nn.conv2d</code>而言，其指定<code>filter</code>的方式相比较<code>tf.contrib.slim.conv2d</code>来说更加得复杂。去除掉少用的初始化部分，其实两者的<code>API</code>可以简化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.slim.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    num_outputs,  <span class="comment"># [卷积核个数]</span></span><br><span class="line">    kernel_size,  <span class="comment"># [卷积核的高度，卷积核的宽度]</span></span><br><span class="line">    stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>,)</span><br><span class="line">​</span><br><span class="line">tf.nn.conv2d(</span><br><span class="line">    input,  <span class="comment"># 与上述一致</span></span><br><span class="line">    filter,  <span class="comment"># [卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</span></span><br><span class="line">    strides, padding,)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之variable scope/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/TensorFlow之variable scope/" itemprop="url">TensorFlow之variable scope</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T11:10:48+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>Tensorflow</code>为了更好的管理变量，提供了<code>variable scope</code>机制，其官方解释如下：<br>&emsp;&emsp;Variable scope object to carry defaults to provide to <code>get_variable</code>.<br>&emsp;&emsp;Many of the arguments we need for <code>get_variable</code> in a variable store are most easily handled with a context. This object is used for the defaults. Attributes:</p>
<ul>
<li><code>name</code>: <code>name</code> of the current scope, used as prefix in <code>get_variable</code>.</li>
<li><code>initializer</code>: 传给<code>get_variable</code>的默认<code>initializer</code>。如果<code>get_variable</code>的时候指定了<code>initializer</code>，那么将覆盖这个默认的<code>initializer</code>。</li>
<li><code>regularizer</code>: 传给<code>get_variable</code>的默认<code>regulizer</code>。</li>
<li><code>reuse</code>: <code>Boolean</code> or <code>None</code>, setting the reuse in <code>get_variable</code>.</li>
<li><code>caching_device</code>: <code>string</code>, <code>callable</code>, or <code>None</code>: the caching device passed to <code>get_variable</code>.</li>
<li><code>partitioner</code>: <code>callable</code> or <code>None</code>: the partitioner passed to <code>get_variable</code>.</li>
<li><code>custom_getter</code>: default custom getter passed to <code>get_variable</code>.</li>
<li><code>name_scope</code>: The name passed to <code>tf.name_scope</code>.</li>
<li><code>dtype</code>: default type passed to <code>get_variable</code> (defaults to <code>DT_FLOAT</code>).</li>
</ul>
<p>&emsp;&emsp;<code>regularizer</code>参数的作用是给在本<code>variable_scope</code>下创建的<code>weights</code>加上正则项，这样我们就可以不同<code>variable_scope</code>下的参数加不同的正则项了。可以看出，用<code>variable scope</code>管理<code>get_varibale</code>是很方便的。</p>
<h3 id="确定get-variable的prefixed-name"><a href="#确定get-variable的prefixed-name" class="headerlink" title="确定get_variable的prefixed name"></a>确定get_variable的prefixed name</h3><p>&emsp;&emsp;代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"tet1"</span>):</span><br><span class="line">    var3 = tf.get_variable(<span class="string">"var3"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">    print(var3.name)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"tet2"</span>):</span><br><span class="line">    var4 = tf.get_variable(<span class="string">"var4"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">    print(var4.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tet1/var3:<span class="number">0</span></span><br><span class="line">tet2/var4:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><code>variable scope</code>是可以嵌套的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"tet1"</span>):</span><br><span class="line">    var3 = tf.get_variable(<span class="string">"var3"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">    print(var3.name)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"tet2"</span>):</span><br><span class="line">        var4 = tf.get_variable(<span class="string">"var4"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">        print(var4.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tet1/var3:<span class="number">0</span></span><br><span class="line">tet1/tet2/var4:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>get_varibale.name</code>以创建变量的<code>scope</code>作为名字的<code>prefix</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">te2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"te2"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"var2"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">        print(var2.name)</span><br><span class="line">​</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">te1</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"te1"</span>):</span><br><span class="line">                var1 = tf.get_variable(<span class="string">"var1"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">            <span class="keyword">return</span> var1</span><br><span class="line">​</span><br><span class="line">        <span class="keyword">return</span> te1()  <span class="comment"># 在scope的te2内调用的</span></span><br><span class="line">​</span><br><span class="line">res = te2()</span><br><span class="line">print(res.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te2/var2:<span class="number">0</span></span><br><span class="line">te2/te1/var1:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>对比下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">te2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"te2"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"var2"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">        print(var2.name)</span><br><span class="line">​</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">te1</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">"te1"</span>):</span><br><span class="line">                var1 = tf.get_variable(<span class="string">"var1"</span>, shape=[<span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">            <span class="keyword">return</span> var1</span><br><span class="line">    <span class="keyword">return</span> te1()  <span class="comment"># 在scope的te2外面调用的</span></span><br><span class="line">​</span><br><span class="line">res = te2()</span><br><span class="line">print(res.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te2/var2:<span class="number">0</span></span><br><span class="line">te1/var1:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;需要注意一点的是<code>tf.variable_scope(&quot;name&quot;)</code>与<code>tf.variable_scope(scope)</code>的区别，代码<code>1</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope"</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)  <span class="comment"># 执行结果“scope/w”</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"scope"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        print(var2.name)  <span class="comment"># 执行结果“scope/scope/w”</span></span><br></pre></td></tr></table></figure>
<p>代码<code>2</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope"</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)  <span class="comment"># 执行结果“scope/w”</span></span><br><span class="line">    scope = tf.get_variable_scope()</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):  <span class="comment"># 这种方式设置的scope，是用的外部的scope</span></span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])  <span class="comment"># 这个变量的name也是“scope/w”，因此会报出错误</span></span><br><span class="line">        print(var2.name)</span><br></pre></td></tr></table></figure>
<h3 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h3><p>&emsp;&emsp;共享变量的前提是变量的名字是一样的，变量的名字是由变量名和其<code>scope</code>前缀一起构成，<code>tf.get_variable_scope().reuse_variables()</code>允许共享当前<code>scope</code>下的所有变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"level1"</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)</span><br><span class="line">    scope = tf.get_variable_scope()</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"level2"</span>):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        print(var2.name)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"level1"</span>, reuse=<span class="keyword">True</span>):  <span class="comment"># 即使嵌套variable_scop，e也会被reuse</span></span><br><span class="line">    var1 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">    print(var1.name)</span><br><span class="line">    scope = tf.get_variable_scope()</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">        var2 = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">1</span>])</span><br><span class="line">        print(var2.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">level1/w:<span class="number">0</span></span><br><span class="line">level1/level2/w:<span class="number">0</span></span><br><span class="line">level1/w:<span class="number">0</span></span><br><span class="line">level1/w:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<hr>
<p>&emsp;&emsp;在<code>Tensorflow</code>中，有两个<code>scope</code>，一个是<code>name_scope</code>，另一个是<code>variable_scope</code>，这两个<code>scope</code>到底有什么区别呢？<br>&emsp;&emsp;先看第一个程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"hello"</span>) <span class="keyword">as</span> name_scope:</span><br><span class="line">    arr1 = tf.get_variable(<span class="string">"arr1"</span>, shape=[<span class="number">2</span>, <span class="number">10</span>], dtype=tf.float32)</span><br><span class="line">    print(name_scope)</span><br><span class="line">    print(arr1.name)</span><br><span class="line">    print(<span class="string">"scope_name:%s "</span> % tf.get_variable_scope().original_name_scope)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello/</span><br><span class="line">arr1:<span class="number">0</span></span><br><span class="line">scope_name:</span><br></pre></td></tr></table></figure>
<p>可以看出，<code>tf.name_scope</code>返回的是一个<code>string</code>，在<code>name_scope</code>中定义的<code>variable</code>的<code>name</code>并没有<code>hello/</code>前缀；<code>tf.get_variable_scope</code>的<code>original_name_scope</code>是空。<br>&emsp;&emsp;第二个程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"hello"</span>) <span class="keyword">as</span> variable_scope:</span><br><span class="line">    arr1 = tf.get_variable(<span class="string">"arr1"</span>, shape=[<span class="number">2</span>, <span class="number">10</span>], dtype=tf.float32)</span><br><span class="line">    print(variable_scope)</span><br><span class="line">    print(variable_scope.name)  <span class="comment"># 打印出变量空间名字</span></span><br><span class="line">    print(arr1.name)</span><br><span class="line">    print(tf.get_variable_scope().original_name_scope)  <span class="comment"># tf.get_variable_scope获取的就是variable_scope</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"xixi"</span>) <span class="keyword">as</span> v_scope2:</span><br><span class="line">        print(tf.get_variable_scope().original_name_scope)  <span class="comment"># tf.get_variable_scope获取的就是v_scope2</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;tensorflow.python.ops.variable_scope.VariableScope object at <span class="number">0x00000000086649B0</span>&gt;</span><br><span class="line">hello</span><br><span class="line">hello/arr1:<span class="number">0</span></span><br><span class="line">hello/</span><br><span class="line">hello/xixi/</span><br></pre></td></tr></table></figure>
<p>可以看出，<code>tf.variable_scope</code>返回的是一个<code>op</code>对象，<code>variable_scope</code>中定义的<code>variable</code>的<code>name</code>加上了<code>hello/</code>前缀；<code>tf.get_variable_scope</code>的<code>original_name_scope</code>是嵌套后的<code>scope name</code>。<br>&emsp;&emsp;第三个程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"name1"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"var1"</span>):</span><br><span class="line">        w = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">2</span>])</span><br><span class="line">        res = tf.add(w, [<span class="number">3</span>])</span><br><span class="line">​</span><br><span class="line">print(w.name)</span><br><span class="line">print(res.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var1/w:<span class="number">0</span></span><br><span class="line">name1/var1/Add:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看出，<code>variable scope</code>和<code>name scope</code>都会给<code>op</code>的<code>name</code>加上前缀。<br>&emsp;&emsp;对比三个个程序可以看出：</p>
<ul>
<li><code>name_scope</code>对<code>get_variable</code>创建的变量的名字不会有任何影响，而创建的<code>op</code>会被加上前缀。</li>
<li><code>tf.get_variable_scope</code>返回的只是<code>variable_scope</code>，不管<code>name_scope</code>，所以以后我们在使用<code>tf.get_variable_scope().reuse_variables()</code>时可以无视<code>name_scope</code>。</li>
</ul>
<p>&emsp;&emsp;其它情况如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"scope1"</span>) <span class="keyword">as</span> scope1:</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"scope2"</span>) <span class="keyword">as</span> scope2:</span><br><span class="line">        print(scope2)  <span class="comment"># 结果为“scope1/scope2/”</span></span><br></pre></td></tr></table></figure>
<p>另外一个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>) <span class="keyword">as</span> scope1:</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"scope2"</span>) <span class="keyword">as</span> scope2:</span><br><span class="line">        print(scope2.name)  <span class="comment"># 结果为“scope1/scope2”</span></span><br></pre></td></tr></table></figure>
<h3 id="name-scope可以用来干什么？"><a href="#name-scope可以用来干什么？" class="headerlink" title="name_scope可以用来干什么？"></a>name_scope可以用来干什么？</h3><p>&emsp;&emsp;典型的<code>TensorFlow</code>可以有数以千计的节点，如此多而难以一下全部看到，甚至无法使用标准图表工具来展示。为简单起见，我们为<code>op/tensor</code>名划定范围，并且可视化把该信息用于在图表中的节点上定义一个层级。默认情况下，只有顶层节点会显示。下面这个例子使用<code>tf.name_scope</code>在<code>hidden</code>命名域下定义了三个操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'hidden'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    a = tf.constant(<span class="number">5</span>, name=<span class="string">'alpha'</span>)</span><br><span class="line">    W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">'biases'</span>)</span><br><span class="line">    print(a.name)</span><br><span class="line">    print(W.name)</span><br><span class="line">    print(b.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hidden/alpha:<span class="number">0</span></span><br><span class="line">hidden/weights:<span class="number">0</span></span><br><span class="line">hidden/biases:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><code>name_scope</code>是给<code>op_name</code>加前缀，<code>variable_scope</code>是给<code>get_variable</code>创建的变量的名字加前缀。<br>&emsp;&emsp;<code>tf.variable_scope</code>有时也会处理命名冲突：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(name=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name, default_name=<span class="string">"scope"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        w = tf.get_variable(<span class="string">"w"</span>, shape=[<span class="number">2</span>, <span class="number">10</span>])</span><br><span class="line">​</span><br><span class="line">test()</span><br><span class="line">test()</span><br><span class="line">ws = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> ws:</span><br><span class="line">    print(w.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scope/w:<span class="number">0</span></span><br><span class="line">scope_1/w:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看出，如果只是使用<code>default_name</code>这个属性来创建<code>variable_scope</code>的时候，会处理命名冲突。<br>&emsp;&emsp;<code>tf.name_scope(None)</code>有清除<code>name scope</code>的作用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"hehe"</span>):</span><br><span class="line">    w1 = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="keyword">None</span>):</span><br><span class="line">        w2 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">​</span><br><span class="line">print(w1.name)</span><br><span class="line">print(w2.name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hehe/Variable:<span class="number">0</span></span><br><span class="line">Variable:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;简单来看：</p>
<ul>
<li>使用<code>tf.Variable</code>的时候，<code>tf.name_scope</code>和<code>tf.variable_scope</code>都会给<code>Variable</code>和<code>op</code>的<code>name</code>属性加上前缀。</li>
<li>使用<code>tf.get_variable</code>的时候，<code>tf.name_scope</code>就不会给<code>tf.get_variable</code>创建出来的<code>Variable</code>加前缀。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/Keras之Regressor回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/Keras之Regressor回归/" itemprop="url">Keras之Regressor回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T11:00:46+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;神经网络可以用来模拟回归问题(<code>regression</code>)，例如给一组数据，用一条线来对数据进行拟合，并可以预测新输入x的输出值。</p>
<h3 id="导入模块并创建数据"><a href="#导入模块并创建数据" class="headerlink" title="导入模块并创建数据"></a>导入模块并创建数据</h3><p>&emsp;&emsp;<code>models.Sequential</code>用来一层一层地去建立神经层，<code>layers.Dense</code>意思是这个神经层是全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 可视化模块</span></span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># create some data</span></span><br><span class="line">X = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>)</span><br><span class="line">np.random.shuffle(X)  <span class="comment"># randomize the data</span></span><br><span class="line">Y = <span class="number">0.5</span> * X + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, (<span class="number">200</span>,))</span><br><span class="line"><span class="comment"># plot data</span></span><br><span class="line">plt.scatter(X, Y)</span><br><span class="line">plt.show()</span><br><span class="line">​</span><br><span class="line">X_train, Y_train = X[:<span class="number">160</span>], Y[:<span class="number">160</span>]  <span class="comment"># train前160个“data points”</span></span><br><span class="line">X_test, Y_test = X[<span class="number">160</span>:], Y[<span class="number">160</span>:]  <span class="comment"># test后40个“data points”</span></span><br></pre></td></tr></table></figure>
<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>&emsp;&emsp;使用<code>Sequential</code>建立<code>model</code>，再用<code>model.add</code>添加神经层，第二行添加的是<code>Dense</code>全连接神经层。<code>Dense</code>的参数有两个，分别是输入数据和输出数据的维度，本例中的<code>x</code>和<code>y</code>是一维的。<br>&emsp;&emsp;如果需要添加下一个神经层，则不用再定义输入的维度，因为它默认就把前一层的输出作为当前层的输入。在这个例子里，只需要一层就够了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(output_dim=<span class="number">1</span>, input_dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="激活模型"><a href="#激活模型" class="headerlink" title="激活模型"></a>激活模型</h3><p>&emsp;&emsp;接下来要激活神经网络，上一步只是定义模型。在<code>compile</code>的参数中，误差函数用的是<code>mse</code>均方误差；优化器用的是<code>sgd</code>随机梯度下降法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># choose loss function and optimizing method</span></span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'sgd'</span>)</span><br></pre></td></tr></table></figure>
<p>于是就构建好了一个神经网络，它比<code>Tensorflow</code>要少了很多代码。</p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>&emsp;&emsp;训练的时候用<code>model.train_on_batch</code>一批一批地训练<code>X_train</code>、<code>Y_train</code>。默认的返回值是<code>cost</code>，每<code>100</code>步输出一下结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training</span></span><br><span class="line">print(<span class="string">'Training -----------'</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">301</span>):</span><br><span class="line">    cost = model.train_on_batch(X_train, Y_train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'train cost: '</span>, cost)</span><br></pre></td></tr></table></figure>
<h3 id="检验模型"><a href="#检验模型" class="headerlink" title="检验模型"></a>检验模型</h3><p>&emsp;&emsp;检验用到的函数是<code>model.evaluate</code>，输入测试集的<code>x</code>和<code>y</code>，输出<code>cost</code>、<code>weights</code>和<code>biases</code>，其中<code>weights</code>和<code>biases</code>是取在模型的第一层<code>model.layers[0]</code>学习到的参数。从学习到的结果可以看到，<code>weights</code>比较接近<code>0.5</code>，<code>bias</code>接近<code>2</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line">cost = model.evaluate(X_test, Y_test, batch_size=<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'test cost:'</span>, cost)</span><br><span class="line">W, b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line">print(<span class="string">'Weights='</span>, W, <span class="string">'\nbiases='</span>, b)</span><br></pre></td></tr></table></figure>
<h3 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h3><p>&emsp;&emsp;最后可以画出预测结果，与测试集的值进行对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plotting the prediction</span></span><br><span class="line">Y_pred = model.predict(X_test)</span><br><span class="line">plt.scatter(X_test, Y_test)</span><br><span class="line">plt.plot(X_test, Y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/13/深度学习/Keras之Regressor回归/1.png" height="299" width="399"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/tf.profiler.ProfileOptionBuilder/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/深度学习/tf.profiler.ProfileOptionBuilder/" itemprop="url">tf.profiler.ProfileOptionBuilder</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T00:23:29+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Class-ProfileOptionBuilder"><a href="#Class-ProfileOptionBuilder" class="headerlink" title="Class ProfileOptionBuilder"></a>Class ProfileOptionBuilder</h3><p>&emsp;&emsp;Defined in <code>tensorflow/python/profiler/option_builder.py</code>. Option builder for profiling <code>API</code>. For tutorial on the options, see <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Users can use pre-built options:</span></span><br><span class="line">opts = (tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())</span><br><span class="line"><span class="comment"># Or, build your own options:</span></span><br><span class="line">opts = (tf.profiler.ProfileOptionBuilder().with_max_depth(<span class="number">10</span>).with_min_micros(<span class="number">1000</span>) \</span><br><span class="line">        .select([<span class="string">'accelerator_micros'</span>]).with_stdout_output().build())</span><br><span class="line"><span class="comment"># Or customize the pre-built options:</span></span><br><span class="line">opts = (tf.profiler.ProfileOptionBuilder(tf.profiler.ProfileOptionBuilder.time_and_memory()) \</span><br><span class="line">        .with_displaying_options(show_name_regexes=[<span class="string">'.*rnn.*'</span>]).build())</span><br><span class="line"><span class="comment"># Finally, profiling with the options:</span></span><br><span class="line">_ = tf.profiler.profile(tf.get_default_graph(), run_meta=run_meta, cmd=<span class="string">'scope'</span>, options=opts)</span><br></pre></td></tr></table></figure>
<h3 id="init"><a href="#init" class="headerlink" title="__init__"></a>__init__</h3><p>&emsp;&emsp;Constructor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__init__(options=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>options</code>: Optional initial option dict to start with.</li>
</ul>
<h3 id="account-displayed-op-only"><a href="#account-displayed-op-only" class="headerlink" title="account_displayed_op_only"></a>account_displayed_op_only</h3><p>&emsp;&emsp;Whether only account the statistics of displayed profiler nodes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">account_displayed_op_only(is_true)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>is_true</code>: If <code>true</code>, only account statistics of nodes eventually displayed by the outputs. Otherwise, a node’s statistics are accounted by its parents as long as it’s types match <code>account_type_regexes</code>, even if it is hidden from the output, say, by <code>hide_name_regexes</code>.</li>
</ul>
<h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><p>&emsp;&emsp;Build a profiling option:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">build()</span><br></pre></td></tr></table></figure>
<p>Return a dict of profiling options.</p>
<h3 id="float-operation"><a href="#float-operation" class="headerlink" title="float_operation"></a>float_operation</h3><p>&emsp;&emsp;Options used to profile float operations:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">float_operation()</span><br></pre></td></tr></table></figure>
<p>Please see <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md</code> on the caveats of calculating <code>float</code> operations. Return a dict of profiling options.</p>
<h3 id="order-by"><a href="#order-by" class="headerlink" title="order_by"></a>order_by</h3><p>&emsp;&emsp;Order the displayed profiler nodes based on a attribute:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">order_by(attribute)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>attribute</code>: An attribute the profiler node has.</li>
</ul>
<p>Supported attribute includes <code>micros</code>, <code>bytes</code>, <code>occurrence</code>, <code>params</code>, etc. See <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md</code> for supported attributes.</p>
<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p>&emsp;&emsp;Select the attributes to display:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select(attributes)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>attributes</code>: A list of attribute the profiler node has.</li>
</ul>
<h3 id="time-and-memory"><a href="#time-and-memory" class="headerlink" title="time_and_memory"></a>time_and_memory</h3><p>&emsp;&emsp;Show operation time and memory consumptions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">time_and_memory(</span><br><span class="line">        min_micros=<span class="number">1</span>, min_bytes=<span class="number">1</span>, min_accelerator_micros=<span class="number">0</span>, min_cpu_micros=<span class="number">0</span>,</span><br><span class="line">        min_peak_bytes=<span class="number">0</span>, min_residual_bytes=<span class="number">0</span>, min_output_bytes=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_micros</code>: Only show profiler nodes with execution time no less than this. It sums accelerator and cpu times.</li>
<li><code>min_bytes</code>: Only show profiler nodes requested to allocate no less bytes than this.</li>
<li><code>min_accelerator_micros</code>: Only show profiler nodes spend no less than this time on accelerator (e.g. <code>GPU</code>).</li>
<li><code>min_cpu_micros</code>: Only show profiler nodes spend no less than this time on cpu.</li>
<li><code>min_peak_bytes</code>: Only show profiler nodes using no less than this bytes at peak (high watermark). For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>peak_bytes</code>.</li>
<li><code>min_residual_bytes</code>: Only show profiler nodes have no less than this bytes not being <code>de-allocated</code> after <code>Compute()</code> ends. For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>residual_bytes</code>.</li>
<li><code>min_output_bytes</code>: Only show profiler nodes have no less than this bytes output. The output are not necessarily allocated by this profiler nodes.</li>
</ul>
<p>Return a dict of profiling options.</p>
<h3 id="trainable-variables-parameter"><a href="#trainable-variables-parameter" class="headerlink" title="trainable_variables_parameter"></a>trainable_variables_parameter</h3><p>&emsp;&emsp;Options used to profile trainable variable parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">trainable_variables_parameter()</span><br></pre></td></tr></table></figure>
<p>Normally used together with <code>scope</code> view. Return a dict of profiling options.</p>
<h3 id="with-accounted-types"><a href="#with-accounted-types" class="headerlink" title="with_accounted_types"></a>with_accounted_types</h3><p>&emsp;&emsp;Selectively counting statistics based on node types:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_accounted_types(account_type_regexes)</span><br></pre></td></tr></table></figure>
<p>Here, <code>types</code> means the profiler nodes’ properties. Profiler by default consider device name (e.g. <code>/job:xx/.../device:GPU:0</code>) and operation type (e.g. <code>MatMul</code>) as profiler nodes’ properties. User can also associate customized <code>types</code> to profiler nodes through <code>OpLogProto</code> proto.<br>&emsp;&emsp;For example, user can select profiler nodes placed on <code>gpu:0</code> with <code>account_type_regexes=[&#39;.*gpu:0.*&#39;]</code><br>&emsp;&emsp;If none of a node’s properties match the specified regexes, the node is not displayed nor accounted.</p>
<ul>
<li><code>account_type_regexes</code>: A list of regexes specifying the types.</li>
</ul>
<h3 id="with-empty-output"><a href="#with-empty-output" class="headerlink" title="with_empty_output"></a>with_empty_output</h3><p>&emsp;&emsp;Do not generate <code>side-effect</code> outputs:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_empty_output()</span><br></pre></td></tr></table></figure>
<h3 id="with-file-output"><a href="#with-file-output" class="headerlink" title="with_file_output"></a>with_file_output</h3><p>&emsp;&emsp;Print the result to a file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_file_output(outfile)</span><br></pre></td></tr></table></figure>
<h3 id="with-max-depth"><a href="#with-max-depth" class="headerlink" title="with_max_depth"></a>with_max_depth</h3><p>&emsp;&emsp;Set the maximum depth of display:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_max_depth(max_depth)</span><br></pre></td></tr></table></figure>
<p>The depth depends on profiling view. For <code>scope</code> view, it’s the depth of name scope hierarchy (<code>tree</code>), for <code>op</code> view, it’s the number of operation types (<code>list</code>), etc.</p>
<ul>
<li><code>max_depth</code>: Maximum depth of the data structure to display.</li>
</ul>
<h3 id="with-min-execution-time"><a href="#with-min-execution-time" class="headerlink" title="with_min_execution_time"></a>with_min_execution_time</h3><p>&emsp;&emsp;Only show profiler nodes consuming no less than <code>min_micros</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_execution_time(min_micros=<span class="number">0</span>, min_accelerator_micros=<span class="number">0</span>, min_cpu_micros=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_micros</code>: Only show profiler nodes with execution time no less than this. It sums accelerator and cpu times.</li>
<li><code>min_accelerator_micros</code>: Only show profiler nodes spend no less than this time on accelerator (e.g. <code>GPU</code>).</li>
<li><code>min_cpu_micros</code>: Only show profiler nodes spend no less than this time on cpu.</li>
</ul>
<h3 id="with-min-float-operations"><a href="#with-min-float-operations" class="headerlink" title="with_min_float_operations"></a>with_min_float_operations</h3><p>&emsp;&emsp;Only show profiler nodes consuming no less than <code>min_float_ops</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_float_operations(min_float_ops)</span><br></pre></td></tr></table></figure>
<p>Please see <code>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md</code> on the caveats of calculating float operations.</p>
<ul>
<li><code>min_float_ops</code>: Only show profiler nodes with float operations no less than this.</li>
</ul>
<h3 id="with-min-memory"><a href="#with-min-memory" class="headerlink" title="with_min_memory"></a>with_min_memory</h3><p>&emsp;&emsp;Only show profiler nodes consuming no less than <code>min_bytes</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_memory(min_bytes=<span class="number">0</span>, min_peak_bytes=<span class="number">0</span>, min_residual_bytes=<span class="number">0</span>, min_output_bytes=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_bytes</code>: Only show profiler nodes requested to allocate no less bytes than this.</li>
<li><code>min_peak_bytes</code>: Only show profiler nodes using no less than this bytes at peak (high watermark). For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>peak_bytes</code>.</li>
<li><code>min_residual_bytes</code>: Only show profiler nodes have no less than this bytes not being <code>de-allocated</code> after <code>Compute()</code> ends. For profiler nodes consist of multiple graph nodes, it sums the graph nodes’ <code>residual_bytes</code>.</li>
<li><code>min_output_bytes</code>: Only show profiler nodes have no less than this bytes output. The output are not necessarily allocated by this profiler nodes.</li>
</ul>
<h3 id="with-min-occurrence"><a href="#with-min-occurrence" class="headerlink" title="with_min_occurrence"></a>with_min_occurrence</h3><p>&emsp;&emsp;Only show profiler nodes including no less than <code>min_occurrence</code> graph nodes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_occurrence(min_occurrence)</span><br></pre></td></tr></table></figure>
<p>A <code>node</code> means a profiler output node, which can be a python line (code view), an operation type (op view), or a graph node (<code>graph/scope</code> view). A python line includes all graph nodes created by that line, while an operation type includes all graph nodes of that type.</p>
<ul>
<li><code>min_occurrence</code>: Only show nodes including no less than this.</li>
</ul>
<h3 id="with-min-parameters"><a href="#with-min-parameters" class="headerlink" title="with_min_parameters"></a>with_min_parameters</h3><p>&emsp;&emsp;Only show profiler nodes holding no less than <code>min_params</code> parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_min_parameters(min_params)</span><br></pre></td></tr></table></figure>
<p><code>Parameters</code> normally refers the weights of in <code>TensorFlow</code> variables. It reflects the <code>capacity</code> of models.</p>
<ul>
<li><code>min_params</code>: Only show profiler nodes holding number parameters no less than this.</li>
</ul>
<h3 id="with-node-names"><a href="#with-node-names" class="headerlink" title="with_node_names"></a>with_node_names</h3><p>&emsp;&emsp;Regular expressions used to select profiler nodes to display:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_node_names(start_name_regexes=<span class="keyword">None</span>, show_name_regexes=<span class="keyword">None</span>, hide_name_regexes=<span class="keyword">None</span>, trim_name_regexes=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>After <code>with_accounted_types</code> is evaluated, <code>with_node_names</code> are evaluated as follows:<br>&emsp;&emsp;For a profile data structure, profiler first finds the profiler nodes matching <code>start_name_regexes</code>, and starts displaying profiler nodes from there. Then, if a node matches <code>show_name_regexes</code> and doesn’t match <code>hide_name_regexes</code>, it’s displayed. If a node matches <code>trim_name_regexes</code>, profiler stops further searching that branch.</p>
<ul>
<li><code>start_name_regexes</code>: list of node name regexes to start displaying.</li>
<li><code>show_name_regexes</code>: list of node names regexes to display.</li>
<li><code>hide_name_regexes</code>: list of node_names regexes that should be hidden.</li>
<li><code>trim_name_regexes</code>: list of node name regexes from where to stop.</li>
</ul>
<h3 id="with-pprof-output"><a href="#with-pprof-output" class="headerlink" title="with_pprof_output"></a>with_pprof_output</h3><p>&emsp;&emsp;Generate a pprof profile <code>gzip</code> file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_pprof_output(pprof_file)</span><br></pre></td></tr></table></figure>
<p>To use the pprof file: <code>pprof -png --nodecount=100 --sample_index=1</code>.</p>
<ul>
<li><code>pprof_file</code>: filename for output, usually suffixed with <code>.pb.gz</code>.</li>
</ul>
<h3 id="with-stdout-output"><a href="#with-stdout-output" class="headerlink" title="with_stdout_output"></a>with_stdout_output</h3><p>&emsp;&emsp;Print the result to <code>stdout</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_stdout_output()</span><br></pre></td></tr></table></figure>
<h3 id="with-step"><a href="#with-step" class="headerlink" title="with_step"></a>with_step</h3><p>&emsp;&emsp;Which profile step to use for profiling:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_step(step)</span><br></pre></td></tr></table></figure>
<p>The <code>step</code> here refers to the step defined by <code>Profiler.add_step() API</code>.</p>
<ul>
<li><code>step</code>: When multiple steps of profiles are available, select which step’s profile to use. If <code>-1</code>, use average of all available steps.</li>
</ul>
<h3 id="with-timeline-output"><a href="#with-timeline-output" class="headerlink" title="with_timeline_output"></a>with_timeline_output</h3><p>&emsp;&emsp;Generate a timeline json file：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with_timeline_output(timeline_file)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="How-to-calculate-a-net’s-FLOPs-in-CNN"><a href="#How-to-calculate-a-net’s-FLOPs-in-CNN" class="headerlink" title="How to calculate a net’s FLOPs in CNN"></a>How to calculate a net’s FLOPs in CNN</h3><p>&emsp;&emsp;Problem: I want to design a convolutional neural network which occupy <code>GPU</code> resource no more than <code>Alexnet</code>. I want to use <code>FLOPs</code> to measure it but I don’t know how to calculate it. Is there any tools to do it, please?<br>&emsp;&emsp;Answer: For future visitors, if you use <code>Keras</code> and <code>TensorFlow</code> as Backend, then you can try the following example. It calculates the <code>FLOPs</code> for the <code>MobileNet</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.applications.mobilenet <span class="keyword">import</span> MobileNet</span><br><span class="line">​</span><br><span class="line">run_meta = tf.RunMetadata()</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=tf.Graph()) <span class="keyword">as</span> sess:</span><br><span class="line">    K.set_session(sess)</span><br><span class="line">    net = MobileNet(alpha=<span class="number">.75</span>, input_tensor=tf.placeholder(<span class="string">'float32'</span>, shape=(<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">    opts = tf.profiler.ProfileOptionBuilder.float_operation()</span><br><span class="line">    flops = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd=<span class="string">'op'</span>, options=opts)</span><br><span class="line">    opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()</span><br><span class="line">    params = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd=<span class="string">'op'</span>, options=opts)</span><br><span class="line">    print(<span class="string">"&#123;:,&#125; --- &#123;:,&#125;"</span>.format(flops.total_float_ops, params.total_parameters))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/12/深度学习/TensorFlow优化器/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/12/深度学习/TensorFlow优化器/" itemprop="url">TensorFlow优化器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-12T17:59:27+08:00">
                2019-02-12
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;深度学习常见的是对于梯度的优化，优化器最后其实就是各种对于梯度下降算法的优化。<code>TensorFLow</code>提供了很多优化器的支持。</p>
<h3 id="tf-train-GradientDescentOptimizer"><a href="#tf-train-GradientDescentOptimizer" class="headerlink" title="tf.train.GradientDescentOptimizer"></a>tf.train.GradientDescentOptimizer</h3><p>&emsp;&emsp;这个类是实现梯度下降算法的优化器。</p>
<h3 id="tf-train-AdadeltaOptimizer"><a href="#tf-train-AdadeltaOptimizer" class="headerlink" title="tf.train.AdadeltaOptimizer"></a>tf.train.AdadeltaOptimizer</h3><p>&emsp;&emsp;实现了<code>Adadelta</code>算法的优化器，该算法不需要手动调优学习速率，抗噪声能力强，可以选择不同的模型结构。<code>Adadelta</code>是对<code>Adagrad</code>的扩展。<code>Adadelta</code>只累加固定大小的项，并且也不直接存储这些项，仅仅是计算对应的平均值。</p>
<h3 id="tf-train-AdagradOptimizer"><a href="#tf-train-AdagradOptimizer" class="headerlink" title="tf.train.AdagradOptimizer"></a>tf.train.AdagradOptimizer</h3><p>&emsp;&emsp;实现了<code>AdagradOptimizer</code>算法的优化器，<code>Adagrad</code>会累加之前所有的梯度平方。它用于处理大的稀疏矩阵，<code>Adagrad</code>可以自动变更学习速率，只是需要设定一个全局的学习速率，但这并非是实际学习速率，实际的速率是与以往参数的模之和的开方成反比的。这样使得每个参数都有一个自己的学习率。</p>
<h3 id="tf-train-MomentumOptimizer"><a href="#tf-train-MomentumOptimizer" class="headerlink" title="tf.train.MomentumOptimizer"></a>tf.train.MomentumOptimizer</h3><p>&emsp;&emsp;实现了<code>MomentumOptimizer</code>算法的优化器，如果梯度长时间保持一个方向，则增大参数更新幅度；反之，如果频繁发生符号翻转，则说明这是要减小参数更新幅度。可以把这一过程理解成从山顶放下一个球，会滑的越来越快。</p>
<h3 id="tf-train-RMSPropOptimizer"><a href="#tf-train-RMSPropOptimizer" class="headerlink" title="tf.train.RMSPropOptimizer"></a>tf.train.RMSPropOptimizer</h3><p>&emsp;&emsp;实现了<code>RMSPropOptimizer</code>算法的优化器，它与<code>Adam</code>类似，只是使用了不同的滑动均值。</p>
<h3 id="tf-train-AdamOptimizer"><a href="#tf-train-AdamOptimizer" class="headerlink" title="tf.train.AdamOptimizer"></a>tf.train.AdamOptimizer</h3><p>&emsp;&emsp;实现了<code>AdamOptimizer</code>算法的优化器，它综合了<code>Momentum</code>和<code>RMSProp</code>方法，对每个参数保留一个学习率与一个根据过去梯度信息求得的指数衰减均值。</p>
<h3 id="如何选用optimizer"><a href="#如何选用optimizer" class="headerlink" title="如何选用optimizer"></a>如何选用optimizer</h3><p>&emsp;&emsp;对于稀疏数据，使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值。<code>SGD</code>通常训练时间更长，容易陷入鞍点，但是在好的初始化和学习率调度方案的情况下，结果更可靠。<br>&emsp;&emsp;如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。<code>Adadelta</code>、<code>RMSprop</code>和<code>Adam</code>是比较相近的算法，在相似的情况下表现差不多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line">train_X = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">train_Y = <span class="number">2</span> * train_X + np.random.randn(*train_X.shape) * <span class="number">0.33</span> + <span class="number">10</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the model</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">w = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"weight"</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"bias"</span>)</span><br><span class="line">loss = tf.square(Y - X * w - b)</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    epoch = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_X, train_Y):</span><br><span class="line">            _, w_value, b_value = sess.run([train_op, w, b], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">        print(<span class="string">"Epoch: &#123;&#125;, w: &#123;&#125;, b: &#123;&#125;"</span>.format(epoch, w_value, b_value))</span><br><span class="line">        epoch += <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">plt.plot(train_X, train_Y, <span class="string">"+"</span>)</span><br><span class="line">plt.plot(train_X, train_X.dot(w_value) + b_value)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: <span class="number">1</span>, w: <span class="number">-0.9016109108924866</span>, b: <span class="number">9.692331314086914</span></span><br><span class="line">Epoch: <span class="number">2</span>, w: <span class="number">0.23304520547389984</span>, b: <span class="number">10.46933364868164</span></span><br><span class="line">Epoch: <span class="number">3</span>, w: <span class="number">1.0166758298873901</span>, b: <span class="number">10.325897216796875</span></span><br><span class="line">Epoch: <span class="number">4</span>, w: <span class="number">1.4419575929641724</span>, b: <span class="number">10.183098793029785</span></span><br><span class="line">Epoch: <span class="number">5</span>, w: <span class="number">1.6616896390914917</span>, b: <span class="number">10.101426124572754</span></span><br><span class="line">Epoch: <span class="number">6</span>, w: <span class="number">1.7738741636276245</span>, b: <span class="number">10.058719635009766</span></span><br><span class="line">Epoch: <span class="number">7</span>, w: <span class="number">1.8309777975082397</span>, b: <span class="number">10.036850929260254</span></span><br><span class="line">Epoch: <span class="number">8</span>, w: <span class="number">1.860023021697998</span>, b: <span class="number">10.025708198547363</span></span><br><span class="line">Epoch: <span class="number">9</span>, w: <span class="number">1.8747929334640503</span>, b: <span class="number">10.020042419433594</span></span><br><span class="line">Epoch: <span class="number">10</span>, w: <span class="number">1.8823031187057495</span>, b: <span class="number">10.017163276672363</span></span><br></pre></td></tr></table></figure>
<p><img src="/2019/02/12/深度学习/TensorFlow优化器/1.png" height="263" width="352"></p>
<hr>
<p>&emsp;&emsp;这里使用<code>tensorflow</code>的优化器解决最优化问题。定义目标函数<code>loss = (x - 3)^2</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.Variable(tf.truncated_normal([<span class="number">1</span>]), name=<span class="string">"x"</span>)</span><br><span class="line">goal = tf.pow(x - <span class="number">3</span>, <span class="number">2</span>, name=<span class="string">"goal"</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x.initializer.run()</span><br><span class="line">    print(x.eval())</span><br><span class="line">    print(goal.eval())</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0.20199603</span>]</span><br><span class="line">[<span class="number">7.828826</span>]</span><br></pre></td></tr></table></figure>
<p>求<code>goal</code>最小时的<code>x</code>值，使用梯度下降优化器可以解决问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.Variable(tf.truncated_normal([<span class="number">1</span>]), name=<span class="string">"x"</span>)</span><br><span class="line">goal = tf.pow(x - <span class="number">3</span>, <span class="number">2</span>, name=<span class="string">"goal"</span>)</span><br><span class="line">​</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">train_step = optimizer.minimize(goal)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        x.initializer.run()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            print(<span class="string">"x:"</span>, x.eval())</span><br><span class="line">            train_step.run()</span><br><span class="line">            print(<span class="string">"goal:"</span>, goal.eval())</span><br><span class="line">​</span><br><span class="line">train()</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">-0.83802974</span>]</span><br><span class="line">goal: [<span class="number">9.427503</span>]</span><br><span class="line">x: [<span class="number">-0.07042378</span>]</span><br><span class="line">goal: [<span class="number">6.033601</span>]</span><br><span class="line">x: [<span class="number">0.543661</span>]</span><br><span class="line">goal: [<span class="number">3.8615048</span>]</span><br><span class="line">x: [<span class="number">1.0349288</span>]</span><br><span class="line">goal: [<span class="number">2.4713633</span>]</span><br><span class="line">x: [<span class="number">1.427943</span>]</span><br><span class="line">goal: [<span class="number">1.5816724</span>]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>minimize</code>实际上是<code>compute_gradients</code>与<code>apply_gradients</code>的和，即拆分成计算梯度和应用梯度两个步骤：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.Variable(tf.truncated_normal([<span class="number">1</span>]), name=<span class="string">"x"</span>)</span><br><span class="line">goal = tf.pow(x - <span class="number">3</span>, <span class="number">2</span>, name=<span class="string">"goal"</span>)</span><br><span class="line">​</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># compute_gradients返回的是“A list of (gradient, variable) pairs”</span></span><br><span class="line">gra_and_var = optimizer.compute_gradients(goal)</span><br><span class="line">train_step = optimizer.apply_gradients(gra_and_var)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        x.initializer.run()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            print(<span class="string">"x: "</span>, x.eval())</span><br><span class="line">            train_step.run()</span><br><span class="line">            print(<span class="string">"goal:"</span>, goal.eval())</span><br><span class="line">​</span><br><span class="line">train()</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x: [<span class="number">-0.10070122</span>]</span><br><span class="line">goal: [<span class="number">6.153183</span>]</span><br><span class="line">x: [<span class="number">0.51943904</span>]</span><br><span class="line">goal: [<span class="number">3.938037</span>]</span><br><span class="line">x: [<span class="number">1.0155512</span>]</span><br><span class="line">goal: [<span class="number">2.5203435</span>]</span><br><span class="line">x: [<span class="number">1.412441</span>]</span><br><span class="line">goal: [<span class="number">1.6130198</span>]</span><br><span class="line">x: [<span class="number">1.7299528</span>]</span><br><span class="line">goal: [<span class="number">1.0323327</span>]</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/12/深度学习/tensorpack函数总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/12/深度学习/tensorpack函数总结/" itemprop="url">tensorpack函数总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-12T15:46:41+08:00">
                2019-02-12
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="dataflow-TestDataSpeed"><a href="#dataflow-TestDataSpeed" class="headerlink" title="dataflow.TestDataSpeed"></a>dataflow.TestDataSpeed</h3><p>&emsp;&emsp;class <code>dataflow.TestDataSpeed(ds, size=5000, warmup=0)</code>: Test the speed of some <code>DataFlow</code>.<br>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>ds</code> (<code>DataFlow</code>): the <code>DataFlow</code> to test.</li>
<li><code>size</code> (<code>int</code>): number of datapoints to fetch.</li>
<li><code>warmup</code> (<code>int</code>): warmup iterations</li>
</ul>
<p>&emsp;&emsp;Function:</p>
<ul>
<li><code>get_data()</code>: Will run testing at the beginning, then produce data normally.</li>
<li><code>start()</code>: Alias of <code>start_test</code>.</li>
<li><code>start_test()</code>: Start testing with a progress bar.</li>
</ul>
<h3 id="dataflow-MultiThreadMapData"><a href="#dataflow-MultiThreadMapData" class="headerlink" title="dataflow.MultiThreadMapData"></a>dataflow.MultiThreadMapData</h3><p>&emsp;&emsp;class <code>dataflow.MultiThreadMapData(ds, nr_thread, map_func, buffer_size=200, strict=False)</code>: Same as <code>MapData</code>, but start threads to run the mapping function. This is useful when the mapping function is the bottleneck, but you don’t want to start processes for the entire dataflow pipeline.<br>&emsp;&emsp;<strong>Note</strong>: There is tiny communication overhead with threads, but you should avoid starting many threads in your main process to reduce GIL contention.<br>&emsp;&emsp;The threads will only start in the process which calls <code>reset_state()</code>. Therefore you can use <code>PrefetchDataZMQ(MultiThreadMapData(...), 1)</code> to reduce <code>GIL</code> contention.<br>&emsp;&emsp;Threads run in parallel and can take different time to run the mapping function. Therefore the order of datapoints won’t be preserved, and datapoints from one pass of <code>df.get_data()</code> might get mixed with datapoints from the next pass.<br>&emsp;&emsp;You can use strict mode, where <code>MultiThreadMapData.get_data()</code> is guaranteed to produce the exact set which <code>df.get_data()</code> produces. Although the order of data still isn’t preserved.</p>
<p>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>ds</code> (<code>DataFlow</code>): the dataflow to map.</li>
<li><code>nr_thread</code> (<code>int</code>): number of threads to use.</li>
<li><code>map_func</code> (<code>callable</code>): <code>datapoint -&gt; datapoint | None</code>.</li>
<li><code>buffer_size</code> (<code>int</code>): number of datapoints in the buffer.</li>
<li><code>strict</code> (<code>bool</code>): use <code>strict mode</code>.</li>
</ul>
<h3 id="dataflow-MapDataComponent"><a href="#dataflow-MapDataComponent" class="headerlink" title="dataflow.MapDataComponent"></a>dataflow.MapDataComponent</h3><p>&emsp;&emsp;class <code>dataflow.MapDataComponent(ds, func, index=0)</code>: Apply a <code>mapper/filter</code> on a datapoint component.<br>&emsp;&emsp;<strong>Note</strong>: This dataflow itself doesn’t modify the datapoints. But please make sure func doesn’t modify the components unless you’re certain it’s safe.<br>&emsp;&emsp;If you discard some datapoints, <code>ds.size()</code> will be incorrect.</p>
<p>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>ds</code> (<code>DataFlow</code>): input DataFlow.</li>
<li><code>func</code> (<code>TYPE -&gt; TYPE | None</code>): takes <code>dp[index]</code>, returns a new value for <code>dp[index]</code>. return <code>None</code> to discard this datapoint.</li>
<li><code>index</code> (<code>int</code>): index of the component.</li>
</ul>
<h3 id="dataflow-BatchData"><a href="#dataflow-BatchData" class="headerlink" title="dataflow.BatchData"></a>dataflow.BatchData</h3><p>&emsp;&emsp;class <code>dataflow.BatchData(ds, batch_size, remainder=False, use_list=False)</code>: Stack datapoints into batches. It produces datapoints of the same number of components as <code>ds</code>, but each component has one new extra dimension of size <code>batch_size</code>. The batch can be either a list of original components, or (by default) a numpy array of original components.</p>
<p>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>ds</code> (<code>DataFlow</code>): When <code>use_list=False</code>, the components of <code>ds</code> must be either scalars or <code>np.ndarray</code>, and have to be consistent in shapes.</li>
<li><code>batch_size</code> (<code>int</code>): batch size.</li>
<li><code>remainder</code> (<code>bool</code>): When the remaining datapoints in <code>ds</code> is not enough to form a batch, whether or not to also produce the remaining data as a smaller batch. If set to <code>False</code>, all produced datapoints are guaranteed to have the same batch size. If set to <code>True</code>, <code>ds.size()</code> must be accurate.</li>
<li><code>use_list</code> (<code>bool</code>): if <code>True</code>, each component will contain a list of datapoints instead of an numpy array of an extra dimension.</li>
</ul>
<p>&emsp;&emsp;Function:</p>
<ul>
<li><code>get_data()</code>: Yields: Batched data by stacking each component on an extra 0th dimension.</li>
</ul>
<h3 id="dataflow-MapData"><a href="#dataflow-MapData" class="headerlink" title="dataflow.MapData"></a>dataflow.MapData</h3><p>&emsp;&emsp;class <code>dataflow.MapData(ds, func)</code>: Apply a <code>mapper/filter</code> on the <code>DataFlow</code>.<br>&emsp;&emsp;<strong>Note</strong>: Please make sure <code>func</code> doesn’t modify the components unless you’re certain it’s safe.<br>&emsp;&emsp;If you discard some datapoints, <code>ds.size()</code> will be incorrect.<br>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>ds</code> (<code>DataFlow</code>): input <code>DataFlow</code>.</li>
<li><code>func</code> (<code>datapoint -&gt; datapoint | None</code>): takes a datapoint and returns a new datapoint. Return <code>None</code> to discard this datapoint.</li>
</ul>
<h3 id="dataflow-PrefetchData"><a href="#dataflow-PrefetchData" class="headerlink" title="dataflow.PrefetchData"></a>dataflow.PrefetchData</h3><p>&emsp;&emsp;alias of <code>tensorpack.dataflow.parallel.MultiProcessPrefetchData</code>.</p>
<h3 id="dataflow-MultiProcessPrefetchData"><a href="#dataflow-MultiProcessPrefetchData" class="headerlink" title="dataflow.MultiProcessPrefetchData"></a>dataflow.MultiProcessPrefetchData</h3><p>&emsp;&emsp;class <code>dataflow.MultiProcessPrefetchData(ds, nr_prefetch, nr_proc)</code>: Prefetch data from a <code>DataFlow</code> using <code>Python</code> multiprocessing utilities. It will fork the process calling <code>__init__()</code>, collect datapoints from <code>ds</code> in each process by a <code>Python multiprocessing.Queue</code>.<br>&emsp;&emsp;<strong>Note</strong>: An iterator cannot run faster automatically, what’s happening is that the underlying dataflow will be forked <code>nr_proc</code> times. As a result, we have the following guarantee on the dataflow correctness:<br>&emsp;&emsp;When <code>nr_proc = 1</code>, the dataflow produces the same data as <code>ds</code> in the same order.<br>&emsp;&emsp;When <code>nr_proc &gt; 1</code>, the dataflow produces the same distribution of data as <code>ds</code> if each sample from <code>ds</code> is <code>i.i.d.</code> (e.g. fully shuffled). You probably only want to use it for training.<br>&emsp;&emsp;This has more serialization overhead than <code>PrefetchDataZMQ</code> when data is large.<br>&emsp;&emsp;You can nest like this: <code>PrefetchDataZMQ(PrefetchData(df, nr_proc = a), nr_proc = b)</code>. A total of a instances of df worker processes will be created.<br>&emsp;&emsp;fork happens in <code>__init__</code>. <code>reset_state()</code> is a <code>no-op</code>. The worker processes won’t get called.<br>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>ds</code> (<code>DataFlow</code>): input <code>DataFlow</code>.</li>
<li><code>nr_prefetch</code> (<code>int</code>): size of the queue to hold prefetched datapoints.</li>
<li><code>nr_proc</code> (<code>int</code>): number of processes to use.</li>
</ul>
<h3 id="dataflow-RNGDataFlow"><a href="#dataflow-RNGDataFlow" class="headerlink" title="dataflow.RNGDataFlow"></a>dataflow.RNGDataFlow</h3><p>&emsp;&emsp;class <code>dataflow.RNGDataFlow</code>: A <code>DataFlow</code> with <code>RNG</code>.<br>&emsp;&emsp;Function:</p>
<ul>
<li><code>reset_state()</code>: Reset the <code>RNG</code>.</li>
</ul>
<h3 id="dataflow-DataFlowTerminated"><a href="#dataflow-DataFlowTerminated" class="headerlink" title="dataflow.DataFlowTerminated"></a>dataflow.DataFlowTerminated</h3><p>&emsp;&emsp;exception <code>dataflow.DataFlowTerminated</code>: An exception indicating that the <code>DataFlow</code> is unable to produce any more data, i.e. something wrong happened so that <code>calling get_data()</code> cannot give a valid iterator any more. In most <code>DataFlow</code> this will never be raised.</p>
<h3 id="dataflow-RemoteDataZMQ"><a href="#dataflow-RemoteDataZMQ" class="headerlink" title="dataflow.RemoteDataZMQ"></a>dataflow.RemoteDataZMQ</h3><p>&emsp;&emsp;class <code>dataflow.RemoteDataZMQ(addr1, addr2=None, hwm=50, bind=True)</code>: Produce data from <code>ZMQ PULL socket(s)</code>. It is the <code>receiver-side</code> counterpart of <code>send_dataflow_zmq()</code>, which uses <code>tensorpack.utils.serialize</code> for serialization. See <code>http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html#distributed-dataflow</code>.<br>&emsp;&emsp;Parameters:</p>
<ul>
<li><code>addr1</code>, <code>addr2</code> (<code>str</code>): addr of the <code>zmq</code> endpoint to connect to. Use both if you need two protocols (e.g. both <code>IPC</code> and <code>TCP</code>). I don’t think you’ll ever need <code>3</code>.</li>
<li><code>hwm</code> (<code>int</code>): <code>ZMQ</code> <code>high-water</code> mark (buffer size).</li>
<li><code>bind</code> (<code>bool</code>): whether to connect or bind the endpoint.</li>
</ul>
<p>&emsp;&emsp;Function:</p>
<ul>
<li><code>bind_or_connect(socket, addr)</code></li>
</ul>
<h3 id="dataflow-imgaug-RotationAndCropValid"><a href="#dataflow-imgaug-RotationAndCropValid" class="headerlink" title="dataflow.imgaug.RotationAndCropValid"></a>dataflow.imgaug.RotationAndCropValid</h3><p>&emsp;&emsp;class <code>dataflow.imgaug.RotationAndCropValid(max_deg, interp=cv2.INTER_LINEAR, step_deg=None)</code>: Random rotate and then crop the largest possible rectangle. Note that this will produce images of different shapes.</p>
<p>&emsp;&emsp;Function:</p>
<ul>
<li><code>largest_rotated_rect(w, h, angle)</code>: Get largest rectangle after rotation.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/12/机器学习/Logistic回归sklearn/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/12/机器学习/Logistic回归sklearn/" itemprop="url">Logistic回归sklearn</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-12T14:04:32+08:00">
                2019-02-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>sklearn.linear_model</code>模块提供了很多模型供我们使用，比如<code>Logistic</code>回归、<code>Lasso</code>回归、贝叶斯脊回归等：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>linear_model.ARDRegression([n_iter, tol, ...])</code></td>
<td><code>Bayesian ARD regression</code></td>
</tr>
<tr>
<td><code>linear_model.BayesianRidge([n_iter, tol, ...])</code></td>
<td><code>Bayesian ridge regression</code></td>
</tr>
<tr>
<td><code>linear_model.ElasticNet([alpha, l1_ratio, ...])</code></td>
<td><code>Linear regression</code> with combined <code>L1</code> and <code>L2</code> priors as regularizer</td>
</tr>
<tr>
<td><code>linear_model.ElasticNetCV([l1_ratio, eps, ...])</code></td>
<td><code>Elastic Net model</code> with iterative fitting along a regularization path</td>
</tr>
<tr>
<td><code>linear_model.HuberRegressor([epsilon, ...])</code></td>
<td><code>Linear regression</code> model that is robust to outliers</td>
</tr>
<tr>
<td><code>linear_model.Lars([fit_intercept, verbose, ...])</code></td>
<td><code>Least Angle Regression</code> model <code>a.k.a</code></td>
</tr>
<tr>
<td><code>linear_model.LarsCV([fit_intercept, ...])</code></td>
<td><code>Cross-validated Least Angle Regression</code> model</td>
</tr>
<tr>
<td><code>linear_model.Lasso([alpha, fit_intercept, ...])</code></td>
<td><code>Linear Model</code> trained with <code>L1</code> prior as regularizer</td>
</tr>
<tr>
<td><code>linear_model.LassoCV([eps, n_alphas, ...])</code></td>
<td><code>Lasso</code> linear model with iterative fitting along a regularization path</td>
</tr>
<tr>
<td><code>linear_model.LassoLars([alpha, ...])</code></td>
<td><code>Lasso</code> model fit with <code>Least Angle Regression</code></td>
</tr>
<tr>
<td><code>linear_model.LassoLarsCV([fit_intercept, ...])</code></td>
<td><code>Cross-validated Lasso</code>, using the <code>LARS</code> algorithm</td>
</tr>
<tr>
<td><code>linear_model.LassoLarsIC([criterion, ...])</code></td>
<td><code>Lasso</code> model fit with <code>Lars</code> using <code>BIC</code> or <code>AIC</code> for model selection</td>
</tr>
<tr>
<td><code>linear_model.LinearRegression([...])</code></td>
<td>Ordinary least squares <code>Linear Regression</code></td>
</tr>
<tr>
<td><code>linear_model.LogisticRegression([penalty, ...])</code></td>
<td><code>Logistic Regression</code> (<code>aka logit</code>, <code>MaxEnt</code>) classifier</td>
</tr>
<tr>
<td><code>linear_model.LogisticRegressionCV([Cs, ...])</code></td>
<td><code>Logistic Regression CV</code> (<code>aka logit</code>, <code>MaxEnt</code>) classifier</td>
</tr>
<tr>
<td><code>linear_model.MultiTaskLasso([alpha, ...])</code></td>
<td><code>Multi-task Lasso</code> model trained with <code>L1/L2</code> <code>mixed-norm</code> as regularizer</td>
</tr>
<tr>
<td><code>linear_model.MultiTaskElasticNet([alpha, ...])</code></td>
<td><code>Multi-task ElasticNet</code> model trained with <code>L1/L2</code> <code>mixed-norm</code> as regularizer</td>
</tr>
<tr>
<td><code>linear_model.MultiTaskLassoCV([eps, ...])</code></td>
<td><code>Multi-task L1/L2 Lasso</code> with <code>built-in</code> <code>cross-validation</code></td>
</tr>
<tr>
<td><code>linear_model.MultiTaskElasticNetCV([...])</code></td>
<td><code>Multi-task L1/L2 ElasticNet</code> with <code>built-in</code> <code>cross-validation</code></td>
</tr>
<tr>
<td><code>linear_model.OrthogonalMatchingPursuit([...])</code></td>
<td><code>Orthogonal Matching Pursuit</code> model</td>
</tr>
<tr>
<td><code>linear_model.OrthogonalMatchingPursuitCV([...])</code></td>
<td><code>Cross-validated</code> <code>Orthogonal Matching Pursuit</code> model</td>
</tr>
<tr>
<td><code>linear_model.PassiveAggressiveClassifier([...])</code></td>
<td><code>Passive Aggressive Classifier</code></td>
</tr>
<tr>
<td><code>linear_model.PassiveAggressiveRegressor([C, ...])</code></td>
<td><code>Passive Aggressive Regressor</code></td>
</tr>
<tr>
<td><code>linear_model.RANSACRegressor([...])</code></td>
<td><code>RANSAC</code> (<code>RANdom SAmple Consensus</code>) algorithm</td>
</tr>
<tr>
<td><code>linear_model.Ridge([alpha, fit_intercept, ...])</code></td>
<td><code>Linear</code> least squares with <code>l2</code> regularization</td>
</tr>
<tr>
<td><code>linear_model.RidgeClassifier([alpha, ...])</code></td>
<td>Classifier using <code>Ridge</code> regression</td>
</tr>
<tr>
<td><code>linear_model.RidgeClassifierCV([alphas, ...])</code></td>
<td>Ridge classifier with <code>built-in</code> <code>cross-validation</code></td>
</tr>
<tr>
<td><code>linear_model.RidgeCV([alphas, ...])</code></td>
<td>Ridge regression with <code>built-in</code> <code>cross-validation</code></td>
</tr>
<tr>
<td><code>linear_model.SGDClassifier([loss, penalty, ...])</code></td>
<td><code>Linear</code> classifiers (<code>SVM</code>, <code>logistic regression</code>) with <code>SGD</code> training</td>
</tr>
<tr>
<td><code>linear_model.SGDRegressor([loss, penalty, ...])</code></td>
<td><code>Linear</code> model fitted by minimizing a regularized empirical loss with <code>SGD</code></td>
</tr>
<tr>
<td><code>linear_model.TheilSenRegressor([...])</code></td>
<td><code>Theil-Sen Estimator</code>: robust multivariate regression model</td>
</tr>
<tr>
<td><code>linear_model.enet_path(X, y[, l1_ratio, ...])</code></td>
<td>Compute elastic net path with coordinate descent</td>
</tr>
<tr>
<td><code>linear_model.lars_path(X, y[, Xy, Gram, ...])</code></td>
<td>Compute <code>Least Angle Regression</code> or <code>Lasso</code> path using <code>LARS</code> algorithm</td>
</tr>
<tr>
<td><code>linear_model.lasso_path(X, y[, eps, ...])</code></td>
<td>Compute <code>Lasso</code> path with coordinate descent</td>
</tr>
<tr>
<td><code>linear_model.logistic_regression_path(X, y)</code></td>
<td>Compute a <code>Logistic Regression</code> model for a list of regularization parameters</td>
</tr>
<tr>
<td><code>linear_model.orthogonal_mp(X, y[, ...])</code></td>
<td><code>Orthogonal Matching Pursuit</code></td>
</tr>
<tr>
<td><code>linear_model.orthogonal_mp_gram(Gram, Xy[, ...])</code></td>
<td><code>Gram Orthogonal Matching Pursuit</code></td>
</tr>
</tbody>
</table>
</div>
<p>先看一下<code>LogisticRegression</code>这个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">linear_model</span>.<span class="title">LogisticRegression</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    penalty=<span class="string">"l2"</span>, dual=False, tol=<span class="number">0.0001</span>, C=<span class="number">1.0</span>, fit_intercept=True, intercept_scaling=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    class_weight=None, random_state=None, solver=<span class="string">"liblinear"</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">"ovr"</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    verbose=<span class="number">0</span>, warm_start=False, n_jobs=<span class="number">1</span>)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>penalty</code>：惩罚项，<code>str</code>类型，可选参数为<code>l1</code>和<code>l2</code>，用于指定惩罚项中使用的规范。<code>newton-cg</code>、<code>sag</code>和<code>lbfgs</code>求解算法只支持<code>L2</code>规范。<code>L1</code>规范假设的是模型的参数满足拉普拉斯分布，<code>L2</code>假设的模型参数满足高斯分布。所谓的规范就是加上对参数的约束，使得模型更不会过拟合(<code>overfit</code>)，但是如果要说是不是加了约束就会好，这个没有人能回答，只能说在加约束的情况下，理论上应该可以获得泛化能力更强的结果。</li>
<li><code>dual</code>：对偶或原始方法，<code>bool</code>类型。对偶方法只用在求解线性多核(<code>liblinear</code>)的<code>L2</code>惩罚项上。当样本数量大于样本特征的时候，<code>dual</code>通常设置为<code>False</code>。</li>
<li><code>tol</code>：停止求解的标准，<code>float</code>类型。就是求解到多少的时候停止，认为已经求出最优解。</li>
<li><code>c</code>：正则化系数λ的倒数，<code>float</code>类型，必须是正浮点型数。像<code>SVM</code>一样，越小的数值表示越强的正则化。</li>
<li><code>fit_intercept</code>：是否存在截距或偏差，<code>bool</code>类型。</li>
<li><code>intercept_scaling</code>：<code>float</code>类型。仅在正则化项为<code>liblinear</code>且<code>fit_intercept</code>设置为<code>True</code>时有用。</li>
<li><code>class_weight</code>：用于标示分类模型中各种类型的权重，可以是一个字典或者<code>balanced</code>字符串，默认为不输入，也就是不考虑权重。如果选择输入的话，可以选择<code>balanced</code>让类库自己计算类型权重，或者自己输入各个类型的权重。举个例子，比如对于<code>0</code>和<code>1</code>的二元模型，我们可以定义<code>class_weight = {0:0.9, 1:0.1}</code>，这样类型<code>0</code>的权重为<code>90%</code>，而类型<code>1</code>的权重为<code>10%</code>；如果<code>class_weight</code>选择<code>balanced</code>，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。当<code>class_weight</code>为<code>balanced</code>时，类权重计算方法为<code>n_samples / (n_classes * np.bincount(y))</code>，<code>n_samples</code>为样本数，<code>n_classes</code>为类别数量，<code>np.bincount(y)</code>会输出每个类的样本数，例如<code>y = [1, 0, 0, 1, 1]</code>，则<code>np.bincount(y) = [2, 3]</code>。</li>
</ul>
<p>&emsp;&emsp;那么<code>class_weight</code>有什么作用呢？在分类模型中，我们经常会遇到两类问题：</p>
<ol>
<li>误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时我们可以适当提高非法用户的权重。</li>
<li>样本是高度失衡。比如我们有合法用户和非法用户的二元样本数据<code>10000</code>条，里面合法用户有<code>9995</code>条，非法用户只有<code>5</code>条，如果我们不考虑权重，则可以将所有的测试集都预测为合法用户，这样预测准确率理论上有<code>99.95%</code>，但是却没有任何意义。这时我们可以选择<code>balanced</code>，让类库自动提高非法用户样本的权重。提高了某种分类的权重，相比不考虑权重，会有更多的样本分类划分到高权重的类别，从而可以解决上面两类问题。</li>
</ol>
<ul>
<li><code>random_state</code>：随机数种子，<code>int</code>类型，可选参数，仅在正则化优化算法为<code>sag</code>、<code>liblinear</code>时有用。</li>
<li><code>solver</code>：优化算法选择参数，只有五个可选参数，即<code>newton-cg</code>、<code>lbfgs</code>、<code>liblinear</code>、<code>sag</code>和<code>saga</code>，默认为<code>liblinear</code>。<code>solver</code>参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：</li>
</ul>
<ol>
<li><code>liblinear</code>：使用了开源的<code>liblinear</code>库实现，内部使用了坐标轴下降法来迭代优化损失函数。</li>
<li><code>lbfgs</code>：拟牛顿法的一种，利用损失函数二阶导数矩阵(即海森矩阵)来迭代优化损失函数。</li>
<li><code>newton-cg</code>：也是牛顿法家族的一种，利用损失函数二阶导数矩阵(即海森矩阵)来迭代优化损失函数。</li>
<li><code>sag</code>：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</li>
<li><code>saga</code>：线性收敛的随机优化算法的的变重。</li>
</ol>
<p>&emsp;&emsp;<code>liblinear</code>适用于小数据集，而<code>sag</code>和<code>saga</code>适用于大数据集，因为速度更快。<br>&emsp;&emsp;对于多分类问题，只有<code>newton-cg</code>、<code>sag</code>、<code>saga</code>和<code>lbfgs</code>能够处理多项损失，而<code>liblinear</code>受限于一对剩余(<code>OvR</code>)。意思就是用<code>liblinear</code>的时候，如果是多分类问题，得先把一种类别作为一个类别，剩余的所有类别作为另外一个类别。依次类推，最终遍历所有类别，进行分类。<br>&emsp;&emsp;<code>newton-cg</code>、<code>sag</code>和<code>lbfgs</code>这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的<code>L1</code>正则化，只能用于<code>L2</code>正则化。而<code>liblinear</code>和<code>saga</code>通吃<code>L1</code>正则化和<code>L2</code>正则化。<br>&emsp;&emsp;同时，<code>sag</code>每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它。而如果样本量非常大，比如大于<code>10</code>万，<code>sag</code>是第一选择。但是<code>sag</code>不能用于<code>L1</code>正则化，所以当你有大量的样本又需要<code>L1</code>正则化的话，就要自己做取舍了。要么通过对样本采样来降低样本量，要么回到<code>L2</code>正则化。<br>&emsp;&emsp;从上面的描述，大家可能觉得既然<code>newton-cg</code>、<code>lbfgs</code>和<code>sag</code>这么多限制，如果不是大样本，我们选择<code>liblinear</code>不就行了吗？这是错误的，因为<code>liblinear</code>也有自己的弱点！我们知道，逻辑回归有二元逻辑回归和多元逻辑回归。对于多元逻辑回归常见的有<code>one-vs-rest</code>(<code>OvR</code>)和<code>many-vs-many</code>(<code>MvM</code>)两种，而<code>MvM</code>一般比<code>OvR</code>分类相对准确一些。郁闷的是<code>liblinear</code>只支持<code>OvR</code>却不支持<code>MvM</code>，这样如果我们需要相对精确的多元逻辑回归时，就不能选择<code>liblinear</code>了，即不能使用<code>L1</code>正则化。</p>
<ul>
<li><code>max_iter</code>：算法收敛最大迭代次数，<code>int</code>类型。仅在正则化优化算法为<code>newton-cg</code>、<code>sag</code>和<code>lbfgs</code>才有用，算法收敛的最大迭代次数。</li>
<li><code>multi_class</code>：分类方式选择参数，<code>str</code>类型，可选参数为<code>ovr</code>和<code>multinomial</code>。<code>ovr</code>即<code>one-vs-rest</code>，而<code>multinomial</code>即<code>many-vs-many</code>。如果是二元逻辑回归，<code>ovr</code>和<code>multinomial</code>并没有任何区别，区别主要在多元逻辑回归上。</li>
</ul>
<p>&emsp;&emsp;<code>OvR</code>和<code>MvM</code>有什么不同？<code>OvR</code>的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第<code>K</code>类的分类决策，我们把所有第<code>K</code>类的样本作为正例，除了第<code>K</code>类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第<code>K</code>类的分类模型。其他类的分类模型获得以此类推。而<code>MvM</code>则相对复杂，这里举<code>MvM</code>的特例<code>one-vs-one</code>(<code>OvO</code>)作讲解。如果模型有<code>T</code>类，我们每次在所有的<code>T</code>类样本里面选择两类样本出来，不妨记为<code>T1</code>类和<code>T2</code>类，把所有的输出为<code>T1</code>和<code>T2</code>的样本放在一起，把<code>T1</code>作为正例，<code>T2</code>作为负例，进行二元逻辑回归，得到模型参数，一共需要<code>T(T - 1) / 2</code>次分类。可以看出<code>OvR</code>相对简单，但分类效果相对略差(这里指大多数样本分布情况，某些样本分布下<code>OvR</code>可能更好)；<code>MvM</code>分类相对精确，但是分类速度没有<code>OvR</code>快。如果选择了<code>ovr</code>，则<code>4</code>种损失函数的优化方法<code>liblinear</code>、<code>newton-cg</code>、<code>lbfgs</code>和<code>sag</code>都可以选择。但是如果选择了<code>multinomial</code>，则只能选择<code>newton-cg</code>、<code>lbfgs</code>和<code>sag</code>了。</p>
<ul>
<li><code>verbose</code>：日志冗长度，<code>int</code>类型，就是不输出训练过程；<code>1</code>的时候偶尔输出结果；如果大于<code>1</code>，对于每个子模型都输出。</li>
<li><code>warm_start</code>：热启动参数，<code>bool</code>类型。如果为<code>True</code>，则下一次训练是以追加树的形式进行(重新使用上一次的调用作为初始化)。</li>
<li><code>n_jobs</code>：并行数，<code>int</code>类型。<code>1</code>的时候，用<code>CPU</code>的一个内核运行程序；<code>2</code>的时候，用<code>CPU</code>的<code>2</code>个内核运行程序；<code>-1</code>的时候，用所有<code>CPU</code>的内核运行程序。</li>
</ul>
<p>&emsp;&emsp;<code>LogisticRegression</code>也提供了一些方法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>decision_function(X)</code></td>
<td>Predict confidence scores for samples</td>
</tr>
<tr>
<td><code>densify()</code></td>
<td>Convert coefficient matrix to dense array format</td>
</tr>
<tr>
<td><code>fit(X, y[, sample_weight])</code></td>
<td>Fit the model according to the given training data</td>
</tr>
<tr>
<td><code>get_params([deep])</code></td>
<td>Get parameters for this estimator</td>
</tr>
<tr>
<td><code>predict(X)</code></td>
<td>Predict class labels for samples in <code>X</code></td>
</tr>
<tr>
<td><code>predict_log_proba(X)</code></td>
<td>Log of probability estimates</td>
</tr>
<tr>
<td><code>predict_proba(X)</code></td>
<td>Probability estimates</td>
</tr>
<tr>
<td><code>score(X, y[, sample_weight])</code></td>
<td>Returns the mean accuracy on the given test data and labels</td>
</tr>
<tr>
<td><code>set_params(**params)</code></td>
<td>Set the parameters of this estimator</td>
</tr>
<tr>
<td><code>sparsify()</code></td>
<td>Convert coefficient matrix to sparse format</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;针对从疝气病症状预测病马的死亡率的案例，使用<code>sklearn</code>的解决方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">​</span><br><span class="line"><span class="string">""" 使用Sklearn构建Logistic回归分类器 """</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicSklearn</span><span class="params">()</span>:</span></span><br><span class="line">    frTrain = open(<span class="string">'horseColicTraining.txt'</span>)  <span class="comment"># 打开训练集</span></span><br><span class="line">    frTest = open(<span class="string">'horseColicTest.txt'</span>)  <span class="comment"># 打开测试集</span></span><br><span class="line">    trainingSet = []</span><br><span class="line">    trainingLabels = []</span><br><span class="line">    testSet = []</span><br><span class="line">    testLabels = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(currLine) - <span class="number">1</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(currLine) - <span class="number">1</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        testSet.append(lineArr)</span><br><span class="line">        testLabels.append(float(currLine[<span class="number">-1</span>]))</span><br><span class="line">    classifier = LogisticRegression(solver=<span class="string">'liblinear'</span>, max_iter=<span class="number">10</span>).fit(trainingSet, trainingLabels)</span><br><span class="line">    test_accurcy = classifier.score(testSet, testLabels) * <span class="number">100</span></span><br><span class="line">    print(<span class="string">'正确率:%f%%'</span> % test_accurcy)  <span class="comment"># 正确率:73.134328%</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    colicSklearn()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/12/机器学习/sklearn的数据预处理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/12/机器学习/sklearn的数据预处理/" itemprop="url">sklearn的数据预处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-12T12:04:27+08:00">
                2019-02-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>klearn.preprocessing</code>提供了各种公共函数，用于将<code>raw feature vector</code>转换成另外一种更适合评估器工作的格式。</p>
<h3 id="标准化-Standardization-、平均移除法-mean-removal-和方差归一化-variance-scaling"><a href="#标准化-Standardization-、平均移除法-mean-removal-和方差归一化-variance-scaling" class="headerlink" title="标准化(Standardization)、平均移除法(mean removal)和方差归一化(variance scaling)"></a>标准化(Standardization)、平均移除法(mean removal)和方差归一化(variance scaling)</h3><p>&emsp;&emsp;<code>scale</code>函数提供了一个快速而简单的方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>              [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>              [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled = preprocessing.scale(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled</span><br><span class="line">array([[ <span class="number">0.</span>  ..., <span class="number">-1.22</span>...,  <span class="number">1.33</span>...],</span><br><span class="line">       [ <span class="number">1.22</span>...,  <span class="number">0.</span>  ..., <span class="number">-0.26</span>...],</span><br><span class="line">       [<span class="number">-1.22</span>...,  <span class="number">1.22</span>..., <span class="number">-1.06</span>...]])</span><br></pre></td></tr></table></figure>
<p>归一化后的数据其均值为<code>0</code>，方差为<code>1</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled.mean(axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled.std(axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<p>一般会把<code>train</code>和<code>test</code>集放在一起做标准化，或者在<code>train</code>集上做标准化后，用同样的标准化器去标准化<code>test</code>集，此时可以用<code>StandardScaler</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = preprocessing.StandardScaler().fit(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler</span><br><span class="line">StandardScaler(copy=<span class="keyword">True</span>, with_mean=<span class="keyword">True</span>, with_std=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.mean_</span><br><span class="line">array([ <span class="number">1.</span> ...,  <span class="number">0.</span> ...,  <span class="number">0.33</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.scale_</span><br><span class="line">array([ <span class="number">0.81</span>...,  <span class="number">0.81</span>...,  <span class="number">1.24</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.transform(X)</span><br><span class="line">array([[ <span class="number">0.</span>  ..., <span class="number">-1.22</span>...,  <span class="number">1.33</span>...],</span><br><span class="line">       [ <span class="number">1.22</span>...,  <span class="number">0.</span>  ..., <span class="number">-0.26</span>...],</span><br><span class="line">       [<span class="number">-1.22</span>...,  <span class="number">1.22</span>..., <span class="number">-1.06</span>...]])</span><br></pre></td></tr></table></figure>
<p>通过在<code>StandardScaler</code>的构造函数中设置<code>with_mean=False</code>或者<code>with_std=False</code>，可以禁止均值中心化(<code>centering</code>)和归一化(<code>scaling</code>)。</p>
<h3 id="将feature归一化到一个范围内"><a href="#将feature归一化到一个范围内" class="headerlink" title="将feature归一化到一个范围内"></a>将feature归一化到一个范围内</h3><p>&emsp;&emsp;另一种标准化方式是将<code>feature</code>归一化到给定的范围内(比如<code>[0, 1]</code>之间)，可以使用<code>MinMaxScaler</code>或者<code>MaxAbsScaler</code>函数。<br>&emsp;&emsp;归一化至<code>[0, 1]</code>的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_minmax</span><br><span class="line">array([[ <span class="number">0.5</span>       ,  <span class="number">0.</span>        ,  <span class="number">1.</span>        ],</span><br><span class="line">       [ <span class="number">1.</span>        ,  <span class="number">0.5</span>       ,  <span class="number">0.33333333</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">1.</span>        ,  <span class="number">0.</span>        ]])</span><br></pre></td></tr></table></figure>
<p>如果<code>MinMaxScaler</code>给出了显式的范围，例如<code>feature_range=(min, max)</code>，那么对应的公式为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_std = (X - X.min(axis=<span class="number">0</span>)) / (X.max(axis=<span class="number">0</span>) - X.min(axis=<span class="number">0</span>))</span><br><span class="line">X_scaled = X_std / (max - min) + min</span><br></pre></td></tr></table></figure>
<p><code>MaxAbsScaler</code>以类似的方式工作，它的归一化范围在<code>[-1, 1]</code>之间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>                    [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>max_abs_scaler = preprocessing.MaxAbsScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_maxabs</span><br><span class="line">array([[ <span class="number">0.5</span>, <span class="number">-1.</span> ,  <span class="number">1.</span> ],</span><br><span class="line">       [ <span class="number">1.</span> ,  <span class="number">0.</span> ,  <span class="number">0.</span> ],</span><br><span class="line">       [ <span class="number">0.</span> ,  <span class="number">1.</span> , <span class="number">-0.5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test = np.array([[ <span class="number">-3.</span>, <span class="number">-1.</span>,  <span class="number">4.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_test_maxabs</span><br><span class="line">array([[<span class="number">-1.5</span>, <span class="number">-1.</span> ,  <span class="number">2.</span> ]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>max_abs_scaler.scale_</span><br><span class="line">array([ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure>
<h3 id="正态分布化-Normalization"><a href="#正态分布化-Normalization" class="headerlink" title="正态分布化(Normalization)"></a>正态分布化(Normalization)</h3><p>&emsp;&emsp;<code>Normalization</code>用于将各个样本归一化为正态分布，函数<code>normalize</code>提供了这一功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_normalized = preprocessing.normalize(X, norm=<span class="string">'l2'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_normalized</span><br><span class="line">array([[ <span class="number">0.40</span>..., <span class="number">-0.40</span>...,  <span class="number">0.81</span>...],</span><br><span class="line">       [ <span class="number">1.</span>  ...,  <span class="number">0.</span>  ...,  <span class="number">0.</span>  ...],</span><br><span class="line">       [ <span class="number">0.</span>  ...,  <span class="number">0.70</span>..., <span class="number">-0.70</span>...]])</span><br></pre></td></tr></table></figure>
<p><code>Normalizer</code>类也可以实现这一功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalizer = preprocessing.Normalizer().fit(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalizer</span><br><span class="line">Normalizer(copy=<span class="keyword">True</span>, norm=<span class="string">'l2'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalizer.transform(X)</span><br><span class="line">array([[ <span class="number">0.40</span>..., <span class="number">-0.40</span>...,  <span class="number">0.81</span>...],</span><br><span class="line">       [ <span class="number">1.</span>  ...,  <span class="number">0.</span>  ...,  <span class="number">0.</span>  ...],</span><br><span class="line">       [ <span class="number">0.</span>  ...,  <span class="number">0.70</span>..., <span class="number">-0.70</span>...]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>normalizer.transform([[<span class="number">-1.</span>,  <span class="number">1.</span>, <span class="number">0.</span>]])</span><br><span class="line">array([[<span class="number">-0.70</span>...,  <span class="number">0.70</span>...,  <span class="number">0.</span>  ...]])</span><br></pre></td></tr></table></figure>
<h3 id="二值化-Binarization"><a href="#二值化-Binarization" class="headerlink" title="二值化(Binarization)"></a>二值化(Binarization)</h3><p>&emsp;&emsp;二值化可以将数值形(<code>numerical</code>)的<code>feature</code>进行阀值化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>binarizer = preprocessing.Binarizer().fit(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>binarizer</span><br><span class="line">Binarizer(copy=<span class="keyword">True</span>, threshold=<span class="number">0.0</span>)  <span class="comment"># 调整binarizer的threshold</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>binarizer.transform(X)</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="补充缺失值"><a href="#补充缺失值" class="headerlink" title="补充缺失值"></a>补充缺失值</h3><p>&emsp;&emsp;现实世界中有许多数据集中包含着缺失值(<code>missing values</code>)，经常被编码成空格、<code>NaN</code>或者其它占位符。这样的数据集对于<code>sklearn</code>来说是不兼容的，因为它的输入数据必须是全是数值型的。<br>&emsp;&emsp;一个基本策略是使用不完整的数据，即抛弃掉那些带缺失值的行。然而，缺失的数据中也可能包含有价值的信息。一个更好地策略是补充缺失值，比如从已知的数据中去模拟它们。<br>&emsp;&emsp;<code>Imputer</code>类提供了基本策略来补充缺失值，或者使用均值、中值，或者是行中或列中最常用的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>imp = Imputer(missing_values=<span class="string">'NaN'</span>, strategy=<span class="string">'mean'</span>, axis=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>imp.fit([[<span class="number">1</span>, <span class="number">2</span>], [np.nan, <span class="number">3</span>], [<span class="number">7</span>, <span class="number">6</span>]])</span><br><span class="line">Imputer(axis=<span class="number">0</span>, copy=<span class="keyword">True</span>, missing_values=<span class="string">'NaN'</span>, strategy=<span class="string">'mean'</span>, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[np.nan, <span class="number">2</span>], [<span class="number">6</span>, np.nan], [<span class="number">7</span>, <span class="number">6</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(imp.transform(X))</span><br><span class="line">[[ <span class="number">4.</span>          <span class="number">2.</span>        ]</span><br><span class="line"> [ <span class="number">6.</span>          <span class="number">3.666</span>...]</span><br><span class="line"> [ <span class="number">7.</span>          <span class="number">6.</span>        ]]</span><br></pre></td></tr></table></figure>
<h3 id="多项式特征生成"><a href="#多项式特征生成" class="headerlink" title="多项式特征生成"></a>多项式特征生成</h3><p>&emsp;&emsp;很多情况下，考虑输入数据中的非线性特征来增加模型的复杂性是非常有效的。一个简单常用的方法就是使用多项式特征，它能捕捉到特征中高阶和相互作用的项。<code>PolynomialFeatures</code>类可以实现该功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.arange(<span class="number">6</span>).reshape(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly = PolynomialFeatures(<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly.fit_transform(X)</span><br><span class="line">array([[  <span class="number">1.</span>,   <span class="number">0.</span>,   <span class="number">1.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">1.</span>],</span><br><span class="line">       [  <span class="number">1.</span>,   <span class="number">2.</span>,   <span class="number">3.</span>,   <span class="number">4.</span>,   <span class="number">6.</span>,   <span class="number">9.</span>],</span><br><span class="line">       [  <span class="number">1.</span>,   <span class="number">4.</span>,   <span class="number">5.</span>,  <span class="number">16.</span>,  <span class="number">20.</span>,  <span class="number">25.</span>]])</span><br></pre></td></tr></table></figure>
<p>特征向量<code>X</code>从<code>(X1, X2)</code>被转换成<code>(1, X1, X2, X1^2, X1*X2, X2^2)</code>。<br>&emsp;&emsp;在一些情况下，我们只需要特征中的相互作用项(<code>interaction terms</code>)，它可以通过传入参数<code>interaction_only=True</code>获得：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly = PolynomialFeatures(degree=<span class="number">3</span>, interaction_only=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly.fit_transform(X)</span><br><span class="line">array([[   <span class="number">1.</span>,    <span class="number">0.</span>,    <span class="number">1.</span>,    <span class="number">2.</span>,    <span class="number">0.</span>,    <span class="number">0.</span>,    <span class="number">2.</span>,    <span class="number">0.</span>],</span><br><span class="line">       [   <span class="number">1.</span>,    <span class="number">3.</span>,    <span class="number">4.</span>,    <span class="number">5.</span>,   <span class="number">12.</span>,   <span class="number">15.</span>,   <span class="number">20.</span>,   <span class="number">60.</span>],</span><br><span class="line">       [   <span class="number">1.</span>,    <span class="number">6.</span>,    <span class="number">7.</span>,    <span class="number">8.</span>,   <span class="number">42.</span>,   <span class="number">48.</span>,   <span class="number">56.</span>,  <span class="number">336.</span>]])</span><br></pre></td></tr></table></figure>
<p>特征向量<code>X</code>从<code>(X1, X2, X3)</code>被转换成<code>(1, X1, X2, X3, X1*X2, X1*X3, X2*X3, X1*X2*X3)</code>。</p>
<hr>
<h3 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h3><p>&emsp;&emsp;<code>One-Hot</code>编码又称为<code>一位有效编码</code>，主要是采用位状态寄存器来对个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。<br>&emsp;&emsp;在实际的机器学习任务中，特征有时候并不总是连续值，有可能是一些分类值，例如性别可分为<code>male</code>和<code>female</code>。对于这样的特征，通常需要对其进行特征数字化：</p>
<ul>
<li>性别：<code>[male, female]</code></li>
<li>地区：<code>[Europe, US, Asia]</code></li>
<li>浏览器：<code>[Firefox, Chrome, Safari, Internet Explorer]</code></li>
</ul>
<p>对于某一个样本，如<code>[male, US, Internet Explorer]</code>，我们需要将这个分类值的特征数字化，最直接的方法就是采用序列化的方式，即<code>[0, 1, 3]</code>。但是，这样的特征处理并不能直接放入机器学习算法中。<br>&emsp;&emsp;对于上述问题，<code>性别</code>属性是二维的，<code>地区</code>属性是三维的，而<code>浏览器</code>属性则是四维的。我们可以采用<code>One-Hot</code>编码的方式对上述的样本<code>[male, US, Internet Explorer]</code>进行编码，例如<code>male</code>对应<code>[1, 0]</code>，<code>US</code>对应<code>[0, 1, 0]</code>，<code>Internet Explorer</code>对应<code>[0, 0, 0, 1]</code>，则完整的特征数字化的结果为<code>[1, 0, 0, 1, 0, 0, 0, 0, 1]</code>。这样导致的一个结果就是数据会变得非常得稀疏。<br>&emsp;&emsp;可以这样理解：对于每一个特征，如果它有<code>m</code>个可能值，那么经过独热编码后，就变成了<code>m</code>个二元特征，并且这些特征互斥，每次只有一个激活，因此数据会变成稀疏的。这样做的好处主要有：解决了分类器不好处理属性数据的问题；一定程度上也起到了扩充特征的作用。<br>&emsp;&emsp;实际的<code>Python</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">​</span><br><span class="line">enc = preprocessing.OneHotEncoder()</span><br><span class="line">enc.fit([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">array = enc.transform([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>]]).toarray()</span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure>
<p>我们将矩阵排起来看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">0</span> <span class="number">0</span> <span class="number">3</span> ];</span><br><span class="line"> [ <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> ];</span><br><span class="line"> [ <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> ];</span><br><span class="line"> [ <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> ]]</span><br></pre></td></tr></table></figure>
<p>该矩阵的每一列代表一个特征：</p>
<ul>
<li>第一列只有<code>0</code>或<code>1</code>出现，共有两种情况，所以<code>one-hot</code>编码前两维代表第一个特征，也恰好说明了是性别的分类。</li>
<li>第二列有<code>0</code>、<code>1</code>和<code>2</code>出现，共有三种情况，所以<code>one-hot</code>编码中间三维代表第二个特征，恰好证明了地区所对应的特征。</li>
<li>第三列有<code>0</code>、<code>1</code>、<code>2</code>和<code>3</code>出现，共有四种情况，所以<code>one-hot</code>编码最后四维代表第三个特征，也是最后一个特征，证明了浏览器的对应的特征。</li>
</ul>
<p>这里也很好地解释了<code>一定程度上也起到了扩充特征的作用</code>这句话，其实就是将所有的特征都融入到一个向量里面构成<code>one-hot</code>意义下的特征，一下子变成了<code>9</code>维的向量。特征列如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[male, female, Europe, US, Asia, Firefox, Chrome, Safari, Internet Explorer]]</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/23/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><span class="space">&hellip;</span><a class="page-number" href="/page/95/">95</a><a class="extend next" rel="next" href="/page/25/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">949</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
