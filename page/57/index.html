<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="泥腿子出身">
<meta property="og:url" content="http://fukangwei.gitee.io/page/57/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="泥腿子出身">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/page/57/">





  <title>泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/网络编程/UDP服务器/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/网络编程/UDP服务器/" itemprop="url">UDP服务器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T21:28:24+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>udp_client.c</code>如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line">​</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT 8888</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_BUF_SIZE 1024</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">udpc_requ</span> <span class="params">( <span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr_in *addr, <span class="keyword">int</span> len )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> buffer[MAX_BUF_SIZE];</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">while</span> ( <span class="number">1</span> ) &#123;</span><br><span class="line">        <span class="comment">/* 从键盘读入，写到服务端 */</span></span><br><span class="line">        <span class="built_in">printf</span> ( <span class="string">"Please input char:\r\n"</span> );</span><br><span class="line">        fgets ( buffer, MAX_BUF_SIZE, <span class="built_in">stdin</span> );</span><br><span class="line">        sendto ( sockfd, buffer, <span class="built_in">strlen</span> ( buffer ), <span class="number">0</span>, addr, len );</span><br><span class="line">        bzero ( buffer, MAX_BUF_SIZE );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">( <span class="keyword">int</span> argc, <span class="keyword">char</span> **argv )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sockfd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( argc != <span class="number">2</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Usage:%s server_ip\n"</span>, argv[<span class="number">0</span>] );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 建立sockfd描述符 */</span></span><br><span class="line">    sockfd = socket ( AF_INET, SOCK_DGRAM, <span class="number">0</span> );</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( sockfd &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Socket Error:%s\r\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 填充服务端的资料 */</span></span><br><span class="line">    bzero ( &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in ) );</span><br><span class="line">    addr.sin_family = AF_INET;</span><br><span class="line">    addr.sin_port = htons ( SERVER_PORT );</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* inet_aton函数用于把字符串型的IP地址转化成网络二进制数字 */</span></span><br><span class="line">    <span class="keyword">if</span> ( inet_aton ( argv[<span class="number">1</span>], &amp;addr.sin_addr ) &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Ip error:%s\r\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    udpc_requ ( sockfd, &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in ) ); <span class="comment">/* 进行读写操作 */</span></span><br><span class="line">    close ( sockfd );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>udp_server.c</code>如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line">​</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT 8888</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_MSG_SIZE 1024</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">udps_respon</span> <span class="params">( <span class="keyword">int</span> sockfd )</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">    <span class="keyword">int</span> addrlen, n;</span><br><span class="line">    <span class="keyword">char</span> msg[MAX_MSG_SIZE];</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">while</span> ( <span class="number">1</span> ) &#123;</span><br><span class="line">        bzero ( msg, <span class="keyword">sizeof</span> ( msg ) ); <span class="comment">/* 初始化，清零 */</span></span><br><span class="line">        addrlen = <span class="keyword">sizeof</span> ( struct sockaddr );</span><br><span class="line">        <span class="comment">/* 从客户端接收消息 */</span></span><br><span class="line">        n = recvfrom ( sockfd, msg, MAX_MSG_SIZE, <span class="number">0</span>, ( struct sockaddr * ) &amp;addr, &amp;addrlen );</span><br><span class="line">        msg[n] = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stdout</span>, <span class="string">"Server have received %s\r\n"</span>, msg ); <span class="comment">/* 显示服务端已经收到了信息 */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">( <span class="keyword">void</span> )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sockfd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">    <span class="comment">/* 服务器端开始建立socket描述符 */</span></span><br><span class="line">    sockfd = socket ( AF_INET, SOCK_DGRAM, <span class="number">0</span> );</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( sockfd &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Socket Error:%s\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 服务器端填充sockaddr结构 */</span></span><br><span class="line">    bzero ( &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in ) );</span><br><span class="line">    addr.sin_family = AF_INET;</span><br><span class="line">    addr.sin_addr.s_addr = htonl ( INADDR_ANY );</span><br><span class="line">    addr.sin_port = htons ( SERVER_PORT );</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 捆绑sockfd描述符 */</span></span><br><span class="line">    <span class="keyword">if</span> ( bind ( sockfd, ( struct sockaddr * ) &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in ) ) &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Bind Error:%s\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    udps_respon ( sockfd ); <span class="comment">/* 进行读写操作 */</span></span><br><span class="line">    close ( sockfd );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>uip_client_ipv6.c</code>如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line">​</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT  8888</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_BUF_SIZE 1024</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">udpc_requ</span> <span class="params">( <span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr_in6 *addr, <span class="keyword">int</span> len )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> buffer[MAX_BUF_SIZE];</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">while</span> ( <span class="number">1</span> ) &#123;</span><br><span class="line">        <span class="comment">/* 从键盘读入，写到服务端 */</span></span><br><span class="line">        <span class="built_in">printf</span> ( <span class="string">"Please input char:\n"</span> );</span><br><span class="line">        fgets ( buffer, MAX_BUF_SIZE, <span class="built_in">stdin</span> );</span><br><span class="line">        sendto ( sockfd, buffer, <span class="built_in">strlen</span> ( buffer ), <span class="number">0</span>, ( struct sockaddr * ) addr, len );</span><br><span class="line">        bzero ( buffer, MAX_BUF_SIZE );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">( <span class="keyword">int</span> argc, <span class="keyword">char</span> **argv )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sockfd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in6</span> <span class="title">addr</span>;</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( argc != <span class="number">2</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Usage:%s server_ip\n"</span>, argv[<span class="number">0</span>] );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 建立sockfd描述符 */</span></span><br><span class="line">    sockfd = socket ( AF_INET6, SOCK_DGRAM, <span class="number">0</span> );</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( sockfd &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Socket Error:%s\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 填充客户端的资料 */</span></span><br><span class="line">    bzero ( &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in6 ) );</span><br><span class="line">    addr.sin6_family = AF_INET6;</span><br><span class="line">    addr.sin6_port = htons ( SERVER_PORT );</span><br><span class="line">    inet_pton ( AF_INET6, argv[<span class="number">1</span>], &amp;addr.sin6_addr );</span><br><span class="line">    udpc_requ ( sockfd, &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in6 ) ); <span class="comment">/* 进行读写操作 */</span></span><br><span class="line">    close ( sockfd );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>uip_server_ipv6.c</code>如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line">​</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT 8888</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_MSG_SIZE 1024</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">udps_respon</span> <span class="params">( <span class="keyword">int</span> sockfd )</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in6</span> <span class="title">addr</span>;</span></span><br><span class="line">    <span class="keyword">int</span> addrlen, n;</span><br><span class="line">    <span class="keyword">char</span> msg[MAX_MSG_SIZE];</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">while</span> ( <span class="number">1</span> ) &#123;</span><br><span class="line">        bzero ( msg, <span class="keyword">sizeof</span> ( msg ) ); <span class="comment">/* 初始化，清零 */</span></span><br><span class="line">        addrlen = <span class="keyword">sizeof</span> ( struct sockaddr_in6 );</span><br><span class="line">        <span class="comment">/* 从客户端接收消息 */</span></span><br><span class="line">        n = recvfrom ( sockfd, msg, MAX_MSG_SIZE, <span class="number">0</span>, ( struct sockaddr * ) &amp;addr, &amp;addrlen );</span><br><span class="line">        msg[n] = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stdout</span>, <span class="string">"Server have received %s"</span>, msg ); <span class="comment">/* 显示服务端已经收到了信息 */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">( <span class="keyword">void</span> )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sockfd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in6</span> <span class="title">addr</span>;</span></span><br><span class="line">    <span class="comment">/* 服务器端开始建立socket描述符 */</span></span><br><span class="line">    sockfd = socket ( AF_INET6, SOCK_DGRAM, <span class="number">0</span> );</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">if</span> ( sockfd &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Socket Error:%s\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 服务器端填充sockaddr结构 */</span></span><br><span class="line">    bzero ( &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in6 ) );</span><br><span class="line">    addr.sin6_family = AF_INET6;</span><br><span class="line">    addr.sin6_addr = in6addr_any;</span><br><span class="line">    addr.sin6_port = htons ( SERVER_PORT );</span><br><span class="line">​</span><br><span class="line">    <span class="comment">/* 捆绑sockfd描述符 */</span></span><br><span class="line">    <span class="keyword">if</span> ( bind ( sockfd, ( struct sockaddr * ) &amp;addr, <span class="keyword">sizeof</span> ( struct sockaddr_in6 ) ) &lt; <span class="number">0</span> ) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span> ( <span class="built_in">stderr</span>, <span class="string">"Bind Error:%s\n"</span>, strerror ( errno ) );</span><br><span class="line">        <span class="built_in">exit</span> ( <span class="number">1</span> );</span><br><span class="line">    &#125;</span><br><span class="line">​</span><br><span class="line">    udps_respon ( sockfd ); <span class="comment">/* 进行读写操作 */</span></span><br><span class="line">    close ( sockfd );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/网络编程/网络基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/网络编程/网络基础/" itemprop="url">网络基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T21:15:36+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="私有IP"><a href="#私有IP" class="headerlink" title="私有IP"></a>私有IP</h3><p>&emsp;&emsp;私有<code>IP</code>就是在本地局域网上的<code>IP</code>，与之对应的是公有<code>IP</code>(在互联网上的<code>IP</code>)。随着私有<code>IP</code>网络的发展，为节省可分配的注册<code>IP</code>地址，有一组<code>IP</code>地址被拿出来专门用于私有<code>IP</code>网络，称为私有<code>IP</code>地址。私有<code>IP</code>地址范围如下：</p>
<ul>
<li><code>A</code>类：<code>10.0.0.0</code>至<code>10.255.255.255</code>，即<code>10.0.0.0/8</code>。</li>
<li><code>B</code>类：<code>172.16.0.0</code>至<code>172.31.255.255</code>，即<code>172.16.0.0/12</code>。</li>
<li><code>C</code>类：<code>192.168.0.0</code>至<code>192.168.255.255</code>，即<code>192.168.0.0/16</code>。</li>
</ul>
<p>&emsp;&emsp;这些地址是不会被<code>Internet</code>分配的，它们在<code>Internet</code>上也不会被路由。虽然它们不能直接和<code>Internet</code>网连接，但通过技术手段仍旧可以和<code>Internet</code>通讯(即<code>NAT</code>技术)。我们可以根据需要来选择适当的地址类，在内部局域网中将这些地址像公用<code>IP</code>地址一样地使用。在<code>Internet</code>上，有些不需要与<code>Internet</code>通讯的设备，如打印机、可管理集线器等也可以使用这些地址，以节省<code>IP</code>地址资源。</p>
<h3 id="Hosts"><a href="#Hosts" class="headerlink" title="Hosts"></a>Hosts</h3><p>&emsp;&emsp;<code>Hosts</code>是一个没有扩展名的系统文件，可以用记事本等工具打开。其作用就是将一些常用的网址域名与其对应的<code>IP</code>地址建立一个<code>关联数据库</code>，当用户在浏览器中输入一个需要登录的网址时，系统会首先自动从<code>Hosts</code>文件中寻找对应的<code>IP</code>地址，一旦找到，系统会立即打开对应网页；如果没有找到，则系统会再将网址提交<code>DNS</code>域名解析服务器进行<code>IP</code>地址的解析。也就是说<code>Hosts</code>的<code>IP</code>解析优先级比<code>DNS</code>要高。</p>
<h3 id="主机名"><a href="#主机名" class="headerlink" title="主机名"></a>主机名</h3><p>&emsp;&emsp;主机名就是计算机的名字，网上邻居就是根据主机名来识别的。这个名字可以随时更改，从<code>我的电脑</code>属性的计算机名就可以更改。用户登陆时候用的是操作系统的个人用户帐号，这个也可以更改，在控制面板的用户界面里就可以修改，但这个用户名和计算机名无关。<br>&emsp;&emsp;因特网上的主机或<code>Web</code>站点是用主机名识别的，主机名有时称为域名。主机名会映射到<code>IP</code>地址，但是主机名和<code>IP</code>地址之间没有一对一关系。当<code>Web</code>客户机发出到主机的<code>HTTP</code>请求时，使用的就是主机名。发出请求的用户可能会指定服务器的<code>IP</code>地址，而不是主机名，但现在在因特网中不常见。对于用户来说，主机名比数字<code>IP</code>地址更方便。公司、组织和个人常常选择其<code>Web</code>站点的主机名，用户能很容易地记住这些主机名。在<code>HTTP</code>请求中使用主机名意味着：</p>
<ul>
<li>一个主机名中的服务可以由许多服务器提供，它们有不同的<code>IP</code>地址。</li>
<li>具有一个<code>IP</code>地址的一台服务器可以提供许多主机名中的服务，这称为<code>虚拟主机</code>。</li>
</ul>
<p>&emsp;&emsp;主机名由<code>DNS</code>服务器或域名服务器的服务器映射到<code>IP</code>地址，<code>DNS</code>代表域名服务。在大型网络中，许多<code>DNS</code>服务器可以相互协作，以提供主机名和<code>IP</code>地址之间的映射。<br>&emsp;&emsp;在<code>Windows</code>中，使用命令<code>ipconfig/all</code>可以看到自己的主机名；在<code>linux</code>中，使用<code>hostname</code>可以查看自己的主机名。</p>
<h3 id="IP地址“0-0-0-0”"><a href="#IP地址“0-0-0-0”" class="headerlink" title="IP地址“0.0.0.0”"></a>IP地址“0.0.0.0”</h3><p>&emsp;&emsp;参考<code>RFC</code>文档：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">8</span> - Addresses in <span class="keyword">this</span> block refer to source hosts on <span class="string">"this"</span> network.</span><br><span class="line">Address <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">32</span> may be used as a source address <span class="keyword">for</span> <span class="keyword">this</span> host on <span class="keyword">this</span> network;</span><br><span class="line">other addresses within <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">8</span> may be used to refer to specified hosts on <span class="keyword">this</span></span><br><span class="line">network ([RFC1122], Section <span class="number">3.2</span><span class="number">.1</span><span class="number">.3</span>).</span><br></pre></td></tr></table></figure>
<p>根据<code>RFC</code>文档描述，它不只是代表本机，<code>0.0.0.0/8</code>可以表示本网络中的所有主机，<code>0.0.0.0/32</code>可以用作本机的源地址，<code>0.0.0.0/8</code>也可表示本网络上的某个特定主机。综合起来，可以说<code>0.0.0.0</code>表示整个网络。它的作用是帮助路由器发送路由表中无法查询的包。如果设置了全零网络的路由，路由表中无法查询的包都将送到全零网络的路由中去。严格说来，<code>0.0.0.0</code>已经不是一个真正意义上的<code>IP</code>地址了。它表示的是这样一个集合：所有未知的主机和目的网络。这里的<code>未知</code>是指在本机的路由表里没有特定条目指明如何到达。<br>&emsp;&emsp;通常情况下，网络中所说的<code>0.0.0.0</code>的<code>IP</code>地址表示整个网络，即网络中的所有主机。在一些老的软件中，它们可能将<code>0.0.0.0</code>做为广播地址使用，即它们发送广播数据包时，目标地址址不是<code>255.255.255.255</code>，而是<code>0.0.0.0</code>。所以，当协议分析软件抓到<code>IP</code>是<code>0.0.0.0</code>的主机时，表示网络存在将<code>0.0.0.0</code>做为广播地址进行通讯的情况，而不是代表整个网络。<br>&emsp;&emsp;注意，<code>0.0.0.0</code>做为广播地址已经基本上被废弃，当前的网络程序或设备一般都不会将<code>0.0.0.0</code>做为广播地址。在网络中出现<code>0.0.0.0</code>时，我们需要检查该数据包的源主机，检查其是否是人为手动使用某些老的软件产生的这种数据包，或者是由于某些非法攻击产生的该数据包。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/网络编程/FTP问题总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/网络编程/FTP问题总结/" itemprop="url">FTP问题总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T20:42:58+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="FTP端口号"><a href="#FTP端口号" class="headerlink" title="FTP端口号"></a>FTP端口号</h3><p>&emsp;&emsp;<code>FTP</code>端口号是<code>20</code>和<code>21</code>，它的端口号是可以修改的。<code>ftp</code>的控制端口一般为<code>21</code>，而数据端口不一定是<code>20</code>，这和<code>FTP</code>的应用模式有关。如果是主动模式，应该为<code>20</code>；如果为被动模式，由服务器端和客户端协商而定。<br>&emsp;&emsp;<code>21</code>端口主要用于<code>FTP</code>(<code>File Transfer Protocol</code>，文件传输协议)服务，<code>FTP</code>服务主要是为了在两台计算机之间实现文件的上传与下载，一台计算机作为<code>FTP</code>客户端，另一台计算机作为<code>FTP</code>服务器。可以采用匿名<code>anonymous</code>登录和授权用户名与密码登录两种方式登录<code>FTP</code>服务器。一个主动模式的<code>FTP</code>连接建立要遵循以下步骤：</p>
<ol>
<li>客户端打开一个随机的端口(端口号大于<code>1024</code>，在这里我们称它为<code>x</code>)，同时一个<code>FTP</code>进程连接至服务器的<code>21</code>号命令端口。此时源端口为随机端口<code>x</code>，在客户端上；远程端口为<code>21</code>，在服务器上。</li>
<li>客户端开始监听端口(<code>x + 1</code>)，同时向服务器发送一个端口命令(通过服务器的<code>21</code>号命令端口)。此命令告诉服务器，客户端正在监听的端口号并且已准备好从此端口接收数据。这个端口就是我们所知的数据端口。</li>
<li>服务器打开<code>20</code>号源端口并且建立和客户端数据端口的连接。此时，源端口为<code>20</code>，远程数据端口为(<code>x + 1</code>)。</li>
<li>客户端通过本地的数据端口建立一个和服务器<code>20</code>号端口的连接，然后向服务器发送一个应答，告诉服务器它已经建立好了一个连接。</li>
</ol>
<hr>
<p>&emsp;&emsp;<code>FTP</code>是仅基于<code>TCP</code>的服务，不支持<code>UDP</code>。与众不同的是，<code>FTP</code>使用<code>2</code>个端口，一个数据端口和一个命令端口(也可叫做控制端口)。通常来说这两个端口是<code>21</code>(命令端口)和<code>20</code>(数据端口)。但<code>FTP</code>工作方式的不同，数据端口并不总是<code>20</code>。这就是主动与被动<code>FTP</code>的最大不同之处。</p>
<h3 id="主动FTP"><a href="#主动FTP" class="headerlink" title="主动FTP"></a>主动FTP</h3><p>&emsp;&emsp;主动方式的<code>FTP</code>是这样的：客户端从一个任意的非特权端口<code>N</code>(<code>N &gt; 1024</code>)连接到<code>FTP</code>服务器的命令端口，也就是<code>21</code>端口。然后客户端开始监听端口<code>N + 1</code>，并发送<code>FTP</code>命令<code>port N+1</code>到<code>FTP</code>服务器。接着服务器会从它自己的数据端口<code>20</code>连接到客户端指定的数据端口<code>N + 1</code>。针对<code>FTP</code>服务器前面的防火墙来说，必须允许以下通讯才能支持主动方式<code>FTP</code>：</p>
<ul>
<li>任何大于<code>1024</code>的端口到<code>FTP</code>服务器的<code>21</code>端口(客户端初始化的连接)。</li>
<li><code>FTP</code>服务器的<code>21</code>端口到大于<code>1024</code>的端口(服务器响应客户端的控制端口)。</li>
<li><code>FTP</code>服务器的<code>20</code>端口到大于<code>1024</code>的端口(服务器端初始化数据连接到客户端的数据端口)。</li>
<li>大于<code>1024</code>端口到<code>FTP</code>服务器的<code>20</code>端口(客户端发送<code>ACK</code>响应到服务器的数据端口)。</li>
</ul>
<h3 id="被动FTP"><a href="#被动FTP" class="headerlink" title="被动FTP"></a>被动FTP</h3><p>&emsp;&emsp;为了解决服务器发起到客户的连接的问题，人们开发了一种不同的<code>FTP</code>连接方式。这就是所谓的被动方式，或者叫做<code>PASV</code>，当客户端通知服务器它处于被动模式时才启用。在被动方式<code>FTP</code>中，命令连接和数据连接都由客户端发起，这样就可以解决从服务器到客户端的数据端口的入方向连接被防火墙过滤掉的问题。<br>&emsp;&emsp;当开启一个<code>FTP</code>连接时，客户端打开两个任意的非特权本地端口(<code>N &gt; 1024</code>和<code>N + 1</code>）。第一个端口连接服务器的<code>21</code>端口，但与主动方式的<code>FTP</code>不同，客户端不会提交<code>PORT</code>命令并允许服务器来回连它的数据端口，而是提交<code>PASV</code>命令。这样做的结果是服务器会开启一个任意的非特权端口(<code>P &gt; 1024</code>)，并发送<code>PORT P</code>命令给客户端。然后客户端发起从本地端口<code>N + 1</code>到服务器的端口P的连接用来传送数据。<br>&emsp;&emsp;对于服务器端的防火墙来说，必须允许下面的通讯才能支持被动方式的<code>FTP</code>：</p>
<ul>
<li>从任何大于<code>1024</code>的端口到服务器的<code>21</code>端口(客户端初始化的连接)。</li>
<li>服务器的<code>21</code>端口到任何大于<code>1024</code>的端口(服务器响应到客户端的控制端口的连接)。</li>
<li>从任何大于<code>1024</code>端口到服务器的大于<code>1024</code>端口(客户端初始化数据连接到服务器指定的任意端口)。</li>
<li>服务器的大于<code>1024</code>端口到远程的大于<code>1024</code>的端口(服务器发送<code>ACK</code>响应和数据到客户端的数据端口)。</li>
</ul>
<h3 id="主动与被动FTP优缺点"><a href="#主动与被动FTP优缺点" class="headerlink" title="主动与被动FTP优缺点"></a>主动与被动FTP优缺点</h3><p>&emsp;&emsp;主动<code>FTP</code>对<code>FTP</code>服务器的管理有利，但对客户端的管理不利。因为<code>FTP</code>服务器企图与客户端的高位随机端口建立连接，而这个端口很有可能被客户端的防火墙阻塞掉。被动<code>FTP</code>对<code>FTP</code>客户端的管理有利，但对服务器端的管理不利。因为客户端要与服务器端建立两个连接，其中一个连到一个高位随机端口，而这个端口很有可能被服务器端的防火墙阻塞掉。<br>&emsp;&emsp;幸运的是，有折衷的办法。既然<code>FTP</code>服务器的管理员需要他们的服务器有最多的客户连接，那么必须得支持被动<code>FTP</code>。我们可以通过为<code>FTP</code>服务器指定一个有限的端口范围来减小服务器高位端口的暴露。这样，不在这个范围的任何端口会被服务器的防火墙阻塞。虽然这没有消除所有针对服务器的危险，但它大大减少了危险。<br>&emsp;&emsp;简而言之，主动模式是从服务器端向客户端发起连接；被动模式是客户端向服务器端发起连接。两者的共同点是都使用<code>21</code>端口进行用户验证及管理，差别在于传送数据的方式不同，<code>PORT</code>模式的<code>FTP</code>服务器数据端口固定在<code>20</code>，而<code>PASV</code>模式则在<code>1025</code>至<code>65535</code>之间随机。<br>&emsp;&emsp;<code>PORT</code>(主动)方式的连接过程是：客户端向服务器的<code>FTP</code>端口(默认是<code>21</code>)发送连接请求，服务器接受连接，建立一条命令链路。当需要传送数据时，客户端在命令链路上用<code>PORT</code>命令告诉服务器<code>我打开了XXXX端口，你过来连接我</code>。于是服务器从<code>20</code>端口向客户端的<code>XXXX</code>端口发送连接请求，建立一条数据链路来传送数据。<br>&emsp;&emsp;<code>PASV</code>(被动)方式的连接过程是：客户端向服务器的<code>FTP</code>端口(默认是<code>21</code>)发送连接请求，服务器接受连接，建立一条命令链路。当需要传送数据时，服务器在命令链路上用<code>PASV</code>命令告诉客户端<code>我打开了XXXX端口，你过来连接我</code>。于是客户端向服务器的<code>XXXX</code>端口发送连接请求，建立一条数据链路来传送数据。<br>&emsp;&emsp;所以如果你是如果通过代理上网的话，就不能用主动模式，因为服务器敲的是上网代理服务器的门，而不是敲客户端的门。而且有时候，客户端也不是轻易就开门的，因为有防火墙阻挡，除非客户端开放大于<code>1024</code>的高端端口。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/网络编程/ifreq结构体/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/网络编程/ifreq结构体/" itemprop="url">ifreq结构体</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T20:33:54+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;结构原型如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> &#123;</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">define</span> IFHWADDRLEN 6</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">union</span> &#123;</span><br><span class="line">        <span class="keyword">char</span> ifrn_name[IFNAMSIZ];</span><br><span class="line">    &#125; ifr_ifrn;</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">union</span> &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> <span class="title">ifru_addr</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> <span class="title">ifru_dstaddr</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> <span class="title">ifru_broadaddr</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> <span class="title">ifru_netmask</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> <span class="title">ifru_hwaddr</span>;</span></span><br><span class="line">        <span class="keyword">short</span> ifru_flags;</span><br><span class="line">        <span class="keyword">int</span> ifru_ivalue;</span><br><span class="line">        <span class="keyword">int</span> ifru_mtu;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ifmap</span> <span class="title">ifru_map</span>;</span></span><br><span class="line">        <span class="keyword">char</span> ifru_slave[IFNAMSIZ];</span><br><span class="line">        <span class="keyword">char</span> ifru_newname[IFNAMSIZ];</span><br><span class="line">        <span class="keyword">void</span> __user *ifru_data;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">if_settings</span> <span class="title">ifru_settings</span>;</span></span><br><span class="line">    &#125; ifr_ifru;</span><br><span class="line">&#125;;</span><br><span class="line">​</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_name      ifr_ifrn.ifrn_name</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_hwaddr    ifr_ifru.ifru_hwaddr</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_addr      ifr_ifru.ifru_addr</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_dstaddr   ifr_ifru.ifru_dstaddr</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_broadaddr ifr_ifru.ifru_broadaddr</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_netmask   ifr_ifru.ifru_netmask</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_flags     ifr_ifru.ifru_flags</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_metric    ifr_ifru.ifru_ivalue</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_mtu       ifr_ifru.ifru_mtu</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_map       ifr_ifru.ifru_map</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_slave     ifr_ifru.ifru_slave</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_data      ifr_ifru.ifru_data</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_ifindex   ifr_ifru.ifru_ivalue</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_bandwidth ifr_ifru.ifru_ivalue</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_qlen      ifr_ifru.ifru_ivalue</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_newname   ifr_ifru.ifru_newname</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ifr_settings  ifr_ifru.ifru_settings</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>ifreq</code>结构定义在<code>/usr/include/net/if.h</code>中，用来配置<code>ip</code>地址、激活接口、配置<code>MTU</code>等接口信息的。其中包含了一个接口的名 字和具体内容(它是个共用体，有可能是<code>IP</code>地址、广播地址、子网掩码、<code>MAC</code>号、<code>MTU</code>或其他内容)。<code>ifreq</code>包含在<code>ifconf</code>结构中，而<code>ifconf</code>结构通常是用来保存所有接口的信息的。<br>&emsp;&emsp;在<code>Linux</code>系统中，<code>ifconfig</code>命令是通过<code>ioctl</code>接口与内核通信。例如当系统管理员输入如下命令来改变接口<code>eth0</code>的<code>MTU</code>大小：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 mtu 1250</span><br></pre></td></tr></table></figure>
<p><code>ifconfig</code>命令首先打开一个<code>socket</code>，然后通过系统管理员输入的参数初始化一个数据结构，并通过<code>ioctl</code>调用将数据传送到内核。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> <span class="title">data</span>;</span></span><br><span class="line">fd = socket ( PF_INET, SOCK_DGRAM, <span class="number">0</span> );</span><br><span class="line"><span class="comment">/* initialize "data" */</span></span><br><span class="line">err = ioctl ( fd, SIOCSIFMTU, &amp;data ); <span class="comment">/* SIOCSIFMTU是命令标识符 */</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/Keras之函数式模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/Keras之函数式模型/" itemprop="url">Keras之函数式模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T18:34:38+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>Keras</code>函数式<code>API</code>是定义复杂模型(如多输出模型、有向无环图，或具有共享层的模型)的方法。只要你的模型不是类似<code>VGG</code>一条路走到黑的模型，或者你的模型需要多于一个的输出，那么你总应该选择函数式模型。函数式模型是最广泛的一类模型，序贯模型(<code>Sequential</code>)只是它的一种特殊情况。这部分的文档假设你已经对<code>Sequential</code>模型已经比较熟悉</p>
<h3 id="全连接网络"><a href="#全连接网络" class="headerlink" title="全连接网络"></a>全连接网络</h3><p>&emsp;&emsp;<code>Sequential</code>模型可能是实现这种网络的一个更好选择，但这个例子能够帮助我们进行一些简单的理解。在开始前，有几个概念需要说明：</p>
<ul>
<li>网络层的实例是可调用的，它以张量为参数，并且返回一个张量。</li>
<li>输入和输出均为张量，它们都可以用来定义一个模型。</li>
<li>这样的模型同<code>Keras</code>的<code>Sequential</code>模型一样，都可以被训练。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line">​</span><br><span class="line">inputs = Input(shape=(<span class="number">784</span>,))  <span class="comment"># 这部分返回一个张量</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 层的实例是可调用的，它以张量为参数，并且返回一个张量</span></span><br><span class="line">x = Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>)(inputs)</span><br><span class="line">x = Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>)(x)</span><br><span class="line">predictions = Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 这部分创建了一个包含输入层和三个全连接层的模型</span></span><br><span class="line">model = Model(inputs=inputs, outputs=predictions)</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(data, labels)  <span class="comment"># 开始训练</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;利用函数式<code>API</code>，可以轻松地重用训练好的模型：可以将任何模型看作是一个层，然后通过传递一个张量来调用它。注意，在调用模型时，您不仅重用模型的结构，还重用了它的权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = Input(shape=(<span class="number">784</span>,))</span><br><span class="line">y = model(x)  <span class="comment"># 这是可行的，并且返回上面定义的“10-way softmax”。</span></span><br></pre></td></tr></table></figure>
<p>这种方式能允许我们快速创建可以处理序列输入的模型。只需一行代码，你就将图像分类模型转换为视频分类模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> TimeDistributed</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 输入张量是20个时间步的序列，每一个时间为一个784维的向量</span></span><br><span class="line">input_sequences = Input(shape=(<span class="number">20</span>, <span class="number">784</span>))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 这部分将我们之前定义的模型应用于输入序列中的每个时间步。之前定义的模型的输出</span></span><br><span class="line"><span class="comment"># 是一个“10-way softmax”，因而下面的层的输出将是维度为10的20个向量的序列</span></span><br><span class="line">processed_sequences = TimeDistributed(model)(input_sequences)</span><br></pre></td></tr></table></figure>
<h3 id="层“节点”的概念"><a href="#层“节点”的概念" class="headerlink" title="层“节点”的概念"></a>层“节点”的概念</h3><p>&emsp;&emsp;每当你在某个输入上调用一个层时，都将创建一个新的张量(层的输出)，并且为该层添加一个<code>节点</code>，将输入张量连接到输出张量。当多次调用同一个图层时，该图层将拥有多个节点索引(<code>0, 1, 2...</code>)。<br>&emsp;&emsp;在之前版本的<code>Keras</code>中，可以通过<code>layer.get_output</code>来获得层实例的输出张量，或者通过<code>layer.output_shape</code>来获取其输出形状。现在你依然可以这么做(除了<code>get_output</code>已经被<code>output</code>属性替代)。但是如果一个层与多个输入连接呢？只要一个层只连接到一个输入，就不会有困惑，<code>output</code>会返回层的唯一输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line">​</span><br><span class="line">lstm = LSTM(<span class="number">32</span>)</span><br><span class="line">encoded_a = lstm(a)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">assert</span> lstm.output == encoded_a</span><br></pre></td></tr></table></figure>
<p>但是如果该层有多个输入，那就会出现问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line">b = Input(shape=(<span class="number">140</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">lstm = LSTM(<span class="number">32</span>)</span><br><span class="line">encoded_a = lstm(a)</span><br><span class="line">encoded_b = lstm(b)</span><br><span class="line"></span><br><span class="line">lstm.output</span><br><span class="line"></span><br><span class="line">&gt;&gt; AttributeError: Layer lstm_1 has multiple inbound nodes,</span><br><span class="line">hence the notion of <span class="string">"layer output"</span> <span class="keyword">is</span> ill-defined.</span><br><span class="line">Use `get_output_at(node_index)` instead.</span><br></pre></td></tr></table></figure>
<p>好吧，通过下面的方法可以解决：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> lstm.get_output_at(<span class="number">0</span>) == encoded_a</span><br><span class="line"><span class="keyword">assert</span> lstm.get_output_at(<span class="number">1</span>) == encoded_b</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>input_shape</code>和<code>output_shape</code>这两个属性也是如此：只要该层只有一个节点，或者只要所有节点具有相同的输入/输出尺寸，那么<code>input_shape</code>和<code>output_shape</code>都是没有歧义的，并且将由<code>layer.output_shape/layer.input_shape</code>返回。但如果将一个<code>Conv2D</code>层先应用于尺寸为(<code>32, 32, 3</code>)的输入，再应用于尺寸为(<code>64, 64, 3</code>)的输入，那么这个层就会有多个输入/输出尺寸，你将不得不通过指定它们所属节点的索引来获取它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))</span><br><span class="line">b = Input(shape=(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">conv = Conv2D(<span class="number">16</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>)</span><br><span class="line">conved_a = conv(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 到目前为止只有一个输入，以下可行：</span></span><br><span class="line"><span class="keyword">assert</span> conv.input_shape == (<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">conved_b = conv(b)</span><br><span class="line"><span class="comment"># 现在input_shape属性不可行，但是这样可以：</span></span><br><span class="line"><span class="keyword">assert</span> conv.get_input_shape_at(<span class="number">0</span>) == (<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">assert</span> conv.get_input_shape_at(<span class="number">1</span>) == (<span class="keyword">None</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Inception模型"><a href="#Inception模型" class="headerlink" title="Inception模型"></a>Inception模型</h3><p>&emsp;&emsp;有关<code>Inception</code>结构的更多信息，请参阅<code>Going Deeper with Convolutions</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D, Input</span><br><span class="line">​</span><br><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line">​</span><br><span class="line">tower_1 = Conv2D(<span class="number">64</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)(input_img)</span><br><span class="line">tower_1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)(tower_1)</span><br><span class="line">​</span><br><span class="line">tower_2 = Conv2D(<span class="number">64</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)(input_img)</span><br><span class="line">tower_2 = Conv2D(<span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)(tower_2)</span><br><span class="line">​</span><br><span class="line">tower_3 = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(input_img)</span><br><span class="line">tower_3 = Conv2D(<span class="number">64</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)(tower_3)</span><br><span class="line">​</span><br><span class="line">output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="卷积层上的残差连接"><a href="#卷积层上的残差连接" class="headerlink" title="卷积层上的残差连接"></a>卷积层上的残差连接</h3><p>&emsp;&emsp;有关残差网络(<code>Residual Network</code>)的更多信息，请参阅<code>Deep Residual Learning for Image Recognition</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, Input</span><br><span class="line">​</span><br><span class="line">x = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))  <span class="comment"># 输入张量为3通道“256 * 256”图像</span></span><br><span class="line"><span class="comment"># 3输出通道(与输入通道相同)的“3 * 3”卷积核</span></span><br><span class="line">y = Conv2D(<span class="number">3</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">z = keras.layers.add([x, y])  <span class="comment"># 返回“x + y”</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Model类API"><a href="#Model类API" class="headerlink" title="Model类API"></a>Model类API</h3><p>&emsp;&emsp;在函数式<code>API</code>中，给定一些输入张量和输出张量，可以通过以下方式实例化一个<code>Model</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</span><br><span class="line"></span><br><span class="line">a = Input(shape=(<span class="number">32</span>,))</span><br><span class="line">b = Dense(<span class="number">32</span>)(a)</span><br><span class="line">model = Model(inputs=a, outputs=b)</span><br></pre></td></tr></table></figure>
<p>这个模型将包含从<code>a</code>到<code>b</code>的计算的所有网络层。在多输入或多输出模型的情况下，你也可以使用列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Model(inputs=[a1, a2], outputs=[b1, b3, b3])</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>Model</code>的实用属性：<code>model.layers</code>是组成模型图的各个层；<code>model.inputs</code>是输入张量的列表；<code>model.outputs</code>是输出张量的列表。</p>
<h3 id="Model类模型方法"><a href="#Model类模型方法" class="headerlink" title="Model类模型方法"></a>Model类模型方法</h3><h4 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h4><p>&emsp;&emsp;该函数用于配置训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">compile(</span><br><span class="line">    self, optimizer, loss, metrics=<span class="keyword">None</span>, loss_weights=<span class="keyword">None</span>,</span><br><span class="line">    sample_weight_mode=<span class="keyword">None</span>, weighted_metrics=<span class="keyword">None</span>, target_tensors=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>optimizer</code>：字符串(预定义优化器名)或者优化器对象。</li>
<li><code>loss</code>：字符串(预定义损失函数名)或目标函数。如果模型具有多个输出，则可以通过传递损失函数的字典或列表，在每个输出上使用不同的损失。模型将最小化的损失值将是所有单个损失的总和。</li>
<li><code>metrics</code>：在训练和测试期间的模型评估标准，通常你会使用<code>metrics = [&#39;accuracy&#39;]</code>。要为多输出模型的不同输出指定不同的评估标准，还可以传递一个字典，如<code>metrics = {&#39;output_a&#39;: &#39;accuracy&#39;}</code>。</li>
<li><code>loss_weights</code>：可选的指定标量系数(<code>Python</code>浮点数)的列表或字典，用以衡量损失函数对不同的模型输出的贡献。模型将最小化的误差值是由<code>loss_weights</code>系数加权的加权总和误差。如果是列表，那么它应该是与模型输出相对应的<code>1:1</code>映射。如果是张量，那么应该把输出的名称(字符串)映到标量系数。</li>
<li><code>sample_weight_mode</code>：如果你需要执行按时间步采样权重(<code>2D</code>权重)，请将其设置为<code>temporal</code>。默认为<code>None</code>，为采样权重(<code>1D</code>)。如果模型有多个输出，则可以通过传递<code>mode</code>的字典或列表，以在每个输出上使用不同的<code>sample_weight_mode</code>。</li>
<li><code>weighted_metrics</code>：在训练和测试期间，这些<code>metrics</code>将由<code>sample_weight</code>或<code>clss_weight</code>计算并赋权。</li>
<li><code>target_tensors</code>：默认情况下，<code>Keras</code>将为模型的目标创建一个占位符，该占位符在训练过程中将被目标数据代替。相反，如果你想使用自己的目标张量(反过来说，<code>Keras</code>在训练期间不会载入这些目标张量的外部<code>Numpy</code>数据)，您可以通过<code>target_tensors</code>参数指定它们。它可以是单个张量(单输出模型)、张量列表、或一个映射输出名称到目标张量的字典。</li>
<li><code>**kwargs</code>：当使用<code>Theano/CNTK</code>后端时，这些参数被传入<code>K.function</code>。当使用<code>TensorFlow</code>后端时，这些参数被传递到<code>tf.Session.run</code>。</li>
</ul>
<p>&emsp;&emsp;如果你只是载入模型并利用其<code>predict</code>，可以不用进行<code>compile</code>。在<code>Keras</code>中，<code>compile</code>主要完成损失函数和优化器的一些配置，是为训练服务的。<code>predict</code>会在内部进行符号函数的编译工作(通过调用<code>_make_predict_function</code>生成函数)。</p>
<h4 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h4><p>&emsp;&emsp;该函数的作用是以固定数量的轮次(数据集上的迭代)训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fit(</span><br><span class="line">    self, x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, batch_size=<span class="keyword">None</span>, epochs=<span class="number">1</span>, verbose=<span class="number">1</span>, callbacks=<span class="keyword">None</span>,</span><br><span class="line">    validation_split=<span class="number">0.0</span>, validation_data=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>, class_weight=<span class="keyword">None</span>,</span><br><span class="line">    sample_weight=<span class="keyword">None</span>, initial_epoch=<span class="number">0</span>, steps_per_epoch=<span class="keyword">None</span>, validation_steps=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：训练数据的<code>Numpy</code>数组(如果模型只有一个输入)，或者是<code>Numpy</code>数组的列表(如果模型有多个输入)。如果模型中的输入层被命名，你也可以传递一个字典，将输入层名称映射到<code>Numpy</code>数组。如果从本地框架张量馈送(例如<code>TensorFlow</code>数据张量)数据，<code>x</code>可以是<code>None</code>(默认)。</li>
<li><code>y</code>：目标(标签)数据的<code>Numpy</code>数组(如果模型只有一个输出)，或者是<code>Numpy</code>数组的列表(如果模型有多个输出)。如果模型中的输出层被命名，你也可以传递一个字典，将输出层名称映射到<code>Numpy</code>数组。如果从本地框架张量馈送(例如<code>TensorFlow</code>数据张量)数据，<code>y</code>可以是<code>None</code>(默认)。</li>
<li><code>batch_size</code>：整数或<code>None</code>，每次梯度更新的样本数。</li>
<li><code>epochs</code>：整数，训练模型迭代轮次。一个轮次是在整个<code>x</code>或<code>y</code>上的一轮迭代。请注意，与<code>initial_epoch</code>一起，<code>epochs</code>被理解为<code>最终轮次</code>。模型并不是训练了<code>epochs</code>轮，而是到第<code>epochs</code>轮停止训练。</li>
<li><code>verbose</code>：日志显示模式，<code>0</code>为不在标准输出流输出日志信息，<code>1</code>为输出进度条记录，<code>2</code>为每个<code>epoch</code>输出一行记录。</li>
<li><code>callbacks</code>：一系列的<code>keras.callbacks.Callback</code>实例，一系列可以在训练时使用的回调函数。</li>
<li><code>validation_split</code>：在<code>0</code>和<code>1</code>之间浮动，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个<code>epoch</code>结束后测试的模型的指标，如损失函数、精确度等。注意，<code>validation_split</code>的划分在<code>shuffle</code>之后，因此如果你的数据本身是有序的，需要先手工打乱再指定<code>validation_split</code>，否则可能会出现验证集样本不均匀。</li>
<li><code>validation_data</code>：元组(<code>x_val, y_val</code>)或元组(<code>x_val, y_val, val_sample_weights</code>)，是指定的验证集。这个参数会覆盖<code>validation_split</code>。</li>
<li><code>shuffle</code>：布尔值(是否在每轮迭代之前混洗数据)或者字符串(<code>batch</code>)。<code>batch</code>是处理<code>HDF5</code>数据限制的特殊选项，它对一个<code>batch</code>内部的数据进行混洗。当<code>steps_per_epoch</code>非<code>None</code>时，这个参数无效。</li>
<li><code>class_weight</code>：可选的字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数(只能用于训练)。该参数在处理非平衡的训练数据(某些类的训练样本数很少)时，可以使得损失函数对样本数不足的数据更加关注。</li>
<li><code>sample_weight</code>：训练样本的可选<code>Numpy</code>权重数组，用于在训练时调整损失函数(仅用于训练)。可以传递一个<code>1D</code>的与样本等长的向量用于对样本进行<code>1</code>对<code>1</code>的加权，或者在面对时序数据时，传递一个的形式为(<code>samples, sequence_length</code>)的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了<code>sample_weight_mode = &#39;temporal&#39;</code>。</li>
<li><code>initial_epoch</code>：整数，从该参数指定的<code>epoch</code>开始训练，有助于恢复之前的训练。</li>
<li><code>steps_per_epoch</code>：整数或<code>None</code>，在声明一个轮次完成并开始下一个轮次之前的总步数(样品批次数)。使用<code>TensorFlow</code>数据张量等输入张量进行训练时，默认值<code>None</code>等于数据集中样本的数量除以<code>batch</code>的大小。</li>
<li><code>validation_steps</code>：只有在指定了<code>steps_per_epoch</code>时才有用，在验证集上的<code>step</code>总数。</li>
</ul>
<p>该函数返回一个<code>History</code>的对象，其<code>History.history</code>属性记录了损失函数和其他指标的数值随<code>epoch</code>变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况。</p>
<h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><p>&emsp;&emsp;该函数的作用是在测试模式下返回模型的误差值和评估标准值(计算是分批进行的)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evaluate(self, x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, batch_size=<span class="keyword">None</span>, verbose=<span class="number">1</span>, sample_weight=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：测试数据的<code>Numpy</code>数组(如果模型只有一个输入)，或者是<code>Numpy</code>数组的列表(如果模型有多个输入)。如果模型中的输入层被命名，你也可以传递一个字典，将输入层名称映射到<code>Numpy</code>数组。如果从本地框架张量馈送(例如<code>TensorFlow</code>数据张量)数据，<code>x</code>可以是<code>None</code>(默认)。</li>
<li><code>y</code>：目标(标签)数据的<code>Numpy</code>数组，或<code>Numpy</code>数组的列表(如果模型具有多个输出)。如果模型中的输出层被命名，你也可以传递一个字典，将输出层名称映射到<code>Numpy</code>数组。如果从本地框架张量馈送(例如<code>TensorFlow</code>数据张量)数据，<code>y</code>可以是<code>None</code>(默认)。</li>
<li><code>batch_size</code>：整数或<code>None</code>，每次梯度更新的样本数。</li>
<li><code>verbose</code>：日志显示模式，<code>0</code>为不在标准输出流输出日志信息，<code>1</code>为输出进度条记录。</li>
<li><code>sample_weight</code>：训练样本的可选<code>Numpy</code>权重数组，用于在训练时调整损失函数(仅用于训练)。可以传递一个<code>1D</code>的与样本等长的向量用于对样本进行<code>1</code>对<code>1</code>的加权，或者在面对时序数据时，传递一个的形式为(<code>samples, sequence_length</code>)的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了<code>sample_weight_mode = &#39;temporal&#39;</code>。</li>
</ul>
<p>该函数返回标量测试误差(如果模型只有一个输出且没有评估标准)或标量列表(如果模型具有多个输出和/或评估指标)。属性<code>model.metrics_names</code>将提供标量输出的显示标签。</p>
<h4 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h4><p>&emsp;&emsp;该函数的作用是为输入样本生成输出预测(计算是分批进行的)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(self, x, batch_size=<span class="keyword">None</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>该函数返回预测的<code>Numpy</code>数组(或数组列表)。</p>
<h4 id="train-on-batch"><a href="#train-on-batch" class="headerlink" title="train_on_batch"></a>train_on_batch</h4><p>&emsp;&emsp;该函数在一个<code>batch</code>的数据上进行一次参数更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_on_batch(self, x, y, sample_weight=<span class="keyword">None</span>, class_weight=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>该函数返回标量训练误差(如果模型只有一个输入且没有评估标准)，或者标量的列表(如果模型有多个输出和/或评估标准)。属性<code>model.metrics_names</code>将提供标量输出的显示标签。</p>
<h4 id="test-on-batch"><a href="#test-on-batch" class="headerlink" title="test_on_batch"></a>test_on_batch</h4><p>&emsp;&emsp;该函数在一个<code>batch</code>的样本上对模型进行评估：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_on_batch(self, x, y, sample_weight=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>该函数返回标量测试误差(如果模型只有一个输入且没有评估标准)，或者标量的列表(如果模型有多个输出<code>和/或</code>评估标准)。属性<code>model.metrics_names</code>将提供标量输出的显示标签。</p>
<h4 id="predict-on-batch"><a href="#predict-on-batch" class="headerlink" title="predict_on_batch"></a>predict_on_batch</h4><p>&emsp;&emsp;该函数返回一个<code>batch</code>的样本上的模型预测值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_on_batch(self, x)</span><br></pre></td></tr></table></figure>
<p>参数<code>x</code>是输入数据，<code>Numpy</code>数组。该函数返回预测值的<code>Numpy</code>数组(或数组列表)。</p>
<h4 id="fit-generator"><a href="#fit-generator" class="headerlink" title="fit_generator"></a>fit_generator</h4><p>&emsp;&emsp;该函数利用<code>Python</code>的生成器，逐个生成数据的<code>batch</code>并进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fit_generator(</span><br><span class="line">    self, generator, steps_per_epoch=<span class="keyword">None</span>, epochs=<span class="number">1</span>, verbose=<span class="number">1</span>, callbacks=<span class="keyword">None</span>,</span><br><span class="line">    validation_data=<span class="keyword">None</span>, validation_steps=<span class="keyword">None</span>, class_weight=<span class="keyword">None</span>, max_queue_size=<span class="number">10</span>,</span><br><span class="line">    workers=<span class="number">1</span>, use_multiprocessing=<span class="keyword">False</span>, shuffle=<span class="keyword">True</span>, initial_epoch=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>生成器与模型并行运行，以提高效率。例如，该函数允许我们在<code>CPU</code>上进行实时的数据提升，同时在<code>GPU</code>上进行模型训练。该函数返回一个<code>History</code>对象。<br>&emsp;&emsp;<code>keras.utils.Sequence</code>的使用可以保证数据的顺序，以及当<code>use_multiprocessing = True</code>时，保证每个输入在每个<code>epoch</code>只使用一次。</p>
<ul>
<li><code>generator</code>：一个生成器，或者一个<code>Sequence(keras.utils.Sequence)</code>对象的实例，为了在使用多进程时避免数据的重复。生成器的输出应该为以下之一：</li>
</ul>
<ol>
<li>一个(<code>inputs, targets</code>)元组。</li>
<li>一个(<code>inputs, targets, sample_weights</code>)元组。所有的返回值都应该包含相同数目的样本。生成器将无限在数据集上循环。每个<code>epoch</code>以经过模型的样本数达到<code>samples_per_epoch</code>时，记一个<code>epoch</code>结束。</li>
</ol>
<ul>
<li><code>steps_per_epoch</code>：在声明一个<code>epoch</code>完成并开始下一个<code>epoch</code>之前从<code>generator</code>产生的总步数(批次样本)。它通常应该等于你的数据集的样本数量除以批量大小。对于<code>Sequence</code>，它是可选的：如果未指定，将使用<code>len(generator)</code>作为步数。</li>
<li><code>epochs</code>：整数，数据的迭代总轮数。请注意，与<code>initial_epoch</code>一起，参数<code>epochs</code>应被理解为<code>最终轮数</code>。模型并不是训练了<code>epochs</code>轮，而是到第<code>epochs</code>轮停止训练。</li>
<li><code>verbose</code>：日志显示模式，<code>0</code>为不在标准输出流输出日志信息，<code>1</code>为输出进度条记录，<code>2</code>为每个<code>epoch</code>输出一行记录。</li>
<li><code>callbacks</code>：在训练时调用的一系列回调函数。</li>
<li><code>validation_data</code>：它可以是以下之一：</li>
</ul>
<ol>
<li>验证数据的生成器。</li>
<li>一个(<code>inputs, targets</code>)元组。</li>
<li>一个(<code>inputs, targets, sample_weights</code>)元组。</li>
</ol>
<ul>
<li><code>validation_steps</code>：当<code>validation_data</code>为生成器时，本参数指定验证集的生成器返回次数。它通常应该等于你的数据集的样本数量除以批量大小。可选参数<code>Sequence</code>：如果未指定，将使用<code>len(generator)</code>作为步数。</li>
<li><code>class_weight</code>：将类别索引映射为权重的字典。</li>
<li><code>max_queue_size</code>：整数，生成器队列的最大尺寸。如未指定，<code>max_queue_size</code>将默认为<code>10</code>。</li>
<li><code>workers</code>：整数，使用的最大进程数量，如果使用基于进程的多线程。如未指定，<code>workers</code>将默认为<code>1</code>。如果为<code>0</code>，将在主线程上执行生成器。</li>
<li><code>use_multiprocessing</code>：布尔值。如果<code>True</code>，则使用基于进程的多线程。如未指定，<code>use_multiprocessing</code>将默认为<code>False</code>。请注意，由于此实现依赖于多进程，所以不应将不可传递的参数传递给生成器，因为它们不能被轻易地传递给子进程。</li>
<li><code>shuffle</code>：是否在每轮迭代之前打乱<code>batch</code>的顺序，只能与<code>Sequence(keras.utils.Sequence)</code>实例同用。</li>
<li><code>initial_epoch</code>：从该参数指定的<code>epoch</code>开始训练，有助于恢复之前的训练。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_arrays_from_file</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        f = open(path)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="comment"># 从文件中的每一行生成输入数据和标签的numpy数组</span></span><br><span class="line">            x1, x2, y = process_line(line)</span><br><span class="line">            <span class="keyword">yield</span> (&#123;<span class="string">'input_1'</span>: x1, <span class="string">'input_2'</span>: x2&#125;, &#123;<span class="string">'output'</span>: y&#125;)</span><br><span class="line">        f.close()</span><br><span class="line">​</span><br><span class="line">model.fit_generator(generate_arrays_from_file(<span class="string">'/my_file.txt'</span>), steps_per_epoch=<span class="number">10000</span>, epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h4 id="evaluate-generator"><a href="#evaluate-generator" class="headerlink" title="evaluate_generator"></a>evaluate_generator</h4><p>&emsp;&emsp;该函数在数据生成器上评估模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">evaluate_generator(</span><br><span class="line">    self, generator, steps=<span class="keyword">None</span>, max_queue_size=<span class="number">10</span>,</span><br><span class="line">    workers=<span class="number">1</span>, use_multiprocessing=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>这个生成器应该返回与<code>test_on_batch</code>的输入数据相同类型的数据。</p>
<ul>
<li><code>generator</code>：一个生成(<code>inputs, targets</code>)或(<code>inputs, targets, sample_weights</code>)的生成器，或一个<code>Sequence(keras.utils.Sequence)</code>对象的实例，为了避免在使用多进程时数据的重复。</li>
<li><code>steps</code>：在声明一个<code>epoch</code>完成并开始下一个<code>epoch</code>之前从<code>generator</code>产生的总步数(批次样本数)。它通常应该等于你的数据集的样本数量除以批量大小。对于<code>Sequence</code>，它是可选的：如果未指定，将使用<code>len(generator)</code>作为步数。</li>
</ul>
<p>该函数返回标量测试误差(如果模型只有一个输入且没有评估标准)，或者标量的列表(如果模型有多个输出和/或评估标准)。属性<code>model.metrics_names</code>将提供标量输出的显示标签。</p>
<h4 id="predict-generator"><a href="#predict-generator" class="headerlink" title="predict_generator"></a>predict_generator</h4><p>&emsp;&emsp;该函数为来自数据生成器的输入样本生成预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predict_generator(</span><br><span class="line">    self, generator, steps=<span class="keyword">None</span>, max_queue_size=<span class="number">10</span>,</span><br><span class="line">    workers=<span class="number">1</span>, use_multiprocessing=<span class="keyword">False</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>这个生成器应返回与<code>predict_on_batch</code>的输入数据相同类型的数据。该函数返回预测值的<code>Numpy</code>数组(或数组列表)。</p>
<h4 id="get-layer"><a href="#get-layer" class="headerlink" title="get_layer"></a>get_layer</h4><p>&emsp;&emsp;该函数根据名称(唯一)或索引值查找网络层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_layer(self, name=<span class="keyword">None</span>, index=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>根据网络层的名称(唯一)或其索引返回该层，索引是基于水平图遍历的顺序(自下而上)。参数<code>name</code>是字符串，即层的名字；参数<code>index</code>是整数，即层的索引。该函数返回一个层实例。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/TensorFlow之layers模块/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/TensorFlow之layers模块/" itemprop="url">TensorFlow之layers模块</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T14:56:39+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;<code>TensorFlow</code>的<code>layers</code>模块提供用于深度学习的更高层次封装的<code>API</code>，利用它可以轻松地构建模型。<code>tf.layers</code>模块提供的方法有：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Input</code></td>
<td>用于实例化一个输入<code>Tensor</code>，作为神经网络的输入</td>
</tr>
<tr>
<td><code>average_pooling1d</code></td>
<td>一维平均池化层</td>
</tr>
<tr>
<td><code>average_pooling2d</code></td>
<td>二维平均池化层</td>
</tr>
<tr>
<td><code>average_pooling3d</code></td>
<td>三维平均池化层</td>
</tr>
<tr>
<td><code>batch_normalization</code></td>
<td>批量标准化层</td>
</tr>
<tr>
<td><code>conv1d</code></td>
<td>一维卷积层</td>
</tr>
<tr>
<td><code>conv2d</code></td>
<td>二维卷积层</td>
</tr>
<tr>
<td><code>conv2d_transpose</code></td>
<td>二维反卷积层</td>
</tr>
<tr>
<td><code>conv3d</code></td>
<td>三维卷积层</td>
</tr>
<tr>
<td><code>conv3d_transpose</code></td>
<td>三维反卷积层</td>
</tr>
<tr>
<td><code>dense</code></td>
<td>全连接层</td>
</tr>
<tr>
<td><code>dropout</code></td>
<td><code>Dropout</code>层</td>
</tr>
<tr>
<td><code>flatten</code></td>
<td><code>Flatten</code>层，把一个<code>Tensor</code>展平</td>
</tr>
<tr>
<td><code>max_pooling1d</code></td>
<td>一维最大池化层</td>
</tr>
<tr>
<td><code>max_pooling2d</code></td>
<td>二维最大池化层</td>
</tr>
<tr>
<td><code>max_pooling3d</code></td>
<td>三维最大池化层</td>
</tr>
<tr>
<td><code>separable_conv2d</code></td>
<td>二维深度可分离卷积层</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>&emsp;&emsp;<code>tf.layers.Input</code>(目前已更名为<code>tf.keras.Input</code>)这个方法用于输入数据，类似于<code>tf.placeholder</code>，相当于一个占位符，可以通过传入<code>tensor</code>参数来进行赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input(</span><br><span class="line">    shape=<span class="keyword">None</span>, batch_size=<span class="keyword">None</span>, name=<span class="keyword">None</span>,</span><br><span class="line">    dtype=tf.float32, sparse=<span class="keyword">False</span>, tensor=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>shape</code>：可选参数，是一个由数字组成的元组或列表。这个<code>shape</code>比较特殊，它不包含<code>batch_size</code>，比如传入的<code>shape</code>为<code>[32]</code>，那么它会将<code>shape</code>转化为<code>[?, 32]</code>。</li>
<li><code>batch_size</code>：可选参数，代表输入数据的<code>batch_size</code>，可以是数字或者<code>None</code>。</li>
<li><code>name</code>：可选参数，输入层的名称。</li>
<li><code>dtype</code>：可选参数，元素的类型。</li>
<li><code>sparse</code>：可选参数，指定是否以稀疏矩阵的形式来创建<code>placeholder</code>。</li>
<li><code>tensor</code>：可选参数，如果指定的话，那么创建的内容便不再是一个<code>placeholder</code>，会用此<code>Tensor</code>初始化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.dense(x, <span class="number">16</span>, activation=tf.nn.softmax)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>首先用<code>Input</code>方法初始化了一个<code>placeholder</code>，注意这时我们没有传入<code>tensor</code>参数。然后调用了<code>dense</code>方法构建了一个全连接网络，激活函数使用<code>softmax</code>。执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 32), dtype=float32)</span><br><span class="line">Tensor("dense/Softmax:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>注意此时<code>shape</code>给我们做了转化，本来是<code>[32]</code>，结果转化成了<code>[?, 32]</code>，第一维代表<code>batch_size</code>。所以我们需要注意，在调用此方法时不需要去关心<code>batch_size</code>这一维。<br>&emsp;&emsp;如果我们在初始化时传入一个<code>Tensor</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">data = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = tf.keras.Input(tensor=data)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<p>执行结果如下，可以看到它可以自动计算出其<code>shape</code>和<code>dtype</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"Const:0"</span>, shape=(<span class="number">3</span>,), dtype=int32)</span><br></pre></td></tr></table></figure>
<h3 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch_normalization"></a>batch_normalization</h3><p>&emsp;&emsp;此方法是批量标准化的方法，对数据经过处理之后可以加快训练速度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_normalization(</span><br><span class="line">    inputs, axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="keyword">True</span>, scale=<span class="keyword">True</span>,</span><br><span class="line">    beta_initializer=tf.zeros_initializer(), gamma_initializer=tf.ones_initializer(),</span><br><span class="line">    moving_mean_initializer=tf.zeros_initializer(), moving_variance_initializer=tf.ones_initializer(),</span><br><span class="line">    beta_regularizer=<span class="keyword">None</span>, gamma_regularizer=<span class="keyword">None</span>, beta_constraint=<span class="keyword">None</span>, gamma_constraint=<span class="keyword">None</span>,</span><br><span class="line">    training=<span class="keyword">False</span>, trainable=<span class="keyword">True</span>, name=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>, renorm=<span class="keyword">False</span>, renorm_clipping=<span class="keyword">None</span>,</span><br><span class="line">    renorm_momentum=<span class="number">0.99</span>, fused=<span class="keyword">None</span>, virtual_batch_size=<span class="keyword">None</span>, adjustment=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：输入数据。</li>
<li><code>axis</code>：可选参数，进行标准化操作时操作数据的维度。</li>
<li><code>momentum</code>：可选参数，动态均值的动量。</li>
<li><code>epsilon</code>：可选参数，大于<code>0</code>的小浮点数，用于防止除<code>0</code>错误。</li>
<li><code>center</code>：可选参数，若设为<code>True</code>，将会把<code>beta</code>作为偏置加上去，否则忽略参数<code>beta</code>。</li>
<li><code>scale</code>：可选参数，若设为<code>True</code>，则会乘以<code>gamma</code>，否则不使用<code>gamma</code>。</li>
<li><code>beta_initializer</code>：可选参数，<code>beta</code>权重的初始方法。</li>
<li><code>gamma_initializer</code>：可选参数，<code>gamma</code>的初始化方法。</li>
<li><code>moving_mean_initializer</code>：可选参数，动态均值的初始化方法。</li>
<li><code>moving_variance_initializer</code>：可选参数，动态方差的初始化方法。</li>
<li><code>beta_regularizer</code>: 可选参数，<code>beta</code>的正则化方法。</li>
<li><code>gamma_regularizer</code>: 可选参数，<code>gamma</code>的正则化方法。</li>
<li><code>beta_constraint</code>: 可选参数，加在<code>beta</code>上的约束项。</li>
<li><code>gamma_constraint</code>: 可选参数，加在<code>gamma</code>上的约束项。</li>
<li><code>training</code>：可选参数，返回结果是<code>training</code>模式。</li>
<li><code>trainable</code>：可选参数，布尔类型。如果为<code>True</code>，则将变量添加到<code>GraphKeys.TRAINABLE_VARIABLES</code>中。</li>
<li><code>name</code>：可选参数，层名称。</li>
<li><code>reuse</code>：可选参数，根据层名判断是否重复利用。</li>
<li><code>renorm</code>：可选参数，是否要用<code>Batch Renormalization</code>。</li>
<li><code>renorm_clipping</code>：可选参数，是否要用<code>rmax</code>、<code>rmin</code>、<code>dmax</code>来<code>scalar Tensor</code>。</li>
<li><code>renorm_momentum</code>：可选参数，用来更新动态均值和标准差的<code>Momentum</code>值。</li>
<li><code>fused</code>：可选参数，是否使用一个更快的、融合的实现方法。</li>
<li><code>virtual_batch_size</code>：可选参数，<code>int</code>类型数字，指定一个虚拟<code>batch size</code>。</li>
<li><code>adjustment</code>：可选参数，对标准化后的结果进行适当调整。详细用法参考<code>https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization</code>。</li>
</ul>
<p>该函数的用法是在输入数据后面加一层<code>batch_normalization</code>即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">x = tf.layers.batch_normalization(x)</span><br><span class="line">y = tf.layers.dense(x, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h3 id="dense"><a href="#dense" class="headerlink" title="dense"></a>dense</h3><p>&emsp;&emsp;<code>dense</code>是全连接网络，<code>layers</code>模块提供了一个<code>dense</code>方法来实现此操作，定义在<code>tensorflow/python/layers/core.py</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dense(</span><br><span class="line">    inputs, units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(), kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">    bias_constraint=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, name=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：输入数据。</li>
<li><code>units</code>：神经元的数量。</li>
<li><code>activation</code>：可选参数，如果为<code>None</code>，则是线性激活。</li>
<li><code>use_bias</code>：可选参数，是否使用偏置。</li>
<li><code>kernel_initializer</code>：可选参数，权重的初始化方法。如果为<code>None</code>，则使用默认的<code>Xavier</code>初始化方法。</li>
<li><code>bias_initializer</code>：可选参数，偏置的初始化方法。</li>
<li><code>kernel_regularizer</code>：可选参数，施加在权重上的正则项。</li>
<li><code>bias_regularizer</code>：可选参数，施加在偏置上的正则项。</li>
<li><code>activity_regularizer</code>：可选参数，施加在输出上的正则项。</li>
<li><code>kernel_constraint</code>：可选参数，施加在权重上的约束项。</li>
<li><code>bias_constraint</code>：可选参数，施加在偏置上的约束项。</li>
<li><code>trainable</code>：可选参数，布尔类型。如果为<code>True</code>，则将变量添加到<code>GraphKeys.TRAINABLE_VARIABLES</code>中。</li>
<li><code>name</code>：可选参数，卷积层的名称。</li>
<li><code>reuse</code>：可选参数，布尔类型。</li>
</ul>
<p>该函数返回全连接网络处理后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">print(x)</span><br><span class="line">y1 = tf.layers.dense(x, <span class="number">16</span>, activation=tf.nn.relu)</span><br><span class="line">print(y1)</span><br><span class="line">y2 = tf.layers.dense(y1, <span class="number">5</span>, activation=tf.nn.sigmoid)</span><br><span class="line">print(y2)</span><br></pre></td></tr></table></figure>
<p>首先我们用<code>Input</code>定义了<code>[?, 32]</code>的输入数据，然后经过第一层全连接网络，此时指定了神经元个数为<code>16</code>，激活函数为<code>relu</code>。接着输出结果经过第二层全连接网络，此时指定了神经元个数为<code>5</code>，激活函数为<code>sigmoid</code>。执行结果如下，可以看到输出结果的最后一维度就等于神经元的个数：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 32), dtype=float32)</span><br><span class="line">Tensor("dense/Relu:0", shape=(?, 16), dtype=float32)</span><br><span class="line">Tensor("dense_1/Sigmoid:0", shape=(?, 5), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="convolution"><a href="#convolution" class="headerlink" title="convolution"></a>convolution</h3><p>&emsp;&emsp;<code>convolution</code>就是卷积，<code>layers</code>层提供了多个卷积方法，例如<code>conv1d</code>、<code>conv2d</code>和<code>conv3d</code>分别代表一维、二维、三维卷积。另外还有<code>conv2d_transpose</code>、<code>conv3d_transpose</code>，分别代表二维和三维反卷积，还有<code>separable_conv2d</code>方法代表二维深度可分离卷积。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    inputs, filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(), kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">    trainable=<span class="keyword">True</span>, name=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：需要进行操作的输入数据。</li>
<li><code>filters</code>：输出通道的个数，即<code>output_channels</code>。</li>
<li><code>kernel_size</code>：卷积核大小，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>strides</code>：可选参数，卷积步长，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>padding</code>：可选参数，<code>padding</code>的模式，有<code>valid</code>和<code>same</code>两种，大小写不区分。</li>
<li><code>data_format</code>：可选参数，分为<code>channels_last</code>和<code>channels_first</code>两种模式，代表了输入数据的维度类型。如果是<code>channels_last</code>，那么输入数据的<code>shape</code>为(<code>batch, height, width, channels</code>)；如果是<code>channels_first</code>，那么输入数据的<code>shape</code>为(<code>batch, channels, height, width</code>)。</li>
<li><code>dilation_rate</code>：可选参数，卷积的扩张率。例如当扩张率为<code>2</code>时，卷积核内部就会有边距，<code>3 * 3</code>的卷积核就会变成<code>5 * 5</code>。</li>
<li><code>activation</code>：可选参数。如果为<code>None</code>，则是线性激活。</li>
<li><code>use_bias</code>：可选参数，是否使用偏置。</li>
<li><code>kernel_initializer</code>：可选参数，权重的初始化方法。如果为<code>None</code>，则使用默认的<code>Xavier</code>初始化方法。</li>
<li><code>bias_initializer</code>：可选参数，偏置的初始化方法。</li>
<li><code>kernel_regularizer</code>：可选参数，施加在权重上的正则项。</li>
<li><code>bias_regularizer</code>：可选参数，施加在偏置上的正则项。</li>
<li><code>activity_regularizer</code>：可选参数，施加在输出上的正则项。</li>
<li><code>kernel_constraint</code>：可选参数，施加在权重上的约束项。</li>
<li><code>bias_constraint</code>：可选参数，施加在偏置上的约束项。</li>
<li><code>trainable</code>：可选参数，布尔类型。如果为<code>True</code>，则将变量添加到<code>GraphKeys.TRAINABLE_VARIABLES</code>中。</li>
<li><code>name</code>：可选参数，卷积层的名称。</li>
<li><code>reuse</code>：可选参数，布尔类型。</li>
</ul>
<p>该函数返回卷积后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>首先声明了一个<code>[?, 20, 20, 3]</code>的输入<code>x</code>，然后将其传给<code>conv2d</code>方法。<code>filters</code>设定为<code>6</code>，即输出通道为<code>6</code>；<code>kernel_size</code>为<code>2</code>，即卷积核大小为<code>2 * 2</code>；<code>padding</code>方式设置为<code>same</code>，那么输出结果的宽高和原来一定是相同的，结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 20, 20, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果我们让<code>padding</code>使用默认的<code>valid</code>模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果如下所示，这是因为步长默认为<code>1</code>，卷积核大小为<code>2 * 2</code>，所以得到的结果的高宽即为<code>(20 - (2 - 1)) * (20 - (2 - 1)) = 19 * 19</code>：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 19, 19, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>对于卷积核的大小，我们可以传入一个列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 19, 18, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>这时卷积核的大小变成了<code>2 * 3</code>，即高为<code>2</code>，宽为<code>3</code>，结果就变成了<code>[?, 19, 18, 6]</code>。这是因为步长默认为<code>1</code>，卷积核大小为<code>2 * 2</code>，所以结果的高宽即为<code>(20 - (2 - 1)) * (20 - (3 - 1)) = 19 * 18</code>。<br>&emsp;&emsp;对于步长，我们也可以传入一个列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=[<span class="number">2</span>, <span class="number">3</span>], strides=[<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>这时卷积核大小变成了<code>2 * 3</code>，步长变成了<code>2 * 2</code>，所以结果的高宽为<code>ceil(20 - (2 - 1)) / 2 * ceil(20 - (3 - 1)) / 2 = 10 * 9</code>，得到的结果即为<code>[?, 10, 9, 6]</code>：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 10, 9, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>我们还可以传入激活函数，或者禁用<code>bias</code>等操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, activation=tf.nn.relu, use_bias=<span class="keyword">False</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d/Relu:0", shape=(?, 19, 19, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;另外还有反卷积操作，反卷积顾名思义即卷积的反向操作，即输入卷积的结果，输出卷积前的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">y = tf.layers.conv2d_transpose(x, filters=<span class="number">6</span>, kernel_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>例如此处输入的图像高宽为<code>20 * 20</code>，经过卷积核为<code>2</code>，步长为<code>2</code>的反卷积处理，得到的结果高宽就变为了<code>40 * 40</code>，执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("conv2d_transpose/BiasAdd:0", shape=(?, 40, 40, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><p>&emsp;&emsp;<code>pooling</code>即池化层，<code>layers</code>模块提供了多个池化方法，这些池化方法都是类似的，包括<code>max_pooling1d</code>、<code>max_pooling2d</code>、<code>max_pooling3d</code>、<code>average_pooling1d</code>、<code>average_pooling2d</code>和<code>average_pooling3d</code>，分别代表一维、二维、三维、最大和平均池化方法，它们都定义在<code>tensorflow/python/layers/pooling.py</code>中。这里以<code>max_pooling2d</code>方法为例进行介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">max_pooling2d(</span><br><span class="line">    inputs, pool_size, strides, padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>: 需要池化的输入对象，必须是4维的。</li>
<li><code>pool_size</code>：池化窗口大小，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>strides</code>：池化步长，必须是一个数字(高和宽都是此数字)或者长度为<code>2</code>的列表(分别代表高、宽)。</li>
<li><code>padding</code>：可选参数，<code>padding</code>的方法，可选<code>valid</code>或者<code>same</code>，大小写不区分。</li>
<li><code>data_format</code>：可选参数，分为<code>channels_last</code>和<code>channels_first</code>两种模式，代表了输入数据的维度类型。如果是<code>channels_last</code>，那么输入数据的<code>shape</code>为(<code>batch, height, width, channels</code>)；如果是<code>channels_first</code>，那么输入数据的<code>shape</code>为(<code>batch, channels, height, width</code>)。</li>
<li><code>name</code>：可选参数，池化层的名称。</li>
</ul>
<p>该函数返回经过池化处理后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">20</span>, <span class="number">20</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.conv2d(x, filters=<span class="number">6</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">print(y)</span><br><span class="line">p = tf.layers.max_pooling2d(y, pool_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br><span class="line">print(p)</span><br></pre></td></tr></table></figure>
<p>首先指定了输入<code>x</code>的<code>shape</code>为<code>[20, 20, 3]</code>，然后对其进行了卷积以及池化操作，最后得到池化后的结果。执行结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 20, 20, 3), dtype=float32)</span><br><span class="line">Tensor("conv2d/BiasAdd:0", shape=(?, 20, 20, 6), dtype=float32)</span><br><span class="line">Tensor("max_pooling2d/MaxPool:0", shape=(?, 10, 10, 6), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>这里池化窗口的大小是<code>2 * 2</code>，步长也是<code>2</code>，所以原本卷积后的<code>shape</code>为<code>[?, 20, 20, 6]</code>，结果就变成了<code>[?, 10, 10, 6]</code>。</p>
<h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>&emsp;&emsp;<code>dropout</code>是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，可以用来防止过拟合。<code>layers</code>模块提供了<code>dropout</code>方法来实现这一操作，定义在<code>tensorflow/python/layers/core.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dropout(inputs, rate=<span class="number">0.5</span>, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>, training=<span class="keyword">False</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：输入数据。</li>
<li><code>rate</code>：可选参数，即<code>dropout rate</code>。如果设置为<code>0.1</code>，则会丢弃<code>10%</code>的神经元。</li>
<li><code>noise_shape</code>：可选参数，<code>int32</code>类型的一维<code>Tensor</code>，它代表了<code>dropout mask</code>的<code>shape</code>。</li>
<li><code>seed</code>：可选参数，产生随机数的种子值。</li>
<li><code>training</code>：可选参数，布尔类型，代表是否标志为<code>training</code>模式。</li>
<li><code>name</code>：可选参数，<code>dropout</code>层的名称。</li>
</ul>
<p>该函数返回经过<code>dropout</code>层之后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">32</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.dense(x, <span class="number">16</span>, activation=tf.nn.softmax)</span><br><span class="line">print(y)</span><br><span class="line">d = tf.layers.dropout(y, rate=<span class="number">0.2</span>)</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure>
<p>这里我们使用<code>dropout</code>方法实现了<code>droput</code>操作，并制定<code>dropout rate</code>为<code>0.2</code>，最后输出结果的<code>shape</code>和原来是一致的：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 32), dtype=float32)</span><br><span class="line">Tensor("dense/Softmax:0", shape=(?, 16), dtype=float32)</span><br><span class="line">Tensor("dropout/Identity:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h3><p>&emsp;&emsp;<code>flatten</code>方法可以对<code>Tensor</code>进行展平操作，定义在<code>tensorflow/python/layers/core.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatten(inputs, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>inputs</code>是输入数据，<code>name</code>是该层的名称，该函数返回展平后的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.keras.Input(shape=[<span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.flatten(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>这里输入数据的<code>shape</code>为<code>[?, 5, 6]</code>，经过<code>flatten</code>层之后，就会变成<code>[?, 30]</code>，也就是将除了第一维的数据维度相乘，对原<code>Tensor</code>进行展平：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor("input_1:0", shape=(?, 5, 6), dtype=float32)</span><br><span class="line">Tensor("flatten/Reshape:0", shape=(?, 30), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>如果第一维是一个已知数据的话，它依然进行同样的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(shape=[<span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>], dtype=tf.float32)</span><br><span class="line">print(x)</span><br><span class="line">y = tf.layers.flatten(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"Placeholder:0"</span>, shape=(<span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">"flatten/Reshape:0"</span>, shape=(<span class="number">5</span>, <span class="number">12</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/TensorFlow指定设备/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/TensorFlow指定设备/" itemprop="url">TensorFlow指定设备</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T14:08:23+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="支持的设备"><a href="#支持的设备" class="headerlink" title="支持的设备"></a>支持的设备</h3><p>&emsp;&emsp;在一套标准的系统上通常有多个计算设备，<code>TensorFlow</code>支持<code>CPU</code>和GPU这两种设备。我们用指定字符串<code>strings</code>来标识这些设备：</p>
<ul>
<li><code>/cpu:0</code>：机器中的<code>CPU</code>。</li>
<li><code>/gpu:0</code>：机器中的<code>GPU</code>，如果你有一个的话。</li>
<li><code>/gpu:1</code>：机器中的第二个<code>GPU</code>，以此类推。</li>
</ul>
<p>如果一个<code>TensorFlow</code>的<code>operation</code>中兼有<code>CPU</code>和<code>GPU</code>的实现，当这个算子被指派设备时，<code>GPU</code>有优先权。</p>
<h3 id="记录设备指派情况"><a href="#记录设备指派情况" class="headerlink" title="记录设备指派情况"></a>记录设备指派情况</h3><p>&emsp;&emsp;为了获取你的<code>operations</code>和<code>Tensor</code>被指派到哪个设备上运行，用<code>log_device_placement</code>新建一个<code>session</code>，并设置为<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 新建一个graph</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># 新建“session with log_device_placement”，并设置为True</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(c))  <span class="comment"># 运行这个op</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, \</span><br><span class="line">    name: GeForce MX150,</span><br><span class="line">pci bus id: <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span>, compute capability: <span class="number">6.1</span></span><br><span class="line">MatMul: (MatMul): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">b: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">a: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">[[<span class="number">22.</span> <span class="number">28.</span>]</span><br><span class="line"> [<span class="number">49.</span> <span class="number">64.</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="手工指派设备"><a href="#手工指派设备" class="headerlink" title="手工指派设备"></a>手工指派设备</h3><p>&emsp;&emsp;如果你不想使用系统来为<code>operation</code>指派设备，而是手工指派设备，可以用<code>with tf.device</code>创建一个设备环境，这个环境下的<code>operation</code>都统一运行在环境指定的设备上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">    b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">​</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, \</span><br><span class="line">    name: GeForce MX150,</span><br><span class="line">pci bus id: <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span>, compute capability: <span class="number">6.1</span></span><br><span class="line">MatMul: (MatMul): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span></span><br><span class="line">b: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span></span><br><span class="line">a: (Const): /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span></span><br><span class="line">[[<span class="number">22.</span> <span class="number">28.</span>]</span><br><span class="line"> [<span class="number">49.</span> <span class="number">64.</span>]]</span><br></pre></td></tr></table></figure>
<p>你会发现<code>a</code>和<code>b</code>操作都被指派给了<code>cpu:0</code>。<br>&emsp;&emsp;为了避免出现指定设备不存在的情况，可以在创建的<code>session</code>里把参数<code>allow_soft_placement</code>设置为<code>True</code>，这样<code>tensorFlow</code>会自动选择一个存在并且支持的设备来运行<code>operation</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:2'</span>):</span><br><span class="line">    a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">    b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>,</span><br><span class="line">                                        log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure>
<h3 id="使用多个GPU"><a href="#使用多个GPU" class="headerlink" title="使用多个GPU"></a>使用多个GPU</h3><p>&emsp;&emsp;如果你想让<code>TensorFlow</code>在多个<code>GPU</code>上运行，你可以建立<code>multi-tower</code>结构，在这个结构里，每个<code>tower</code>分别被指配给不同的<code>GPU</code>运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">c = []</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> [<span class="string">'/gpu:2'</span>, <span class="string">'/gpu:3'</span>]:</span><br><span class="line">    <span class="keyword">with</span> tf.device(d):</span><br><span class="line">        a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">        b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">        c.append(tf.matmul(a, b))</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    sum = tf.add_n(c)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 新建“session with log_device_placement”，并设置为True</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line">print(sess.run(sum))  <span class="comment"># 运行这个op</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">02</span>:<span class="number">00.0</span></span><br><span class="line">/job:localhost/replica:0/task:0/gpu:1 -&gt; device: 1, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">03</span>:<span class="number">00.0</span></span><br><span class="line">/job:localhost/replica:0/task:0/gpu:2 -&gt; device: 2, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">83</span>:<span class="number">00.0</span></span><br><span class="line">/job:localhost/replica:0/task:0/gpu:3 -&gt; device: 3, name: Tesla K20m, \</span><br><span class="line">    pci bus id: <span class="number">0000</span>:<span class="number">84</span>:<span class="number">00.0</span></span><br><span class="line">Const_3: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">3</span></span><br><span class="line">Const_2: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">3</span></span><br><span class="line">MatMul_1: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">3</span></span><br><span class="line">Const_1: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">2</span></span><br><span class="line">Const: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">2</span></span><br><span class="line">MatMul: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">2</span></span><br><span class="line">AddN: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/cpu:<span class="number">0</span></span><br><span class="line">[[<span class="number">44.</span> <span class="number">56.</span>]</span><br><span class="line"> [<span class="number">98.</span> <span class="number">128.</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="限制GPU资源使用"><a href="#限制GPU资源使用" class="headerlink" title="限制GPU资源使用"></a>限制GPU资源使用</h3><p>&emsp;&emsp;为了加快运行效率，<code>TensorFlow</code>在初始化时会尝试分配所有可用的<code>GPU</code>显存资源给自己，这在多人使用的服务器上工作时就会导致<code>GPU</code>占用，别人无法使用<code>GPU</code>工作的情况。<br>&emsp;&emsp;<code>tf</code>提供了两种控制<code>GPU</code>资源使用的方法，一是让<code>TensorFlow</code>在运行过程中动态申请显存，需要多少就申请多少，第二种方式就是限制<code>GPU</code>的使用率。<br>&emsp;&emsp;动态申请显存如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;限制<code>GPU</code>使用率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line"><span class="comment"># 占用40%显存</span></span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.4</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.4</span>)</span><br><span class="line">config=tf.ConfigProto(gpu_options=gpu_options)</span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;设置使用哪块<code>GPU</code>，可以是在<code>python</code>程序中设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0'</span>  <span class="comment"># 使用“GPU 0”</span></span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0,1'</span>  <span class="comment"># 使用“GPU 0”和“GPU 1”</span></span><br></pre></td></tr></table></figure>
<hr>
<p>&emsp;&emsp;<code>tf.device</code>是<code>tf.Graph.device</code>的一个包装，是一个用于指定新创建的操作(<code>operation</code>)的默认设备的环境管理器。参数为<code>device_name_or_function</code>，可以传入一个设备字符串或者环境操作函数(如<code>tf.DeviceSpec</code>)。</p>
<ul>
<li>如果传入的是一个设备名称字符串，那么在此环境中构造的所有操作都将被分配给带有该名称的设备，除非被其他嵌套的设备环境(其他的<code>tf.device</code>)所覆盖。</li>
<li>如果传入的是一个函数，它将被当作一个从操作对象到设备名称字符串的函数，并在每次创建新操作时调用它。操作将被分配给带有返回名称的设备。</li>
<li>如果是<code>None</code>，所有的来自代码段上下文的设备调用将被忽略。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.device(<span class="string">'/device:GPU:0'</span>):</span><br><span class="line">    <span class="comment"># All operations constructed in this context will be placed on GPU 0</span></span><br><span class="line"><span class="keyword">with</span> g.device(<span class="keyword">None</span>):</span><br><span class="line">    <span class="comment"># All operations constructed in this context will have no assigned device</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Defines a function from "Operation" to device string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matmul_on_gpu</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n.type == <span class="string">"MatMul"</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"/device:GPU:0"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"/cpu:0"</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> g.device(matmul_on_gpu):</span><br><span class="line">    <span class="comment"># All operations of type "MatMul" constructed in this context will be</span></span><br><span class="line">    <span class="comment"># placed on GPU 0; all other operations will be placed on CPU 0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>tf.DeviceSpec</code>返回的是部分或者全部的设备指定，在整个<code>graph</code>中来描述状态存储和计算发生的位置，并且允许解析设备规范的字符串，以验证它们的有效性，然后合并它们或以编码方式组合它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Place the operations on device "GPU:0" in the "ps" job</span></span><br><span class="line">device_spec = DeviceSpec(job=<span class="string">"ps"</span>, device_type=<span class="string">"GPU"</span>, device_index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(device_spec):</span><br><span class="line">    <span class="comment"># Both my_var and squared_var will be placed on /job:ps/device:GPU:0</span></span><br><span class="line">    my_var = tf.Variable(..., name=<span class="string">"my_variable"</span>)</span><br><span class="line">    squared_var = tf.square(my_var)</span><br></pre></td></tr></table></figure>
<p>如果一个<code>DeviceSpec</code>被部分指定，将根据定义的范围与其他<code>DeviceSpecs</code>合并，在内部内定义的<code>DeviceSpec</code>组件优先于在外层内定义的组件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(DeviceSpec(job=<span class="string">"train"</span>,)):</span><br><span class="line">    <span class="keyword">with</span> tf.device(DeviceSpec(job=<span class="string">"ps"</span>, device_type=<span class="string">"GPU"</span>, device_index=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># Nodes created here will be assigned to /job:ps/device:GPU:0</span></span><br><span class="line">    <span class="keyword">with</span> tf.device(DeviceSpec(device_type=<span class="string">"GPU"</span>, device_index=<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Nodes created here will be assigned to /job:train/device:GPU:1</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>DeviceSpec</code>的参数如下：</p>
<ul>
<li><code>job</code>：作业名称。</li>
<li><code>task</code>：任务索引。</li>
<li><code>device_type</code>：设备类型(<code>CPU</code>或<code>GPU</code>)。</li>
<li><code>device_index</code>：设备索引，如果未指定，则可以使用任意的设备。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/CNN与SVM联用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/CNN与SVM联用/" itemprop="url">CNN与SVM联用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T13:31:51+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;本文演示如何使用卷积神经网络来提取手写数字图片特征，并使用<code>SVM</code>进行分类。该实验使用了<code>UCI</code>手写数字数据集，其中前<code>256</code>维是<code>16 * 16</code>的图片，后<code>10</code>维是<code>one hot</code>编码的标签，即<code>1000000000</code>代表<code>0</code>，<code>0010000000</code>代表<code>2</code>。<br>&emsp;&emsp;使用<code>CNN</code>提取特征的原因如下：</p>
<ul>
<li>由于卷积和池化计算的性质，使得图像中的平移部分对于最后的特征向量是没有影响的。从这一角度说，提取到的特征更不容易过拟合。</li>
<li>可以利用不同的卷积、池化和最后输出的特征向量的大小控制整体模型的拟合能力。在过拟合时可以降低特征向量的维数，在欠拟合时可以提高卷积层的输出维数。</li>
</ul>
<p>&emsp;&emsp;算法流程如下：</p>
<ol>
<li>整理训练网络的数据。</li>
<li>建立卷积神经网络。</li>
<li>将数据代入进行训练。</li>
<li>保存训练好的模型。</li>
<li>把数据代入模型获得特征向量。</li>
<li>用特征向量送入<code>SVM</code>进行训练。</li>
<li>使用<code>SVM</code>进行预测，获得结果。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">​</span><br><span class="line">start = time.clock()</span><br><span class="line">​</span><br><span class="line">right0 = <span class="number">0.0</span>  <span class="comment"># 记录预测为1，且实际为1的结果数</span></span><br><span class="line">error0 = <span class="number">0</span>  <span class="comment"># 记录预测为1，但实际为0的结果数</span></span><br><span class="line">right1 = <span class="number">0.0</span>  <span class="comment"># 记录预测为0，且实际为0的结果数</span></span><br><span class="line">error1 = <span class="number">0</span>  <span class="comment"># 记录预测为0，但实际为1的结果数</span></span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span>  <span class="comment"># 初始化权值向量</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span>  <span class="comment"># 初始化偏置向量</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span>  <span class="comment"># 二维卷积运算，步长为1，输出大小不变</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span>  <span class="comment"># 池化运算，将卷积特征缩小为“1/2”</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">for</span> file_num <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'testing NO.%d dataset.......'</span> % file_num)</span><br><span class="line">    ff = open(<span class="string">'digit_train_'</span> + file_num.__str__() + <span class="string">'.data'</span>)</span><br><span class="line">    rr = ff.readlines()</span><br><span class="line">    x_test2 = []</span><br><span class="line">    y_test2 = []</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rr)):</span><br><span class="line">        x_test2.append(list(map(int, map(float, rr[i].split(<span class="string">' '</span>)[:<span class="number">256</span>]))))</span><br><span class="line">        y_test2.append(list(map(int, rr[i].split(<span class="string">' '</span>)[<span class="number">256</span>:<span class="number">266</span>])))</span><br><span class="line"></span><br><span class="line">    ff.close()</span><br><span class="line">    <span class="comment"># 读出测试数据</span></span><br><span class="line">    ff2 = open(<span class="string">'digit_test_'</span> + file_num.__str__() + <span class="string">'.data'</span>)</span><br><span class="line">    rr2 = ff2.readlines()</span><br><span class="line">    x_test3 = []</span><br><span class="line">    y_test3 = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rr2)):</span><br><span class="line">        x_test3.append(list(map(int, map(float, rr2[i].split(<span class="string">' '</span>)[:<span class="number">256</span>]))))</span><br><span class="line">        y_test3.append(list(map(int, rr2[i].split(<span class="string">' '</span>)[<span class="number">256</span>:<span class="number">266</span>])))</span><br><span class="line"></span><br><span class="line">    ff2.close()</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 给x、y留出占位符，以便未来填充数据</span></span><br><span class="line">    x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">256</span>])</span><br><span class="line">    y_ = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 设置输入层的W和b</span></span><br><span class="line">    W = tf.Variable(tf.zeros([<span class="number">256</span>, <span class="number">10</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line">    <span class="comment"># 计算输出，采用的函数是softmax(输入的时候是“one hot”编码)</span></span><br><span class="line">    y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line">    <span class="comment"># 第一个卷积层，“5*5”的卷积核，输出向量是32维</span></span><br><span class="line">    w_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">    <span class="comment"># 图片大小是“16*16”，“-1”代表其他维数自适应</span></span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">1</span>])</span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)</span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 第二层卷积层，输入向量是32维，输出64维，还是“5*5”的卷积核</span></span><br><span class="line">    w_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)</span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 全连接层的w和b</span></span><br><span class="line">    w_fc1 = weight_variable([<span class="number">4</span> * <span class="number">4</span> * <span class="number">64</span>, <span class="number">256</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">256</span>])  <span class="comment"># 此时输出的维数是256维</span></span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">4</span> * <span class="number">4</span> * <span class="number">64</span>])</span><br><span class="line">    <span class="comment"># h_fc1是提取出的256维特征，后面就是用这个参数输入到SVM中</span></span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 设置dropout，否则很容易过拟合</span></span><br><span class="line">    keep_prob = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line">​</span><br><span class="line">    <span class="comment"># 输出层，本次实验只利用它的输出训练CNN</span></span><br><span class="line">    w_fc2 = weight_variable([<span class="number">256</span>, <span class="number">10</span>])</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">​</span><br><span class="line">    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)</span><br><span class="line">    <span class="comment"># 以交叉熵的形式设置误差代价</span></span><br><span class="line">    cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))</span><br><span class="line">    <span class="comment"># 用adma的优化算法优化目标函数</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>):  <span class="comment"># 跑3000轮迭代，每次随机从训练样本中抽出50个进行训练</span></span><br><span class="line">            batch = ([], [])</span><br><span class="line">            p = random.sample(range(<span class="number">795</span>), <span class="number">50</span>)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> p:</span><br><span class="line">                batch[<span class="number">0</span>].append(x_test2[k])</span><br><span class="line">                batch[<span class="number">1</span>].append(y_test2[k])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                train_accuracy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>],</span><br><span class="line">                                                          y_: batch[<span class="number">1</span>],</span><br><span class="line">                                                          keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line"></span><br><span class="line">            train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.6</span>&#125;)</span><br><span class="line">​</span><br><span class="line">        print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;x: x_test3,</span><br><span class="line">                                                            y_: y_test3,</span><br><span class="line">                                                            keep_prob: <span class="number">1.0</span>&#125;))</span><br><span class="line">​</span><br><span class="line">        <span class="comment"># 以下两步都是为了将源数据的“one hot”编码改为1和0，数字“5”可以改为其他值</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(len(y_test2)):</span><br><span class="line">            <span class="keyword">if</span> np.argmax(y_test2[h]) == <span class="number">5</span>:</span><br><span class="line">                y_test2[h] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_test2[h] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(len(y_test3)):</span><br><span class="line">            <span class="keyword">if</span> np.argmax(y_test3[h]) == <span class="number">5</span>:</span><br><span class="line">                y_test3[h] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_test3[h] = <span class="number">0</span></span><br><span class="line">​</span><br><span class="line">        <span class="comment"># 将原来的x带入训练好的CNN中，计算出来全连接层的特征向量，将结果作为SVM中的特征向量</span></span><br><span class="line">        x_temp = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> x_test2:</span><br><span class="line">            x_temp.append(sess.run(h_fc1, feed_dict=&#123;x: np.array(g).reshape((<span class="number">1</span>, <span class="number">256</span>))&#125;)[<span class="number">0</span>])  </span><br><span class="line"></span><br><span class="line">        x_temp2 = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> x_test3:</span><br><span class="line">            x_temp2.append(sess.run(h_fc1, feed_dict=&#123;x: np.array(g).reshape((<span class="number">1</span>, <span class="number">256</span>))&#125;)[<span class="number">0</span>])</span><br><span class="line">​</span><br><span class="line">        clf = svm.SVC(C=<span class="number">0.9</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">        clf.fit(x_temp, y_test2)</span><br><span class="line">​</span><br><span class="line">        print(<span class="string">'svm testing accuracy:'</span>, clf.score(x_temp2, y_test3))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_temp2)):</span><br><span class="line">            <span class="comment"># 在验证时，对出现的四种情况分别使用四个变量进行存储</span></span><br><span class="line">            <span class="keyword">if</span> clf.predict(x_temp2[j].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>] == y_test3[j] == <span class="number">1</span>:</span><br><span class="line">                right0 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> clf.predict(x_temp2[j].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>] == y_test3[j] == <span class="number">0</span>:</span><br><span class="line">                right1 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> clf.predict(x_temp2[j].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">and</span> y_test3[j] == <span class="number">0</span>:</span><br><span class="line">                error0 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                error1 += <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">accuracy = right0 / (right0 + error0)  <span class="comment"># 准确率</span></span><br><span class="line">recall = right0 / (right0 + error1)  <span class="comment"># 召回率</span></span><br><span class="line">print(<span class="string">'svm right ratio:'</span>, (right0 + right1) / (right0 + right1 + error0 + error1))</span><br><span class="line">print(<span class="string">'accuracy:'</span>, accuracy)</span><br><span class="line">print(<span class="string">'recall:'</span>, recall)</span><br><span class="line">print(<span class="string">'F1 score:'</span>, <span class="number">2</span> * accuracy * recall / (accuracy + recall))  <span class="comment"># 计算F1值</span></span><br><span class="line">​</span><br><span class="line">end = time.clock()</span><br><span class="line">print(<span class="string">"time is:"</span>, end - start)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/Keras之回调函数/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/Keras之回调函数/" itemprop="url">Keras之回调函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T10:34:19+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;回调函数是一个函数的合集，会在训练的阶段中所使用。你可以使用回调函数来查看训练模型的内在状态和统计。你可以传递一个列表的回调函数(作为<code>callbacks</code>关键字参数)到<code>Sequential</code>或<code>Model</code>类型的<code>fit</code>方法。在训练时，相应的回调函数的方法就会被在各自的阶段被调用。<br>&emsp;&emsp;虽然我们称之为<code>回调函数</code>，但事实上<code>Keras</code>的回调函数是一个类，回调函数只是习惯性称呼。</p>
<h3 id="Callback"><a href="#Callback" class="headerlink" title="Callback"></a>Callback</h3><p>&emsp;&emsp;该函数用来组建新的回调函数的抽象基类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.Callback()</span><br></pre></td></tr></table></figure>
<p>类属性如下：</p>
<ul>
<li><code>params</code>：字典，训练参数(例如<code>verbosity</code>、<code>batch size</code>、<code>number of epochs</code>等)。</li>
<li><code>model</code>：<code>keras.models.Model</code>的实例，它是正在训练的模型的引用。</li>
</ul>
<p>被回调函数作为参数的<code>logs</code>字典，它会包含了一系列与当前<code>batch</code>或<code>epoch</code>相关的信息。目前，<code>Sequentia</code>模型类的<code>fit</code>方法会在传入到回调函数的<code>logs</code>里面包含以下的数据：</p>
<ul>
<li>在每个<code>epoch</code>的结尾处(<code>on_epoch_end</code>)：<code>logs</code>将包含训练的正确率和误差(<code>acc</code>和<code>loss</code>)，如果指定了验证集，还会包含验证集正确率和误差(<code>val_acc</code>和<code>val_loss</code>)，<code>val_acc</code>还额外需要在<code>compile</code>中启用<code>metrics = [&#39;accuracy&#39;]</code>。</li>
<li>在每个<code>batch</code>的开始处(<code>on_batch_begin</code>)：<code>logs</code>包含<code>size</code>，即当前<code>batch</code>的样本数。</li>
<li>在每个<code>batch</code>的结尾处(<code>on_batch_end</code>)：<code>logs</code>包含<code>loss</code>，若启用<code>accuracy</code>，则还包含<code>acc</code>。</li>
</ul>
<h3 id="BaseLogger"><a href="#BaseLogger" class="headerlink" title="BaseLogger"></a>BaseLogger</h3><p>&emsp;&emsp;该回调函数用来对每个<code>epoch</code>累加<code>metrics</code>指定的监视指标的<code>epoch</code>平均值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.BaseLogger()</span><br></pre></td></tr></table></figure>
<p>这个回调函数被自动应用到每一个<code>Keras</code>模型上面。</p>
<h3 id="TerminateOnNaN"><a href="#TerminateOnNaN" class="headerlink" title="TerminateOnNaN"></a>TerminateOnNaN</h3><p>&emsp;&emsp;该函数是当遇到<code>NaN</code>损失会停止训练的回调函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.TerminateOnNaN()</span><br></pre></td></tr></table></figure>
<h3 id="History"><a href="#History" class="headerlink" title="History"></a>History</h3><p>&emsp;&emsp;该函数是把所有事件都记录到<code>History</code>对象的回调函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.History()</span><br></pre></td></tr></table></figure>
<p>该回调函数在<code>Keras</code>模型上会被自动调用，<code>History</code>对象即为<code>fit</code>方法的返回值。</p>
<h3 id="ModelCheckpoint"><a href="#ModelCheckpoint" class="headerlink" title="ModelCheckpoint"></a>ModelCheckpoint</h3><p>&emsp;&emsp;该函数在每个训练期之后保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.ModelCheckpoint(</span><br><span class="line">    filepath, monitor=<span class="string">'val_loss'</span>, verbose=<span class="number">0</span>, save_best_only=<span class="keyword">False</span>,</span><br><span class="line">    save_weights_only=<span class="keyword">False</span>, mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><code>filepath</code>可以是格式化的字符串，里面的占位符将会被<code>epoch</code>值和传入<code>on_epoch_end</code>的<code>logs</code>关键字所填入。例如，如果<code>filepath</code>是<code>weights.{epoch:02d}-{val_loss:.2f}.hdf5</code>，那么会生成对应<code>epoch</code>和验证集<code>loss</code>的多个文件。</p>
<ul>
<li>filepath：字符串，保存模型的路径。</li>
<li>monitor：被监测的数据。</li>
<li><code>verbose</code>：详细信息模式，<code>0</code>或者<code>1</code>。</li>
<li><code>save_best_only</code>：当设置为<code>True</code>时，将只保存在验证集上性能最好的模型。</li>
<li><code>mode</code>：<code>auto</code>、<code>min</code>和<code>max</code>其中之一。在<code>save_best_only=True</code>时决定性能最佳模型的评判准则，例如当监测值为<code>val_acc</code>时，模式应为<code>max</code>；当检测值为<code>val_loss</code>时，模式应为<code>min</code>。在<code>auto</code>模式下，评价准则由被监测值的名字自动推断。</li>
<li><code>save_weights_only</code>：如果为<code>True</code>，那么只有模型的权重会被保存(<code>model.save_weights(filepath)</code>)；否则的话，整个模型会被保存(<code>model.save(filepath)</code>)。</li>
<li><code>period</code>：每个检查点之间的间隔(训练轮数)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">10</span>, input_dim=<span class="number">784</span>, kernel_initializer=<span class="string">'uniform'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line"><span class="string">""" 如果验证损失下降，那么在每个训练轮之后保存模型 """</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath=<span class="string">'/tmp/weights.hdf5'</span>,</span><br><span class="line">                               verbose=<span class="number">1</span>, save_best_only=<span class="keyword">True</span>)</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">20</span>, verbose=<span class="number">0</span>,</span><br><span class="line">          validation_data=(X_test, Y_test), callbacks=[checkpointer])</span><br></pre></td></tr></table></figure>
<h3 id="EarlyStopping"><a href="#EarlyStopping" class="headerlink" title="EarlyStopping"></a>EarlyStopping</h3><p>&emsp;&emsp;当监测值不再改善时，该回调函数将中止训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>, min_delta=<span class="number">0</span>, patience=<span class="number">0</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>min_delta</code>：在被监测的数据中被认为是提升的最小变化，例如小于<code>min_delta</code>的绝对变化会被认为没有提升。</li>
<li><code>patience</code>：当<code>early stop</code>被激活(例如发现<code>loss</code>相比上一个<code>epoch</code>训练没有下降)，则经过<code>patience</code>个<code>epoch</code>后停止训练。</li>
<li><code>mode</code>：<code>auto</code>、<code>min</code>和<code>max</code>其中之一。在<code>min</code>模式中，当被监测的数据停止下降，训练就会停止；在<code>max</code>模式中，当被监测的数据停止上升，训练就会停止；在<code>auto</code>模式中，方向会自动从被监测的数据的名字中判断出来。</li>
</ul>
<h3 id="LearningRateScheduler"><a href="#LearningRateScheduler" class="headerlink" title="LearningRateScheduler"></a>LearningRateScheduler</h3><p>&emsp;&emsp;学习速率定时器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.LearningRateScheduler(schedule, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>schedule</code>是一个函数，该函数以<code>epoch</code>号为参数(从<code>0</code>算起的整数)，返回一个新学习率(浮点数)。</p>
<h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><p>&emsp;&emsp;该函数用于<code>Tensorboard</code>基本可视化。<code>TensorBoard</code>是由<code>Tensorflow</code>提供的一个可视化工具。这个回调函数为<code>Tensorboard</code>编写一个日志，使得你可以动态地观察训练和测试指标的图像以及不同层的激活值直方图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.TensorBoard(</span><br><span class="line">    log_dir=<span class="string">'./logs'</span>, histogram_freq=<span class="number">0</span>, batch_size=<span class="number">32</span>, write_graph=<span class="keyword">True</span>,</span><br><span class="line">    write_grads=<span class="keyword">False</span>, write_images=<span class="keyword">False</span>, embeddings_freq=<span class="number">0</span>,</span><br><span class="line">    embeddings_layer_names=<span class="keyword">None</span>, embeddings_metadata=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>log_dir</code>：用来保存被<code>TensorBoard</code>分析的日志文件的文件名。</li>
<li><code>histogram_freq</code>：计算各个层激活值直方图的频率(每多少个<code>epoch</code>计算一次)，如果设置为<code>0</code>则不计算。</li>
<li><code>write_graph</code>：是否在<code>TensorBoard</code>中可视化图像。如果<code>write_graph</code>被设置为<code>True</code>，日志文件会变得非常大。</li>
<li><code>write_grads</code>：是否在<code>TensorBoard</code>中可视化梯度值直方图。<code>histogram_freq</code>必须要大于<code>0</code>。</li>
<li><code>batch_size</code>：用以直方图计算的传入神经元网络输入批的大小。</li>
<li><code>write_images</code>：是否将模型权重以图片的形式可视化。</li>
<li><code>embeddings_freq</code>：依据该频率(以<code>epoch</code>为单位)筛选保存的<code>embedding</code>层。</li>
<li><code>embeddings_layer_names</code>：要观察的层名称的列表，若设置为<code>None</code>或空列表，则所有<code>embedding</code>层都将被观察。</li>
<li><code>embeddings_metadata</code>：一个字典，将层名称映射为包含该<code>embedding</code>层元数据的文件名。如果所有的<code>embedding</code>层都使用相同的元数据文件，则可传递字符串。</li>
</ul>
<h3 id="ReduceLROnPlateau"><a href="#ReduceLROnPlateau" class="headerlink" title="ReduceLROnPlateau"></a>ReduceLROnPlateau</h3><p>&emsp;&emsp;当评价指标不再提升时，减少学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">    monitor=<span class="string">'val_loss'</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>, verbose=<span class="number">0</span>,</span><br><span class="line">    mode=<span class="string">'auto'</span>, epsilon=<span class="number">0.0001</span>, cooldown=<span class="number">0</span>, min_lr=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>monitor</code>：被监测的数据。</li>
<li><code>factor</code>：每次减少学习率的因子，学习率将以<code>lr = lr * factor</code>的形式被减少。</li>
<li><code>patience</code>：当经历了<code>patience</code>个<code>epoch</code>，而模型性能不提升时，学习率减少的动作会被触发。</li>
<li><code>mode</code>：<code>auto</code>、<code>min</code>或<code>max</code>其中之一。如果是<code>min</code>模式，如果被监测的数据已经停止下降，学习速率会被降低；在<code>max</code>模式，如果被监测的数据已经停止上升，学习速率会被降低；在<code>auto</code>模式，方向会被从被监测的数据中自动推断出来。</li>
<li><code>epsilon</code>：阈值，用来确定是否进入检测值的<code>平原区</code>。</li>
<li><code>cooldown</code>：学习率减少后，会经过<code>cooldown</code>个<code>epoch</code>才重新进行正常操作。</li>
<li><code>min_lr</code>：学习率的下限。</li>
</ul>
<p>当学习停滞时，减少<code>2</code>倍或<code>10</code>倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在<code>patience</code>个<code>epoch</code>中看不到模型性能提升，则减少学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduce_lr = ReduceLROnPlateau(monitor=<span class="string">'val_loss'</span>, factor=<span class="number">0.2</span>, patience=<span class="number">5</span>, min_lr=<span class="number">0.001</span>)</span><br><span class="line">model.fit(X_train, Y_train, callbacks=[reduce_lr])</span><br></pre></td></tr></table></figure>
<h3 id="CSVLogger"><a href="#CSVLogger" class="headerlink" title="CSVLogger"></a>CSVLogger</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.CSVLogger(filename, separator=<span class="string">','</span>, append=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>filename</code>：<code>csv</code>文件的文件名。</li>
<li><code>separator</code>：用来隔离<code>csv</code>文件中元素的字符串。</li>
<li><code>append</code>：如果为<code>True</code>，则表示如果文件存在则增加(可以被用于继续训练)；如果为<code>False</code>，表示覆盖存在的文件。</li>
</ul>
<p>将<code>epoch</code>的训练结果保存在<code>csv</code>文件中，支持所有可被转换为<code>string</code>的值，包括<code>1D</code>的可迭代数值(例如<code>np.ndarray</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">csv_logger = CSVLogger(<span class="string">'training.log'</span>)</span><br><span class="line">model.fit(X_train, Y_train, callbacks=[csv_logger])</span><br></pre></td></tr></table></figure>
<h3 id="编写自己的回调函数"><a href="#编写自己的回调函数" class="headerlink" title="编写自己的回调函数"></a>编写自己的回调函数</h3><p>&emsp;&emsp;我们可以通过继承<code>keras.callbacks.Callback</code>编写自己的回调函数。如下示例可以保存每个<code>batch</code>的<code>loss</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LossHistory</span><span class="params">(keras.callbacks.Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.losses = []</span><br><span class="line">​</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_end</span><span class="params">(self, batch, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.losses.append(logs.get(<span class="string">'loss'</span>))</span><br><span class="line">​</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">10</span>, input_dim=<span class="number">784</span>, kernel_initializer=<span class="string">'uniform'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line">​</span><br><span class="line">history = LossHistory()</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">128</span>, epochs=<span class="number">20</span>, verbose=<span class="number">0</span>, callbacks=[history])</span><br><span class="line">print(history.losses)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/01/16/深度学习/Keras之卷积层/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/深度学习/Keras之卷积层/" itemprop="url">Keras之卷积层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T08:06:29+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Conv1D"><a href="#Conv1D" class="headerlink" title="Conv1D"></a>Conv1D</h3><p>&emsp;&emsp;该函数是<code>1D</code>卷积层(例如<code>时序卷积</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv1D(</span><br><span class="line">    filters, kernel_size, strides=<span class="number">1</span>, padding=<span class="string">'valid'</span>, dilation_rate=<span class="number">1</span>,</span><br><span class="line">    activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>一维卷积层(即<code>时域卷积</code>)用以在一维输入信号上进行邻域滤波。当使用该层作为首层时，需要提供关键字参数<code>input_shape</code>。例如(<code>10, 128</code>)代表一个长为<code>10</code>的序列，序列中每个信号为<code>128</code>向量，而(<code>None, 128</code>)代表变长的<code>128</code>维向量序列。该层生成将输入信号与卷积核按照单一的空域(或时域)方向进行卷积。如果<code>use_bias = True</code>，则还会加上一个偏置项，若<code>activation</code>不为<code>None</code>，则输出为经过激活函数的输出。</p>
<ul>
<li><code>filters</code>：卷积核的数目(即输出的维度)。</li>
<li><code>kernel_size</code>：一个整数，或者单个整数表示的元组或列表，指明卷积核的空域或时域窗长度。</li>
<li><code>strides</code>：一个整数，或者单个整数表示的元组或列表，指明卷积的步长。任何不为<code>1</code>的<code>strides</code>均与任何不为<code>1</code>的<code>dilation_rate</code>均不兼容。</li>
<li><code>padding</code>：<code>valid</code>、<code>causal</code>或<code>same</code>之一(大小写敏感)。<code>valid</code>表示<code>不填充</code>；<code>same</code>表示填充输入以使输出具有与原始输入相同的长度；<code>causal</code>表示因果(膨胀)卷积，例如<code>output[t]</code>不依赖于<code>input[t + 1:]</code>，当对不能违反时间顺序的时序信号建模时有用。</li>
<li><code>dilation_rate</code>：一个整数，或者单个整数表示的元组或列表，指定用于膨胀卷积的膨胀率。任何不为<code>1</code>的<code>dilation_rate</code>均与任何不为<code>1</code>的<code>strides</code>均不兼容。</li>
<li><code>activation</code>：要使用的激活函数。如果你不指定，则不使用激活函数(即线性激活<code>a(x) = x</code>)。</li>
<li><code>use_bias</code>：布尔值，该层是否使用偏置向量。</li>
<li><code>kernel_initializer</code>：<code>kernel</code>权值矩阵的初始化器。</li>
<li><code>bias_initializer</code>：偏置向量的初始化器。</li>
<li><code>kernel_regularizer</code>：运用到<code>kernel</code>权值矩阵的正则化函数。</li>
<li><code>bias_regularizer</code>：运用到偏置向量的正则化函数。</li>
<li><code>activity_regularizer</code>：运用到层的输出的正则化函数。</li>
<li><code>kernel_constraint</code>：运用到<code>kernel</code>权值矩阵的约束函数。</li>
<li><code>bias_constraint</code>：运用到偏置向量的约束函数。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch_size, steps, input_dim</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch_size, new_steps, filters</code>)。由于填充或窗口按步长滑动，<code>steps</code>值可能已更改。</p>
<h3 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D"></a>Conv2D</h3><p>&emsp;&emsp;该函数是<code>2D</code>卷积层(例如对图像的<code>空间卷积</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>,</span><br><span class="line">    kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>二维卷积层即是对图像的空域卷积，该层对二维输入进行滑动窗卷积。当使用该层作为第一层时，应提供<code>input_shape</code>参数，例如<code>input_shape = (128, 128, 3)</code>代表<code>128 * 128</code>的彩色<code>RGB</code>图像(<code>data_format = &#39;channels_last&#39;</code>)。</p>
<ul>
<li><code>filters</code>：卷积核的数目(即输出的维度)。</li>
<li><code>kernel_size</code>：一个整数，或者<code>2</code>个整数表示的元组或列表，指明<code>2D</code>卷积窗口的宽度和高度。如为单个整数，则表示在各个空间维度的相同长度。</li>
<li><code>strides</code>：一个整数，或者<code>2</code>个整数表示的元组或列表，指明卷积沿宽度和高度方向的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为<code>1</code>的<code>strides</code>均与任何不为<code>1</code>的<code>dilation_rate</code>均不兼容。</li>
<li><code>padding</code>：<code>valid</code>或<code>same</code>。<code>valid</code>代表只进行有效的卷积，即对边界数据不处理；<code>same</code>代表保留边界处的卷积结果，通常会导致输出<code>shape</code>与输入<code>shape</code>相同。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
<li><code>dilation_rate</code>：一个整数或<code>2</code>个整数的元组或列表，指定膨胀卷积的膨胀率。任何不为<code>1</code>的<code>dilation_rate</code>均与任何不为<code>1</code>的<code>strides</code>均不兼容。</li>
<li><code>activation</code>：要使用的激活函数。如果你不指定，则不使用激活函数(即线性激活<code>a(x) = x</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸(注意这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>samples, channels, rows, cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>samples, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(输出的行列数可能会因为填充方法而改变)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>samples, filters, new_rows, new_cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>samples, new_rows, new_cols, filters</code>)。</li>
</ul>
<h3 id="SeparableConv2D"><a href="#SeparableConv2D" class="headerlink" title="SeparableConv2D"></a>SeparableConv2D</h3><p>&emsp;&emsp;该函数在深度方向的可分离<code>2D</code>卷积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SeparableConv2D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    depth_multiplier=<span class="number">1</span>, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    depthwise_initializer=<span class="string">'glorot_uniform'</span>, pointwise_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, depthwise_regularizer=<span class="keyword">None</span>, pointwise_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, depthwise_constraint=<span class="keyword">None</span>,</span><br><span class="line">    pointwise_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>可分离卷积首先按深度方向进行卷积(对每个输入通道分别卷积)，然后逐点进行卷积，将上一步的卷积结果混合到输出通道中。参数<code>depth_multiplier</code>控制了在<code>depthwise</code>卷积(第一步)的过程中，每个输入通道信号产生多少个输出通道。</p>
<ul>
<li><code>depth_multiplier</code>：在按深度卷积的步骤中，每个输入通道使用多少个输出通道。</li>
<li><code>depthwise_initializer</code>：运用到深度方向的核矩阵的初始化器。</li>
<li><code>pointwise_initializer</code>：运用到逐点核矩阵的初始化器。</li>
<li><code>depthwise_regularizer</code>：运用到深度方向的核矩阵的正则化函数。</li>
<li><code>pointwise_regularizer</code>：运用到逐点核矩阵的正则化函数。</li>
<li><code>depthwise_constraint</code>：运用到深度方向的核矩阵的约束函数。</li>
<li><code>pointwise_constraint</code>：运用到逐点核矩阵的约束函数。</li>
</ul>
<p>&emsp;&emsp;输入尺寸(注意这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(输出的行列数可能会因为填充方法而改变)：</p>
<ul>
<li>如果<code>data_format = &#39;channels_first&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, filters, new_rows, new_cols</code>)。</li>
<li>如果<code>data_format = &#39;channels_last&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, new_rows, new_cols, filters</code>)。</li>
</ul>
<h3 id="Conv2DTranspose"><a href="#Conv2DTranspose" class="headerlink" title="Conv2DTranspose"></a>Conv2DTranspose</h3><p>&emsp;&emsp;该函数转置卷积层(有时被称为<code>反卷积</code>)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2DTranspose(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>需要反卷积的情况通常发生在用户想要对一个普通卷积的结果做反方向的变换，例如将具有该卷积层输出<code>shape</code>的<code>tensor</code>转换为具有该卷积层输入<code>shape</code>的<code>tensor</code>，同时保留与卷积层兼容的连接模式。当使用该层作为第一层时，应提供<code>input_shape</code>参数。例如<code>input_shape = (3, 128, 128)</code>代表<code>128 * 128</code>的彩色<code>RGB</code>图像。</p>
<p>&emsp;&emsp;输入尺寸(注意这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(输出的行列数可能会因为填充方法而改变)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, filters, new_rows, new_cols</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输出<code>4D</code>张量，尺寸为(<code>batch, new_rows, new_cols, filters</code>)。</li>
</ul>
<h3 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h3><p>&emsp;&emsp;该函数是<code>3D</code>卷积层(例如立体空间卷积)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv3D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>,</span><br><span class="line">    kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>三维卷积对三维的输入进行滑动窗卷积，当使用该层作为第一层时，应提供<code>input_shape</code>参数。例如<code>input_shape = (3, 10, 128, 128)</code>代表对<code>10</code>帧<code>128 * 128</code>的彩色<code>RGB</code>图像进行卷积。数据的通道位置仍然由<code>data_format</code>参数指定。</p>
<ul>
<li><code>filters</code>：整数，输出空间的维度(即卷积中滤波器的输出数量)。</li>
<li><code>kernel_size</code>：一个整数，或者<code>3</code>个整数表示的元组或列表，指明<code>3D</code>卷积窗口的深度、高度和宽度。如为单个整数，则表示在各个空间维度的相同长度。</li>
<li><code>strides</code>：一个整数，或者<code>3</code>个整数表示的元组或列表，指明卷积沿每一个空间维度的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为<code>1</code>的<code>strides</code>均与任何不为<code>1</code>的<code>dilation_rate</code>均不兼容。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
<li><code>dilation_rate</code>：一个整数或<code>3</code>个整数的元组或列表，指定<code>dilated convolution</code>中的膨胀比例。任何不为<code>1</code>的<code>dilation_rate</code>均与任何不为<code>1</code>的<code>strides</code>均不兼容。</li>
</ul>
<p>&emsp;&emsp;输入尺寸(这里的输入<code>shape</code>指的是函数内部实现的输入<code>shape</code>，而非函数接口应指定的<code>input_shape</code>)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输入<code>5D</code>张量，尺寸为(<code>samples, channels, conv_dim1, conv_dim2, conv_dim3</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输入<code>5D</code>张量，尺寸为(<code>samples, conv_dim1, conv_dim2, conv_dim3, channels</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸(由于填充的原因，<code>new_conv_dim1</code>、<code>new_conv_dim2</code>和<code>new_conv_dim3</code>值可能已更改)：</p>
<ul>
<li>如果<code>data_format=&#39;channels_first&#39;</code>，输出<code>5D</code>张量，尺寸为(<code>samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3</code>)。</li>
<li>如果<code>data_format=&#39;channels_last&#39;</code>，输出<code>5D</code>张量，尺寸为(<code>samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters</code>)。</li>
</ul>
<h3 id="Cropping1D"><a href="#Cropping1D" class="headerlink" title="Cropping1D"></a>Cropping1D</h3><p>&emsp;&emsp;该函数是<code>1D</code>输入的裁剪层(例如时间序列)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Cropping1D(cropping=(<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>它沿着时间维度(第<code>1</code>个轴)对输入进行裁剪。参数<code>cropping</code>是整数或整数元组(长度为<code>2</code>)，决定在裁剪维度(第<code>1</code>个轴)的开始和结束位置应该裁剪多少个单位。如果只提供了一个整数，那么这两个位置将使用相同的值。</p>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch, axis_to_crop, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch, cropped_axis, features</code>)。</p>
<h3 id="Cropping2D"><a href="#Cropping2D" class="headerlink" title="Cropping2D"></a>Cropping2D</h3><p>&emsp;&emsp;该函数是<code>2D</code>输入的裁剪层(例如图像)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Cropping2D(cropping=((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>)), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>它对<code>2D</code>输入(图像)进行裁剪，将在空域维度(即宽和高的方向上)裁剪。</p>
<ul>
<li><code>cropping</code>：整数，或<code>2</code>个整数的元组，或<code>2</code>个整数的<code>2</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对宽度和高度应用相同的对称裁剪。</li>
<li>如果为<code>2</code>个整数的元组：解释为对高度和宽度使用两个不同的裁剪值(<code>symmetric_height_crop, symmetric_width_crop</code>)。</li>
<li>如果为<code>2</code>个整数的<code>2</code>个元组：解释为(<code>(top_crop, bottom_crop), (left_crop, right_crop)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, cropped_rows, cropped_cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, channels, cropped_rows, cropped_cols</code>)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 24, 20, 3)”</span></span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">4</span>)),input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>))</span><br><span class="line"><span class="comment"># 现在“model.output_shape == (None, 20, 16. 64)”</span></span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))))</span><br></pre></td></tr></table></figure>
<h3 id="Cropping3D"><a href="#Cropping3D" class="headerlink" title="Cropping3D"></a>Cropping3D</h3><p>&emsp;&emsp;该函数是<code>3D</code>数据的裁剪层(例如空间或时空)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Cropping3D(cropping=((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>)), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cropping</code>：整数，或<code>3</code>个整数的元组，或<code>2</code>个整数的<code>3</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对深度、高度和宽度应用相同的对称裁剪。</li>
<li>如果为<code>3</code>个整数的元组：解释为对深度、高度和宽度的<code>3</code>个不同的对称裁剪值(<code>symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop</code>)。</li>
<li>如果为<code>2</code>个整数的<code>3</code>个元组：解释为(<code>(left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis</code>)。</li>
</ul>
<h3 id="UpSampling1D"><a href="#UpSampling1D" class="headerlink" title="UpSampling1D"></a>UpSampling1D</h3><p>&emsp;&emsp;该函数是<code>1D</code>输入的上采样层：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.UpSampling1D(size=2)</span><br></pre></td></tr></table></figure>
<p>沿着时间轴重复每个时间步<code>size</code>次。参数<code>size</code>是整数，上采样因子。<br>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch, steps, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch, upsampled_steps, features</code>)。</p>
<h3 id="UpSampling2D"><a href="#UpSampling2D" class="headerlink" title="UpSampling2D"></a>UpSampling2D</h3><p>&emsp;&emsp;该函数是<code>2D</code>输入的上采样层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.UpSampling2D(size=(<span class="number">2</span>, <span class="number">2</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>沿着数据的行和列分别重复<code>size[0]</code>和<code>size[1]</code>次。</p>
<ul>
<li><code>size</code>：整数，或<code>2</code>个整数的元组，分别是行和列的上采样因子。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, upsampled_rows, upsampled_cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, channels, upsampled_rows, upsampled_cols</code>)。</li>
</ul>
<h3 id="UpSampling3D"><a href="#UpSampling3D" class="headerlink" title="UpSampling3D"></a>UpSampling3D</h3><p>&emsp;&emsp;该函数是<code>3D</code>输入的上采样层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.UpSampling3D(size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>沿着数据的第<code>1</code>、<code>2</code>、<code>3</code>维度分别重复<code>size[0]</code>、<code>size[1]</code>和<code>size[2]</code>次。</p>
<ul>
<li><code>size</code>：整数，或<code>3</code>个整数的元组，代表<code>dim1</code>、<code>dim2</code>和<code>dim3</code>的上采样因子。</li>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, dim1, dim2, dim3, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>5D</code>张量，尺寸为(<code>batch, channels, dim1, dim2, dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>5D</code>张量，尺寸为(<code>batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3</code>)。</li>
</ul>
<h3 id="ZeroPadding1D"><a href="#ZeroPadding1D" class="headerlink" title="ZeroPadding1D"></a>ZeroPadding1D</h3><p>&emsp;&emsp;对<code>1D</code>输入的首尾端(如时域序列)填充<code>0</code>，以控制卷积以后向量的长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ZeroPadding1D(padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>参数<code>padding</code>是整数，或长度为<code>2</code>的整数元组。</p>
<ul>
<li>整数：在填充维度(第一个轴)的开始和结束处添加多少个零。</li>
<li>长度为<code>2</code>的整数元组：在填充维度的开始和结尾处添加多少个零(<code>(left_pad, right_pad)</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：<code>3D</code>张量，尺寸为(<code>batch, axis_to_pad, features</code>)。<br>&emsp;&emsp;输出尺寸：<code>3D</code>张量，尺寸为(<code>batch, padded_axis, features</code>)。</p>
<h3 id="ZeroPadding2D"><a href="#ZeroPadding2D" class="headerlink" title="ZeroPadding2D"></a>ZeroPadding2D</h3><p>&emsp;&emsp;对<code>2D</code>输入(如图片)的边界填充<code>0</code>，以控制卷积以后特征图的大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>padding</code>：整数，或<code>2</code>个整数的元组，或<code>2</code>个整数的<code>2</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对宽度和高度运用相同的对称填充。</li>
<li>如果为<code>2</code>个整数的元组：解释为高度和宽度的2个不同的对称裁剪值(<code>symmetric_height_pad, symmetric_width_pad</code>)。</li>
<li>如果为<code>2</code>个整数的<code>2</code>个元组：解释为(<code>(top_pad, bottom_pad), (left_pad, right_pad)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表图像的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, height, width, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, height, width</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, rows, cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输入<code>4D</code>张量，尺寸为(<code>batch, channels, rows, cols</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, padded_rows, padded_cols, channels</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，则输出<code>4D</code>张量，尺寸为(<code>batch, channels, padded_rows, padded_cols</code>)。</li>
</ul>
<h3 id="ZeroPadding3D"><a href="#ZeroPadding3D" class="headerlink" title="ZeroPadding3D"></a>ZeroPadding3D</h3><p>&emsp;&emsp;将数据的三个维度上填充<code>0</code>，本层目前只能在使用<code>Theano</code>为后端时可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ZeroPadding3D(padding=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>padding</code>：整数，或<code>3</code>个整数的元组，或<code>2</code>个整数的<code>3</code>个元组。</li>
</ul>
<ol>
<li>如果为整数：将对深度、高度和宽度运用相同的对称填充。</li>
<li>如果为<code>3</code>个整数的元组：解释为深度、高度和宽度的三个不同的对称填充值(<code>symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad</code>)。</li>
<li>如果为<code>2</code>个整数的<code>3</code>个元组：解释为(<code>(left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)</code>)。</li>
</ol>
<ul>
<li><code>data_format</code>：字符串，<code>channels_last</code>(默认)或<code>channels_first</code>之一，代表数据的通道维的位置。<code>channels_last</code>对应输入尺寸为(<code>batch, spatial_dim1, spatial_dim2, spatial_dim3, channels</code>)，<code>channels_first</code>对应输入尺寸为(<code>batch, channels, spatial_dim1, spatial_dim2, spatial_dim3</code>)。</li>
</ul>
<p>&emsp;&emsp;输入尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，输入<code>5D</code>张量，尺寸为(<code>batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，输入<code>5D</code>张量，尺寸为(<code>batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad</code>)。</li>
</ul>
<p>&emsp;&emsp;输出尺寸：</p>
<ul>
<li>如果<code>data_format</code>为<code>channels_last</code>，输出<code>5D</code>张量，尺寸为(<code>batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth</code>)。</li>
<li>如果<code>data_format</code>为<code>channels_first</code>，输出<code>5D</code>张量，尺寸为(<code>batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad</code>)。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/56/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/56/">56</a><span class="page-number current">57</span><a class="page-number" href="/page/58/">58</a><span class="space">&hellip;</span><a class="page-number" href="/page/96/">96</a><a class="extend next" rel="next" href="/page/58/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">955</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
