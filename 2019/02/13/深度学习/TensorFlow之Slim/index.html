<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="&amp;emsp;&amp;emsp;TF-Slim是Tensorflow中一个轻量级的库，用于定义、训练和评估复杂的模型。TF-Slim中的组件可以与Tensorflow中原生的函数一起使用，与其他的框架(比如tf.contrib.learn)也可以一起使用。使用方法如下： 1import tensorflow.contrib.slim as slim &amp;emsp;&amp;emsp;TF-Slim可以使建立、训练和">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow之Slim">
<meta property="og:url" content="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之Slim/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:description" content="&amp;emsp;&amp;emsp;TF-Slim是Tensorflow中一个轻量级的库，用于定义、训练和评估复杂的模型。TF-Slim中的组件可以与Tensorflow中原生的函数一起使用，与其他的框架(比如tf.contrib.learn)也可以一起使用。使用方法如下： 1import tensorflow.contrib.slim as slim &amp;emsp;&amp;emsp;TF-Slim可以使建立、训练和">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-03-13T07:33:08.245Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow之Slim">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;TF-Slim是Tensorflow中一个轻量级的库，用于定义、训练和评估复杂的模型。TF-Slim中的组件可以与Tensorflow中原生的函数一起使用，与其他的框架(比如tf.contrib.learn)也可以一起使用。使用方法如下： 1import tensorflow.contrib.slim as slim &amp;emsp;&amp;emsp;TF-Slim可以使建立、训练和">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之Slim/">





  <title>TensorFlow之Slim | 泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之Slim/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow之Slim</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T12:16:46+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>&emsp;&emsp;<code>TF-Slim</code>是<code>Tensorflow</code>中一个轻量级的库，用于定义、训练和评估复杂的模型。<code>TF-Slim</code>中的组件可以与<code>Tensorflow</code>中原生的函数一起使用，与其他的框架(比如<code>tf.contrib.learn</code>)也可以一起使用。使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>TF-Slim</code>可以使建立、训练和评估神经网络更加简单，具有如下特点：</p>
<ul>
<li>允许用户通过减少模板代码使得模型更加简洁。这个可以通过使用<code>argument scoping</code>和大量的高层<code>layers</code>、<code>variables</code>来实现。</li>
<li>通过使用常用的正则化(<code>regularizers</code>)使得建立模型更加简单。</li>
<li>一些广泛使用的计算机视觉相关的模型(比如<code>VGG</code>、<code>AlexNet</code>)已经在<code>slim</code>中定义好了，用户可以很方便地使用。这些既可以当成黑盒使用，也可以被扩展使用，比如添加一些<code>multiple heads</code>到不同的内部的层。</li>
<li><code>Slim</code>使得扩展复杂模型变得容易，可以使用已经存在的模型的<code>checkpoints</code>来开始训练算法。</li>
</ul>
<h3 id="What-are-the-various-components-of-TF-Slim"><a href="#What-are-the-various-components-of-TF-Slim" class="headerlink" title="What are the various components of TF-Slim?"></a>What are the various components of TF-Slim?</h3><p>&emsp;&emsp;<code>TF-Slim</code>由几个独立存在的组件组成：</p>
<ul>
<li><code>arg_scope</code>：提供一个新的作用域(<code>scope</code>)，称为<code>arg_scope</code>。在该作用域(<code>scope</code>)中，用户可以定义一些默认的参数，用于特定的操作。</li>
<li><code>data</code>：包含<code>TF-Slim</code>的<code>dataset</code>定义、<code>data providers</code>、<code>parallel_reader</code>和<code>decoding utilities</code>。</li>
<li><code>evaluation</code>：包含用于模型评估的常规函数。</li>
<li><code>layers</code>：包含用于建立模型的高级<code>layers</code>。</li>
<li><code>learning</code>：包含一些用于训练模型的常规函数。</li>
<li><code>losses</code>：包含一些用于<code>loss function</code>的函数。</li>
<li><code>metrics</code>：包含一些热门的评价标准。</li>
<li><code>nets</code>：包含一些热门的网络定义，如<code>VGG</code>、<code>AlexNet</code>等模型。</li>
<li><code>queues</code>：提供一个内容管理者，使得可以很容易、很安全地启动和关闭<code>QueueRunners</code>。</li>
<li><code>regularizers</code>：包含权重正则化。</li>
<li><code>variables</code>：提供一个方便的封装，用于变量创建和使用。</li>
</ul>
<h4 id="Defining-Models"><a href="#Defining-Models" class="headerlink" title="Defining Models"></a>Defining Models</h4><p>&emsp;&emsp;使用<code>TF-Slim</code>，结合<code>variables</code>、<code>layers</code>和<code>scopes</code>，模型可以很简洁地被定义。</p>
<h4 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h4><p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，创建<code>Variable</code>需要一个预定义的值或者一种初始化机制(比如从一个高斯分布中随机采样)。此外，如果一个变量需要在一个特定的设备上(如<code>GPU</code>)创建，那么必须被明确说明。为了减少变量创建所需的代码，<code>TF-Slim</code>提供了一些封装函数(定义在<code>variables.py</code>中)，可以使得用户定义变量变得简单。<br>&emsp;&emsp;举个例子，定义一个权重(<code>weight</code>)变量，然后使用一个截断的正态分布来初始化，使用<code>l2 loss</code>正则化，并将该变量放置在<code>CPU</code>中，只需要声明如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = slim.variable(</span><br><span class="line">    <span class="string">'weights'</span>, shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">3</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">    regularizer=slim.l2_regularizer(<span class="number">0.05</span>), device=<span class="string">'/CPU:0'</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，有两种类型的<code>variables</code>：<code>regular variables</code>和<code>local (transient) variables</code>。绝大部分变量是<code>regular variables</code>，一旦被创建，可以使用<code>saver</code>来将这些变量保存到磁盘中；<code>Local variables</code>是那些仅仅存在于一个<code>session</code>内的变量，并不会被保存到磁盘中。<br>&emsp;&emsp;<code>TF-Slim</code>通过定义<code>model variables</code>来进一步区别变量，这些是表示一个模型参数的变量。<code>Model variables</code>在学习期间被训练或者<code>fine-tuned</code>，在评估或者推断期间可以从一个<code>checkpoint</code>中加载。模型变量包括使用<code>slim.fully_connected</code>或者<code>slim.conv2d</code>创建的变量等。非模型变量(<code>Non-model variables</code>)指的是那些在学习或者评估阶段使用，但是在实际的<code>inference</code>中不需要用到的变量。例如<code>global_step</code>在学习和评估阶段会用到的变量，但是实际上并不是模型的一部分。类似的，<code>moving average variables</code>也是非模型变量。<br>&emsp;&emsp;<code>model variables</code>和<code>regular variables</code>在<code>TF-Slim</code>中很容易地被创建和恢复：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Variables</span></span><br><span class="line">weights = slim.model_variable(</span><br><span class="line">    <span class="string">'weights'</span>, shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">3</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">    regularizer=slim.l2_regularizer(<span class="number">0.05</span>), device=<span class="string">'/CPU:0'</span>)</span><br><span class="line">model_variables = slim.get_model_variables()</span><br><span class="line"><span class="comment"># Regular variables</span></span><br><span class="line">my_var = slim.variable(<span class="string">'my_var'</span>, shape=[<span class="number">20</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">regular_variables_and_model_variables = slim.get_variables()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这是如何工作的呢？当你通过<code>TF-Slim</code>的<code>layer</code>或者直接通过<code>slim.model_variable</code>函数创建一个模型的变量时，<code>TF-Slim</code>将变量添加到<code>tf.GraphKeys.MODEL_VARIABLES</code>集合中。如果你想拥有自己定制化的<code>layers</code>或者<code>variables</code>创建机制，但是仍然想利用<code>TF-Slim</code>来管理你的变量，此时<code>TF-Slim</code>提供一个方便的函数，用于添加模型的变量到集合中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_model_variable = CreateViaCustomCode()</span><br><span class="line">slim.add_model_variable(my_model_variable)  <span class="comment"># Letting TF-Slim know about the additional variable</span></span><br></pre></td></tr></table></figure>
<h4 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h4><p>&emsp;&emsp;在原生的<code>Tensorflow</code>中，要定义一些层(比如说卷积层、全连接层、<code>BatchNorm</code>层等)是比较麻烦的。举个例子，神经网络中的卷积层由以下几个步骤组成：</p>
<ul>
<li>创建权重和偏置变量。</li>
<li>将输入与权重做卷积运算。</li>
<li>将偏置加到第二步的卷积运算得到的结果中。</li>
<li>使用一个激活函数。</li>
</ul>
<p>上面的步骤使用原始的<code>Tensorflow</code>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1_1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    kernel = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=tf.float32, stddev=<span class="number">1e-1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    conv = tf.nn.conv2d(input, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">128</span>], dtype=tf.float32), trainable=<span class="keyword">True</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">    conv1 = tf.nn.relu(bias, name=scope)</span><br></pre></td></tr></table></figure>
<p>为了减少重复代码，<code>TF-Slim</code>提供了一些方便的、高级别的、更抽象的神经网络层。例如卷积层实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line">net = slim.conv2d(input, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1_1'</span>)</span><br></pre></td></tr></table></figure>
<p><code>TF-Slim</code>提供了大量的标准的实现，用于建立神经网络：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Layer</th>
<th>TF-Slim</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BiasAdd</code></td>
<td><code>slim.bias_add</code></td>
</tr>
<tr>
<td><code>BatchNorm</code></td>
<td><code>slim.batch_norm</code></td>
</tr>
<tr>
<td><code>Conv2d</code></td>
<td><code>slim.conv2d</code></td>
</tr>
<tr>
<td><code>Conv2dInPlane</code></td>
<td><code>slim.conv2d_in_plane</code></td>
</tr>
<tr>
<td><code>Conv2dTranspose(Deconv)</code></td>
<td><code>slim.conv2d_transpose</code></td>
</tr>
<tr>
<td><code>FullyConnected</code></td>
<td><code>slim.fully_connected</code></td>
</tr>
<tr>
<td><code>AvgPool2D</code></td>
<td><code>slim.avg_pool2d</code></td>
</tr>
<tr>
<td><code>Dropout</code></td>
<td><code>slim.dropout</code></td>
</tr>
<tr>
<td><code>Flatten</code></td>
<td><code>slim.flatten</code></td>
</tr>
<tr>
<td><code>MaxPool2D</code></td>
<td><code>slim.max_pool2d</code></td>
</tr>
<tr>
<td><code>OneHotEncoding</code></td>
<td><code>slim.one_hot_encoding</code></td>
</tr>
<tr>
<td><code>SeparableConv2</code></td>
<td><code>slim.separable_conv2d</code></td>
</tr>
<tr>
<td><code>UnitNorm</code></td>
<td><code>slim.unit_norm</code></td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;<code>TF-Slim</code>也包含两个操作符，称为<code>repeat</code>和<code>stack</code>，允许用户重复执行相同的操作。例如下面几个卷积层加一个池化层是<code>VGG</code>网络的一部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = ...</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>减少重复代码的其中一种方法是利用<code>for</code>循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = ...</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_%d'</span> % (i + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>另一种方式是使用<code>TF-Slim</code>中的<code>repeat</code>操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure>
<p>在上面例子中，<code>slim.repeat</code>会自动给每一个卷积层的<code>scopes</code>命名为<code>conv3/conv3_1</code>、<code>conv3/conv3_2</code>和<code>conv3/conv3_3</code>。<br>&emsp;&emsp;另外，<code>TF-Slim</code>的<code>slim.stack</code>操作允许用户用不同的参数重复调用同一种操作。<code>slim.stack</code>也为每一个被创建的操作创建一个新的<code>tf.variable_scope</code>。例如下面是一种简单的方式来创建多层感知器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.fully_connected(x, <span class="number">32</span>, scope=<span class="string">'fc/fc_1'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">64</span>, scope=<span class="string">'fc/fc_2'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">128</span>, scope=<span class="string">'fc/fc_3'</span>)</span><br><span class="line"><span class="comment"># Equivalent, TF-Slim way using slim.stack:</span></span><br><span class="line">slim.stack(x, slim.fully_connected, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure>
<p>在上面的例子中，<code>slim.stack</code>调用了<code>slim.fully_connected</code>三次。类似的，我们可以使用<code>stack</code>来简化多层的卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_1'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_2'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_3'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_4'</span>)</span><br><span class="line"><span class="comment"># Using stack:</span></span><br><span class="line">slim.stack(x, slim.conv2d, [(<span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>])], scope=<span class="string">'core'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Scopes"><a href="#Scopes" class="headerlink" title="Scopes"></a>Scopes</h4><p>&emsp;&emsp;除了<code>Tensorflow</code>中作用域(<code>scope</code>)之外(<code>name_scope</code>、<code>variable_scope</code>)，<code>TF-Slim</code>增加了新的作用域机制，称为<code>arg_scope</code>。这个新的作用域允许使用者明确一个或者多个操作和一些参数，这些定义好的操作或者参数会传递给<code>arg_scope</code>内部的每一个操作。先看如下代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(</span><br><span class="line">    inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>从上面的代码中可以清楚地看出来有<code>3</code>层卷积层，其中很多超参数都是一样的。两个卷积层有相同的<code>padding</code>，所有三个卷积层有相同的<code>weights_initializer</code>和<code>weight_regularizer</code>。上面的代码包含了大量重复的值，其中一种解决方法是使用变量来说明一些默认的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">padding = <span class="string">'SAME'</span></span><br><span class="line">initializer = tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">regularizer = slim.l2_regularizer(<span class="number">0.0005</span>)</span><br><span class="line">​</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=padding, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(</span><br><span class="line">    net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=padding, weights_initializer=initializer,</span><br><span class="line">    weights_regularizer=regularizer, scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的解决方案其实并没有减少代码的混乱程度。通过使用<code>arg_scope</code>，我们既可以保证每一层使用相同的值，也可以简化代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d], padding=<span class="string">'SAME'</span>,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv2'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的例子表明，使用<code>arg_scope</code>可以使得代码变得更整洁、更干净并且更加容易维护。注意到，在<code>arg_scope</code>中规定的参数值，它们可以被局部覆盖。例如上面的<code>padding</code>参数被设置成<code>SAME</code>，但是在第二个卷积层中用<code>VALID</code>覆盖了这个参数。<br>&emsp;&emsp;我们也可以嵌套使用<code>arg_scope</code>，在相同的作用域内使用多个操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,</span><br><span class="line">    weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d], stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>):</span><br><span class="line">        net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv1'</span>)</span><br><span class="line">        net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">5</span>, <span class="number">5</span>], weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.03</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">1000</span>, activation_fn=<span class="keyword">None</span>, scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure>
<p>在第一个<code>arg_scope</code>中，卷积层和全连接层被应用于相同的权重初始化和权重正则化；在第二个<code>arg_scope</code>中，额外的参数仅仅对卷积层<code>conv2d</code>起作用。</p>
<h3 id="Working-Example-Specifying-the-VGG16-Layers"><a href="#Working-Example-Specifying-the-VGG16-Layers" class="headerlink" title="Working Example: Specifying the VGG16 Layers"></a>Working Example: Specifying the VGG16 Layers</h3><p>&emsp;&emsp;通过结合<code>TF-Slim</code>的<code>Variables</code>、<code>Operations</code>和<code>scopes</code>，我们可以使用比较少的代码来实现一个比较复杂的网络。例如整个<code>VGG</code>网络定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg16</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">     [slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,</span><br><span class="line">     weights_initializer=tf.truncated_normal_initializer(<span class="number">0.0</span>, <span class="number">0.01</span>),</span><br><span class="line">     weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">        net = slim.repeat(inputs, <span class="number">2</span>, slim.conv2d, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool1'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">2</span>, slim.conv2d, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv2'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool3'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv4'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool4'</span>)</span><br><span class="line">        net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv5'</span>)</span><br><span class="line">        net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool5'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">4096</span>, scope=<span class="string">'fc6'</span>)</span><br><span class="line">        net = slim.dropout(net, <span class="number">0.5</span>, scope=<span class="string">'dropout6'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">4096</span>, scope=<span class="string">'fc7'</span>)</span><br><span class="line">        net = slim.dropout(net, <span class="number">0.5</span>, scope=<span class="string">'dropout7'</span>)</span><br><span class="line">        net = slim.fully_connected(net, <span class="number">1000</span>, activation_fn=<span class="keyword">None</span>, scope=<span class="string">'fc8'</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<h3 id="Training-Models"><a href="#Training-Models" class="headerlink" title="Training Models"></a>Training Models</h3><p>&emsp;&emsp;训练<code>Tensorflow</code>模型要求一个模型、一个<code>loss function</code>、梯度计算和一个训练的程序，用来迭代地根据<code>loss</code>计算模型权重的梯度和更新权重。<code>TF-Slim</code>提供了<code>loss function</code>和一些帮助函数来运行训练和评估。</p>
<h4 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h4><p>&emsp;&emsp;<code>Loss function</code>定义了一个我们需要最小化的量。对于分类问题，主要是计算真正的分布与预测的概率分布之间的交叉熵；对于回归问题，主要是计算预测值与真实值均方误差。<br>&emsp;&emsp;特定的模型，比如说多任务学习模型，要求同时使用多个<code>loss function</code>。换句话说，最终被最小化的<code>loss function</code>是多个其他的<code>loss function</code>之和。例如一个同时预测图像中场景的类型和深度的模型，该模型的<code>loss function</code>就是分类<code>loss</code>和深度预测<code>loss</code>之和。<br>&emsp;&emsp;<code>TF-Slim</code>通过<code>losses</code>模块为用户提供了一种机制，使得定义<code>loss function</code>变得简单。例如下面的是我们想要训练<code>VGG</code>网络的简单示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">images, labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">predictions, _ = vgg.vgg_16(images)  <span class="comment"># Create the model</span></span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">loss = slim.losses.softmax_cross_entropy(predictions, labels)</span><br></pre></td></tr></table></figure>
<p>在上面这个例子中，我们首先创建一个模型(利用<code>TF-Slim</code>的<code>VGG</code>实现)，然后增加了标准的分类<code>loss</code>。现在来看看当我们有一个多个输出的多任务模型的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">images, scene_labels, depth_labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">scene_predictions, depth_predictions = CreateMultiTaskModel(images)  <span class="comment"># Create the model</span></span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)</span><br><span class="line">sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)</span><br><span class="line"><span class="comment"># The following two lines have the same effect:</span></span><br><span class="line">total_loss = classification_loss + sum_of_squares_loss</span><br><span class="line">total_loss = slim.losses.get_total_loss(add_regularization_losses=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中有<code>2</code>个<code>loss</code>，是通过调用<code>slim.losses.softmax_cross_entropy</code>和<code>slim.losses.sum_of_squares</code>得到。我们可以将这两个<code>loss</code>加在一起，或者调用<code>slim.losses.get_total_loss</code>来得到全部的<code>loss</code>(<code>total_loss</code>)。这是如何工作的？当你通过<code>TF-Slim</code>创建一个<code>loss</code>时，<code>TF-Slim</code>将<code>loss</code>加到一个特殊的<code>TensorFlow collection of loss functions</code>。这使得你既可以手动地管理全部的<code>loss</code>，也可以让<code>TF-Slim</code>来替你管理它们。<br>&emsp;&emsp;如果你想让<code>TF-Slim</code>为你管理<code>losses</code>，但是你有一个自己实现的<code>loss</code>该怎么办？<code>loss_ops.py</code>也有一个函数可以将你自己实现的<code>loss</code>加到<code>TF-Slims collection</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">images, scene_labels, depth_labels, pose_labels = ...  <span class="comment"># Load the images and labels</span></span><br><span class="line">scene_predictions, depth_predictions, pose_predictions = CreateMultiTaskModel(images)  <span class="comment"># Create the model</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the loss functions and get the total loss</span></span><br><span class="line">classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)</span><br><span class="line">sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)</span><br><span class="line">pose_loss = MyCustomLossFunction(pose_predictions, pose_labels)</span><br><span class="line">slim.losses.add_loss(pose_loss)  <span class="comment"># Letting TF-Slim know about the additional loss.</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># The following two ways to compute the total loss are equivalent:</span></span><br><span class="line">regularization_loss = tf.add_n(slim.losses.get_regularization_losses())</span><br><span class="line">total_loss1 = classification_loss + sum_of_squares_loss + pose_loss + regularization_loss</span><br><span class="line">​</span><br><span class="line">total_loss2 = slim.losses.get_total_loss()  <span class="comment"># (Regularization Loss is included in the total loss by default)</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们既可以手动地计算的出全部的<code>loss function</code>，也可以让<code>TF-Slim</code>知道这个额外的<code>loss</code>，然后让<code>TF-Slim</code>处理这个<code>loss</code>。</p>
<h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><p>&emsp;&emsp;<code>TF-Slim</code>提供了一个简单但是很强的用于训练模型的工具(在<code>learning.py</code>中)，其中包括一个可以重复测量<code>loss</code>、计算梯度和将模型保存到磁盘的训练函数。举个例子，一旦定义好了模型、<code>loss function</code>和最优化方法，我们可以调用<code>slim.learning.create_train_op</code>和<code>slim.learning.train</code>来实现优化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the model and specify the losses</span></span><br><span class="line">...</span><br><span class="line">total_loss = slim.losses.get_total_loss()</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line"><span class="comment"># create_train_op ensures that each time we ask for the loss,</span></span><br><span class="line"><span class="comment"># the update_ops are run and the gradients being computed are applied too</span></span><br><span class="line">train_op = slim.learning.create_train_op(total_loss, optimizer)</span><br><span class="line">logdir = ...  <span class="comment"># Where checkpoints are stored.</span></span><br><span class="line">slim.learning.train(train_op, logdir, number_of_steps=<span class="number">1000</span>, save_summaries_secs=<span class="number">300</span>, save_interval_secs=<span class="number">600</span>):</span><br></pre></td></tr></table></figure>
<p>在这个例子中，提供给<code>slim.learning.train</code>的参数有<code>train_op</code>(用于计算<code>loss</code>和梯度)；<code>logdir</code>(用于声明<code>checkpoints</code>和<code>event</code>文件保存的路径)；用<code>number_of_steps</code>参数来限制梯度下降的步数；<code>save_summaries_secs=300</code>表明每<code>5</code>分钟计算一次<code>summaries</code>；<code>save_interval_secs=600</code>表明每<code>10</code>分钟保存一次模型的<code>checkpoint</code>。</p>
<h3 id="Working-Example-Training-the-VGG16-Model"><a href="#Working-Example-Training-the-VGG16-Model" class="headerlink" title="Working Example: Training the VGG16 Model"></a>Working Example: Training the VGG16 Model</h3><p>&emsp;&emsp;下面是训练一个<code>VGG</code>网络的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">...</span><br><span class="line">train_log_dir = ...</span><br><span class="line">​</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(train_log_dir):</span><br><span class="line">    tf.gfile.MakeDirs(train_log_dir)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    images, labels = ...  <span class="comment"># Set up the data loading</span></span><br><span class="line">    predictions = vgg.vgg_16(images, is_training=<span class="keyword">True</span>)  <span class="comment"># Define the model</span></span><br><span class="line">    <span class="comment"># Specify the loss function:</span></span><br><span class="line">    slim.losses.softmax_cross_entropy(predictions, labels)</span><br><span class="line">    total_loss = slim.losses.get_total_loss()</span><br><span class="line">    tf.summary.scalar(<span class="string">'losses/total_loss'</span>, total_loss)</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">.001</span>)  <span class="comment"># Specify the optimization scheme</span></span><br><span class="line">    <span class="comment"># create_train_op that ensures that when we evaluate it to get the loss,</span></span><br><span class="line">    <span class="comment"># the update_ops are done and the gradient updates are computed</span></span><br><span class="line">    train_tensor = slim.learning.create_train_op(total_loss, optimizer)</span><br><span class="line">    slim.learning.train(train_tensor, train_log_dir)  <span class="comment"># Actually runs training</span></span><br></pre></td></tr></table></figure>
<h3 id="Fine-Tuning-Existing-Models"><a href="#Fine-Tuning-Existing-Models" class="headerlink" title="Fine-Tuning Existing Models"></a>Fine-Tuning Existing Models</h3><h4 id="Brief-Recap-on-Restoring-Variables-from-a-Checkpoint"><a href="#Brief-Recap-on-Restoring-Variables-from-a-Checkpoint" class="headerlink" title="Brief Recap on Restoring Variables from a Checkpoint"></a>Brief Recap on Restoring Variables from a Checkpoint</h4><p>&emsp;&emsp;当一个模型被训练完毕之后，它可以从一个给定的<code>checkpoint</code>中使用<code>tf.train.Saver</code>来恢复变量。在很多情况下，<code>tf.train.Saver</code>提供一个简单的机制来恢复所有变量或者一部分变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables</span></span><br><span class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</span><br><span class="line">restorer = tf.train.Saver()  <span class="comment"># Add ops to restore all the variables</span></span><br><span class="line">restorer = tf.train.Saver([v1, v2])  <span class="comment"># Add ops to restore some variables</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and do some work with the model</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br><span class="line">    print(<span class="string">"Model restored."</span>)</span><br><span class="line">    <span class="comment"># Do some work with the model</span></span><br></pre></td></tr></table></figure>
<h4 id="Partially-Restoring-Models"><a href="#Partially-Restoring-Models" class="headerlink" title="Partially Restoring Models"></a>Partially Restoring Models</h4><p>&emsp;&emsp;在一个新的数据集或者一个新的任务上<code>fine-tune</code>一个预训练的模型通常是比较流行的，我们可以使用<code>TF-Slim</code>的<code>helper</code>函数来选择想要恢复的一部分变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables.</span></span><br><span class="line">v1 = slim.variable(name=<span class="string">"v1"</span>, ...)</span><br><span class="line">v2 = slim.variable(name=<span class="string">"nested/v2"</span>, ...)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Get list of variables to restore (which contains only 'v2').</span></span><br><span class="line"><span class="comment"># These are all equivalent methods:</span></span><br><span class="line">variables_to_restore = slim.get_variables_by_name(<span class="string">"v2"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_by_suffix(<span class="string">"2"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables(scope=<span class="string">"nested"</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(include=[<span class="string">"nested"</span>])</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(exclude=[<span class="string">"v1"</span>])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the saver which will be used to restore the variables</span></span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br><span class="line">    print(<span class="string">"Model restored."</span>)</span><br><span class="line">    <span class="comment"># Do some work with the model</span></span><br></pre></td></tr></table></figure>
<h4 id="Restoring-models-with-different-variable-names"><a href="#Restoring-models-with-different-variable-names" class="headerlink" title="Restoring models with different variable names"></a>Restoring models with different variable names</h4><p>&emsp;&emsp;当从一个<code>checkpoint</code>中恢复变量时，<code>Saver</code>定位在<code>checkpoint</code>文件中变量的名字，并且将它们映射到当前图(<code>graph</code>)的变量中去。在上面的例子中，我们通过传递给<code>saver</code>一个变量列表来创建一个<code>saver</code>。在这种情况下，在<code>checkpoint</code>文件中定位的变量名隐式地从每个提供的变量的<code>var.op.name</code>中获得。<br>&emsp;&emsp;当<code>checkpoint</code>文件中的变量名与<code>graph</code>匹配时，将会工作良好。然而有时候，我们想要从一个与当前的<code>graph</code>不同变量名的<code>checkpoint</code>中恢复变量，那么在这种情况下，我们必须给<code>Saver</code>提供一个字典，该字典将每个<code>checkpoint</code>中变量名映射到每个<code>graph</code>的变量。下面的例子通过一个简单的函数获得<code>checkpoint</code>中的变量的名字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assuming than 'conv1/weights' should be restored from 'vgg16/conv1/weights'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_in_checkpoint</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'vgg16/'</span> + var.op.name</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Assuming than 'conv1/weights' and 'conv1/bias' should be restored from 'conv1/params1' and 'conv1/params2'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_in_checkpoint</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"weights"</span> <span class="keyword">in</span> var.op.name:</span><br><span class="line">        <span class="keyword">return</span> var.op.name.replace(<span class="string">"weights"</span>, <span class="string">"params1"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">"bias"</span> <span class="keyword">in</span> var.op.name:</span><br><span class="line">        <span class="keyword">return</span> var.op.name.replace(<span class="string">"bias"</span>, <span class="string">"params2"</span>)</span><br><span class="line">​</span><br><span class="line">variables_to_restore = slim.get_model_variables()</span><br><span class="line">variables_to_restore = &#123;name_in_checkpoint(var): var <span class="keyword">for</span> var <span class="keyword">in</span> variables_to_restore&#125;</span><br><span class="line">restorer = tf.train.Saver(variables_to_restore)</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    restorer.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)  <span class="comment"># Restore variables from disk</span></span><br></pre></td></tr></table></figure>
<h4 id="Fine-Tuning-a-Model-on-a-different-task"><a href="#Fine-Tuning-a-Model-on-a-different-task" class="headerlink" title="Fine-Tuning a Model on a different task"></a>Fine-Tuning a Model on a different task</h4><p>&emsp;&emsp;考虑这么一种情况：我们有一个预训练好的<code>VGG16</code>模型，该模型是在<code>ImageNet</code>数据集上训练好的，有<code>1000</code>类。然而我们想要将其应用到只有<code>20</code>类的<code>Pascal VOC</code>数据集上。为了实现这个功能，我们可以使用不包括最后一层的预训练模型来初始化新模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">image, label = MyPascalVocDataLoader(...)  <span class="comment"># Load the Pascal VOC data</span></span><br><span class="line">images, labels = tf.train.batch([image, label], batch_size=<span class="number">32</span>)</span><br><span class="line">​</span><br><span class="line">predictions = vgg.vgg_16(images)  <span class="comment"># Create the model</span></span><br><span class="line">train_op = slim.learning.create_train_op(...)</span><br><span class="line"><span class="comment"># Specify where the Model, trained on ImageNet, was saved</span></span><br><span class="line">model_path = <span class="string">'/path/to/pre_trained_on_imagenet.checkpoint'</span></span><br><span class="line">log_dir = <span class="string">'/path/to/my_pascal_model_dir/'</span>  <span class="comment"># Specify where the new model will live</span></span><br><span class="line"><span class="comment"># Restore only the convolutional layers</span></span><br><span class="line">variables_to_restore = slim.get_variables_to_restore(exclude=[<span class="string">'fc6'</span>, <span class="string">'fc7'</span>, <span class="string">'fc8'</span>])</span><br><span class="line">init_fn = assign_from_checkpoint_fn(model_path, variables_to_restore)</span><br><span class="line">slim.learning.train(train_op, log_dir, init_fn=init_fn)  <span class="comment"># Start training</span></span><br></pre></td></tr></table></figure>
<h3 id="Evaluating-Models"><a href="#Evaluating-Models" class="headerlink" title="Evaluating Models"></a>Evaluating Models</h3><p>&emsp;&emsp;一旦我们已经训练好了一个模型(或者模型正在训练之中)，想要看看模型的实际表现能力，这个可以通过使用一些评估度量来实现，该度量可以对模型的表现能力评分。而评估代码实际上是加载数据、做出预测、将预测结果与真实值做比较，最后得到得分。这个步骤可以运行一次或者周期重复。</p>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>&emsp;&emsp;我们将度量定义为一个性能度量，它不是一个<code>loss</code>函数(<code>losses</code>是在训练的时候直接最优化)，但我们仍然感兴趣的是评估模型的目的。例如我们想要最优化<code>log loss</code>，但是我们感兴趣的度量可能是<code>F1</code>得分(<code>test accuracy</code>)，或者是<code>Intersection Over Union score</code>(这是不可微的，因此不能作为损失使用)。<br>&emsp;&emsp;<code>TF-Slim</code>提供了一些使得评估模型变得简单的度量操作。计算度量的值可以分为以下三个步骤：</p>
<ul>
<li>初始化(<code>Initialization</code>)：初始化用于计算度量的变量。</li>
<li>聚合(<code>Aggregation</code>)：使用操作(比如求和操作)来计算度量。</li>
<li>终止化(<code>Finalization</code>)：这是可选的步骤，使用最终的操作来计算度量值，比如说计算均值、最小值、最大值等。</li>
</ul>
<p>举个例子，为了计算<code>mean_absolute_error</code>，变量<code>count</code>和<code>total</code>变量被初始化为<code>0</code>。在聚合期间，我们观测到一些预测值和标签值，计算它们的绝对差值然后加到<code>total</code>中。每一次我们观测到新的一个数据，我们增加<code>count</code>。最后在<code>Finalization</code>期间，<code>total</code>除以<code>count</code>来获得均值<code>mean</code>。<br>&emsp;&emsp;下面的示例演示了声明度量标准的<code>API</code>。由于度量经常在测试集上进行评估，因此我们假设使用的是测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">images, labels = LoadTestData(...)</span><br><span class="line">predictions = MyModel(images)</span><br><span class="line">​</span><br><span class="line">mae_value_op, mae_update_op = slim.metrics.streaming_mean_absolute_error(predictions, labels)</span><br><span class="line">mre_value_op, mre_update_op = slim.metrics.streaming_mean_relative_error(predictions, labels)</span><br><span class="line">pl_value_op, pl_update_op = slim.metrics.percentage_less(mean_relative_errors, <span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>一个度量的创建返回两个值：<code>value_op</code>和<code>update_op</code>。<code>value_op</code>是一个幂等操作，它返回度量的当前值；<code>update_op</code>是执行上面提到的聚合步骤的操作，以及返回度量的值。<br>&emsp;&emsp;跟踪每个<code>value_op</code>和<code>update_op</code>是很费力的，为了解决这个问题，<code>TF-Slim</code>提供了两个便利功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Aggregates the value and update ops in two lists:</span></span><br><span class="line">value_ops, update_ops = slim.metrics.aggregate_metrics(</span><br><span class="line">    slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    slim.metrics.streaming_mean_squared_error(predictions, labels))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Aggregates the value and update ops in two dictionaries</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">"eval/mean_absolute_error"</span>: slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    <span class="string">"eval/mean_squared_error"</span>: slim.metrics.streaming_mean_squared_error(predictions, labels),</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="Working-example-Tracking-Multiple-Metrics"><a href="#Working-example-Tracking-Multiple-Metrics" class="headerlink" title="Working example: Tracking Multiple Metrics"></a>Working example: Tracking Multiple Metrics</h3><p>&emsp;&emsp;将代码全部放在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim.nets <span class="keyword">as</span> nets</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">vgg = nets.vgg</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Load the data</span></span><br><span class="line">images, labels = load_data(...)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define the network</span></span><br><span class="line">predictions = vgg.vgg_16(images)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Choose the metrics to compute:</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">"eval/mean_absolute_error"</span>: slim.metrics.streaming_mean_absolute_error(predictions, labels),</span><br><span class="line">    <span class="string">"eval/mean_squared_error"</span>: slim.metrics.streaming_mean_squared_error(predictions, labels),</span><br><span class="line">&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Evaluate the model using 1000 batches of data:</span></span><br><span class="line">num_batches = <span class="number">1000</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    sess.run(tf.local_variables_initializer())</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">for</span> batch_id <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        sess.run(names_to_updates.values())</span><br><span class="line">​</span><br><span class="line">    metric_values = sess.run(names_to_values.values())</span><br><span class="line">    <span class="keyword">for</span> metric, value <span class="keyword">in</span> zip(names_to_values.keys(), metric_values):</span><br><span class="line">        print(<span class="string">'Metric %s has value: %f'</span> % (metric, value))</span><br></pre></td></tr></table></figure>
<h3 id="Evaluation-Loop"><a href="#Evaluation-Loop" class="headerlink" title="Evaluation Loop"></a>Evaluation Loop</h3><p>&emsp;&emsp;<code>TF-Slim</code>提供了一个评估模块(<code>evaluation.py</code>)，它包含了使用来自<code>metric_ops.py</code>模块编写模型评估脚本的辅助函数。这些功能包括定期运行评估、对数据批量进行评估、打印和汇总度量结果的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">​</span><br><span class="line">slim = tf.contrib.slim</span><br><span class="line">images, labels = load_data(...)  <span class="comment"># Load the data</span></span><br><span class="line">predictions = MyModel(images)  <span class="comment"># Define the network</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Choose the metrics to compute:</span></span><br><span class="line">names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(&#123;</span><br><span class="line">    <span class="string">'accuracy'</span>: slim.metrics.accuracy(predictions, labels),</span><br><span class="line">    <span class="string">'precision'</span>: slim.metrics.precision(predictions, labels),</span><br><span class="line">    <span class="string">'recall'</span>: slim.metrics.recall(mean_relative_errors, <span class="number">0.3</span>),</span><br><span class="line">&#125;)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the summary ops such that they also print out to std output:</span></span><br><span class="line">summary_ops = []</span><br><span class="line"><span class="keyword">for</span> metric_name, metric_value <span class="keyword">in</span> names_to_values.iteritems():</span><br><span class="line">    op = tf.summary.scalar(metric_name, metric_value)</span><br><span class="line">    op = tf.Print(op, [metric_value], metric_name)</span><br><span class="line">    summary_ops.append(op)</span><br><span class="line">​</span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_batches = math.ceil(num_examples / float(batch_size))</span><br><span class="line">​</span><br><span class="line">slim.get_or_create_global_step()  <span class="comment"># Setup the global step</span></span><br><span class="line">​</span><br><span class="line">output_dir = ...  <span class="comment"># Where the summaries are stored.</span></span><br><span class="line">eval_interval_secs = ...  <span class="comment"># How often to run the evaluation.</span></span><br><span class="line">slim.evaluation.evaluation_loop(</span><br><span class="line">    <span class="string">'local'</span>, checkpoint_dir, log_dir, num_evals=num_batches, eval_op=names_to_updates.values(),</span><br><span class="line">    summary_op=tf.summary.merge(summary_ops), eval_interval_secs=eval_interval_secs)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="tf-nn-conv2d和tf-contrib-slim-conv2d的区别"><a href="#tf-nn-conv2d和tf-contrib-slim-conv2d的区别" class="headerlink" title="tf.nn.conv2d和tf.contrib.slim.conv2d的区别"></a>tf.nn.conv2d和tf.contrib.slim.conv2d的区别</h3><p>&emsp;&emsp;在查看代码的时候，发现有代码用到卷积层是<code>tf.nn.conv2d</code>，但是也有使用的卷积层是<code>tf.contrib.slim.conv2d</code>。<code>tf.nn.conv2d</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：需要做卷积的输入图像，它要求是一个<code>Tensor</code>，具有<code>[batch_size, in_height, in_width, in_channels]</code>这样的<code>shape</code>，具体含义是<code>[训练时一个batch的图片数量，图片高度，图片宽度，图像通道数]</code>。注意这是一个<code>4</code>维的<code>Tensor</code>，要求数据类型为<code>float32</code>和<code>float64</code>其中之一。</li>
<li><code>filter</code>：指定<code>CNN</code>中的卷积核，它要求是一个<code>Tensor</code>，具有<code>[filter_height, filter_width, in_channels, out_channels]</code>这样的<code>shape</code>，具体含义是<code>[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</code>，要求类型与参数<code>input</code>相同。有一个地方需要注意，第三维<code>in_channels</code>就是参数<code>input</code>的第四维，这里是维度一致，不是数值一致。</li>
<li><code>strides</code>：卷积时在图像每一维的步长。这是一个一维的向量，长度为<code>4</code>，对应的是在<code>input</code>的<code>4</code>个维度上的步长。</li>
<li><code>padding</code>：<code>string</code>类型的变量，只能是<code>SAME</code>、<code>VALID</code>其中之一。这个值决定了不同的卷积方式，<code>SAME</code>代表卷积核可以停留图像边缘，<code>VALID</code>表示不能。</li>
<li><code>use_cudnn_on_gpu</code>：指定是否使用<code>cudnn</code>加速。</li>
<li><code>data_format</code>：指定<code>input</code>的格式，默认为<code>NHWC</code>格式。</li>
</ul>
<p>该函数结果返回一个<code>Tensor</code>，这个输出就是我们常说的<code>feature map</code>。<br>&emsp;&emsp;<code>tf.contrib.slim.conv2d</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">convolution(</span><br><span class="line">    inputs, num_outputs, kernel_size, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">    rate=<span class="number">1</span>, activation_fn=nn.relu, normalizer_fn=<span class="keyword">None</span>, normalizer_params=<span class="keyword">None</span>,</span><br><span class="line">    weights_initializer=initializers.xavier_initializer(), weights_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    biases_initializer=init_ops.zeros_initializer(), biases_regularizer=<span class="keyword">None</span>, reuse=<span class="keyword">None</span>,</span><br><span class="line">    variables_collections=<span class="keyword">None</span>, outputs_collections=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, scope=<span class="keyword">None</span>):</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inputs</code>：需要做卷积的输入图像。</li>
<li><code>num_outputs</code>：卷积核的个数(就是<code>filter</code>的个数)。</li>
<li><code>kernel_size</code>：卷积核的维度(卷积核的宽度和高度)。</li>
<li><code>stride</code>：卷积时在图像每一维的步长。</li>
<li><code>padding</code>：<code>padding</code>的方式选择，<code>VALID</code>或者<code>SAME</code>。</li>
<li><code>data_format</code>：指定<code>input</code>的格式。</li>
<li><code>rate</code>：使用<code>atrous convolution</code>的膨胀率。</li>
<li><code>activation_fn</code>：指定激活函数，默认为<code>ReLU</code>函数。</li>
<li><code>normalizer_fn</code>：正则化函数。</li>
<li><code>normalizer_params</code>：正则化函数的参数。</li>
<li><code>weights_initializer</code>：权重的初始化程序。</li>
<li><code>weights_regularizer</code>：权重可选的正则化程序。</li>
<li><code>biases_initializer</code>：<code>biase</code>的初始化程序。</li>
<li><code>biases_regularizer</code>：<code>biases</code>可选的正则化程序。</li>
<li><code>reuse</code>：是否共享层或者核变量。</li>
<li><code>variable_collections</code>：所有变量的集合列表或者字典。</li>
<li><code>outputs_collections</code>：输出被添加的集合。</li>
<li><code>trainable</code>：卷积层的参数是否可被训练。</li>
<li><code>scope</code>：共享变量所指的<code>variable_scope</code>。</li>
</ul>
<p>&emsp;&emsp;在上述的<code>API</code>中，可以看出去两者并没有什么不同。只是<code>tf.contrib.slim.conv2d</code>提供了更多可以指定的初始化部分，而对于<code>tf.nn.conv2d</code>而言，其指定<code>filter</code>的方式相比较<code>tf.contrib.slim.conv2d</code>来说更加得复杂。去除掉少用的初始化部分，其实两者的<code>API</code>可以简化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.slim.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    num_outputs,  <span class="comment"># [卷积核个数]</span></span><br><span class="line">    kernel_size,  <span class="comment"># [卷积核的高度，卷积核的宽度]</span></span><br><span class="line">    stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>,)</span><br><span class="line">​</span><br><span class="line">tf.nn.conv2d(</span><br><span class="line">    input,  <span class="comment"># 与上述一致</span></span><br><span class="line">    filter,  <span class="comment"># [卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</span></span><br><span class="line">    strides, padding,)</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/13/深度学习/TensorFlow之variable scope/" rel="next" title="TensorFlow之variable scope">
                <i class="fa fa-chevron-left"></i> TensorFlow之variable scope
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/13/深度学习/Tf.reduce类函数/" rel="prev" title="Tf.reduce类函数">
                Tf.reduce类函数 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">958</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-are-the-various-components-of-TF-Slim"><span class="nav-number">1.</span> <span class="nav-text">What are the various components of TF-Slim?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Defining-Models"><span class="nav-number">1.1.</span> <span class="nav-text">Defining Models</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Variables"><span class="nav-number">1.2.</span> <span class="nav-text">Variables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Layers"><span class="nav-number">1.3.</span> <span class="nav-text">Layers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scopes"><span class="nav-number">1.4.</span> <span class="nav-text">Scopes</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Working-Example-Specifying-the-VGG16-Layers"><span class="nav-number">2.</span> <span class="nav-text">Working Example: Specifying the VGG16 Layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Models"><span class="nav-number">3.</span> <span class="nav-text">Training Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Losses"><span class="nav-number">3.1.</span> <span class="nav-text">Losses</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-Loop"><span class="nav-number">3.2.</span> <span class="nav-text">Training Loop</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Working-Example-Training-the-VGG16-Model"><span class="nav-number">4.</span> <span class="nav-text">Working Example: Training the VGG16 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fine-Tuning-Existing-Models"><span class="nav-number">5.</span> <span class="nav-text">Fine-Tuning Existing Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Brief-Recap-on-Restoring-Variables-from-a-Checkpoint"><span class="nav-number">5.1.</span> <span class="nav-text">Brief Recap on Restoring Variables from a Checkpoint</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Partially-Restoring-Models"><span class="nav-number">5.2.</span> <span class="nav-text">Partially Restoring Models</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Restoring-models-with-different-variable-names"><span class="nav-number">5.3.</span> <span class="nav-text">Restoring models with different variable names</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-Tuning-a-Model-on-a-different-task"><span class="nav-number">5.4.</span> <span class="nav-text">Fine-Tuning a Model on a different task</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-Models"><span class="nav-number">6.</span> <span class="nav-text">Evaluating Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Metrics"><span class="nav-number">6.1.</span> <span class="nav-text">Metrics</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Working-example-Tracking-Multiple-Metrics"><span class="nav-number">7.</span> <span class="nav-text">Working example: Tracking Multiple Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-Loop"><span class="nav-number">8.</span> <span class="nav-text">Evaluation Loop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-nn-conv2d和tf-contrib-slim-conv2d的区别"><span class="nav-number">9.</span> <span class="nav-text">tf.nn.conv2d和tf.contrib.slim.conv2d的区别</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
