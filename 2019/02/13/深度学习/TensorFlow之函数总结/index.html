<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="get_shape函数&amp;emsp;&amp;emsp;get_shape函数主要用于获取一个张量的维度，并且输出张量每个维度上面的值。如果是二维矩阵，也就是输出行和列的值，使用非常方便。 123456import tensorflow as tfwith tf.Session() as sess:    A = tf.random_normal(shape=[3, 4])    print(A.get_s">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow之函数总结">
<meta property="og:url" content="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之函数总结/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:description" content="get_shape函数&amp;emsp;&amp;emsp;get_shape函数主要用于获取一个张量的维度，并且输出张量每个维度上面的值。如果是二维矩阵，也就是输出行和列的值，使用非常方便。 123456import tensorflow as tfwith tf.Session() as sess:    A = tf.random_normal(shape=[3, 4])    print(A.get_s">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-03-13T07:34:17.042Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow之函数总结">
<meta name="twitter:description" content="get_shape函数&amp;emsp;&amp;emsp;get_shape函数主要用于获取一个张量的维度，并且输出张量每个维度上面的值。如果是二维矩阵，也就是输出行和列的值，使用非常方便。 123456import tensorflow as tfwith tf.Session() as sess:    A = tf.random_normal(shape=[3, 4])    print(A.get_s">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之函数总结/">





  <title>TensorFlow之函数总结 | 泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/13/深度学习/TensorFlow之函数总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow之函数总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T19:48:39+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="get-shape函数"><a href="#get-shape函数" class="headerlink" title="get_shape函数"></a>get_shape函数</h3><p>&emsp;&emsp;<code>get_shape</code>函数主要用于获取一个张量的维度，并且输出张量每个维度上面的值。如果是二维矩阵，也就是输出行和列的值，使用非常方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    A = tf.random_normal(shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    print(A.get_shape())</span><br><span class="line">    print(A.get_shape)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">&lt;bound method Tensor.get_shape of &lt;tf.Tensor <span class="string">'random_normal:0'</span> shape=(<span class="number">3</span>, <span class="number">4</span>) dtype=float32&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>第一个输出是一个元祖，就是数值，而第二输出就是一个张量的对象，里面包含更多的东西。如果你需要输出某一个维度上面的值，那就用下面的这种方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.get_shape()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这就表示第一个维度。该函数经常和<code>as_list</code>一起使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">varX = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">varX_shape = tf.shape(varX)</span><br><span class="line">print(sess.run(varX_shape))  <span class="comment"># 输出“[3 3]”</span></span><br><span class="line">varX_shape = varX.get_shape().as_list()</span><br><span class="line">print(varX_shape)  <span class="comment"># 输出“[3, 3]”</span></span><br></pre></td></tr></table></figure>
<h3 id="random-normal函数"><a href="#random-normal函数" class="headerlink" title="random_normal函数"></a>random_normal函数</h3><p>&emsp;&emsp;<code>tf.random_normal</code>函数用于从服从指定正态分布的数值中取出指定个数的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random_normal(shape, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32, seed=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>shape</code>：输出张量的形状。</li>
<li><code>mean</code>：正态分布的均值。</li>
<li><code>stddev</code>是正态分布的标准差。</li>
<li><code>dtype</code>：输出的类型。</li>
<li><code>seed</code>：随机数种子，是一个整数。</li>
<li><code>name</code>：操作的名称。</li>
</ul>
<p>&emsp;&emsp;以下程序定义一个<code>w1</code>变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(w1)</span><br><span class="line">    print(sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable <span class="string">'Variable:0'</span> shape=(<span class="number">2</span>, <span class="number">3</span>) dtype=float32_ref&gt;</span><br><span class="line">[[<span class="number">-0.8113182</span>   <span class="number">1.4845988</span>   <span class="number">0.06532937</span>]</span><br><span class="line"> [<span class="number">-2.4427042</span>   <span class="number">0.0992484</span>   <span class="number">0.5912243</span> ]]</span><br></pre></td></tr></table></figure>
<p>变量<code>w1</code>声明之后并没有被赋值，需要在<code>Session</code>中调用<code>run(tf.global_variables_initializer())</code>方法初始化之后才会被具体赋值。<code>tf</code>中张量与常规向量不同的是，执行<code>print(w1)</code>输出的是<code>w1</code>的形状和数据类型等属性信息，获取<code>w1</code>的值需要调用<code>sess.run(w1)</code>方法。</p>
<h3 id="TensorFlow的算术操作"><a href="#TensorFlow的算术操作" class="headerlink" title="TensorFlow的算术操作"></a>TensorFlow的算术操作</h3><p>&emsp;&emsp;<code>TensorFlow</code>的算术操作如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tf.add(x, y, name=None)</code></td>
<td>求和</td>
</tr>
<tr>
<td><code>tf.multiply(x, y, name=None)</code></td>
<td>减法</td>
</tr>
<tr>
<td><code>tf.multiply(x, y, name=None)</code></td>
<td>乘法</td>
</tr>
<tr>
<td><code>tf.div(x, y, name=None)</code>，推荐使用<code>tf.divide</code></td>
<td>除法</td>
</tr>
<tr>
<td><code>tf.mod(x, y, name=None)</code></td>
<td>取模</td>
</tr>
<tr>
<td><code>tf.abs(x, name=None)</code></td>
<td>求绝对值</td>
</tr>
<tr>
<td><code>tf.negative(x, name=None)</code></td>
<td>取负(<code>y = -x</code>)</td>
</tr>
<tr>
<td><code>tf.sign(x, name=None)</code></td>
<td>返回符号</td>
</tr>
<tr>
<td><code>tf.inv(x, name=None)</code></td>
<td>取反</td>
</tr>
<tr>
<td><code>tf.square(x, name=None)</code></td>
<td>计算平方</td>
</tr>
<tr>
<td><code>tf.round(x, name=None)</code></td>
<td>四舍五入最接近的整数</td>
</tr>
<tr>
<td><code>tf.sqrt(x, name=None)</code></td>
<td>开根号</td>
</tr>
<tr>
<td><code>tf.pow(x, y, name=None)</code></td>
<td>幂次方</td>
</tr>
<tr>
<td><code>tf.exp(x, name=None)</code></td>
<td>计算<code>e</code>的次方</td>
</tr>
<tr>
<td><code>tf.log(x, name=None)</code></td>
<td>计算<code>log</code></td>
</tr>
<tr>
<td><code>tf.maximum(x, y, name=None)</code></td>
<td>返回最大值</td>
</tr>
<tr>
<td><code>tf.minimum(x, y, name=None)</code></td>
<td>返回最小值</td>
</tr>
<tr>
<td><code>tf.cos(x, name=None)</code></td>
<td>三角函数<code>cos</code></td>
</tr>
<tr>
<td><code>tf.sin(x, name=None)</code></td>
<td>三角函数<code>sin</code></td>
</tr>
<tr>
<td><code>tf.tan(x, name=None)</code></td>
<td>三角函数<code>tan</code></td>
</tr>
<tr>
<td><code>tf.atan(x, name=None)</code></td>
<td>三角函数<code>atan</code></td>
</tr>
<tr>
<td><code>tf.sign(x, name=None)</code></td>
<td><code>y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0</code></td>
</tr>
<tr>
<td><code>tf.round(x, name=None)</code></td>
<td><code>a is [0.9, 2.5, 2.3, -4.4], tf.round(a) is [1.0, 3.0, 2.0, -4.0]</code></td>
</tr>
<tr>
<td><code>tf.pow(x, y, name=None)</code></td>
<td><code>x is [[2, 2], [3, 3]], y is [[8, 16], [2, 3]], tf.pow(x, y) is [[256, 65536], [9, 27]]</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="assign函数"><a href="#assign函数" class="headerlink" title="assign函数"></a>assign函数</h3><p>&emsp;&emsp;<code>tf.assign(A, new_number)</code>的功能是把<code>A</code>的值变为<code>new_number</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)  <span class="comment"># 定义常量one</span></span><br><span class="line">new_value = tf.add(state, one)  <span class="comment"># 定义加法步骤(注意，此步并没有直接计算)</span></span><br><span class="line">update = tf.assign(state, new_value)  <span class="comment"># 将State更新成new_value</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>assign</code>函数也可以用于给图变量赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">In [<span class="number">2</span>]: v = tf.Variable(<span class="number">3</span>, name=<span class="string">'v'</span>)</span><br><span class="line">In [<span class="number">3</span>]: v2 = v.assign(<span class="number">5</span>)</span><br><span class="line">In [<span class="number">4</span>]: sess = tf.InteractiveSession()</span><br><span class="line">In [<span class="number">5</span>]: sess.run(v.initializer)</span><br><span class="line">In [<span class="number">6</span>]: sess.run(v)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">3</span></span><br><span class="line">In [<span class="number">7</span>]: sess.run(v2)</span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-contrib-layers-separable-conv2d"><a href="#tf-contrib-layers-separable-conv2d" class="headerlink" title="tf.contrib.layers.separable_conv2d"></a>tf.contrib.layers.separable_conv2d</h3><p>&emsp;&emsp;Aliases:</p>
<ul>
<li>tf.contrib.layers.separable_conv2d</li>
<li>tf.contrib.layers.separable_convolution2d</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.separable_conv2d(</span><br><span class="line">    inputs, num_outputs, kernel_size, depth_multiplier, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=DATA_FORMAT_NHWC, rate=<span class="number">1</span>, activation_fn=tf.nn.relu, normalizer_fn=<span class="keyword">None</span>,</span><br><span class="line">    normalizer_params=<span class="keyword">None</span>, weights_initializer=initializers.xavier_initializer(),</span><br><span class="line">    weights_regularizer=<span class="keyword">None</span>, biases_initializer=tf.zeros_initializer(), biases_regularizer=<span class="keyword">None</span>,</span><br><span class="line">    reuse=<span class="keyword">None</span>, variables_collections=<span class="keyword">None</span>, outputs_collections=<span class="keyword">None</span>, trainable=<span class="keyword">True</span>, scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/layers.py</code>. Adds a <code>depth-separable</code> <code>2D</code> convolution with optional <code>batch_norm</code> layer.<br>&emsp;&emsp;This op first performs a depthwise convolution that acts separately on channels, creating a variable called <code>depthwise_weights</code>. If <code>num_outputs</code> is not <code>None</code>, it adds a pointwise convolution that mixes channels, creating a variable called <code>pointwise_weights</code>. Then, if <code>normalizer_fn</code> is <code>None</code>, it adds bias to the result, creating a variable called <code>biases</code>, otherwise, <code>the normalizer_fn</code> is applied. It finally applies an activation function to produce the end result.</p>
<ul>
<li><code>inputs</code>: A tensor of size <code>[batch_size, height, width, channels]</code>.</li>
<li><code>num_outputs</code>: The number of pointwise convolution output filters. If <code>num_outputs</code> is <code>None</code>, then we skip the pointwise convolution stage.</li>
<li><code>kernel_size</code>: A list of length <code>2</code>: <code>[kernel_height, kernel_width]</code> of the filters. Can be an <code>int</code> if both values are the same.</li>
<li><code>depth_multiplier</code>: The number of depthwise convolution output channels for each input channel.</li>
<li><code>stride</code>: A list of length <code>2</code>: <code>[stride_height, stride_width]</code>, specifying the depthwise convolution stride. Can be an <code>int</code> if both strides are the same.</li>
<li><code>padding</code>: One of <code>VALID</code> or <code>SAME</code>.</li>
<li><code>data_format</code>: A string. <code>NHWC</code> (default) and <code>NCHW</code> are supported.</li>
<li><code>rate</code>: A list of length <code>2</code>: <code>[rate_height, rate_width]</code>, specifying the dilation rates for atrous convolution. Can be an <code>int</code> if both rates are the same. If any value is larger than one, then both stride values need to be one.</li>
<li><code>activation_fn</code>: Activation function. The default value is a <code>ReLU</code> function. Explicitly set it to <code>None</code> to skip it and maintain a linear activation.</li>
<li><code>normalizer_fn</code>: Normalization function to use instead of biases. If <code>normalizer_fn</code> is provided then <code>biases_initializer</code> and <code>biases_regularizer</code> are ignored and biases are not created nor added. default set to <code>None</code> for no normalizer function</li>
<li><code>normalizer_params</code>: Normalization function parameters.</li>
<li><code>weights_initializer</code>: An initializer for the weights.</li>
<li><code>weights_regularizer</code>: Optional regularizer for the weights.</li>
<li><code>biases_initializer</code>: An initializer for the biases. If <code>None</code>, skip biases.</li>
<li><code>biases_regularizer</code>: Optional regularizer for the biases.</li>
<li><code>reuse</code>: Whether or not the layer and its variables should be reused. To be able to reuse the layer scope must be given.</li>
<li><code>variables_collections</code>: Optional list of collections for all the variables or a dictionary containing a different list of collection per variable.</li>
<li><code>outputs_collections</code>: Collection to add the outputs.</li>
<li><code>trainable</code>: Whether or not the variables should be trainable or not.</li>
<li><code>scope</code>: Optional scope for <code>variable_scope</code>.</li>
</ul>
<p>Returns a <code>Tensor</code> representing the output of the operation.</p>
<h3 id="tf-contrib-layers-xavier-initializer"><a href="#tf-contrib-layers-xavier-initializer" class="headerlink" title="tf.contrib.layers.xavier_initializer"></a>tf.contrib.layers.xavier_initializer</h3><p>&emsp;&emsp;Aliases:</p>
<ul>
<li>tf.contrib.layers.xavier_initializer</li>
<li>tf.contrib.layers.xavier_initializer_conv2d</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.xavier_initializer(uniform=<span class="keyword">True</span>, seed=<span class="keyword">None</span>, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/initializers.py</code>. Returns an initializer performing <code>Xavier</code> initialization for weights. This function implements the weight initialization from <code>Xavier Glorot</code> and <code>Yoshua Bengio</code> (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.<br>&emsp;&emsp;This initializer is designed to keep the scale of the gradients roughly the same in all layers.</p>
<ul>
<li><code>uniform</code>: Whether to use uniform or normal distributed random initialization.</li>
<li><code>seed</code>: A <code>Python</code> integer. Used to create random seeds. See <code>tf.set_random_seed</code> for behavior.</li>
<li><code>dtype</code>: The data type. Only floating point types are supported.</li>
</ul>
<h3 id="tf-contrib-layers-l2-regularizer"><a href="#tf-contrib-layers-l2-regularizer" class="headerlink" title="tf.contrib.layers.l2_regularizer"></a>tf.contrib.layers.l2_regularizer</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.l2_regularizer(scale, scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/contrib/layers/python/layers/regularizers.py</code>. Returns a function that can be used to apply <code>L2</code> regularization to weights. Small values of <code>L2</code> can help prevent overfitting the training data.</p>
<ul>
<li><code>scale</code>: A scalar multiplier <code>Tensor</code>. <code>0.0</code> disables the regularizer.</li>
<li><code>scope</code>: An optional scope name.</li>
</ul>
<h3 id="tf-placeholder-with-default"><a href="#tf-placeholder-with-default" class="headerlink" title="tf.placeholder_with_default"></a>tf.placeholder_with_default</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder_with_default(input, shape, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/gen_array_ops.py</code>. A placeholder op that passes through input when its output is not fed.</p>
<ul>
<li><code>input</code>: A <code>Tensor</code>. The default value to produce when output is not fed.</li>
<li><code>shape</code>: A <code>tf.TensorShape</code> or list of <code>ints</code>. The (possibly partial) shape of the tensor.</li>
<li><code>name</code>: A name for the operation (optional).</li>
</ul>
<p>Returns a <code>Tensor</code>. Has the same type as input.</p>
<h3 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split"></a>tf.split</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.split(value, num_or_size_splits, axis=<span class="number">0</span>, num=<span class="keyword">None</span>, name=<span class="string">'split'</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Splits a tensor into sub tensors.<br>&emsp;&emsp;If <code>num_or_size_splits</code> is an integer type, <code>num_split</code>, then splits <code>value</code> along dimension <code>axis</code> into <code>num_split</code> smaller tensors. Requires that <code>num_split</code> evenly divides <code>value.shape[axis]</code>.<br>&emsp;&emsp;If <code>num_or_size_splits</code> is not an integer type, it is presumed to be a <code>Tensor</code> <code>size_splits</code>, then splits <code>value</code> into <code>len(size_splits)</code> pieces. The shape of the <code>i-th</code> piece has the same size as the <code>value</code> except along dimension <code>axis</code> where the size is <code>size_splits[i]</code>.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 'value' is a tensor with shape [5, 30]. Split 'value' into 3 tensors</span></span><br><span class="line"><span class="comment"># with sizes [4, 15, 11] along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, [<span class="number">4</span>, <span class="number">15</span>, <span class="number">11</span>], <span class="number">1</span>)</span><br><span class="line">tf.shape(split0)  <span class="comment"># [5, 4]</span></span><br><span class="line">tf.shape(split1)  <span class="comment"># [5, 15]</span></span><br><span class="line">tf.shape(split2)  <span class="comment"># [5, 11]</span></span><br><span class="line"><span class="comment"># Split 'value' into 3 tensors along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, num_or_size_splits=<span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">tf.shape(split0)  <span class="comment"># [5, 10]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>: The <code>Tensor</code> to split.</li>
<li><code>num_or_size_splits</code>: Either a <code>0-D</code> integer <code>Tensor</code> indicating the number of splits along <code>split_dim</code> or a <code>1-D</code> integer <code>Tensor</code> containing the sizes of each output tensor along <code>split_dim</code>. If a scalar then it must evenly divide <code>value.shape[axis]</code>; otherwise the sum of sizes along the split dimension must match that of the value.</li>
<li><code>axis</code>: A <code>0-D</code> <code>int32</code> <code>Tensor</code>. The dimension along which to split. Must be in the range <code>[-rank(value), rank(value))</code>.</li>
<li><code>num</code>: Optional, used to specify the number of outputs when it cannot be inferred from the shape of <code>size_splits</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>&emsp;&emsp;Returns: if <code>num_or_size_splits</code> is a scalar returns <code>num_or_size_splits</code> <code>Tensor</code> objects; if <code>num_or_size_splits</code> is a <code>1-D</code> <code>Tensor</code> returns <code>num_or_size_splits.get_shape[0]</code> <code>Tensor</code> objects resulting from splitting value.</p>
<h3 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape"></a>tf.reshape</h3><p>&emsp;&emsp;<code>tf.reshape(tensor, shape, name=None)</code>的作用是将<code>tensor</code>变换为参数<code>shape</code>的形式。其中<code>shape</code>为一个列表形式，特殊的一点是列表中可以存在<code>-1</code>，<code>-1</code>代表的含义是不用我们自己指定这一维的大小，函数会自动计算，但列表中只能存在一个<code>-1</code>(当然如果存在多个<code>-1</code>，就是一个存在多解的方程了)。<br>&emsp;&emsp;<code>TensorFlow</code>根据<code>shape</code>变换矩阵的方式为<code>reshape(t, shape) =&gt; reshape(t, [-1]) =&gt; reshape(t, shape)</code>，首先将矩阵t变为一维矩阵，然后再对矩阵的形式进行更改。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]. tensor 't' has shape [9]</span></span><br><span class="line">reshape(t, [3, 3]) ==&gt; [[1, 2, 3],</span><br><span class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></span><br><span class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</span><br><span class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></span><br><span class="line"><span class="comment">#                 [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3, 3],</span></span><br><span class="line"><span class="comment">#                 [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#                [[5, 5, 5],</span></span><br><span class="line"><span class="comment">#                 [6, 6, 6]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]. pass '[-1]' to flatten 't'</span></span><br><span class="line">reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</span><br><span class="line"><span class="comment"># -1 can also be used to infer the shape</span></span><br><span class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],  # -1 is inferred to be 9</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line">reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],  # -1 is inferred to be 2</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line">reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],  # -1 is inferred to be 3</span><br><span class="line">                              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                              [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">                             [[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">                              [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                              [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line">reshape(t, []) ==&gt; 7  # tensor 't' is [7]. shape `[]` reshapes to a scalar</span><br></pre></td></tr></table></figure>
<h3 id="tf-shape"><a href="#tf-shape" class="headerlink" title="tf.shape"></a>tf.shape</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.shape(input, name=<span class="keyword">None</span>, out_type=tf.int32)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Returns the shape of a tensor. This operation returns a <code>1-D</code> integer tensor representing the shape of input.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]], [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]])</span><br><span class="line">tf.shape(t)  <span class="comment"># [2, 2, 3]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>: A <code>Tensor</code> or <code>SparseTensor</code>.</li>
<li><code>name</code>: A name for the operation (optional).</li>
<li><code>out_type</code>: (Optional) The specified output type of the operation (<code>int32</code> or <code>int64</code>).</li>
</ul>
<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat"></a>tf.concat</h3><p>&emsp;&emsp;该函数用于连接两个矩阵的操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(concat_dim, values, name=<span class="string">'concat'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>concat_dim</code>：<code>tensor</code>连接的方向(维度)，<code>cancat_dim</code>维度可以不一样，其他维度的尺寸必须一样。</li>
<li><code>values</code>：两个或者一组待连接的<code>tensor</code>。</li>
<li><code>name</code>：指定该操作的<code>name</code>。</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">tf.concat(0, [t1, t2]) =&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</span><br><span class="line">tf.concat(1, [t1, t2]) =&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span><br></pre></td></tr></table></figure>
<p>这里要注意的是，如果是两个向量，它们是无法调用<code>tf.concat(1, [t1, t2])</code>来连接的，因为它们对应的<code>shape</code>只有一个维度，当然不能在第二维上连了，虽然实际中两个向量可以在行上连，但是放在程序里是会报错的。如果要连，必须要调用<code>tf.expand_dims</code>来扩维：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t1 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t2 = tf.constant([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># concated = tf.concat(1, [t1, t2])  # 这样会报错</span></span><br><span class="line">t1 = tf.expand_dims(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]),<span class="number">1</span>)</span><br><span class="line">t2 = tf.expand_dims(tf.constant([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]),<span class="number">1</span>)</span><br><span class="line">concated = tf.concat(<span class="number">1</span>, [t1, t2])  <span class="comment"># 这样就是正确的</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-add-n"><a href="#tf-add-n" class="headerlink" title="tf.add_n"></a>tf.add_n</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add_n(inputs, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/math_ops.py</code>. Adds all input tensors <code>element-wise</code>.</p>
<ul>
<li><code>inputs</code>: A list of <code>Tensor</code> objects, each with same shape and type.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>&emsp;&emsp;Returns a <code>Tensor</code> of same shape and type as the elements of <code>inputs</code>.</p>
<h3 id="tf-squeeze"><a href="#tf-squeeze" class="headerlink" title="tf.squeeze"></a>tf.squeeze</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.squeeze(input, squeeze_dims=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Removes dimensions of size <code>1</code> from the shape of a tensor.<br>&emsp;&emsp;Given a tensor <code>input</code>, this operation returns a tensor of the same type with all dimensions of size <code>1</code> removed. If you don’t want to remove all size <code>1</code> dimensions, you can remove specific size <code>1</code> dimensions by specifying axis.<br>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]</span></span><br><span class="line">tf.shape(tf.squeeze(t))  <span class="comment"># [2, 3]</span></span><br><span class="line">Or, to remove specific size <span class="number">1</span> dimensions:</span><br><span class="line"><span class="comment"># 't' is a tensor of shape [1, 2, 1, 3, 1, 1]</span></span><br><span class="line">tf.shape(tf.squeeze(t, [<span class="number">2</span>, <span class="number">4</span>]))  <span class="comment"># [1, 2, 3, 1]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>: A <code>Tensor</code>. The input to squeeze.</li>
<li><code>axis</code>: An optional list of <code>ints</code>. If specified, only squeezes the dimensions listed. The dimension index starts at <code>0</code>. It is an error to squeeze a dimension that is not <code>1</code>. Must be in the range <code>[-rank(input), rank(input))</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>squeeze_dims</code>: Deprecated keyword argument that is now <code>axis</code>.</li>
</ul>
<p>&emsp;&emsp;Return a <code>Tensor</code>. Has the same type as <code>input</code>. Contains the same data as <code>input</code>, but has one or more dimensions of size <code>1</code> removed.</p>
<h3 id="tf-expand-dims"><a href="#tf-expand-dims" class="headerlink" title="tf.expand_dims"></a>tf.expand_dims</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(input, axis=<span class="keyword">None</span>, name=<span class="keyword">None</span>, dim=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Defined in <code>tensorflow/python/ops/array_ops.py</code>. Inserts a dimension of <code>1</code> into a tensor’s shape.<br>&emsp;&emsp;Given a tensor <code>input</code>, this operation inserts a dimension of <code>1</code> at the dimension index <code>axis</code> of <code>input&#39;s</code> shape. The dimension index <code>axis</code> starts at <code>zero</code>; if you specify a negative number for <code>axis</code> it is counted backward from the end.<br>&emsp;&emsp;This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape <code>[height, width, channels]</code>, you can make it a batch of <code>1</code> image with <code>expand_dims(image, 0)</code>, which will make the shape <code>[1, height, width, channels]</code>.<br>&emsp;&emsp;Other examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"><span class="comment"># 't2' is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure>
<p>This operation requires that:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-1</span> - input.dims() &lt;= dim &lt;= input.dims()</span><br></pre></td></tr></table></figure>
<p>This operation is related to <code>squeeze()</code>, which removes dimensions of size <code>1</code>.</p>
<ul>
<li><code>input</code>: A <code>Tensor</code>.</li>
<li><code>axis</code>: <code>0-D</code> (scalar). Specifies the dimension index at which to expand the shape of <code>input</code>. Must be in the range <code>[-rank(input) - 1, rank(input)]</code>.</li>
<li><code>name</code>: The <code>name</code> of the output <code>Tensor</code>.</li>
<li><code>dim</code>: <code>0-D</code> (scalar). Equivalent to <code>axis</code>, to be deprecated.</li>
</ul>
<p>&emsp;&emsp;Return a <code>Tensor</code> with the same data as <code>input</code>, but its shape has an additional dimension of size <code>1</code> added.</p>
<h3 id="tf-Session-as-default"><a href="#tf-Session-as-default" class="headerlink" title="tf.Session.as_default"></a>tf.Session.as_default</h3><p>&emsp;&emsp;如果使用关键字<code>with</code>来指定会话，可以在会话中执行<code>Operation.run</code>或<code>Tensor.eval</code>，以得到运行的<code>tensor</code>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant(..)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="keyword">assert</span> tf.get_default_session() <span class="keyword">is</span> sess</span><br><span class="line">    print(c.eval())</span><br></pre></td></tr></table></figure>
<p>使用函数<code>tf.get_default_session</code>来得到当前默认的会话。需要注意的是，退出<code>as_default</code>上下文管理器时，并没有关闭该会话(<code>session</code>)，所以你必须明确地关闭会话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant(...)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(c.eval())</span><br><span class="line"></span><br><span class="line">sess.close()  <span class="comment"># 关闭会话</span></span><br></pre></td></tr></table></figure>
<p>而使用<code>with tf.Session()</code>的方式可以创建并自动关闭会话。</p>
<h3 id="错误类-Error-classes"><a href="#错误类-Error-classes" class="headerlink" title="错误类(Error classes)"></a>错误类(Error classes)</h3><p>&emsp;&emsp;错误类如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>class tf.OpError</code></td>
<td>一个基本的错误类型，在当TF执行失败时候报错</td>
</tr>
<tr>
<td><code>tf.OpError.op</code></td>
<td>返回执行失败的操作节点，有的操作(如<code>Send</code>或<code>Recv</code>)可能不会返回，那就要用用到<code>node_def</code>方法</td>
</tr>
<tr>
<td><code>tf.OpError.node_def</code></td>
<td>以<code>NodeDef proto</code>形式表示失败的<code>op</code></td>
</tr>
<tr>
<td><code>tf.OpError.error_code</code></td>
<td>描述该错误的整数错误代码</td>
</tr>
<tr>
<td><code>tf.OpError.message</code></td>
<td>返回错误信息</td>
</tr>
<tr>
<td><code>class tf.errors.CancelledError</code></td>
<td>当操作或者阶段被取消时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.UnknownError</code></td>
<td>未知错误类型</td>
</tr>
<tr>
<td><code>class tf.errors.InvalidArgumentError</code></td>
<td>在接收到非法参数时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.NotFoundError</code></td>
<td>当发现不存在所请求的一个实体时候，比如文件或目录</td>
</tr>
<tr>
<td><code>class tf.errors.AlreadyExistsError</code></td>
<td>当创建的实体已经存在的时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.PermissionDeniedError</code></td>
<td>没有执行权限做某操作的时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.FailedPreconditionError</code></td>
<td>系统没有条件执行某个行为时候报错</td>
</tr>
<tr>
<td><code>class tf.errors.AbortedError</code></td>
<td>操作中止时报错，常常发生在并发情形</td>
</tr>
<tr>
<td><code>class tf.errors.OutOfRangeError</code></td>
<td>超出范围报错</td>
</tr>
<tr>
<td><code>class tf.errors.UnimplementedError</code></td>
<td>某个操作没有执行时报错</td>
</tr>
<tr>
<td><code>class tf.errors.InternalError</code></td>
<td>当系统经历了一个内部错误时报出</td>
</tr>
<tr>
<td><code>class tf.errors.ResourceExhaustedError</code></td>
<td>资源耗尽时报错</td>
</tr>
<tr>
<td><code>class tf.errors.DataLossError</code></td>
<td>当出现不可恢复的错误，例如在运行<code>tf.WholeFileReader.read</code>读取整个文件的同时文件被删减</td>
</tr>
<tr>
<td><code>tf.errors.XXXXX.__init__(node_def, op, message)</code></td>
<td>使用该形式方法创建以上各种错误类</td>
</tr>
</tbody>
</table>
</div>
<h3 id="tf-one-hot"><a href="#tf-one-hot" class="headerlink" title="tf.one_hot"></a>tf.one_hot</h3><p>&emsp;&emsp;该函数用于将输入转换成<code>one-hot</code>形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(indices, depth, on_value, off_value, axis)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>indices</code>：非负整数表示的标签列表，<code>len(indices)</code>就是分类的类别数。<code>tf.one_hot</code>返回的张量的阶数为<code>indeces</code>的阶数加上<code>1</code>。当<code>indices</code>的某个分量取<code>-1</code>时，即对应的向量没有独热值。</li>
<li><code>depth</code>：每个独热向量的维度。</li>
<li><code>on_value</code>：独热值。</li>
<li><code>off_value</code>：非独热值。</li>
<li><code>axis</code>：指定第几阶为<code>depth</code>维独热向量，默认为<code>-1</code>，即指定张量的最后一维为独热向量。例如对于一个<code>2</code>阶张量而言，<code>axis = 0</code>时，每个列向量是一个独热的<code>depth</code>维向量；<code>axis = 1</code>时，每个行向量是一个独热的<code>depth</code>维向量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=[<span class="number">10</span>])</span><br><span class="line">y = tf.one_hot(z, <span class="number">10</span>, on_value=<span class="number">1</span>, off_value=<span class="keyword">None</span>, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session()<span class="keyword">as</span> sess:</span><br><span class="line">    print(z)</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4</span> <span class="number">4</span> <span class="number">5</span> <span class="number">8</span> <span class="number">9</span> <span class="number">0</span> <span class="number">5</span> <span class="number">0</span> <span class="number">3</span> <span class="number">1</span>]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="tf-sparse-to-dense"><a href="#tf-sparse-to-dense" class="headerlink" title="tf.sparse_to_dense"></a>tf.sparse_to_dense</h3><p>&emsp;&emsp;函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是将一个稀疏表示转换成一个密集张量。具体将稀疏张量<code>sparse</code>转换成密集张量<code>dense</code>的步骤如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If sparse_indices is scalar</span></span><br><span class="line">dense[i] = (i == sparse_indices ? sparse_values : default_value)</span><br><span class="line"><span class="comment"># If sparse_indices is a vector, then for each i</span></span><br><span class="line">dense[sparse_indices[i]] = sparse_values[i]</span><br><span class="line"><span class="comment"># If sparse_indices is an n by d matrix, then for each i in [0, n)</span></span><br><span class="line">dense[sparse_indices[i][<span class="number">0</span>], ..., sparse_indices[i][d<span class="number">-1</span>]] = sparse_values[i]</span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>dense</code>中的填充值<code>default_value</code>都是<code>0</code>，除非该值被设置成一个标量。</p>
<ul>
<li><code>sparse_indices</code>是稀疏矩阵中那些个别元素对应的索引值，有三种情况：</li>
</ul>
<ol>
<li>如果<code>sparse_indices</code>是个数，那么它只能指定一维矩阵的某一个元素。</li>
<li>如果<code>sparse_indices</code>是个向量，那么它可以指定一维矩阵的多个元素。</li>
<li>如果<code>sparse_indices</code>是个矩阵，那么它可以指定二维矩阵的多个元素。</li>
</ol>
<ul>
<li><code>output_shape</code>是输出的稀疏矩阵的<code>shape</code>。</li>
<li><code>sparse_values</code>是个别元素的值，分为两种情况：</li>
</ul>
<ol>
<li>如果sparse_values是个数，则所有索引指定的位置都用这个数。</li>
<li>如果sparse_values是个向量，则输出矩阵的某一行向量里某一行对应的数(所以这里向量的长度应该和输出矩阵的行数对应，不然报错)。</li>
</ol>
<ul>
<li><code>default_value</code>是未指定元素的默认值，一般如果是稀疏矩阵，就是0了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span></span><br><span class="line"><span class="comment"># 真实标签，shape为[5, 1]</span></span><br><span class="line">label = tf.expand_dims(tf.constant([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]), <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 真实标签的索引，shape为[5, 1]</span></span><br><span class="line">index = tf.expand_dims(tf.range(<span class="number">0</span>, BATCH_SIZE), <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 将标签和索引tensor在第二个维度上连接起来，新的concated的shape为[5, 2]</span></span><br><span class="line">concated = tf.concat([index, label], <span class="number">1</span>)</span><br><span class="line"><span class="comment"># onehot_labels的shape为[5, 10]</span></span><br><span class="line">onehot_labels = tf.sparse_to_dense(concated, [BATCH_SIZE, <span class="number">10</span>], <span class="number">1.0</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(concated))</span><br><span class="line">    print(<span class="string">"----------------"</span>)</span><br><span class="line">    onehot1 = sess.run(onehot_labels)</span><br><span class="line">    print(onehot1)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">9</span>]]</span><br><span class="line">----------------</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;如果<code>output_shape</code>是一个行向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">predicted_class = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line">one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    onehot1 = sess.run(one_hot)</span><br><span class="line">    print(onehot1)  <span class="comment"># 输出“[0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]”</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-nn-in-top-k"><a href="#tf-nn-in-top-k" class="headerlink" title="tf.nn.in_top_k"></a>tf.nn.in_top_k</h3><p>&emsp;&emsp;<code>tf.nn.in_top_k</code>用于计算预测的结果和实际结果的是否相等，并返回一个<code>bool</code>类型的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.in_top_k(prediction, target, K)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>prediction</code>：预测的结果，大小就是预测样本的数量乘以输出的维度。</li>
<li><code>target</code>：实际样本类别的标签，大小就是样本数量的个数。</li>
<li><code>K</code>：每个样本的预测结果的前<code>k</code>个最大的数里面是否包含<code>targets</code>预测中的标签，一般都是取<code>1</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.3</span>], [<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.4</span>]]</span><br><span class="line">B = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">out = tf.nn.in_top_k(A, B, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(out))  <span class="comment"># 输出“[False  True]”</span></span><br></pre></td></tr></table></figure>
<p>因为<code>A</code>张量里面的第一个元素的最大值的标签是<code>0</code>，第二个元素的最大值的标签是<code>1</code>，但实际上是<code>1</code>和<code>1</code>，所以输出就是<code>False</code>和<code>True</code>。如果把<code>K</code>改成<code>2</code>，那么第一个元素的前面<code>2</code>个最大元素的位置是<code>0</code>和<code>1</code>，第二个元素的就是<code>1</code>和<code>2</code>。而<code>B</code>是<code>[1, 1]</code>，包含在里面，所以输出结果就是<code>True</code>和<code>True</code>。</p>
<h3 id="initialized-value"><a href="#initialized-value" class="headerlink" title="initialized_value"></a>initialized_value</h3><p>&emsp;&emsp;你有时候会需要用另一个变量的初始化值给当前变量初始化。由于<code>tf.initialize_all_variables</code>是并行地初始化所有变量，所以在有这种需求的情况下需要小心。<br>&emsp;&emsp;用其它变量的值初始化一个新的变量时，使用其它变量的<code>initialized_value</code>属性。你可以直接把已初始化的值作为新变量的初始值，或者把它当做<code>tensor</code>计算得到一个值赋予新变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a variable with a random value</span></span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=<span class="string">"weights"</span>)</span><br><span class="line"><span class="comment"># Create another variable with the same value as 'weights'</span></span><br><span class="line">w2 = tf.Variable(weights.initialized_value(), name=<span class="string">"w2"</span>)</span><br><span class="line"><span class="comment"># Create another variable with twice the value of 'weights'</span></span><br><span class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">0.2</span>, name=<span class="string">"w_twice"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tf-py-func"><a href="#tf-py-func" class="headerlink" title="tf.py_func"></a>tf.py_func</h3><p>&emsp;&emsp;这是一个可以把<code>TensorFlow</code>和<code>Python</code>原生代码无缝衔接起来的函数，有了它，你就可以在<code>TensorFlow</code>里面自由的实现你想要的功能，而不用考虑<code>TensorFlow</code>有没有实现它的<code>API</code>，并且可以帮助我们实现自由地检查该功能模块的输入输出是否正确，而不受到<code>TensorFlow</code>的先构造计算图再运行导致的不能单独检测单一模块的功能的限制。<br>&emsp;&emsp;它的具体功能描述是包装一个普通的<code>Python</code>函数，这个函数接受<code>numpy</code>的数组作为输入和输出，让这个函数可以作为<code>TensorFlow</code>计算图上的计算节点<code>OP</code>来使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">py_func(func, inp, Tout, stateful=<span class="keyword">True</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>func</code>：一个<code>Python</code>函数，它接受<code>NumPy</code>数组作为输入和输出，并且数组的类型和大小必须和输入和输出用来衔接的<code>Tensor</code>大小和数据类型相匹配。</li>
<li><code>inp</code>：输入的<code>Tensor</code>列表。</li>
<li><code>Tout</code>：输出<code>Tensor</code>数据类型的列表或元祖。</li>
<li><code>stateful</code>：状态，布尔值。</li>
<li><code>name</code>：节点<code>OP</code>的名称。</li>
</ul>
<p>&emsp;&emsp;<code>operation</code>分为有状态的与无状态的<code>operation</code>：无状态的<code>operation</code>主要进行数学计算，比如矩阵乘法、加法等，如果给该<code>OP</code>同一个输入，那么将会得到同一个输出；有状态的<code>operation</code>(<code>stateful operation</code>)分为<code>variable</code>以及<code>queue</code>，<code>variable</code>负责保存机器学习模型的模型参数，<code>queue</code>提供更加复杂的模型架构，给定同一个输入，可能会得到不同的输出。<code>common subexpression elimination</code>(<code>CSE</code>)公共子表达式消除只会在无状态的节点<code>OP</code>上执行。<br>&emsp;&emsp;简单代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sinh(x)</span><br><span class="line"></span><br><span class="line">inp = tf.placeholder(tf.float32)</span><br><span class="line">y = tf.py_func(my_func, [inp], tf.float32)</span><br></pre></td></tr></table></figure>
<p>缺点如下：</p>
<ul>
<li>这个被包装过的的计算函数的内部部分不会被序列化到<code>GraphDef</code>里面去，所以，如果你要序列化存储和恢复模型，就不能使用该函数。</li>
<li>这个被包装的计算节点<code>OP</code>与调用它的<code>Python</code>程序必须运行在同一个物理设备上，也就是说，如果使用分布式<code>TensorFlow</code>，必须使用<code>tf.train.Server</code>和<code>with tf.device</code>来保证二者在同一个服务器内。</li>
</ul>
<h3 id="tf-trainable-variables"><a href="#tf-trainable-variables" class="headerlink" title="tf.trainable_variables"></a>tf.trainable_variables</h3><p>&emsp;&emsp;在创造变量(<code>tf.Variable</code>、<code>tf.get_variable</code>等)时，都会有一个<code>trainable</code>的选项，表示该变量是否可训练，这个函数会返回图中所有<code>trainable=True</code>的变量。<code>tf.get_variable</code>和<code>tf.Variable</code>的默认选项是<code>True</code>，而<code>tf.constant</code>只能是<code>False</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.get_variable(<span class="string">'a'</span>, shape=[<span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>, shape=[<span class="number">2</span>, <span class="number">5</span>], trainable=<span class="keyword">False</span>)</span><br><span class="line">c = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.int32, shape=[<span class="number">8</span>], name=<span class="string">'c'</span>)</span><br><span class="line">d = tf.Variable(tf.random_uniform(shape=[<span class="number">3</span>, <span class="number">3</span>]), name=<span class="string">'d'</span>)</span><br><span class="line">tvar = tf.trainable_variables()</span><br><span class="line">tvar_name = [x.name <span class="keyword">for</span> x <span class="keyword">in</span> tvar]</span><br><span class="line">print(tvar)</span><br><span class="line">print(tvar_name)</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable <span class="string">'a:0'</span> shape=(<span class="number">5</span>, <span class="number">2</span>) dtype=float32_ref&gt;, &lt;tf.Variable <span class="string">'d:0'</span> shape=(<span class="number">3</span>, <span class="number">3</span>) dtype=float32_ref&gt;]</span><br><span class="line">[<span class="string">'a:0'</span>, <span class="string">'d:0'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a>tf.train.ExponentialMovingAverage</h3><p>&emsp;&emsp;该函数用于更新参数，就是采用滑动平均的方法更新参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.ExponentialMovingAverage(decay, steps)</span><br></pre></td></tr></table></figure>
<p>这个函数初始化需要提供一个衰减速率(<code>decay</code>)，用于控制模型的更新速度。这个函数还会维护一个影子变量，也就是更新参数后的参数值，这个影子变量的初始值就是这个变量的初始值，影子变量值的更新方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shadow_variable = decay * shadow_variable + (<span class="number">1</span> - decay) * variable</span><br></pre></td></tr></table></figure>
<p><code>shadow_variable</code>是影子变量；<code>variable</code>表示待更新的变量，也就是变量被赋予的值；<code>decay</code>为衰减速率，一般设为接近于<code>1</code>的数(<code>0.99</code>或<code>0.999</code>)，其越大模型越稳定，因为<code>decay</code>越大，参数更新的速度就越慢，趋于稳定。<br>&emsp;&emsp;<code>tf.train.ExponentialMovingAverage</code>这个函数还提供了自己动更新<code>decay</code>的计算方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decay = min(decay, (<span class="number">1</span> + steps) / (<span class="number">10</span> + steps))</span><br></pre></td></tr></table></figure>
<p><code>steps</code>是迭代的次数，可以自己设定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line">step = tf.Variable(tf.constant(<span class="number">0</span>))</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>, step)</span><br><span class="line">maintain_average = ema.apply([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))  <span class="comment"># 初始的值都为0</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">5</span>))  <span class="comment"># 把v1变为5</span></span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, 1/10) = 0.1”，“v1 = 0.1 * 0 + 0.9 * 5 = 4.5”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    sess.run(tf.assign(step, <span class="number">10000</span>))  <span class="comment"># steps = 10000</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">10</span>))  <span class="comment"># v1 = 10</span></span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, (1 + 10000)/(10 + 10000)) = 0.99”，“v1 = 0.99 * 4.5 + 0.01 * 10 = 4.555”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    sess.run(maintain_average)</span><br><span class="line">    <span class="comment"># “decay = min(0.99, (1 + 10000)/(10 + 10000)) = 0.99”，“v1 = 0.99 * 4.555 + 0.01 * 10 = 4.6”</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure>
<h3 id="tf-moving-average-variables-scope-None"><a href="#tf-moving-average-variables-scope-None" class="headerlink" title="tf.moving_average_variables(scope=None)"></a>tf.moving_average_variables(scope=None)</h3><p>&emsp;&emsp;Returns all variables that maintain their moving averages. If an <code>ExponentialMovingAverage</code> object is created and <code>the apply()</code> method is called on a list of variables, these variables will be added to the <code>GraphKeys.MOVING_AVERAGE_VARIABLES</code> collection. This convenience function returns the contents of that collection.<br>&emsp;&emsp;<code>scope</code> (optional) is a string. If supplied, the resulting list is filtered to include only items whose name attribute matches <code>scope</code> using <code>re.match</code>. Items without a name attribute are never returned if a <code>scope</code> is supplied. The choice of <code>re.match</code> means that a s<code>cope</code> without special tokens filters by prefix.<br>&emsp;&emsp;使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">variable_averages = tf.train.ExponentialMovingAverage(decay, global_step)</span><br><span class="line">variables_to_average = (tf.trainable_variables() + tf.moving_average_variables())</span><br><span class="line">variables_averages_op = variable_averages.apply(variables_to_average)</span><br><span class="line">train_op = tf.group(opt, variables_averages_op)</span><br></pre></td></tr></table></figure>
<h3 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a>tf.control_dependencies</h3><p>&emsp;&emsp;在有些机器学习程序中，我们想要指定某些操作执行的依赖关系，这时可以使用<code>tf.control_dependencies(control_inputs)</code>来实现。该函数返回一个控制依赖的上下文管理器，使用<code>with</code>关键字可以让在这个上下文环境中的操作都在<code>control_inputs</code>执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</span><br><span class="line">    <span class="comment"># 'd' and 'e' will only run after 'a', 'b', and 'c' have executed</span></span><br><span class="line">    d = ...</span><br><span class="line">    e = ...</span><br></pre></td></tr></table></figure>
<p>可以嵌套<code>control_dependencies</code>使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after 'a' and 'b'</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">        <span class="comment"># Ops constructed here run after 'a', 'b', 'c', and 'd'</span></span><br></pre></td></tr></table></figure>
<p>可以传入<code>None</code>来消除依赖：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after 'a' and 'b'</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies(<span class="keyword">None</span>):</span><br><span class="line">        <span class="comment"># Ops constructed here run normally, not waiting for either 'a' or 'b'</span></span><br><span class="line">        <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">            <span class="comment"># Ops constructed here run after 'c' and 'd',</span></span><br><span class="line">            <span class="comment"># also not waiting for either 'a' or 'b'</span></span><br></pre></td></tr></table></figure>
<p>注意，控制依赖只对那些在上下文环境中建立的操作有效，仅仅在<code>context</code>中使用一个操作或张量是没用的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span>  <span class="comment"># WRONG</span></span><br><span class="line">    t = tf.matmul(tensor, tensor)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">        <span class="comment"># The matmul op is created outside the context,</span></span><br><span class="line">        <span class="comment"># so no control dependency will be added</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span>  <span class="comment"># RIGHT</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">        <span class="comment"># The matmul op is created in the context,</span></span><br><span class="line">        <span class="comment"># so a control dependency will be added</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure>
<h3 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a>tf.group</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.group(*inputs, **kwargs)</span><br></pre></td></tr></table></figure>
<p>Create an op that groups multiple operations. When this op finishes, all ops in <code>inputs</code> have finished. This op has no output.</p>
<ul>
<li><code>*inputs</code>: Zero or more tensors to group.</li>
<li><code>name</code>: A name for this operation (optional).</li>
</ul>
<p>Returns an Operation that executes all its <code>inputs</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">b = tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>], minval=<span class="number">10</span>, maxval=<span class="number">11</span>)</span><br><span class="line">w = tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>], minval=<span class="number">10</span>, maxval=<span class="number">11</span>)</span><br><span class="line">mul = tf.multiply(w, <span class="number">2</span>)</span><br><span class="line">add = tf.add(w, <span class="number">2</span>)</span><br><span class="line">group = tf.group(mul, add)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(group))</span><br></pre></td></tr></table></figure>
<h3 id="tf-add-to-collection和tf-get-collection"><a href="#tf-add-to-collection和tf-get-collection" class="headerlink" title="tf.add_to_collection和tf.get_collection"></a>tf.add_to_collection和tf.get_collection</h3><p>&emsp;&emsp;<code>tf.add_to_collection</code>是把变量放入一个集合，把很多变量变成一个列表；<code>tf.get_collection</code>是从一个集合中取出全部变量，返回值是一个列表；<code>tf.add_n</code>是把一个列表的东西都依次加起来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v1 = tf.get_variable(name=<span class="string">'v1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'loss'</span>, v1)</span><br><span class="line">v2 = tf.get_variable(name=<span class="string">'v2'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'loss'</span>, v2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(tf.get_collection(<span class="string">'loss'</span>))</span><br><span class="line">    print(sess.run(tf.add_n(tf.get_collection(<span class="string">'loss'</span>))))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable <span class="string">'v1:0'</span> shape=(<span class="number">1</span>,) dtype=float32_ref&gt;, &lt;tf.Variable <span class="string">'v2:0'</span> shape=(<span class="number">1</span>,) dtype=float32_ref&gt;]</span><br><span class="line">[<span class="number">2.</span>]</span><br></pre></td></tr></table></figure>
<h3 id="tf-stack"><a href="#tf-stack" class="headerlink" title="tf.stack"></a>tf.stack</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack(values, axis=<span class="number">0</span>, name=<span class="string">'stack'</span>)</span><br></pre></td></tr></table></figure>
<p>Stacks a list of <code>rank-R</code> tensors into one <code>rank-(R+1)</code> tensor.</p>
<ul>
<li><code>values</code>: A list of <code>Tensor</code> objects with the same shape and type.</li>
<li><code>axis</code>: An <code>int</code>. The <code>axis</code> to stack along. Defaults to the first dimension. Negative <code>values</code> wrap around, so the valid range is <code>[-(R+1), R+1)</code>.</li>
<li><code>name</code>: A <code>name</code> for this operation (optional).</li>
</ul>
<p>Return a stacked <code>Tensor</code> with the same type as <code>values</code>.<br>&emsp;&emsp;Packs the list of tensors in <code>values</code> into a tensor with rank one higher than each tensor in <code>values</code>, by packing them along the <code>axis</code> dimension.<br>&emsp;&emsp;Given a list of length <code>N</code> of tensors of shape <code>(A, B, C)</code>: if <code>axis == 0</code> then the output tensor will have the shape <code>(N, A, B, C)</code>; if <code>axis == 1</code> then the output tensor will have the shape <code>(A, N, B, C)</code>. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">y = tf.constant([<span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line">z = tf.constant([<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.stack([x, y, z])  <span class="comment"># [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)</span></span><br><span class="line">tf.stack([x, y, z], axis=<span class="number">1</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6]]</span></span><br></pre></td></tr></table></figure>
<p>This is the opposite of <code>unstack</code>. The <code>numpy</code> equivalent is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([x, y, z]) = np.stack([x, y, z])</span><br></pre></td></tr></table></figure>
<h3 id="tf-unstack"><a href="#tf-unstack" class="headerlink" title="tf.unstack"></a>tf.unstack</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.unstack(value, num=<span class="keyword">None</span>, axis=<span class="number">0</span>, name=<span class="string">'unstack'</span>)</span><br></pre></td></tr></table></figure>
<p>Unpacks the given dimension of a <code>rank-R</code> tensor into <code>rank-(R-1)</code> tensors.<br>&emsp;&emsp;Unpacks <code>num</code> tensors from <code>value</code> by chipping it along the <code>axis</code> dimension. If <code>num</code> is not specified (the default), it is inferred from <code>value&#39;s</code> shape. If <code>value.shape[axis]</code> is not known, <code>ValueError</code> is raised.<br>&emsp;&emsp;For example, given a tensor of shape <code>(A, B, C, D)</code>:</p>
<ul>
<li>If <code>axis == 0</code> then the <code>i&#39;th</code> tensor in output is the slice <code>value[i, :, :, :]</code> and each tensor in output will have shape <code>(B, C, D)</code>. Note that the dimension unpacked along is gone, unlike split.</li>
<li>If <code>axis == 1</code> then the <code>i&#39;th</code> tensor in output is the slice <code>value[:, i, :, :]</code> and each tensor in output will have shape <code>(A, C, D)</code>.<br>&emsp;&emsp;Code example:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">d = tf.unstack(c, axis=<span class="number">0</span>)</span><br><span class="line">e = tf.unstack(c, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(d))  <span class="comment"># 输出“[array([1, 2, 3]), array([4, 5, 6])]”</span></span><br><span class="line">    print(sess.run(e))  <span class="comment"># 输出“[array([1, 4]), array([2, 5]), array([3, 6])]”</span></span><br></pre></td></tr></table></figure>
<p>This is the opposite of <code>stack</code>.</p>
<ul>
<li><code>value</code>: A rank <code>R &gt; 0</code> <code>Tensor</code> to be unstacked.</li>
<li><code>num</code>: An <code>int</code>. The length of the dimension <code>axis</code>. Automatically inferred if <code>None</code> (the default).</li>
<li><code>axis</code>: An <code>int</code>. The <code>axis</code> to <code>unstack</code> along. Defaults to the first dimension. Negative values wrap around, so the valid range is <code>[-R, R)</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
</ul>
<p>Returns the list of <code>Tensor</code> objects unstacked from <code>value</code>.</p>
<h3 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose"></a>tf.transpose</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.transpose(a, perm=<span class="keyword">None</span>, name=<span class="string">'transpose'</span>, conjugate=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>Transposes <code>a</code>. Permutes the dimensions according to <code>perm</code>. The returned tensor’s dimension <code>i</code> will correspond to the input dimension <code>perm[i]</code>. If <code>perm</code> is not given, it is set to <code>(n-1...0)</code>, where <code>n</code> is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on <code>2-D</code> input <code>Tensors</code>. If <code>conjugate</code> is <code>True</code> and <code>a.dtype</code> is either <code>complex64</code> or <code>complex128</code> then the values of <code>a</code> are conjugated and transposed.</p>
<ul>
<li><code>a</code>: A <code>Tensor</code>.</li>
<li><code>perm</code>: A permutation of the dimensions of <code>a</code>.</li>
<li><code>name</code>: A <code>name</code> for the operation (optional).</li>
<li><code>conjugate</code>: Optional <code>bool</code>. Setting it to <code>True</code> is mathematically equivalent to <code>tf.conj(tf.transpose(input))</code>.</li>
</ul>
<p>&emsp;&emsp;For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tf.transpose(x)</span><br><span class="line"><span class="comment"># [[1, 4]</span></span><br><span class="line"><span class="comment">#  [2, 5]</span></span><br><span class="line"><span class="comment">#  [3, 6]]</span></span><br><span class="line">tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># Equivalently</span></span><br><span class="line"><span class="comment"># [[1, 4]</span></span><br><span class="line"><span class="comment">#  [2, 5]</span></span><br><span class="line"><span class="comment">#  [3, 6]]</span></span><br><span class="line"><span class="comment"># If x is complex, setting conjugate=True gives the conjugate transpose</span></span><br><span class="line">x = tf.constant([[<span class="number">1</span> + <span class="number">1j</span>, <span class="number">2</span> + <span class="number">2j</span>, <span class="number">3</span> + <span class="number">3j</span>], [<span class="number">4</span> + <span class="number">4j</span>, <span class="number">5</span> + <span class="number">5j</span>, <span class="number">6</span> + <span class="number">6j</span>]])</span><br><span class="line">tf.transpose(x, conjugate=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># [[1 - 1j, 4 - 4j],</span></span><br><span class="line"><span class="comment">#  [2 - 2j, 5 - 5j],</span></span><br><span class="line"><span class="comment">#  [3 - 3j, 6 - 6j]]</span></span><br><span class="line"><span class="comment"># "perm" is more useful for n-dimensional tensors, for n &gt; 2</span></span><br><span class="line">x = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">                 [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]])</span><br><span class="line"><span class="comment"># Take the transpose of the matrices in dimension-0(this common operation has a shorthand "matrix_transpose")</span></span><br><span class="line">tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># [[[1,  4],</span></span><br><span class="line"><span class="comment">#   [2,  5],</span></span><br><span class="line"><span class="comment">#   [3,  6]],</span></span><br><span class="line"><span class="comment">#  [[7, 10],</span></span><br><span class="line"><span class="comment">#   [8, 11],</span></span><br><span class="line"><span class="comment">#   [9, 12]]]</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;In <code>numpy</code>, transposes are <code>memory-efficient</code> constant time operations as they simply return a new view of the same data with adjusted strides. <code>TensorFlow</code> does not support strides, so transpose returns a new tensor with the items permuted.</p>
<h3 id="tf-set-random-seed"><a href="#tf-set-random-seed" class="headerlink" title="tf.set_random_seed"></a>tf.set_random_seed</h3><p>&emsp;&emsp;The function is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.set_random_seed(seed)</span><br></pre></td></tr></table></figure>
<p>Sets the <code>graph-level</code> random <code>seed</code>. Operations that rely on a random <code>seed</code> actually derive it from two seeds: the <code>graph-level</code> and <code>operation-level</code> seeds. This sets the <code>graph-level</code> seed.<br>&emsp;&emsp;Its interactions with <code>operation-level</code> seeds is as follows:</p>
<ul>
<li>If neither the <code>graph-level</code> nor the operation <code>seed</code> is set: A random <code>seed</code> is used for this op.</li>
<li>If the <code>graph-level</code> <code>seed</code> is set, but the operation <code>seed</code> is not: The system deterministically picks an operation <code>seed</code> in conjunction with the <code>graph-level</code> <code>seed</code> so that it gets a unique random sequence.</li>
<li>If the <code>graph-level</code> <code>seed</code> is not set, but the operation <code>seed</code> is set: A default <code>graph-level</code> <code>seed</code> and the specified operation <code>seed</code> are used to determine the random sequence.</li>
<li>If both the <code>graph-level</code> and the operation <code>seed</code> are set: Both seeds are used in conjunction to determine the random sequence.</li>
</ul>
<p>&emsp;&emsp;To generate different sequences across sessions, set neither <code>graph-level</code> nor <code>op-level</code> seeds:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>])</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A3'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A4'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B3'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B4'</span></span><br></pre></td></tr></table></figure>
<p>To generate the same repeatable sequence for an op across sessions, set the <code>seed</code> for the op:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>], seed=<span class="number">1</span>)</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Repeatedly running this block with the same graph will generate the same</span></span><br><span class="line"><span class="comment"># sequence of values for 'a', but different sequences of values for 'b'.</span></span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B3'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B4'</span></span><br></pre></td></tr></table></figure>
<p>To make the random sequences generated by all ops be repeatable across sessions, set a graph-level seed:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.set_random_seed(<span class="number">1234</span>)</span><br><span class="line">a = tf.random_uniform([<span class="number">1</span>])</span><br><span class="line">b = tf.random_normal([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Repeatedly running this block with the same graph</span></span><br><span class="line"><span class="comment"># will generate the same sequences of 'a' and 'b'.</span></span><br><span class="line">print(<span class="string">"Session 1"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess1.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess1.run(b))  <span class="comment"># generates 'B2'</span></span><br><span class="line">print(<span class="string">"Session 2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A1'</span></span><br><span class="line">    print(sess2.run(a))  <span class="comment"># generates 'A2'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B1'</span></span><br><span class="line">    print(sess2.run(b))  <span class="comment"># generates 'B2'</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-identity"><a href="#tf-identity" class="headerlink" title="tf.identity"></a>tf.identity</h3><p>&emsp;&emsp;下面的程序想要执行<code>5</code>次循环，每次循环给<code>x</code>加<code>1</code>并赋值给<code>y</code>，然后打印出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)  <span class="comment"># 返回一个op，表示给变量x加1的操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>打印出的结果都是<code>0.0</code>，也就是说没有达到预期的效果。这是因为<code>y</code>只是复制了<code>x</code>变量内容，并未和<code>tensorflow</code>图上的节点相联系，不能执行节点上的操作。进行如下修改就能实现需要的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = tf.identity(x)  <span class="comment"># 修改部分</span></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>也就是说，<code>tf.identity</code>返回了一个和<code>x</code>相同的的新<code>tensor</code>。</p>
<h3 id="TensorFlow之命令行参数"><a href="#TensorFlow之命令行参数" class="headerlink" title="TensorFlow之命令行参数"></a>TensorFlow之命令行参数</h3><p>&emsp;&emsp;<code>TensorFlow</code>定义了<code>tf.app.flags</code>(也可以用它的别名<code>tf.flags</code>)，用于支持接受命令行传递参数，相当于接收<code>argv</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是参数名称，第二个参数是默认值，第三个是参数描述</span></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'str_name'</span>, <span class="string">'def_v_1'</span>, <span class="string">"descript1"</span>)</span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">'int_name'</span>, <span class="number">10</span>, <span class="string">"descript2"</span>)</span><br><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">'bool_name'</span>, <span class="keyword">False</span>, <span class="string">"descript3"</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    print(FLAGS.str_name)</span><br><span class="line">    print(FLAGS.int_name)</span><br><span class="line">    print(FLAGS.bool_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()  <span class="comment"># run main function</span></span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python test.py --str_name <span class="string">"hello"</span> --int_name <span class="number">12</span> --bool_name <span class="keyword">True</span></span><br><span class="line">hello</span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<h3 id="Session-run和Tensor-eval"><a href="#Session-run和Tensor-eval" class="headerlink" title="Session.run和Tensor.eval"></a>Session.run和Tensor.eval</h3><p>&emsp;&emsp;<code>TensorFlow</code>运行代码时，在会话中需要运行节点，可能会碰到两种方式，即<code>Session.run</code>和<code>Tensor.eval</code>，两者之间的差异如下。<br>&emsp;&emsp;如果你有一个<code>Tensor t</code>，在使用<code>t.eval</code>时，等价于<code>tf.get_default_session().run(t)</code>，实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant(<span class="number">42.0</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():  <span class="comment"># or `with sess:` to close on exit</span></span><br><span class="line">    <span class="keyword">assert</span> sess <span class="keyword">is</span> tf.get_default_session()</span><br><span class="line">    <span class="keyword">assert</span> t.eval() == sess.run(t)</span><br></pre></td></tr></table></figure>
<p>这其中最主要的区别就在于，你可以使用<code>sess.run</code>在同一步获取多个<code>tensor</code>中的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t = tf.constant(<span class="number">42.0</span>)</span><br><span class="line">u = tf.constant(<span class="number">37.0</span>)</span><br><span class="line">tu = tf.mul(t, u)</span><br><span class="line">ut = tf.mul(u, t)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    tu.eval()  <span class="comment"># runs one step</span></span><br><span class="line">    ut.eval()  <span class="comment"># runs one step</span></span><br><span class="line">    sess.run([tu, ut])  <span class="comment"># evaluates both tensors in a single step</span></span><br></pre></td></tr></table></figure>
<p>注意到，每次使用<code>eval</code>和<code>run</code>时，都会执行整个计算图，为了获取计算的结果，将它分配给<code>tf.Variable</code>，然后获取。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/13/深度学习/The Ultimate Guide To Speech Recognition With Python/" rel="next" title="The Ultimate Guide To Speech Recognition With Python">
                <i class="fa fa-chevron-left"></i> The Ultimate Guide To Speech Recognition With Python
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/14/深度学习/TensorFlow之nn的API/" rel="prev" title="TensorFlow之nn的API">
                TensorFlow之nn的API <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">943</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#get-shape函数"><span class="nav-number">1.</span> <span class="nav-text">get_shape函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#random-normal函数"><span class="nav-number">2.</span> <span class="nav-text">random_normal函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow的算术操作"><span class="nav-number">3.</span> <span class="nav-text">TensorFlow的算术操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#assign函数"><span class="nav-number">4.</span> <span class="nav-text">assign函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-contrib-layers-separable-conv2d"><span class="nav-number">5.</span> <span class="nav-text">tf.contrib.layers.separable_conv2d</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-contrib-layers-xavier-initializer"><span class="nav-number">6.</span> <span class="nav-text">tf.contrib.layers.xavier_initializer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-contrib-layers-l2-regularizer"><span class="nav-number">7.</span> <span class="nav-text">tf.contrib.layers.l2_regularizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-placeholder-with-default"><span class="nav-number">8.</span> <span class="nav-text">tf.placeholder_with_default</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-split"><span class="nav-number">9.</span> <span class="nav-text">tf.split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-reshape"><span class="nav-number">10.</span> <span class="nav-text">tf.reshape</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-shape"><span class="nav-number">11.</span> <span class="nav-text">tf.shape</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-concat"><span class="nav-number">12.</span> <span class="nav-text">tf.concat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-add-n"><span class="nav-number">13.</span> <span class="nav-text">tf.add_n</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-squeeze"><span class="nav-number">14.</span> <span class="nav-text">tf.squeeze</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-expand-dims"><span class="nav-number">15.</span> <span class="nav-text">tf.expand_dims</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-Session-as-default"><span class="nav-number">16.</span> <span class="nav-text">tf.Session.as_default</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#错误类-Error-classes"><span class="nav-number">17.</span> <span class="nav-text">错误类(Error classes)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-one-hot"><span class="nav-number">18.</span> <span class="nav-text">tf.one_hot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-sparse-to-dense"><span class="nav-number">19.</span> <span class="nav-text">tf.sparse_to_dense</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-nn-in-top-k"><span class="nav-number">20.</span> <span class="nav-text">tf.nn.in_top_k</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#initialized-value"><span class="nav-number">21.</span> <span class="nav-text">initialized_value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-py-func"><span class="nav-number">22.</span> <span class="nav-text">tf.py_func</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-trainable-variables"><span class="nav-number">23.</span> <span class="nav-text">tf.trainable_variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-train-ExponentialMovingAverage"><span class="nav-number">24.</span> <span class="nav-text">tf.train.ExponentialMovingAverage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-moving-average-variables-scope-None"><span class="nav-number">25.</span> <span class="nav-text">tf.moving_average_variables(scope=None)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-control-dependencies"><span class="nav-number">26.</span> <span class="nav-text">tf.control_dependencies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-group"><span class="nav-number">27.</span> <span class="nav-text">tf.group</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-add-to-collection和tf-get-collection"><span class="nav-number">28.</span> <span class="nav-text">tf.add_to_collection和tf.get_collection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-stack"><span class="nav-number">29.</span> <span class="nav-text">tf.stack</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-unstack"><span class="nav-number">30.</span> <span class="nav-text">tf.unstack</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-transpose"><span class="nav-number">31.</span> <span class="nav-text">tf.transpose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-set-random-seed"><span class="nav-number">32.</span> <span class="nav-text">tf.set_random_seed</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-identity"><span class="nav-number">33.</span> <span class="nav-text">tf.identity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow之命令行参数"><span class="nav-number">34.</span> <span class="nav-text">TensorFlow之命令行参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Session-run和Tensor-eval"><span class="nav-number">35.</span> <span class="nav-text">Session.run和Tensor.eval</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
