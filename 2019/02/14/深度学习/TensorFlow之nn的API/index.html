<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="激活函数&amp;emsp;&amp;emsp;在神经网络中，我们有很多的非线性函数来作为激活函数，比如连续的平滑非线性函数(sigmoid、tanh和softplus)，连续但不平滑的非线性函数(relu、relu6和relu_x)和随机正则化函数(dropout)。所有的激活函数都是单独应用在每个元素上面的，并且输出张量的维度和输入张量的维度一样。&amp;emsp;&amp;emsp;relu函数原型为： 1tf.nn.r">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow之nn的API">
<meta property="og:url" content="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/index.html">
<meta property="og:site_name" content="泥腿子出身">
<meta property="og:description" content="激活函数&amp;emsp;&amp;emsp;在神经网络中，我们有很多的非线性函数来作为激活函数，比如连续的平滑非线性函数(sigmoid、tanh和softplus)，连续但不平滑的非线性函数(relu、relu6和relu_x)和随机正则化函数(dropout)。所有的激活函数都是单独应用在每个元素上面的，并且输出张量的维度和输入张量的维度一样。&amp;emsp;&amp;emsp;relu函数原型为： 1tf.nn.r">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/1.png">
<meta property="og:updated_time" content="2019-03-13T07:34:27.797Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow之nn的API">
<meta name="twitter:description" content="激活函数&amp;emsp;&amp;emsp;在神经网络中，我们有很多的非线性函数来作为激活函数，比如连续的平滑非线性函数(sigmoid、tanh和softplus)，连续但不平滑的非线性函数(relu、relu6和relu_x)和随机正则化函数(dropout)。所有的激活函数都是单独应用在每个元素上面的，并且输出张量的维度和输入张量的维度一样。&amp;emsp;&amp;emsp;relu函数原型为： 1tf.nn.r">
<meta name="twitter:image" content="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '561O3H1PZB',
      apiKey: '7631d3cf19ac49bd39ada7163ec937a7',
      indexName: 'fuxinzi',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/">





  <title>TensorFlow之nn的API | 泥腿子出身</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泥腿子出身</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fukangwei.gitee.io/2019/02/14/深度学习/TensorFlow之nn的API/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="付康为">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泥腿子出身">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow之nn的API</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T11:02:43+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>&emsp;&emsp;在神经网络中，我们有很多的非线性函数来作为激活函数，比如连续的平滑非线性函数(<code>sigmoid</code>、<code>tanh</code>和<code>softplus</code>)，连续但不平滑的非线性函数(<code>relu</code>、<code>relu6</code>和<code>relu_x</code>)和随机正则化函数(<code>dropout</code>)。所有的激活函数都是单独应用在每个元素上面的，并且输出张量的维度和输入张量的维度一样。<br>&emsp;&emsp;<code>relu</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>relu</code>，即<code>max(features, 0)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.relu(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[0. 2.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>和<code>int8</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>relu6</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu6(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>relu6</code>，即<code>min(max(features, 0), 6)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">12.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.relu6(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[0. 6.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是float、double、int32、int64、uint8、int16或者int8。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>softplus</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softplus(features, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算激活函数<code>softplus</code>，即<code>log(exp(features) + 1)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">-1.0</span>, <span class="number">12.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.softplus(a)</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[ 0.31326166 12.000006 ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>features</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>或者<code>int8</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>features</code>相同。<br>&emsp;&emsp;<code>dropout</code>函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(x, keep_prob, noise_shape = <span class="keyword">None</span>, seed = <span class="keyword">None</span>, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算神经网络层的<code>dropout</code>。<br>&emsp;&emsp;一个神经元将以概率<code>keep_prob</code>决定是否放电，如果不放电，那么该神经元的输出将是<code>0</code>，如果该神经元放电，那么该神经元的输出值将被放大到原来的<code>1 / keep_prob</code>倍。这里的放大操作是为了保持神经元输出总个数不变。比如，神经元的值为<code>[1, 2]</code>，<code>keep_prob</code>的值是<code>0.5</code>，并且是第一个神经元是放电的，第二个神经元不放电，那么神经元输出的结果是<code>[2, 0]</code>，也就是相当于，第一个神经元被当做了<code>1 / keep_prob</code>个输出，即<code>2</code>个。这样保证了总和<code>2</code>个神经元保持不变。<br>&emsp;&emsp;默认情况下，每个神经元是否放电是相互独立的。但是，如果<code>noise_shape</code>被修改了，那么它对于变量<code>x</code>就是一个广播形式，而且当且仅当<code>noise_shape[i] == shape(x)[i]</code>，<code>x</code>中的元素是相互独立的。比如，如果<code>shape(x) = [k, l, m, n], noise_shape = [k, 1, 1, n]</code>，那么每个批和通道都是相互独立的，但是每行和每列的数据都是关联的，即要不都为<code>0</code>，要不都还是原来的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[[-2.  0.  6.  0.]]</span></span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape=[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    print(sess.run(b))  <span class="comment"># 结果为[[-2.  4.  6.  8.]]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>keep_prob</code>：一个<code>Python</code>的<code>float</code>类型，表示元素是否放电的概率。</li>
<li><code>noise_shape</code>：一个一维的<code>Tensor</code>，数据类型是<code>int32</code>，代表元素是否独立的标志。</li>
<li><code>seed</code>：一个<code>Python</code>的整数类型，设置随机种子。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和<code>x</code>相同。<br>&emsp;&emsp;<code>bias_add</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.bias_add(value, bias, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是将偏差项<code>bias</code>加到<code>value</code>上面。这个操作你可以看做是<code>tf.add</code>的一个特例，其中<code>bias</code>必须是一维的。该<code>API</code>支持广播形式，因此<code>value</code>可以有任何维度。但是，该<code>API</code>又不像<code>tf.add</code>，可以让<code>bias</code>的维度和<code>value</code>的最后一维不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">1.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">1.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.nn.bias_add(a, b)))</span><br><span class="line">print(<span class="string">"----------"</span>)</span><br><span class="line"><span class="comment"># 因为a最后一维的维度是2，但是c的维度是1，所以以下语句将发生错误</span></span><br><span class="line"><span class="comment"># print(sess.run(tf.nn.bias_add(a, c)))</span></span><br><span class="line">print(sess.run(tf.add(a, c)))  <span class="comment"># 但是tf.add可以正确运行</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int64</code>、<code>int32</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>或者<code>complex64</code>。</li>
<li><code>bias</code>：一个一维的<code>Tensor</code>，数据维度和<code>value</code>的最后一维相同，数据类型必须和<code>value</code>相同。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>sigmoid</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sigmoid(x, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的<code>sigmoid</code>函数，具体计算公式为<code>y = 1 / (1 + exp(-x))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.sigmoid(a)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7310586</span> <span class="number">0.880797</span> ]</span><br><span class="line"> [<span class="number">0.7310586</span> <span class="number">0.880797</span> ]</span><br><span class="line"> [<span class="number">0.7310586</span> <span class="number">0.880797</span> ]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int32</code>、<code>complex64</code>、<code>int64</code>或者<code>qint32</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，如果<code>x.dtype != qint32</code>，那么返回的数据类型和<code>x</code>相同，否则返回的数据类型是<code>quint8</code>。<br>&emsp;&emsp;<code>tanh</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.tanh(x, name = <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的<code>tanh</code>函数。具体计算公式为<code>(exp(x) - exp(-x))/(exp(x) + exp(-x))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(tf.tanh(a)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7615942</span> <span class="number">0.9640276</span>]</span><br><span class="line"> [<span class="number">0.7615942</span> <span class="number">0.9640276</span>]</span><br><span class="line"> [<span class="number">0.7615942</span> <span class="number">0.9640276</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>，数据类型必须是<code>float</code>、<code>double</code>、<code>int32</code>、<code>complex64</code>、<code>int64</code>或者<code>qint32</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，如果<code>x.dtype != qint32</code>，那么返回的数据类型和<code>x</code>相同，否则返回的数据类型是<code>quint8</code>。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>&emsp;&emsp;卷积操作是使用一个二维的卷积核在一个批处理的图片上进行不断扫描。具体操作是将一个卷积核在每张图片上按照一个合适的尺寸在每个通道上面进行扫描。为了达到好的卷积效率，需要在不同的通道和不同的卷积核之间进行权衡。</p>
<ul>
<li><code>conv2d</code>：任意的卷积核，能同时在不同的通道上面进行卷积操作。</li>
<li><code>depthwise_conv2d</code>：卷积核能相互独立的在自己的通道上面进行卷积操作。</li>
<li><code>separable_conv2d</code>：在纵深卷积(<code>depthwise filter</code>)之后进行逐点卷积(<code>separable filter</code>)。</li>
</ul>
<p>注意，虽然这些操作被称之为<code>卷积</code>，但是严格的说，他们只是互相关，因为卷积核没有做一个逆向的卷积过程。<br>&emsp;&emsp;卷积核的卷积过程是按照<code>strides</code>参数来确定的，比如<code>strides = [1, 1, 1, 1]</code>表示卷积核对每个像素点进行卷积，即在二维屏幕上面，两个轴方向的步长都是<code>1</code>。<code>strides = [1, 2, 2, 1]</code>表示卷积核对每隔一个像素点进行卷积，即在二维屏幕上面，两个轴方向的步长都是<code>2</code>。<br>&emsp;&emsp;<code>conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是对一个四维的输入数据<code>input</code>和四维的卷积核<code>filter</code>进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果。<br>&emsp;&emsp;给定的输入张量的维度是<code>[batch, in_height, in_width, in_channels]</code>，卷积核张量的维度是<code>[filter_height, filter_width, in_channels, out_channels]</code>，具体卷积操作如下：</p>
<ol>
<li>将卷积核的维度转换成一个二维的矩阵形状<code>[filter_height * filter_width * in_channels, output_channels]</code>。</li>
<li>对于每个批处理的图片，我们将输入张量转换成一个临时的数据维度<code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>。</li>
<li>对于每个批处理的图片，我们右乘以卷积核，得到最后的输出结果。</li>
</ol>
<p>注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据类型必须是<code>float32</code>或者<code>float64</code>。</li>
<li><code>filter</code>：一个<code>Tensor</code>，数据类型必须是<code>input</code>相同。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>use_cudnn_on_gpu</code>：一个可选布尔值，默认情况下是<code>True</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型是<code>input</code>相同。<br>&emsp;&emsp;<code>depthwise_conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.depthwise_conv2d(input, filter, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数也是一个卷积操作。给定一个输入张量，数据维度是<code>[batch, in_height, in_width, in_channels]</code>，一个卷积核的维度是<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>，在通道<code>in_channels</code>上面的卷积深度是<code>1</code>(我的理解是在每个通道上单独进行卷积)，<code>depthwise_conv2d</code>函数将不同的卷积核独立的应用在<code>in_channels</code>的每个通道上(从通道<code>1</code>到通道<code>channel_multiplier</code>)，然后把所以的结果进行汇总。最后输出通道的总数是<code>in_channels * channel_multiplier</code>。<br>&emsp;&emsp;注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维<code>[batch, in_height, in_width, in_channels]</code>。</li>
<li><code>filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如，<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>use_cudnn_on_gpu</code>：一个可选布尔值，默认情况下是<code>True</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个四维的<code>Tensor</code>，数据维度为<code>[batch, out_height, out_width, in_channels * channel_multiplier]</code>。<br>&emsp;&emsp;<code>separable_conv2d</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用几个分离的卷积核去做卷积，下图是常规卷积和分离卷积的区别：</p>
<p><img src="/2019/02/14/深度学习/TensorFlow之nn的API/1.png" height="210" width="560"></p>
<p>这个卷积是为了避免卷积核在全通道的情况下进行卷积，这样非常浪费时间。使用这个<code>API</code>，你将应用一个二维的卷积核，在每个通道上，以深度<code>channel_multiplier</code>进行卷积。其实如上图<code>Separable Convolution</code>中，就是先利用<code>depthwise_filter</code>，将<code>ID</code>的通道数映射到<code>ID * DM</code>的通道数上面，之后从<code>ID * DM</code>的通道数映射到<code>OD</code>的通道数上面，这也就是上面说的深度<code>channel_multiplier</code>对应于<code>DM</code>。<br>&emsp;&emsp;<code>strides</code>只是仅仅控制<code>depthwise convolution</code>的卷积步长，因为<code>pointwise convolution</code>的卷积步长是确定的<code>[1, 1, 1, 1]</code>。注意，必须有<code>strides[0] = strides[3] = 1</code>。在大部分处理过程中，卷积核的水平移动步数和垂直移动步数是相同的，即<code>strides = [1, stride, stride, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">depthwise_filter = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">pointwise_filter = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">20</span>), dtype=np.float32)</span><br><span class="line"><span class="comment"># out_channels &gt;= channel_multiplier * in_channels</span></span><br><span class="line">y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    print(sess.run(tf.shape(y)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维<code>[batch, in_height, in_width, in_channels]</code>。</li>
<li><code>depthwise_filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>。其中，<code>in_channels</code>的卷积深度是<code>1</code>。</li>
<li><code>pointwise_filter</code>：一个<code>Tensor</code>，数据维度是四维<code>[1, 1, channel_multiplier * in_channels, out_channels]</code>。其中，<code>pointwise_filter</code>是在<code>depthwise_filter</code>卷积之后的混合卷积。</li>
<li><code>strides</code>：一个长度是<code>4</code>的一维整数类型数组，每一维度对应的是<code>input</code>中每一维的对应移动步数，比如，<code>strides[1]</code>对应<code>input[1]</code>的移动步数。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个四维的<code>Tensor</code>，数据维度为<code>[batch, out_height, out_width, out_channels]</code>。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>&emsp;&emsp;池化操作是利用一个矩阵窗口在输入张量上进行扫描，并且将每个矩阵窗口中的值通过取最大值，平均值或者<code>XXXX</code>来减少元素个数。每个池化操作的矩阵窗口大小是由<code>ksize</code>来指定的，并且根据步长参数<code>strides</code>来决定移动步长。比如，如果<code>strides</code>中的值都是<code>1</code>，那么每个矩阵窗口都将被使用。如果<code>strides</code>中的值都是<code>2</code>，那么每一维度上的矩阵窗口都是每隔一个被使用。以此类推。更具体的输出结果是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output[i] = reduce(value[strides * i: strides * i + ksize])</span><br></pre></td></tr></table></figure>
<p>输出数据维度是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shape(output) = (shape(value) - ksize + <span class="number">1</span>) / strides</span><br></pre></td></tr></table></figure>
<p>其中，取舍方向取决于参数<code>padding</code>：</p>
<ul>
<li><code>padding = &#39;SAME&#39;</code>：向下取舍，仅适用于全尺寸操作，即输入数据维度和输出数据维度相同。</li>
<li><code>padding = &#39;VALID</code>：向上取舍，适用于部分窗口，即输入数据维度和输出数据维度不同。</li>
</ul>
<p>&emsp;&emsp;<code>avg_pool</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.avg_pool(value, ksize, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.avg_pool(value=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>、<code>float64</code>、<code>qint8</code>、<code>quint8</code>和<code>qint32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>max_pool</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool(value, ksize, strides, padding, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.max_pool(value=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>、<code>float64</code>、<code>qint8</code>、<code>quint8</code>或<code>qint32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>value</code>相同。<br>&emsp;&emsp;<code>max_pool_with_argmax</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax = <span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置。因为在计算位置<code>argmax</code>的时候，我们将<code>input</code>铺平了进行计算，所以如果<code>input = [b, y, x, c]</code>，那么索引位置是<code>((b * height + y) * width + x) * channels + c</code>。<br>&emsp;&emsp;使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">filter_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">output, argmax = tf.nn.max_pool_with_argmax(input=y, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个四维的<code>Tensor</code>，数据维度是<code>[batch, height, width, channels]</code>，数据类型是<code>float32</code>。</li>
<li><code>ksize</code>：一个长度不小于<code>4</code>的整型数组。每一位上面的值对应于输入数据张量中每一维的窗口对应值。</li>
<li><code>strides</code>：一个长度不小于<code>4</code>的整型数组。该参数指定滑动窗口在输入数据张量每一维上面的步长。</li>
<li><code>padding</code>：一个字符串，取值为<code>SAME</code>或者<code>VALID</code>。</li>
<li><code>Targmax</code>：一个可选的数据类型，即<code>tf.int32</code>或者<code>tf.int64</code>。默认情况下是<code>tf.int64</code>。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个元祖张量<code>(output, argmax)</code>：</p>
<ul>
<li><code>output</code>：一个<code>Tensor</code>，数据类型是<code>float32</code>，表示池化区域的最大值。</li>
<li><code>argmax</code>：一个<code>Tensor</code>，数据类型是<code>Targmax</code>，数据维度是四维的。</li>
</ul>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>&emsp;&emsp;标准化是能防止模型过拟合的好方法，特别是在大数据的情况下。<br>&emsp;&emsp;<code>l2_normalize</code>的函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_normalize(x, dim, epsilon=<span class="number">1e-12</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用<code>L2</code>范数对指定维度<code>dim</code>进行标准化。比如，对于一个一维的张量，指定维度<code>dim = 0</code>，那么计算结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = x / sqrt(max(sum(x ** <span class="number">2</span>), epsilon))</span><br></pre></td></tr></table></figure>
<p>假设<code>x</code>是多维度的，那么标准化只会独立的对维度<code>dim</code>进行，不会影响到别的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.l2_normalize(input_data, dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>dim</code>：需要标准化的维度。</li>
<li><code>epsilon</code>：一个很小的值，确定标准化的下边界。如果<code>norm &lt; sqrt(epsilon)</code>，那么我们将使用<code>sqrt(epsilon)</code>进行标准化。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和x相同。<br>&emsp;&emsp;<code>local_response_normalization</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.local_response_normalization(input, depth_radius=<span class="keyword">None</span>, bias=<span class="keyword">None</span>, alpha=<span class="keyword">None</span>, beta=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算局部数据标准化。输入的数据<code>input</code>是一个四维的张量，但该张量被看做是一个一维的向量(<code>input</code>的最后一维作为向量)，向量中的每一个元素都是一个三维的数组(对应<code>input</code>的前三维)。向量的每一个元素都是独立的被标准化的。具体数学形式如下：<br>&emsp;&emsp;使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.local_response_normalization(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个<code>Tensor</code>，数据维度是四维的，数据类型是<code>float32</code>。</li>
<li><code>depth_radius</code>：可选项，一个整型，默认情况下是<code>5</code>。</li>
<li><code>bias</code>：可选项，一个浮点型，默认情况下是<code>1</code>。一个偏移项，为了避免除<code>0</code>，一般情况下取正值。</li>
<li><code>alpha</code>：可选项，一个浮点型，默认情况下是<code>1</code>。一个比例因子，一般情况下取正值。</li>
<li><code>beta</code>：可选项，一个浮点型，默认情况下是0.5。一个指数。</li>
<li><code>name</code>：可选项，为这个操作取一个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型是<code>float32</code>。<br>&emsp;&emsp;<code>moments</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.moments(x, axes, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>x</code>的均值和方差。沿着<code>axes</code>维度，计算<code>x</code>的均值和方差。如果<code>x</code>是一维的，并且<code>axes = [0]</code>，那么就是计算整个向量的均值和方差。如果我们取<code>axes = [0, 1, 2]</code>(<code>batch, height, width</code>)，那么我们就是计算卷积的全局标准化。如果只是计算批处理的标准化，那么我们取<code>axes = [0]</code>(<code>batch</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">mean, variance = tf.nn.moments(input_data, [<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(mean))</span><br><span class="line">    print(sess.run(tf.shape(mean)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：一个<code>Tensor</code>。</li>
<li><code>axes</code>：一个整型的数组，确定计算均值和方差的维度。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出两个<code>Tensor</code>，分别是均值<code>mean</code>和方差<code>variance</code>。</p>
<h3 id="误差值"><a href="#误差值" class="headerlink" title="误差值"></a>误差值</h3><p>&emsp;&emsp;度量两个张量或者一个张量和零之间的损失误差，这个可用于在一个回归任务或者用于正则的目的(权重衰减)。<br>&emsp;&emsp;<code>l2_loss</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.l2_loss(t, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是利用<code>L2</code>范数来计算张量的误差值，但是没有开方并且只取<code>L2</code>范数的值的一半：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = sum(t ** <span class="number">2</span>) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.l2_loss(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.03965811</span> <span class="number">0.9202959</span>  <span class="number">0.83564</span>   ]</span><br><span class="line"> [<span class="number">0.23268144</span> <span class="number">0.77983814</span> <span class="number">0.8602118</span> ]]</span><br><span class="line"><span class="number">1.474532</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>t</code>：一个<code>Tensor</code>。数据类型必须是以下之一：<code>float32</code>、<code>float64</code>、<code>int64</code>、<code>int32</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>、<code>complex64</code>、<code>qint8</code>、<code>quint8</code>或<code>qint32</code>。虽然一般情况下，数据维度是二维的。但是，数据维度可以取任意维度。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>t</code>相同，是一个标量。</p>
<h3 id="分类操作"><a href="#分类操作" class="headerlink" title="分类操作"></a>分类操作</h3><p>&emsp;&emsp;<code>sigmoid_cross_entropy_with_logits</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>logits</code>经<code>sigmoid</code>函数激活之后的交叉熵。对于一个不相互独立的离散分类任务，这个函数作用是去度量概率误差。比如在一张图片中，同时包含多个分类目标(大象和狗)，那么就可以使用这个函数。<br>&emsp;&emsp;为了描述简洁，我们规定<code>x = logits</code>，<code>z = targets</code>，那么<code>Logistic</code>损失值为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x - x * z + log(<span class="number">1</span> + exp(-x))</span><br></pre></td></tr></table></figure>
<p>为了确保计算稳定，避免溢出，真实的计算实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max(x, <span class="number">0</span>) - x * z + log(<span class="number">1</span> + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<p><code>logits</code>和<code>targets</code>必须有相同的数据类型和数据维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.random.rand传入一个shape，返回一个在[0,1)区间符合均匀分布的array</span></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">1</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">output = tf.nn.sigmoid_cross_entropy_with_logits(logits=input_data, labels=[[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(output))  <span class="comment"># 结果为[[0.3589966 1.1557628 0.9552358]]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个<code>Tensor</code>，数据类型是<code>float32</code>或者<code>float64</code>之一。</li>
<li><code>targets</code>：一个<code>Tensor</code>，数据类型和数据维度都和<code>logits</code>相同。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和<code>logits</code>相同。<br>&emsp;&emsp;<code>softmax</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(logits, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>softmax</code>激活函数。对于每个批<code>i</code>和分类<code>j</code>，我们可以得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))</span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable([[<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.9</span>]], dtype=tf.float32)</span><br><span class="line">output = tf.nn.softmax(input_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.2</span> <span class="number">0.1</span> <span class="number">0.9</span>]]</span><br><span class="line">[[<span class="number">0.25519383</span> <span class="number">0.23090893</span> <span class="number">0.51389724</span>]]</span><br><span class="line">[<span class="number">1</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个<code>Tensor</code>。数据类型是<code>float32</code>或者<code>float64</code>之一。数据维度是二维<code>[batch_size, num_classes]</code>。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度和数据类型都和<code>logits</code>相同。<br>&emsp;&emsp;<code>softmax_cross_entropy_with_logits</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是计算<code>logits</code>经<code>softmax</code>函数激活之后的交叉熵。对于每个独立的分类任务，这个函数是去度量概率误差。比如，在<code>CIFAR-10</code>数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车，但绝对不可能两者都在一张图中(这也是和<code>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)</code>这个<code>API</code>的区别)。<br>&emsp;&emsp;输入<code>API</code>的数据<code>logits</code>不能进行缩放，因为在这个<code>API</code>的执行中会进行<code>softmax</code>计算，如果<code>logits</code>进行了缩放，那么会影响计算正确率。不要调用这个<code>API</code>去计算<code>softmax</code>的值，因为这个<code>API</code>最终输出的结果并不是经过<code>softmax</code>函数的值。<br>&emsp;&emsp;<code>logits</code>和<code>labels</code>必须有相同的数据维度<code>[batch_size, num_classes]</code>，和相同的数据类型(<code>float32</code>或者<code>float64</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable([[<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.9</span>]], dtype=tf.float32)</span><br><span class="line">output = tf.nn.softmax_cross_entropy_with_logits(logits=input_data, labels=[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(input_data))</span><br><span class="line">    print(sess.run(output))</span><br><span class="line">    print(sess.run(tf.shape(output)))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.2</span> <span class="number">0.1</span> <span class="number">0.9</span>]]</span><br><span class="line">[<span class="number">1.365732</span>]</span><br><span class="line">[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>logits</code>：一个没有缩放的对数张量。</li>
<li><code>labels</code>：每一行<code>labels[i]</code>必须是一个有效的概率分布值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据维度是一维的，长度是<code>batch_size</code>，数据类型都和<code>logits</code>相同。</p>
<h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h3><p>&emsp;&emsp;<code>Tensorflow</code>提供了从张量中嵌入查找的库。<br>&emsp;&emsp;`embedding_lookup函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(params, ids, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是查询<code>params</code>中索引是<code>ids</code>的值。这个操作是<code>tf.gather</code>的一个泛化，但它可以被并行计算处理，其中<code>params</code>被认为是一个大型的张量库，<code>ids</code>中的值对应于各个分区。如果<code>len(params) &gt; 1</code>，<code>ids</code>中每个元素<code>id</code>对应于<code>params</code>中每个分区<code>p</code>，即<code>p = id % len(params)</code>。那么，我们得到的每个切片是<code>params[p][id // len(params), ...]</code>。最后得到的切片结果被重新连接成一个稠密张量，最后返回的张量维度是<code>shape(ids) + shape(params)[1:]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">params = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">ids = tf.constant([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">output = tf.nn.embedding_lookup(params, ids)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">'params:'</span>, sess.run(params))</span><br><span class="line">    print(<span class="string">'-------------------------------'</span>)</span><br><span class="line">    print(<span class="string">'输出第0行和第2行:'</span>, sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params: [[<span class="number">0.59815351</span> <span class="number">0.65806083</span> <span class="number">0.1297678</span>  <span class="number">0.8921319</span> ]</span><br><span class="line"> [<span class="number">0.29017073</span> <span class="number">0.32162826</span> <span class="number">0.12856839</span> <span class="number">0.14271805</span>]</span><br><span class="line"> [<span class="number">0.3123268</span>  <span class="number">0.8437347</span>  <span class="number">0.49344986</span> <span class="number">0.29818202</span>]]</span><br><span class="line">-------------------------------</span><br><span class="line">输出第<span class="number">0</span>行和第<span class="number">2</span>行: [[<span class="number">0.59815351</span> <span class="number">0.65806083</span> <span class="number">0.1297678</span>  <span class="number">0.8921319</span> ]</span><br><span class="line"> [<span class="number">0.3123268</span>  <span class="number">0.8437347</span>  <span class="number">0.49344986</span> <span class="number">0.29818202</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>params</code>：一个拥有相同数据维度和数据类型的张量。</li>
<li><code>ids</code>：一个张量，数据类型是<code>int32</code>。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个<code>Tensor</code>，数据类型和<code>params</code>相同。</p>
<h3 id="评估操作"><a href="#评估操作" class="headerlink" title="评估操作"></a>评估操作</h3><p>&emsp;&emsp;评估操作对于测量网络的性能是有用的。由于它们是不可微分的，所以它们通常只是被用在评估阶段。<br>&emsp;&emsp;<code>top_k</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.top_k(input, k, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是返回<code>input</code>中每行最大的k个数，并且返回它们所在位置的索引。<code>value(i, j)</code>表示输入数据<code>input(i)</code>的第<code>j</code>大的元素。<code>indices(i, j)</code>给出对应元素的列索引，即<code>input(i, indices(i, j)) = values(i, j)</code>。如果遇到两个相等的元素，那么我们先取索引小的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">k = <span class="number">2</span></span><br><span class="line">output = tf.nn.top_k(input, k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(input))</span><br><span class="line">    print(<span class="string">'--------------------'</span>)</span><br><span class="line">    print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.9522081</span>  <span class="number">0.59109382</span> <span class="number">0.44528462</span> <span class="number">0.34926363</span>]</span><br><span class="line"> [<span class="number">0.46024959</span> <span class="number">0.59693116</span> <span class="number">0.36231259</span> <span class="number">0.55662777</span>]</span><br><span class="line"> [<span class="number">0.37875119</span> <span class="number">0.4205928</span>  <span class="number">0.43581744</span> <span class="number">0.489631</span>  ]]</span><br><span class="line">--------------------</span><br><span class="line">TopKV2(values=array([[<span class="number">0.9522081</span> , <span class="number">0.59109382</span>],</span><br><span class="line">       [<span class="number">0.59693116</span>, <span class="number">0.55662777</span>],</span><br><span class="line">       [<span class="number">0.489631</span>  , <span class="number">0.43581744</span>]]), indices=array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">2</span>]]))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>input</code>：一个张量，数据类型必须是以下之一：<code>float32</code>、<code>float64</code>、<code>int32</code>、<code>int64</code>、<code>uint8</code>、<code>int16</code>、<code>int8</code>。数据维度是<code>batch_size</code>乘上<code>x</code>个类别。</li>
<li><code>k</code>：一个整型，必须大于等于<code>1</code>。在每行中，查找最大的<code>k</code>个值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个元组<code>Tensor</code>，数据元素是<code>(values, indices)</code>：</p>
<ul>
<li><code>values</code>：一个张量，数据类型和<code>input</code>相同。数据维度是<code>batch_size</code>乘上<code>k</code>个最大值。</li>
<li><code>indices</code>：一个张量，数据类型是<code>int32</code>。每个最大值在<code>input</code>中的索引位置。</li>
</ul>
<p>&emsp;&emsp;<code>in_top_k</code>函数原型为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.in_top_k(predictions, targets, k, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个函数的作用是返回一个布尔向量，说明目标值是否存在于预测值之中。输出数据是一个<code>batch_size</code>长度的布尔向量，如果目标值存在于预测值之中，那么<code>out[i] = true</code>。注意，<code>targets</code>是<code>predictions</code>中的索引位，并不是<code>predictions</code>中具体的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input = tf.constant(np.random.rand(<span class="number">3</span>, <span class="number">4</span>), tf.float32)</span><br><span class="line">k = <span class="number">2</span></span><br><span class="line">output = tf.nn.in_top_k(input, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(input))</span><br><span class="line">    print(<span class="string">'--------------------'</span>)</span><br><span class="line">    print(sess.run(output))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.34494555</span> <span class="number">0.9111343</span>  <span class="number">0.93085057</span> <span class="number">0.419403</span>  ]</span><br><span class="line"> [<span class="number">0.14929643</span> <span class="number">0.46961188</span> <span class="number">0.72727567</span> <span class="number">0.04639981</span>]</span><br><span class="line"> [<span class="number">0.13701458</span> <span class="number">0.83330005</span> <span class="number">0.33437857</span> <span class="number">0.1281736</span> ]]</span><br><span class="line">--------------------</span><br><span class="line">[<span class="keyword">False</span> <span class="keyword">False</span> <span class="keyword">False</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>predictions</code>：一个张量，数据类型是<code>float32</code>。数据维度是<code>batch_size</code>乘上<code>x</code>个类别。</li>
<li><code>targets</code>：一个张量，数据类型是<code>int32</code>。一个长度是<code>batch_size</code>的向量，里面的元素是目标<code>class ID</code>。</li>
<li><code>k</code>：一个整型。在每行中，查找最大的<code>k</code>个值。</li>
<li><code>name</code>：为这个操作取个名字。</li>
</ul>
<p>输出一个张量，数据类型是<code>bool</code>，判断是否预测正确。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/13/深度学习/TensorFlow之函数总结/" rel="next" title="TensorFlow之函数总结">
                <i class="fa fa-chevron-left"></i> TensorFlow之函数总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/14/软件与硬件问题/ubuntu服务器/" rel="prev" title="ubuntu服务器">
                ubuntu服务器 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">付康为</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">939</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数"><span class="nav-number">1.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积层"><span class="nav-number">2.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#池化层"><span class="nav-number">3.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#标准化"><span class="nav-number">4.</span> <span class="nav-text">标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#误差值"><span class="nav-number">5.</span> <span class="nav-text">误差值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类操作"><span class="nav-number">6.</span> <span class="nav-text">分类操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#嵌入层"><span class="nav-number">7.</span> <span class="nav-text">嵌入层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评估操作"><span class="nav-number">8.</span> <span class="nav-text">评估操作</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">付康为</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
